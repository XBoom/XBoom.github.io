{"meta":{"title":"XBoom Dove","subtitle":"","description":"","author":"XBoom Dove","url":"http://xboom.github.io","root":"/"},"pages":[{"title":"Repositories","date":"2023-02-02T12:27:35.937Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"repository/index.html","permalink":"http://xboom.github.io/repository/index.html","excerpt":"","text":""},{"title":"关于","date":"2023-02-02T12:27:35.796Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"about/index.html","permalink":"http://xboom.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2023-02-02T12:27:35.934Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"categories/index.html","permalink":"http://xboom.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-02-02T12:27:35.940Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"tags/index.html","permalink":"http://xboom.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Go入门22-协程池问题","slug":"Go/Go入门23-协程池问题","date":"2023-03-05T06:46:45.000Z","updated":"2023-03-09T13:27:49.809Z","comments":true,"path":"2023/03/05/Go/Go入门23-协程池问题/","link":"","permalink":"http://xboom.github.io/2023/03/05/Go/Go%E5%85%A5%E9%97%A823-%E5%8D%8F%E7%A8%8B%E6%B1%A0%E9%97%AE%E9%A2%98/","excerpt":"","text":"之前在整理 GoZero4-协程池的过程中发现 字节跳动开源的协程池在使用中需要注意的地方： ​ 使用双向链表存储任务，表示它理论上支持无限个任务。后面的任务可能存在长时间等待的情况，并不存在任务过期处理逻辑 没想到在实际应用中真的没注意踩了一遍，这里记录一下这个原因与后续的解决思路，排查过程就不赘述了。 伪代码如下： 1234567891011121314151617181920212223242526//伪代码func Send(conn *grpc.ClientConn) &#123; c := pb.NewGreeterClient(conn) ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) gopool.CtxGo(ctx, func() &#123; defer cancel() //1. 通信消息自定义处理 Handle(ctx) //2. 发送消息 _, err := c.SayHello(ctx, &amp;pb.HelloRequest&#123;Name: *name&#125;) if err != nil &#123; log.Fatalf(\"could not greet: %v\", err) &#125; &#125;) select &#123; case &lt;-ctx.Done(): //log.Printf(\"end success\") return case &lt;-time.After(time.Second * 5): log.Printf(\"time out\") return &#125;&#125; 问题原因：负责底层通信的 grpc-go(进行过二次开发) 使用了上述的协程池进行发送接口的超时控制。当突然大量的消息进入，消息接受速率 大于 消息发送-响应速率。导致过量的消息全部堆积到这个双向链表中，而本身消息是有超时时间的，就导致堆积的消息越来越多，消息等到发送的时候已经超时了，所以就出现了服务OOM，消息也无法发送出去。 类似上图所示，模拟客户端通过协程池的发送协程 work 实现超时控制消息任务 t 的发送 第一阶段，发送协程 work 的消费速率 大于 消息任务 t 的生产速率，一切正常 第二阶段，发送协程 work 的消费速率 小于 消息任务 t 的生产速率的时候，消息开始在双向链表中堆积，出现内存快速增长 第三阶段，当 消息任务 t 的堆积 超过自定义超时时间的时候，所有消息都发送超时失败 第四阶段，当 双向链表中的任务堆积导致内存超时服务上限的时候，服务OOM 这里产生了如下几个思考 能否在任务双向链表中限制数量大小当发送超过限制的时候直接失败? 为什么不直接使用管道来代替底层的双向链表？ 能否自动丢弃超时的消息并返回结果，注意这个时候是在协程里面运行的 如何像linux一样保存 pod因为OOM而产生的core方便后续分析 第一个问题是可以解决的，协程池对象存在 taskCount 记录当前任务数量，也就是说可以增加协程池容量判断来达到限制目的 增加协程池任务容量字段 taskCap 增加接口，协程失败返回错误 error 添加协程池容量判断 1234567func (p *pool) CtxGo2(ctx context.Context, f func()) error &#123; cur := atomic.AddInt32(&amp;p.taskCount, 1) if cur &gt; p.taskCap &#123; //当前任务容量判断 return errors.New &#125; //....&#125; 第二个问题管道的缺点是管道数量是固定的，也就是达不到根据任务数量动态扩容 work 的功效 第三个问题由于任务放入协程中相当于异步处理，并不能直接将任务待执行超时的情况返回给客户端，可以通过自定义处理函数或者内部的panic处理任务超时的情况；但又能如何发现任务超时，时间轮貌似是个好东西 第四个问题找到一篇实践，实操之后再单独记录 总结： 我在使用过程中忽视了双向队列任务堆积的情况，令牌桶的使用也可能存在这样的情况(正常情况不会配置速率上千万的情况)，注意异常情况下的配置范围 上述需要改进与实操的部分尝试解决之后再来补充这边文章","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"算法-E10C-最大公约数","slug":"Algorithms To Live By/算法-E10C-最大公约数","date":"2023-02-02T12:25:25.494Z","updated":"2023-02-15T13:39:32.337Z","comments":true,"path":"2023/02/02/Algorithms To Live By/算法-E10C-最大公约数/","link":"","permalink":"http://xboom.github.io/2023/02/02/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95-E10C-%E6%9C%80%E5%A4%A7%E5%85%AC%E7%BA%A6%E6%95%B0/","excerpt":"","text":"今天碰到一道有意思的题目，学到了很多东西，所以记录一下 检查好数组 碰到之后完全不知道如何入手，最后通过看解答才弄明白，但是中间有很多知识点可以记录一下 首先 需要知道裴蜀定理，又称贝祖定理，是一个最大公约数定理：设 a, b 是不全为零的整数，则存在整数 x , y , 使得 ax+by=gcd(a,b)ax + by = gcd(a, b) ax+by=gcd(a,b) 若任何一个等于0，则 gcd(a,b)==agcd(a, b) == agcd(a,b)==a 对任何整数a、b和它们的最大公约数d，关于未知数x和y的线性丢番图方程 ax+by=max + by = m ax+by=m 有解当前仅当 m 是 d 的倍数 这里有一个奇特的点 0 因为没有因数，所以 0 不与其他数存在最大公约数，等我看了 类欧几里德算法 再补充 所以 ax+by=1ax + by = 1ax+by=1 有解，那么 gcd(a,b)=1gcd(a, b) = 1gcd(a,b)=1 其次 我们如何求一个两个数的最大公约数，经典的 辗转相除法(又称 欧几里德算法) 1234//也可以使用现有函数 gcd(a, b)int GCD(int a, int b) &#123; return b ? GCD(b, a % b) : a;&#125; 证明 gcd(a,b)=gcd(b,a%b)gcd(a, b) = gcd(b, a \\% b)gcd(a,b)=gcd(b,a%b) 过程如下 设 a=bk+ca = bk + ca=bk+c，所以 c=a%bc = a \\% bc=a%b 设 d 是 a , b 的公约数 所以 a/d=b/d∗k+c/da / d = b / d * k + c / da/d=b/d∗k+c/d 因为 a/da / da/d 与 b/d∗kb / d * kb/d∗k 是整数，所以 c/dc / dc/d 也是整数 将 c=a%bc = a \\% bc=a%b 带入第3步得 a/d−b/d∗k=a%b/da / d - b / d * k = a \\% b / da/d−b/d∗k=a%b/d，因为 左边是整数，所有等式右边也是整数 那么 a/d=a%b/d+b/d∗ka / d = a \\% b / d + b / d * ka/d=a%b/d+b/d∗k，那么 d 也是 b, a%ba \\% ba%b 的约数，也是 a, b 的约数 尽然两式的公约数是相同的，那么最大公约数也会相同 所以 gcd(a,b)=gcd(b,amodb)gcd(a, b) = gcd(b, a mod b)gcd(a,b)=gcd(b,amodb) 最后就是这个题目的解答 1234567891011121314class Solution &#123;public: int Gcd(int a, int b) &#123; return b ? Gcd(b, a % b) : a; &#125; bool isGoodArray(vector&lt;int&gt;&amp; nums) &#123; int v = nums[0]; for(int i = 1; i &lt; nums.size(); i++) &#123; if(v == 1) return true; //1与任意正整数的最大公约数是1 v = Gcd(nums[i], v); &#125; return v == 1; &#125;&#125;; 参考链接 https://oi-wiki.org/math/number-theory/gcd/ https://leetcode.cn/problems/check-if-it-is-a-good-array/ https://oi-wiki.org/math/number-theory/bezouts/","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"Go入门21-arena","slug":"Go/Go入门22-arena","date":"2023-01-10T08:00:20.000Z","updated":"2023-03-09T13:28:29.807Z","comments":true,"path":"2023/01/10/Go/Go入门22-arena/","link":"","permalink":"http://xboom.github.io/2023/01/10/Go/Go%E5%85%A5%E9%97%A822-arena/","excerpt":"","text":"最近 Go1.20 更新，中间讲到了一个特性 arena ，这里看看加入 arena 的作用 在Go的内存管理中，arena 其实就是所谓的堆区，然后将这个区域分割成 8KB 大小的页，组合起来称为 mspan，mspan就是Go中内存管理的基本单元，是由一片连续的 8KB 的页组成的大块内存。其实 mspan 是一个包含起始地址、规格、页的数量等内容的双端链表 虽然 Go 的垃圾回收机制能够正常的进行内存管理，但是存在以下问题 垃圾回收机制需要花费大量CPU进行垃圾回收操作 垃圾回收机制有一定的延迟性，导致花费的内存比实际的内存要大 而 arena 的优势 允许从连续的内存空间中分配对象并一次性进行释放 注意：在 github arena 的话题中，有一个最新的笔记提醒, 即处在测试阶段的 arena 功能随时可能被去除掉 1Note, 2023-01-17. This proposal is on hold indefinitely due to serious API concerns. The GOEXPERIMENT=arena code may be changed incompatibly or removed at any time, and we do not recommend its use in production. 带着问题看世界： 它跟内存池的区别 由于需要连续的内存空间，那么当需要分配的内存比较多，没有这么大的连续内存空间怎么办？ 如果它分配的对象有一部分存在内存逃逸，那么该如何处理？ 能支持并发吗 基础功能 由于是实验功能，所以需要配置环境变量 GOEXPERIMENT=arenas 123456789101112131415161718192021import \"arena\"type T struct&#123; Foo string Bar [16]byte&#125;func processRequest(req *http.Request) &#123; // Create an arena in the beginning of the function. mem := arena.NewArena() // Free the arena in the end. defer mem.Free() // Allocate a bunch of objects from the arena. for i := 0; i &lt; 10; i++ &#123; obj := arena.New[T](mem) &#125; // Or a slice with length and capacity. slice := arena.MakeSlice[T](mem, 100, 200)&#125; 如果想在arena释放时候继续使用它分配的对象，则可以通过 Clone 从堆中浅拷贝一个对象 12345obj1 := arena.New[T](mem) // arena-allocatedobj2 := arena.Clone(obj1) // heap-allocatedfmt.Println(obj2 == obj1) // falsemem.Free() 其他接口包括 NewArena：创建一个新的 arena 内存空间。 Free：释放 arena 及其关联对象。 New：基于 arena，创建新对象。 MakeSlice：基于 arena，创建新切片。 Clone：克隆一个 arena 的对象，并移动到内存堆上。只能是指针、slice或者字符串 实现原理 代码路径： src/runtime/arena.go src/arena/arena.go 123456789//arena 表示多个Go一起分配与释放的内存集合，当其中的对象不在被引用那么将会自动释放type Arena struct &#123; a unsafe.Pointer&#125;// NewArena allocates a new arena.func NewArena() *Arena &#123; return &amp;Arena&#123;a: runtime_arena_newArena()&#125;&#125; 根据描述这里注意点有两个： arena 分配的对象需要及时释放 既然是自动释放，然后在使用中 defer arena.Free() 可以任务是，不用等到二次垃圾回收，直接将资源释放，并将可重复使用的mspan放入reused中 查看Arena 内部结构 12345678910111213type userArena struct &#123; // 表示一系列没有足够空心内存的 mspan,当arena被释放他们也会被释放 fullList *mspan //内存组件 mspan // 未满的mspan active *mspan //arena 对象引用报活 refs []unsafe.Pointer //如果arena被释放，那么 defunct=true,避免重复释放 defunct atomic.Bool&#125; Arena 如果重复释放也没有关系，判断释放过则直接结束 **第一步：**分配一个 Arena 12345678910111213141516171819202122232425262728293031323334353637// newUserArena creates a new userArena ready to be used.func newUserArena() *userArena &#123; a := new(userArena) SetFinalizer(a, func(a *userArena) &#123; //g // If arena handle is dropped without being freed, then call // free on the arena, so the arena chunks are never reclaimed // by the garbage collector. a.free() &#125;) a.refill() return a&#125;func (a *userArena) refill() *mspan &#123; // If there's an active chunk, assume it's full. s := a.active //... var x unsafe.Pointer // Check the partially-used list. lock(&amp;userArenaState.lock) if len(userArenaState.reuse) &gt; 0 &#123; //当前存在可重用的就使用重用的 &#125; unlock(&amp;userArenaState.lock) if s == nil &#123; // Allocate a new one. 否则分配一个新的mspan x, s = newUserArenaChunk() if s == nil &#123; throw(\"out of memory\") &#125; &#125; a.refs = append(a.refs, x) //记录mspan.base()，报活mspan a.active = s //记录当前使用的mspan return s&#125; SetFinalizer 函数可参考文章，当gc检测到unreachable对象有关联的SetFinalizer函数时，会执行关联的SetFinalizer函数， 同时取消关联。 这样当下一次gc的时候，对象重新处于unreachable状态并且没有SetFinalizer关联， 就会被回收。 **第二步：**从 arena 中分配具体类型的对象 1234567891011121314151617181920212223242526272829303132333435func (a *userArena) refill() *mspan &#123; // If there's an active chunk, assume it's full. s := a.active //上次分配了mspan if s != nil &#123; if s.userArenaChunkFree.size() &gt; userArenaChunkMaxAllocBytes &#123; // It's difficult to tell when we're actually out of memory // in a chunk because the allocation that failed may still leave // some free space available. However, that amount of free space // should never exceed the maximum allocation size. throw(\"wasted too much memory in an arena chunk\") &#125; s.next = a.fullList //将这个mspan放到fullList的链表头部 a.fullList = s a.active = nil //active置为空 s = nil &#125; var x unsafe.Pointer // Check the partially-used list. lock(&amp;userArenaState.lock) if len(userArenaState.reuse) &gt; 0 &#123; // //如果有可以重用的mspan则放到s中 &#125; unlock(&amp;userArenaState.lock) if s == nil &#123; // Allocate a new one. x, s = newUserArenaChunk() //否则新分配一个新的 if s == nil &#123; throw(\"out of memory\") &#125; &#125; a.refs = append(a.refs, x) a.active = s return s&#125; **第三步：**释放的核心是这块代码 123456789s := a.fullList //获取这个mspani := len(a.refs) - 2for s != nil &#123; //不为空 a.fullList = s.next //指向下一个节点 s.next = nil freeUserArenaChunk(s, a.refs[i]) //释放这个mspan s = a.fullList //指向下一个节点 i--&#125; 释放的时候仅仅是将fullList中所有的都释放掉了，而active中的则会进去到全局reuse对象中用于下次使用 这个全局变量就是 userArenaState 用于存放可重复使用的mspan以及回收的mspan 123456789var userArenaState struct &#123; lock mutex //可重复使用 reuse []liveUserArenaChunk //回收释放的 fault []liveUserArenaChunk&#125; 对比Sync.Pool arena 与 Sync.Pool 同样都是为了解决频繁分配对象和大量对象GC带来的开销 Sync.Pool相同类型的对象，使用完后暂时缓存不GC，下次再有相同的对象分配时直接用之前的缓存的对象，避免频繁创建大量对象。不承诺这些缓存对象的生命周期，GC时会释放之前的缓存，适合解决频繁创建相同对象带来的压力，短时间(两次GC之间)大量创建可能还是会有较大冲击，使用相对简单，但只能用于相同结构创建，不能创建slice等复杂结构 arena手动管理连续内容并统一释放，对象的生命周期完全自己控制，使用相对复杂，支持slice等复杂结构，也不是一个真正意义的连续超大空间，而是通过管理不同的mspan实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package mainimport ( \"arena\" \"sync\" \"testing\")type MyObj struct &#123; Index int&#125;func BenchmarkCreateObj(b *testing.B) &#123; b.ReportAllocs() var p *MyObj for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; 1000; j++ &#123; p = new(MyObj) p.Index = j &#125; &#125;&#125;var ( objPool = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return &amp;MyObj&#123;&#125; &#125;, &#125;)func BenchmarkCreateObj_SyncPool(b *testing.B) &#123; b.ReportAllocs() var p *MyObj for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; 1000; j++ &#123; p = objPool.Get().(*MyObj) p.Index = 23 objPool.Put(p) &#125; &#125;&#125;func BenchmarkCreateObj_Arena(b *testing.B) &#123; b.ReportAllocs() var p *MyObj a := arena.NewArena() defer a.Free() for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; 1000; j++ &#123; p = arena.New[MyObj](a) p.Index = 23 &#125; &#125;&#125; 性能对比的结果 1234cpu: Intel(R) Core(TM) i7-8559U CPU @ 2.70GHzBenchmarkCreateObj-8 100518 11370 ns/op 8000 B/op 1000 allocs/opBenchmarkCreateObj_SyncPool-8 110017 11523 ns/op 0 B/op 0 allocs/opBenchmarkCreateObj_Arena-8 80409 15340 ns/op 8032 B/op 0 allocs/op Sync.Pool 不需要重复分配且每次操作时间短，而Arena执行时间会长一点且每次还是需要分配内存的，因为需要引入新的 mspan 总结 Arena 不支持并发，可以看出操作同一个 arena的时候并不存在锁操作 Arena 强制 Free()之后的对象无法继续使用 优点： 一旦被释放但仍然被访问则会显示的导致程序错误 arena 地址空间除非没有指针指向，否则将不能被重用 arena 永远不会被垃圾回收机制回收(如果GC不可达它会执行 SetFinalizer 自己释放掉，那我们手动free的意义在哪里 --&gt; 也就是构建arena的目的，提前释放内存，降低GC扫描频率) 参考文档 https://uptrace.dev/blog/golang-memory-arena.html https://colobu.com/2022/10/17/a-first-look-at-arena/ https://zhuanlan.zhihu.com/p/604686258","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门21-SetFinalizer","slug":"Go/Go入门21-SetFinalizer","date":"2023-01-04T13:45:13.344Z","updated":"2023-02-18T10:15:47.300Z","comments":true,"path":"2023/01/04/Go/Go入门21-SetFinalizer/","link":"","permalink":"http://xboom.github.io/2023/01/04/Go/Go%E5%85%A5%E9%97%A821-SetFinalizer/","excerpt":"","text":"在阅读Go 1.20 新特性 arena 的时候，看到在构建 arena 对象的时候使用了 SetFinalizer 这样一个函数 带着问题看世界 它有什么用 它怎么用 它有什么缺点导致不是随处可见这种用法 这里来详细看看它的作用，备注如下 12SetFinalizer sets the finalizer associated with obj to the provided finalizer function. When the garbage collector finds an unreachable block with an associated finalizer, it clears the association and runs finalizer(obj) in a separate goroutine. This makes obj reachable again,but now without an associated finalizer. Assuming that SetFinalizer is not called again, the next time the garbage collector sees that obj is unreachable, it will free obj. 大意是：为对象提供一个析构函数，当GC发现不可达对象带有析构函数的时候，会单独使用协程执行这个析构函数。这样对GC来说对象是可达但没有了析构函数，下次GC发现对象不可达就会释放掉对象 有一些协程的生命周期是与整个服务一致的，比如定时清理机制，它的好处是自动处理一些业务而不需要人工调用，但如果是在一些与业务完全分离的场景。比如为业务提供一个缓存池，缓存池中为了清理过期的缓存而设计了一个常驻协程。是否可以使用 SetFinalizer 的过期删除机制 首先看一个栗子： 一般情况我们会提供对象一个 Close()函数用于业务在不需要的时候清理对象，这里就可以用到这个 SetFinalizer，如 os.NewFile 就注册了 SetFinalizer逻辑 1234567891011121314151617181920func newFile(fd uintptr, name string, kind newFileKind) *File &#123; fdi := int(fd) if fdi &lt; 0 &#123; return nil &#125; f := &amp;File&#123;&amp;file&#123; pfd: poll.FD&#123; Sysfd: fdi, IsStream: true, ZeroReadIsEOF: true, &#125;, name: name, stdoutOrErr: fdi == 1 || fdi == 2, &#125;&#125; //... runtime.SetFinalizer(f.file, (*file).close) return f&#125; 调用方式 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( \"fmt\" \"runtime\" \"time\")type Foo struct &#123; name string num int&#125;func finalizer(f *Foo) &#123; fmt.Println(\"a finalizer has run for \", f.name, f.num)&#125;var counter intfunc MakeFoo(name string) (a_foo *Foo) &#123; a_foo = &amp;Foo&#123;name, counter&#125; counter++ runtime.SetFinalizer(a_foo, finalizer) return&#125;func Bar() &#123; f1 := MakeFoo(\"one\") f2 := MakeFoo(\"two\") fmt.Println(\"f1 is: \", f1.name) fmt.Println(\"f2 is: \", f2.name)&#125;func main() &#123; for i := 0; i &lt; 3; i++ &#123; Bar() time.Sleep(time.Second) runtime.GC() &#125; fmt.Println(\"done.\")&#125; 执行结果 12345678910111213f1 is: onef2 is: twoa finalizer has run for two 1a finalizer has run for one 0f1 is: onef2 is: twoa finalizer has run for two 3a finalizer has run for one 2f1 is: onef2 is: twoa finalizer has run for two 5a finalizer has run for one 4done. 注意事项 obj 必须是指针 SetFinalizer 执行顺序按照类似对象的出栈顺序 可以通过 SetFinalizer(obj, nil) 清理对象的析构器 栗子2，它的实际效果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( \"fmt\" \"math/rand\" \"runtime\" \"runtime/debug\" \"strconv\" \"time\")type Foo struct &#123; a int&#125;func main() &#123; debug.SetGCPercent(-1) var ms runtime.MemStats runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects) for i := 0; i &lt; 1000000; i++ &#123; f := NewFoo(i) _ = fmt.Sprintf(\"%d\", f.a) &#125; runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects) runtime.GC() time.Sleep(time.Second) runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects) runtime.GC() time.Sleep(time.Second) runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects)&#125;//go:noinlinefunc NewFoo(i int) *Foo &#123; f := &amp;Foo&#123;a: rand.Intn(50)&#125; runtime.SetFinalizer(f, func(f *Foo) &#123; _ = fmt.Sprintf(\"foo \" + strconv.Itoa(i) + \" has been garbage collected\") &#125;) return f&#125; 运行结果 1234Allocation: 0.121063 Mb, Number of allocation: 140Allocation: 29.111671 Mb, Number of allocation: 1899990Allocation: 128.025635 Mb, Number of allocation: 4382420Allocation: 0.122147 Mb, Number of allocation: 155 可以看出，正如它功能说的一样，在第二次GC之后，分配的内存被释放 它也有缺点： SetFinalizer 最大的问题是延长了对象生命周期。在第一次回收时执行 Finalizer 函数，且目标对象重新变成可达状态，直到第二次才真正 “销毁”。这对于有大量对象分配的高并发算法，可能会造成很大麻烦 指针构成的 “循环引⽤” 加上 runtime.SetFinalizer 会导致内存泄露 SetFinalizer只在GC 发现对象不可达之后的任意时间执行，所以如果程序正常结束或者发生错误，而对象还没有被GC选中那么 SetFinalizer 也不会执行。 所以保险起见还是提供了 Close() 逻辑供业务调用 参考链接 https://zhuanlan.zhihu.com/p/76504936 https://go.dev/play/p/jWhRSPNvxJ https://medium.com/a-journey-with-go/go-finalizers-786df8e17687 runtime/mfinal.go","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"算法之美-八数码问题","slug":"Algorithms To Live By/算法之美-八数码问题","date":"2022-12-11T09:47:28.000Z","updated":"2022-12-11T09:47:28.000Z","comments":true,"path":"2022/12/11/Algorithms To Live By/算法之美-八数码问题/","link":"","permalink":"http://xboom.github.io/2022/12/11/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E5%85%AB%E6%95%B0%E7%A0%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"八数码问题也称为九宫问题。在3×3的棋盘，摆有八个棋子，每个棋子上标有1至8的某一数字，不同棋子上标的数字不相同。棋盘上还有一个空格，与空格相邻的棋子可以移到空格中。要求解决的问题是：给出一个初始状态和一个目标状态，找出一种从初始转变成目标状态的移动棋子步数最少的移动步骤 所谓一个状态就是棋子在棋盘上的一种摆法。棋子移动后，状态就会发生改变。实际上就是找出从初始状态到达目标状态所经过的一系列中间过渡状态 首先这个九宫格的状态数量是 9!，那么存在问题 是否存在从一个状态转移到另外一个状态无解的情况？ 如何计算或者衡量从一个转移到另外一个状态的需要走多少步？ 排列的性质 为了搞清楚上述问题，需要知道几个基本定义与引理 把n个不同的元素按一定的顺序排列成一行，成为这n个元素的一个排列。n个不同元素的排列共有 n! 种 对于n个自然数的一个排列，如果一个大数排在一个小数之前，就称这两个数构成一个逆序。一个排列的逆序总和称为该排列的逆序对，记为τ(j1j2⋯jn)∗τ∗(∗j∗1∗j∗2⋯∗j∗∗n∗)τ(j1j2⋯jn)*τ*(*j*1*j*2⋯*j**n*)τ(j1j2⋯jn)∗τ∗(∗j∗1∗j∗2⋯∗j∗∗n∗) 如5阶排列31542的逆序是(3,1),(3,2),(5,4),(5,2),(4,2),故 τ(31542)=5∗τ∗(31542)=5τ(31542)=5*τ*(31542)=5τ(31542)=5∗τ∗(31542)=5 逆序对为奇数的排列称为奇排列。逆序对为偶数的排列称为偶排列。自然排列 123⋯n的逆序对为0，故它是偶排列 在一个排列中，把某两个数的位置互换（其他数不动）变成另一个排列的变动称为一个对换，将相邻的两个数对换称为相邻对换 性质的证明 性质1：一个排列中的任意两个数对换后，排列改变奇偶性。即经过一次对换，奇排列变成偶排列，偶排列变成奇排列 证明： 先证明相邻对换的情形 设排列 a1a2...asabb1b2...bta_1 a_2 ... a_s a b b_1 b_2 ... b_ta1​a2​...as​abb1​b2​...bt​，对换a 与 b 的排列变为 a1a2...asbab1b2...bta_1 a_2 ... a_s b a b_1 b_2 ... b_ta1​a2​...as​bab1​b2​...bt​ a 与 b 的 对换不影响 a1a2...asa_1 a_2 ... a_sa1​a2​...as​， b1b2...btb_1 b_2 ... b_tb1​b2​...bt​ 与 其他数的关系 但a与b的序关系变为： 当 a &lt; b 时，在新排列中 a 、b 构成逆序 当 a &gt; b 时，在新排列中 a、b 不构成逆序 因此 a1a2...asabb1b2...bta_1 a_2 ... a_s a b b_1 b_2 ... b_ta1​a2​...as​abb1​b2​...bt​ 比 a1a2...asbab1b2...bta_1 a_2 ... a_s b a b_1 b_2 ... b_ta1​a2​...as​bab1​b2​...bt​ 的逆序多1或者少1 再证一般对换情形 设存在排列 a1a2…asab1b2…bmbc1c2…cta_1 a_2 … a_s a b_1 b_2 … b_m b c_1 c_2 … c_ta1​a2​…as​ab1​b2​…bm​bc1​c2​…ct​ 将 b 做 m 次相邻对换变为 a1a2…asabb1b2…bmc1c2…cta_1 a_2 … a_s a b b_1 b_2 … b_m c_1 c_2 … c_ta1​a2​…as​abb1​b2​…bm​c1​c2​…ct​ 再将 a 做 (m + 1) 次相邻对换变为 a1a2…asbb1b2…bmac1c2…cta_1 a_2 … a_s b b_1 b_2 … b_m a c_1 c_2 … c_ta1​a2​…as​bb1​b2​…bm​ac1​c2​…ct​ 所以 经过 (2m + 1) 次相邻兑换，可以把排列 a1a2…asab1b2…bmbc1c2…cta_1 a_2 … a_s a b_1 b_2 … b_m b c_1 c_2 … c_ta1​a2​…as​ab1​b2​…bm​bc1​c2​…ct​转变成a1a2…asbb1b2…bmac1c2…cta_1 a_2 … a_s b b_1 b_2 … b_m a c_1 c_2 … c_ta1​a2​…as​bb1​b2​…bm​ac1​c2​…ct​，这两个排列的奇偶性相反 性质2：在全部的 n(n≥2)阶排列中，奇偶排列各占一半，各有 n!/2 个 证明： 假设在全部n级排列中共有t个奇排列，s个偶排列 将t个奇排列中的前两个数字对换，得到t个互不相同的偶排列。因此 t≤s 同理可证 s≤t 于是 s=t，即奇、偶排列的总数相等，各有 n!/2个 性质3：任意一个n阶排列都可以经过一系列对换变成自然排列，并且所作对换的次数与这个排列有相同的奇偶性 证明(归纳法)： 1阶排列只有一个，结论显然成立 假设对n-1阶排列已经成立，证对n阶排列的情形结论也成立 设j1j2...jnj_1 j_2 ... j_nj1​j2​...jn​ 是一个n阶排列 如果 jn=nj_n=njn​=n ，假设n-1级排列 j1j2…jn−1j_1 j_2 … j_n−1j1​j2​…jn​−1 可以经过一系列变换变成自然序列，即 1 2 … n−1，于是这一系列对换也就把 j1j2⋯jn 变成 12⋯n 这种自然序列的形式。 如果 jn≠njn≠njn​=n，那么对 j1j2…jnj_1 j_2 … j_nj1​j2​…jn​ 作 jnj_njn​ 和 n 的对换，它就变成 j1j2…jn−1nj_1 j_2 … j_n−1 nj1​j2​…jn​−1n ，这就归结成上面的情形，因此 性质4：奇偶性与可达性关系与证明 必要性证明：排列的奇偶性不同则对应在八数码问题中不可达 在满足上述约定的八数码问题中，空格与相邻棋子的交换不会改变棋局中棋子数列的逆序对的奇偶性 空格与左右棋子交换：是不改变棋子数列的逆序对的(因为数列并没有改变) 空格与上下棋子交换：也是不改变棋子数列的逆序对的 假设交换棋子为c[i]=X 原数列p=c[1]… X c[i+1]c[i+2]…c[8]将变为新数列q=c[1]…c[i+1]c[i+2]X …c[8]（注意：在棋盘中，上下相邻的两棋格之间隔有两个棋格）。可以解释为用X与c[i+1]、 c[i+2]先后进行两次相邻交换而完成状态转变。 由p状态到q状态并不会改变改变棋子数列的逆序对的奇偶性。同理可证空格与下方棋子交换也不会改变棋子数列的逆序对的奇偶性。所以，空格与相邻棋子的交换不会改变棋局中棋子数列的逆序对的奇偶性 得出 定理1：对于任意两个状态映射的序列，如果这两个状态等价，那么它们的逆序数相同 充分性证明：排列的奇偶性相同则对应在八数码问题中也可达 首先明确几个定义： 状态(state)：八数码中8个数字与0的排列被定义为八数码的状态，比如： 状态空间(state space)：八数码所有状态的合集被称为状态空间 完全态(completeness)：当状态空间的子集中任何两个状态都能通过一定步骤得到，那么称这个状态空间的子集是完全态 状态映射(a sequence mapped by a state)：一个状态{}可以映射(忽略0)成一个排序，那么这个排列就称为这个状态的映射 标准格式(a standard form)：如果 0 在正中间，那么称这个状态为标准格式 区域(field)：对于任意状态(state)，4个位置中任意两个都相邻，那么称之为 区域，特别的，当两个区域有相同的两个位置，则称为 同边区域 转圈(circle moving)：0在一个区域内移动称 转圈 引理1：对于标准格式中的任意区域，转圈可以得到两个等价的区域 考虑到 标准格式 的对称性，只需要考虑一个区域的的变化 第一种情况(顺时针方向部分先后): a b d 第二种情况(顺时针方向不分先后): a d b 接着考虑在一个标准格式中，在两个具有相同边的区域交换数据，由于标准格式的对称性，所以只考虑上半部分 这个图形转换根据上述原则手动绘制交换流程更容易理解 得出的规律1: m 属于 {a ,d}，n 属于 {c, e}，p 属于 {a, d} 但 p != m，q 属于 {c, e}但 q != n，那么交换 m n，则 n 来到 m 之前的位置，而 m 来到 q 之前的位置，q 来到 n 之前的位置 得出的规律2：abc 可以经过转换编程 cab 或者 bac，不影响其他行且逆序对不变 对于标准格式，如果它们的逆序对的奇偶性相同，那么它们是等价的 证明： 首先，将九宫格分为A、B、C、D分为四个区域 第一步，将 h 移动到位置 位置 9，这是肯定可以的 第二步，将 g、e 移动到区域D 为什么不考虑g、e的顺序，因为可以在不影响h的情况下 g 在 位置 6 与 位置8 任意切换 2.1 如果 g、e 已经在区域D，那么不需要移动 2.2 如果其中 g 在 区域D，而e 在其他区域 2.2.1 e 在区域A 2.2.2 e 在区域B 2.2.3 e 在区域C 第三步，将 b、c 移动到 B区域 3.1 如果 b、c 都在区域 B，那么不需要移动 3.2 如果 b、c 有一个在区域B，那么 b 、c 的位置也同样可以忽略，如果b、c是正常顺序，那么也可以在不影响 g e h 位置的情况下调整 b c 的顺序 3.1.1 如果在位置3 3.1.2 如果在位置2，那么先将c移动位置3再执行上述步骤 如果 b、c 都不在区域B，同理将c 移动到位置3，然后在执行上述3.1.1 步骤 第四步，在不影响bcehg的情况下调整adf的顺序 规律二在 此场景下仍然适用，所以首先将防止到位置7 根据定理1，如果两者等价，那么它们的逆序对是相同的。而如果位置1与位置3交换其他位置不变，那边整个序列的逆序对是变化的。所以a一定在位置1，d在位置4 所以，如果奇偶性相同，那么两个状态都能转换成一个相同的标准状态，那么两个状态之间是可达的 得出结论：两个排列的逆序对奇偶性相同，那么在八数码中必可达 参考链接 https://blog.csdn.net/u011008379/article/details/40144147 https://chengfeng96.com/blog/2018/05/26/利用BFS，DFS，A-解决八数码难题/ http://www.huaying1988.com/blogs/8_Puzzle_StrictProof_SolutionAlgorithm_AndOthers/detail.html 《A constructive proof for the subsets completeness of 8-Puzzle state space》","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"Grpc-03-解析器原理","slug":"Grpc/Grpc-03-解析器原理","date":"2022-10-18T16:18:01.000Z","updated":"2023-02-03T13:42:16.534Z","comments":true,"path":"2022/10/19/Grpc/Grpc-03-解析器原理/","link":"","permalink":"http://xboom.github.io/2022/10/19/Grpc/Grpc-03-%E8%A7%A3%E6%9E%90%E5%99%A8%E5%8E%9F%E7%90%86/","excerpt":"","text":"解析器的的作用 通过对字符串的解析得到后端提供服务的地址列表 解析地址之后触发平衡器流程，平衡器开始根据解析的地址们与服务端建立链接 下面会反复讲到几个词语，这里提前定义 链接 或 ConnectClient：这里代表的是 target 对应连接，可能实际存在多个子连接 addrConn 子链接 或 addrConn：这里代表具体的客户端与服务器建立的一个TCP链接 整理流程 通过 Target 解析获取 的scheme指定解析器构建器名称 解析器构建器在Build过程中构建解析器，获取地址信息，触发平衡器 生成解析器之后再通过 协程for循环建立子链接 实现原理 代码路径:/resolver/resolver.go，在这个文件中一共定义了2个接口 Builder 接口，主要用来构建解析器 Resolver 接口，主要用来让解析器进行解析 解析器构建器注册 解析器构建器通过注册的方式将解析器的构建方法存入全局变量中，当需要使用的时候则直接根据名称获取 使用一个全局的 map var m = make(map[string]Builder)进行解析器构建器 Builder 的存储。k/v 分别对应的是构建 构建器的名称以及构建器的实现方法。 解析器构建器可以通过 Register 函数存储到这个 map 中 123456789101112//注册解析器构建器func Register(b Builder) &#123; m[b.Scheme()] = b&#125;//返回解析器构建器 schemefunc Get(scheme string) Builder &#123; if b, ok := m[scheme]; ok &#123; return b &#125; return nil&#125; 在获取构建中不是通过名称，而是通过scheme做为解析器构建器的名称 1var defaultScheme = \"passthrough\" //默认是passthrough解析器 前面说过解析器是通过 字符串解析获取对应的服务地址 这个字符串的格式 Scheme://Authority/Endpoint，其中 Scheme，Authorith，Endpoint 是需要用户自己设置的。Scheme，Authorith不是必须设置的，默认使用 passthrouth 解析器 字符串格式例子 etcd://auth/config.bar，其中 Scheme = etcd，Authorith = auth，Endpoint = config.bar dns:///config.bar，其中 Scheme = dns，Authorith = &quot;&quot;，Endpoint = config.bar localhost:8080，其中 Scheme = &quot;&quot;，Authorith = &quot;&quot;，Endpoint = localhost:8080 服务地址有可能是具体的服务地址，也可能是负载均衡的地址 而一个普通的解析器构建器(已DNS解析器构建器为例rrPickerBuilder)，则是利用 init 函数直接初始化到这个全局map中，那么就可以在需要使用的时候，直接从全局map 中获取 1234567func init() &#123; resolver.Register(NewBuilder())&#125;func NewBuilder() resolver.Builder &#123; return &amp;dnsBuilder&#123;&#125;&#125; 构建解析器 有了解析器构建器之后就可以构建解析器，默认解析器是 passthrough 设置通过哪个平衡器构建器获取平衡器，由于grpc.Dial(target)中 target 就是由 Scheme://Authority/Endpoint，这样来指定的解析器 指定了构建器之后就会在构建链接过程中调用构建函数 resolver.Builder 123456789101112131415161718192021222324252627282930313233//根据 target 获取解析器构建器func (cc *ClientConn) parseTargetAndFindResolver() (resolver.Builder, error) &#123; var rb resolver.Builder //將target 转换成 resolver.Target 结构 parsedTarget, err := parseTarget(cc.target) if err != nil &#123; //target格式不是这样 [scheme]://[authority]/endpoint //log &#125; else &#123; rb = cc.getResolver(parsedTarget.Scheme) if rb != nil &#123; //根据Scheme找到了对应的解析器构建者 cc.parsedTarget = parsedTarget return rb, nil &#125; &#125; //如果target名称不规范，或者根据scheme找不到解析的构建者，那么就会尝试使用默认的解析器进行解析 passthrouth defScheme := resolver.GetDefaultScheme() canonicalTarget := defScheme + \":///\" + cc.target parsedTarget, err = parseTarget(canonicalTarget) if err != nil &#123; //使用默认的解析器格式还是错误的，那么就直接退出 return nil, err &#125; rb = cc.getResolver(parsedTarget.Scheme) //否则使用默认解析器的构建者 if rb == nil &#123; return nil, fmt.Errorf(\"could not get resolver for default scheme: %q\", parsedTarget.Scheme) &#125; cc.parsedTarget = parsedTarget return rb, nil&#125; 通过 newCCResolverWrapper 解析器构建器的来返回一个构建器，ccResolverWrapper 是内部封装解析器功能对象 12345678910111213141516171819func newCCResolverWrapper(cc *ClientConn, rb resolver.Builder) (*ccResolverWrapper, error) &#123; ccr := &amp;ccResolverWrapper&#123; cc: cc, done: grpcsync.NewEvent(), &#125; //... 可选参数 var err error // 这里加锁的原因是： // rb.Build--&gt;ccr.ReportError--&gt;ccr.poll--&gt;ccr.resolveNow, would end up ccr.resolverMu.Lock() defer ccr.resolverMu.Unlock() ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, rbo) //在这里出发解析器构建器的Build 进行构建解析器 if err != nil &#123; return nil, err &#125; return ccr, nil&#125; ​ 传入的是连接指针与解析器构建器，返回的是包含了解析器的 ccResolverWrapper 对象 这个 ccResolverWrapper 结构其实就是包含了这个连接的解析信息，下面看下包含的内容 1234567891011121314151617181920type ccResolverWrapper struct &#123; cc *ClientConn //target 对应的连接 resolverMu sync.Mutex //解析原子锁 resolver resolver.Resolver //解析器 done *grpcsync.Event //同步事件 curState resolver.State //解析状态 incomingMu sync.Mutex // 原子锁所有的调用&#125;type State struct &#123; // target 对应的地址信息，后续平衡器根据这些Address 建立连接 Addresses []Address //配置信息 ServiceConfig *serviceconfig.ParseResult //解析器属性 Attributes *attributes.Attributes&#125; 在第三步中，通过构建器解析器的构建，接下来就是进行链接操作 12345678910111213141516// A blocking dial blocks until the clientConn is ready.if cc.dopts.block &#123; for &#123; cc.Connect() s := cc.GetState() if s == connectivity.Ready &#123; break &#125; else if cc.dopts.copts.FailOnNonTempDialError &amp;&amp; s == connectivity.TransientFailure &#123; //... &#125; if !cc.WaitForStateChange(ctx, s) &#123; //... &#125; &#125;&#125; 这里会 for 循环执行三个操作 第一个是 Connect，这里通过第三步的 解析器 Build 已经获取了target 对应的地址信息，这里根据地址进行连接 第二个是 GetState，用于获取链接状态，如果链接是就绪状态 connectivity.Ready 表示整个链接就绪 第三个是 WaitForStateChange在后续还有一个状态改变的过程 第四个是 cc.dopts.block，表示阻塞情况下才会直接进行地址链接。 应用解析器 生成了解析器之后就需要放入合适的时间进行解析，解析器的接口包括 123456type Resolver interface &#123; //立即we ResolveNow(ResolveNowOptions) //关闭解析器 Close()&#125; 这里的 ResolveNow 可以不用实现，因为 ResolveNow 其实是在建立连接失败之后立即尝试重新建立链接 12345// Connect -&gt; connect -&gt; resetTransportif err := ac.tryAllAddrs(addrs, connectDeadline); err != nil &#123; ac.cc.resolveNow(resolver.ResolveNowOptions&#123;&#125;) //如果失败了，就执行resolveNow(可以由用户自定义，在连接失败之后可以立即重新更新地址已重新建立连接) //... 退避算法&#125; 如果不实现resolveNow ，那么就需要通过退避算法等待一段时间再次进行链接 实际应用 这里已内部解析器的实现 passthrough 为例，代码路径：internal/resolver/passthrough/passthrough.go 第一步，为了实现解析器就需要解析器构建器与解析器 1234567type passthroughBuilder struct&#123;&#125; //解析器构建器type passthroughResolver struct &#123; //解析器 target resolver.Target cc resolver.ClientConn&#125; 解析器结构字段可自定义 解析器构建器需要实现 Builder 1234567891011121314151617181920212223242526type Builder interface &#123; Build(target Target, cc ClientConn, opts BuildOptions) (Resolver, error) Scheme() string&#125;//构建解析器func (*passthroughBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) &#123; r := &amp;passthroughResolver&#123; //构建解析器 target: target, cc: cc, &#125; r.start() return r, nil&#125;//触发平衡器func (r *passthroughResolver) start() &#123; r.cc.UpdateState(resolver.State&#123;Addresses: []resolver.Address&#123;&#123;Addr: r.target.Endpoint&#125;&#125;&#125;)&#125;const scheme = \"passthrough\"//返回构建器名称func (*passthroughBuilder) Scheme() string &#123; return scheme&#125; 解析器的实现 123func (*passthroughResolver) ResolveNow(o resolver.ResolveNowOptions) &#123;&#125;func (*passthroughResolver) Close() &#123;&#125; 这里就没有进行任务的实现 最后将解析器注册到全局内存中 123func init() &#123; resolver.Register(&amp;passthroughBuilder&#123;&#125;)&#125; 总结 解析器其实包含了解析器构建器与解析器 可以同时定义多个解析器，然后通过target指定解析器 解析器中获取地址的方式自定义还能加入注册中心来获取地址信息 参考文档 https://zhuanlan.zhihu.com/p/377860784","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Go入门20-Goland激活","slug":"Go/Go入门20-Goland激活","date":"2022-10-17T15:51:31.000Z","updated":"2022-10-17T15:51:31.000Z","comments":true,"path":"2022/10/17/Go/Go入门20-Goland激活/","link":"","permalink":"http://xboom.github.io/2022/10/17/Go/Go%E5%85%A5%E9%97%A820-Goland%E6%BF%80%E6%B4%BB/","excerpt":"","text":"Goland好用但是激活太贵，网上找了一个使用激活服务器的办法， Goland 2022.2.2 激活成功 最好支持正版！！！ 第一步：首先打开网址 https://search.censys.io 第二步：搜索信息 services.http.response.headers.location: account.jetbrains.com/fls-auth 第三步：点击搜索到的网址信息 第四步：找到状态为302的信息网址信息，如果没有找到请重复第三步 第五步：复制上一步信息中的Details信息，这里是 https://188.210.42.106 第六步：将地址信息填入 goland 中的 Licence Service 并点击 Acitve 激活 第七步：如果激活成功将出现下图所示，否则重复第三步 参考文档 https://dushusir.com/jetbrains/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门18-Fuzzing","slug":"Go/Go入门18-Fuzzing","date":"2022-10-14T15:31:50.000Z","updated":"2022-10-14T15:31:50.000Z","comments":true,"path":"2022/10/14/Go/Go入门18-Fuzzing/","link":"","permalink":"http://xboom.github.io/2022/10/14/Go/Go%E5%85%A5%E9%97%A818-Fuzzing/","excerpt":"","text":"Golang在1.18引入的第三个特性就是 Fuzzing 模糊测试：构造随机数据来找出代码里的漏洞或者可能导致程序崩溃的输入。 单元测试有局限性，每个测试输入必须由开发者指定加到单元测试的测试用例里。fuzzing的优点之一是可以基于开发者代码里指定的测试输入作为基础数据，进一步自动生成新的随机测试数据，用来发现指定测试输入没有覆盖到的边界情况。 通过fuzzing可以找出的漏洞包括SQL注入、缓冲区溢出、拒绝服务(Denial of Service)攻击和XSS(cross-site scripting)攻击等 这里通过编写反转字符串函数通过 fuzz test 来发现并修改问题 第一步：实现基本功能 12345678910111213141516func Reverse(s string) string &#123; b := []byte(s) for i, j := 0, len(b)-1; i &lt; len(b)/2; i, j = i+1, j-1 &#123; b[i], b[j] = b[j], b[i] &#125; return string(b)&#125;func main() &#123; input := \"The quick brown fox jumped over the lazy dog\" rev := Reverse(input) doubleRev := Reverse(rev) fmt.Printf(\"original: %q\\n\", input) fmt.Printf(\"reversed: %q\\n\", rev) fmt.Printf(\"reversed again: %q\\n\", doubleRev)&#125; 输出的结果是 123original: \"The quick brown fox jumped over the lazy dog\"reversed: \"god yzal eht revo depmuj xof nworb kciuq ehT\"reversed again: \"The quick brown fox jumped over the lazy dog\" 第二步：编写单元测试 123456789101112131415func TestReverse(t *testing.T) &#123; testcases :&#x3D; []struct &#123; in, want string &#125;&#123; &#123;&quot;Hello, world&quot;, &quot;dlrow ,olleH&quot;&#125;, &#123;&quot; &quot;, &quot; &quot;&#125;, &#123;&quot;!12345&quot;, &quot;54321!&quot;&#125;, &#125; for _, tc :&#x3D; range testcases &#123; rev :&#x3D; Reverse(tc.in) if rev !&#x3D; tc.want &#123; t.Errorf(&quot;Reverse: %q, want %q&quot;, rev, tc.want) &#125; &#125;&#125; 执行单元测试发现，一切正常 1ok example/fuzz 0.003s 第三步：添加模糊测试 123456789101112131415161718func FuzzReverse(f *testing.F) &#123; testcases := []string&#123;\"Hello, world\", \" \", \"!12345\"&#125; for _, tc := range testcases &#123; f.Add(tc) // Use f.Add to provide a seed corpus &#125; f.Fuzz(func(t *testing.T, orig string) &#123; rev := Reverse(orig) doubleRev := Reverse(rev) //这一行是为了解释后面的错误加的，暂时可以忽略 t.Logf(\"Number of runes: orig=%d, rev=%d, doubleRev=%d\", utf8.RuneCountInString(orig), utf8.RuneCountInString(rev), utf8.RuneCountInString(doubleRev)) if orig != doubleRev &#123; t.Errorf(\"Before: %q, after: %q\", orig, doubleRev) &#125; if utf8.ValidString(orig) &amp;&amp; !utf8.ValidString(rev) &#123; t.Errorf(\"Reverse produced invalid UTF-8 string %q\", rev) &#125; &#125;)&#125; 注意点： Reverse函数如果是一个错误的版本(直接return返回输入的字符串)，虽然可以通过上面的模糊测试，但没法通过第二步的单元测试，所以模糊测试与单元测试是互补的关系 go test 只会使用种子语料库，而不会生成随机测试数据。通过这种方式可以用来验证种子语料库的测试数据是否可以测试通过 如果reverse_test.go文件里有其它单元测试函数或者模糊测试函数，但只想运行FuzzReverse模糊测试函数，我们可以执行go test -run=FuzzReverse命令 如果要基于种子语料库生成随机测试数据用于模糊测试，需要给go test命令增加-fuzz参数 123456789101112131415root@13ce5bc74ac3:/code/fuzz# go test -fuzz .fuzz: elapsed: 0s, gathering baseline coverage: 0/7 completedfuzz: elapsed: 0s, gathering baseline coverage: 7/7 completed, now fuzzing with 2 workersfuzz: elapsed: 0s, execs: 648 (17784/sec), new interesting: 1 (total: 8)--- FAIL: FuzzReverse (0.04s) --- FAIL: FuzzReverse (0.00s) hello_test.go:32: Number of runes: orig=1, rev=2, doubleRev=1 hello_test.go:36: Reverse produced invalid UTF-8 string \"\\x9e\\xdb\" Failing input written to testdata/fuzz/FuzzReverse/d6a654c77ca8db001e0bbe2cbb5493efcd20e17777911523bf59bd30bd33e199 To re-run: go test -run=FuzzReverse/d6a654c77ca8db001e0bbe2cbb5493efcd20e17777911523bf59bd30bd33e199FAILexit status 1FAIL example/fuzz 0.048s 运行之后会生成testdata的文件夹，那么下次即使没有带上 -fuzz 参数，也会使用该数据进行模糊测试 路径：./testdata/fuzz/FuzzReverse/d6a654c77ca8db001e0bbe2cbb5493efcd20e17777911523bf59bd30bd33e199，内容是： 12go test fuzz v1 //语料库文件里的第1行标识的是编码版本string(\"۞\") 从第2行开始，每一行数据对应的是语料库的每条测试数据(corpus entry)的其中一个参数，按照参数先后顺序排列，因为fuzz target函数func(t *testing.T, orig string)只有orig这1个参数作为真正的测试输入，也就是每条测试数据其实就1个输入，因此在上面示例的testdata/fuzz/FuzzReverse目录下的文件里只有string(“۞”)这一行 第四步：修复Bug 模糊测试中得出的错误为 Reverse produced invalid UTF-8 string &quot;\\x9e\\xdb&quot; Reverse函数是按照字节(byte)为维度进行字符串反转，这就是问题所在。比如字符string(&quot;۞&quot;) 如果按照字节反转，反转后得到的就是一个无效的字符串了。因此为了保证字符串反转后得到的仍然是一个有效的UTF-8编码的字符串，需要按照rune进行字符串反转。 1234567func Reverse(s string) string &#123; r := []rune(s) for i, j := 0, len(r)-1; i &lt; len(r)/2; i, j = i+1, j-1 &#123; r[i], r[j] = r[j], r[i] &#125; return string(r)&#125; 运行 go test 命令单元测试通过，表示老的数据能够正常的处理，但是如果再次执行 go test -fuzz 会出现新的错误 1234567891011121314root@13ce5bc74ac3:/code/fuzz# go test -fuzz .fuzz: elapsed: 0s, gathering baseline coverage: 0/9 completedfuzz: minimizing 38-byte failing input filefuzz: elapsed: 0s, gathering baseline coverage: 4/9 completed--- FAIL: FuzzReverse (0.01s) --- FAIL: FuzzReverse (0.00s) hello_test.go:34: Before: \"\\xe5\", after: \"�\" Failing input written to testdata/fuzz/FuzzReverse/91862839dc552bd95b4e42be6576a6c198f0d4c8fc2884c953030d898573b014 To re-run: go test -run=FuzzReverse/91862839dc552bd95b4e42be6576a6c198f0d4c8fc2884c953030d898573b014FAILexit status 1FAIL example/fuzz 0.011s 结构就是对一个字符串做了2次反转后得到的和原字符串不一样，这次测试输入本身是非法的unicode 12345678910func Reverse(s string) (string, error) &#123; if !utf8.ValidString(s) &#123; //判断是否是合法的 utf-8编码 return s, errors.New(\"input is not valid UTF-8\") &#125; r := []rune(s) for i, j := 0, len(r)-1; i &lt; len(r)/2; i, j = i+1, j-1 &#123; r[i], r[j] = r[j], r[i] &#125; return string(r), nil&#125; 修改对应的引用和单元测试后，通过测试 语法 Go模糊测试和单元测试在语法上有如下差异： Go模糊测试函数以FuzzXxx开头，单元测试函数以TestXxx开头 Go模糊测试函数以 *testing.F作为入参，单元测试函数以*testing.T作为入参 Go模糊测试会调用f.Add函数和f.Fuzz函数。 f.Add函数把指定输入作为模糊测试的种子语料库(seed corpus)，fuzzing基于种子语料库生成随机输入。 f.Fuzz函数接收一个fuzz target函数作为入参。fuzz target函数有多个参数，第一个参数是*testing.T，其它参数是被模糊的类型(注意：被模糊的类型目前只支持部分内置类型, string, []byte int, int8, int16, int32/rune, int64 uint, uint8/byte, uint16, uint32, uint64 float32, float64 bool 参考链接 https://go.dev/doc/tutorial/fuzz https://segmentfault.com/a/1190000041650681 https://segmentfault.com/a/1190000041467510 https://go.googlesource.com/proposal/+/master/design/draft-fuzzing.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门14-泛型","slug":"Go/Go入门14-泛型","date":"2022-09-17T07:45:35.000Z","updated":"2022-09-17T07:45:35.000Z","comments":true,"path":"2022/09/17/Go/Go入门14-泛型/","link":"","permalink":"http://xboom.github.io/2022/09/17/Go/Go%E5%85%A5%E9%97%A814-%E6%B3%9B%E5%9E%8B/","excerpt":"","text":"问题背景 Golang 在1.18 推出了新特性 泛型 Generics 为什么要有泛型 什么是泛型 如何使用泛型 使用的golang版本 go version go1.18.5 linux/amd6 为什么要有泛型从一个经典的问题开始说起，实现计算两数之和的函数 123func Add(a int, b int) int &#123; return a + b&#125; 很快就写完了，那么要计算浮点型或者字符串怎么办？ 1234567func AddFloat32(a float32, b float32) float32 &#123; return a + b&#125;func AddString(a string, b string) string &#123; return a + b&#125; 重复代码太多，有没有办法只写一个函数实现它们 12345678910111213141516171819202122232425func AddInterface(a, b interface&#123;&#125;) interface&#123;&#125; &#123; at := reflect.TypeOf(a) bt := reflect.TypeOf(b) if at != bt &#123; return nil &#125; switch a.(type) &#123; case int: return a.(int) + b.(int) case string: return a.(string) + b.(string) case float32: return a.(float32) + b.(float32) &#125; return nil&#125;func AddInt(a, b int) int &#123; return a + b&#125;fmt.Printf(\"no Generic1 %v \\n\", AddInterface(1, 2)) //3fmt.Printf(\"no Generic2 %v \\n\", AddInterface(\"1\", \"2\")) //\"12\"fmt.Printf(\"no Generic3 %v \\n\", AddInterface(float32(0.1), float32(0.2))) //0.3 上述函数有两个要求： 两者的类型要一样，否则无法相加 需要是函数支持的类型，否则无法相加 同时，也存在两个问题 需要利用反射进行类型判断写起来很繁琐 添加反射操作必然导致性能的降低 1234567891011func BenchmarkAddInterface(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInterface(30, 30) // run AddInterface(30, 30) b.N times &#125;&#125;func BenchmarkAddInt(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInt(30, 30) // AddInt(30, 30) b.N times &#125;&#125; 执行 go test -bench &quot;Add*&quot; 得到如下图所示，性能大约降低20倍 泛型就能很好的解决上述问题 1234567func AddT[T int | string | float32](a, b T) T &#123; return a + b&#125;fmt.Printf(\"Generic1 %v \\n\", AddT(1, 2)) //3fmt.Printf(\"Generic2 %v \\n\", AddT(\"1\", \"2\")) //\"12\"fmt.Printf(\"Generic3 %v \\n\", AddT(float32(0.1), float32(0.2))) //0.3 那么是否泛型的性能又是怎么样 1234567891011121314151617func BenchmarkAddInterface(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInterface(30, 30) // run AddInterface(30, 30) b.N times &#125;&#125;func BenchmarkAddInt(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInt(30, 30) // AddInt(30, 30) b.N times &#125;&#125;func BenchmarkAddT(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddT(30, 30) // AddT(30, 30) b.N times &#125;&#125; 得到的结果如下图，与固定类型的增加基本没有差别 泛型 泛型的基本概念 函数存在 形参(parameter) 和 实参(argument) 这一基本概念 123456func Add(a int, b int) int &#123; // 变量a,b是函数的形参 \"a int, b int\" 这一串被称为形参列表 return a + b&#125;Add(100,200) // 调用函数时，传入的100和200是实参 为了替代固定的 int 类型，Go 引入了 类型形参 与 类型实参，让一个函数获取了处理多种不同类型数据的能力，这种编程方式就叫做 泛型编程 1234// 假设 T 是类型形参，在定义函数时它的类型是不确定的，类似占位符func AddT[T int | string | float32](a, b T) T &#123; return a + b&#125; 类型形参(Type parameter)：T 表示代表的具体类型并不确定，类似一个占位符(可以使用其他符号代替) 类型约束(Type constraint): 表示类型形参 T 只可以接收 int 或 float32 或 string 类型的实参 类型形参列表(type parameter list): T int | string | float32这一整串定义了所有的类型形参 泛型类型(Generic type)：类型定义中带 类型形参 的类型 类型实参(Type argument)：泛型类型不能直接拿来使用，必须传入类型实参(Type argument) 将其确定为具体的类型之后才可使用。 实例化(Instantiations) ：传入类型实参确定具体类型的操作被称为 实例化(Instantiations) 类型参数的声明紧随： 函数名之后 类型名之后。类型参数通过类型集进行约束。 类型集本质上就是接口，类型集可以作为类型参数的约束，一个接口也可以具有类型参数。 类型集的规则是：同行并集，不同行交集 使用方法 基本类型 123456789101112131415161718type Slice[T int|float32|float64 ] []T // 这里传入了类型实参int，泛型类型Slice[T]被实例化为具体的类型 Slice[int]var a Slice[int] = []int&#123;1, 2, 3&#125; fmt.Printf(\"Type Name: %T\",a) //输出：Type Name: Slice[int]// 传入类型实参float32, 将泛型类型Slice[T]实例化为具体的类型 Slice[string]var b Slice[float32] = []float32&#123;1.0, 2.0, 3.0&#125; fmt.Printf(\"Type Name: %T\",b) //输出：Type Name: Slice[float32]// ✗ 错误。因为变量a的类型为Slice[int]，b的类型为Slice[float32]，两者类型不同a = b // ✗ 错误。string不在类型约束 int|float32|float64 中，不能用来实例化泛型类型var c Slice[string] = []string&#123;\"Hello\", \"World\"&#125; // ✗ 错误。Slice[T]是泛型类型，不可直接使用必须实例化为具体的类型var x Slice[T] = []int&#123;1, 2, 3&#125; map 的使用 123456789// MyMap类型定义了两个类型形参 KEY 和 VALUE。分别为两个形参指定了不同的类型约束// 这个泛型类型的名字叫： MyMap[KEY, VALUE]type MyMap[KEY int | string, VALUE float32 | float64] map[KEY]VALUE // 用类型实参 string 和 flaot64 替换了类型形参 KEY 、 VALUE，泛型类型被实例化为具体的类型：MyMap[string, float64]var a MyMap[string, float64] = map[string]float64&#123; \"jack_score\": 9.6, \"bob_score\": 8.4,&#125; KEY和 VALUE 是类型形参 int|string 是KEY的类型约束， float32|float64 是VALUE的类型约束 KEY int|string, VALUE float32|float64 整个一串文本因为定义了所有形参所以被称为类型形参列表 Map[KEY, VALUE] 是泛型类型，类型的名字就叫 Map[KEY, VALUE] var a MyMap[string, float64] = xx 中的string和float64是类型实参，用于分别替换KEY和VALUE，实例化出了具体的类型 MyMap[string, float64] 其他类型 struct 的使用 123456789101112131415161718192021222324252627// 一个泛型类型的结构体。可用 int 或 sring 类型实例化type MyStruct[T int | string] struct &#123; Name string Data T&#125;//实例话的时候需要指定T的类型var mystruct1 = []MyStruct[int]&#123; &#123; \"1\", 1, &#125;, &#123; \"2\", 2, &#125;,&#125;//✗ 错误。var mystruct2 = []MyStruct[int]&#123; &#123; \"1\", 1, &#125;, &#123; \"2\", \"2\", //cannot use \"2\" (untyped string constant) as int value in struct literal &#125;,&#125; 接口的泛型使用 1234567891011// 一个泛型接口(关于泛型接口在后半部分会详细讲解）type IPrintData[T int | float32 | string] interface &#123; Print(data T)&#125;//实现类型func (m *MyStruct[string]) Print(data string) &#123;&#125;var _ IPrintData[string] = (*MyStruct[string])(nil) chan 的泛型使用 1234// 一个泛型通道，可用类型实参 int 或 string 实例化type MyChan[T int | string] chan Tvar myChan = make(MyChan[int], 1) 类型的互相嵌套 123456789101112131415161718type WowStruct[T int | float32, S []T] struct &#123; Data S MaxValue T MinValue T&#125;var wowStruct1 = &amp;WowStruct[int, []int]&#123; Data: []int&#123;&#125;, MaxValue: 1, MinValue: 1,&#125;//✗ 错误。[]float32 dose not implement []intvar wowStruct2 = &amp;WowStruct[int, []float32]&#123; Data: []float32&#123;&#125;, MaxValue: 1, MinValue: 1,&#125; 其他错误的使用 定义泛型类型的时候，基础类型不能只有类型形参，如下： 12// ✗ 错误，类型形参不能单独使用 cannot use a type parameter as RHS in type declarationtype CommonType[T int|string|float32] T 当类型约束的一些写法会被编译器误认为是表达式时会报错。如下 12345678910//✗ 错误。T *int会被编译器误认为是表达式 T乘以int，而不是int指针type NewType[T *int] []T// 上面代码再编译器眼中：它认为你要定义一个存放切片的数组，数组长度由 T 乘以 int 计算得到type NewType [T * int][]T //✗ 错误。和上面一样，这里不光*被会认为是乘号，| 还会被认为是按位或操作type NewType2[T *int|*float64] []T //✗ 错误。 undeclare name: Ttype NewType2 [T (int)] []T 为了避免这种误解，解决办法就是给类型约束包上 interface{} 或加上逗号消除歧义 12345678type NewType[T interface&#123;*int&#125;] []Ttype NewType2[T interface&#123;*int|*float64&#125;] []T // 如果类型约束中只有一个类型，可以添加个逗号消除歧义type NewType3[T *int,] []T //没错，这样可以而 type NewType3[T *int] []T 不行//✗ 错误。如果类型约束不止一个类型，加逗号是不行的, unexpected comma; expecting ]type NewType4[T *int|*float32,] []T 特殊的泛型类型 12345type Wow[T int | string] intvar a Wow[int] = 123 // 编译正确var b Wow[string] = 123 // 编译正确var c Wow[string] = \"hello\" // 编译错误，因为\"hello\"不能赋值给底层类型int 这里虽然使用了类型形参，但因为类型定义是 type Wow[T int|string] int ，所以无论传入什么类型实参，实例化后的新类型的底层类型都是 int 。所以int类型的数字123可以赋值给变量a和b，但string类型的字符串 “hello” 不能赋值给c，没有什么具体意义，但可以让我们理解泛型类型的实例化的机制 泛型类型的套娃 1234567891011121314151617181920// 先定义个泛型类型 Slice[T]type Slice[T int|string|float32|float64] []T// ✗ 错误。泛型类型Slice[T]的类型约束中不包含uint, uint8type UintSlice[T uint|uint8] Slice[T] // ✓ 正确。基于泛型类型Slice[T]定义了新的泛型类型 FloatSlice[T] 。FloatSlice[T]只接受float32和float64两种类型type FloatSlice[T float32|float64] Slice[T] // ✓ 正确。基于泛型类型Slice[T]定义的新泛型类型 IntAndStringSlice[T]type IntAndStringSlice[T int|string] Slice[T] // ✓ 正确 基于IntAndStringSlice[T]套娃定义出的新泛型类型type IntSlice[T int] IntAndStringSlice[T] // 在map中套一个泛型类型Slice[T]type WowMap[T int|string] map[string]Slice[T]// 在map中套Slice[T]的另一种写法type WowMap2[T Slice[int] | Slice[string]] map[string]T //!!! T 直接代替了 Slice[T] 匿名结构体不支持泛型 123456789testCase := struct &#123; caseName string got int want int&#125;&#123; caseName: \"test OK\", got: 100, want: 100,&#125; 那么匿名结构体能不能使用泛型呢？答案是不能，下面的用法是错误的： 12345678910//✗ 错误。 expected expressiontestCase := struct[T int|string] &#123; caseName string got T want T&#125;[int]&#123; caseName: \"test OK\", got: 100, want: 100,&#125; 在使用泛型的时候我们只能放弃使用匿名结构体，对于很多场景来说这会造成麻烦(最主要麻烦集中在单元测试的时候，为泛型做单元测试会非常麻烦) 泛型receiver 为泛型类型 MySlice[T] 添加了一个计算成员总和的方法 Sum() 。注意观察这个方法的定义 123456789type MySlice[T int | float32] []Tfunc (s MySlice[T]) Sum() T &#123; var sum T for _, value := range s &#123; sum += value &#125; return sum&#125; 首先看receiver (s MySlice[T]) ，直接把类型名称 MySlice[T] 写入了receiver中 然后方法的返回参数我们使用了类型形参 T (方法的接收参数也可以实用类型形参) 在方法的定义中，我们也可以使用类型形参 T (在这个例子里，我们通过 var sum T 定义了一个新的变量 sum ) 泛型类型无论如何都需要先用类型实参实例化!!! 12345var s MySlice[int] = []int&#123;1, 2, 3, 4&#125;fmt.Println(s.Sum()) // 输出：10var s2 MySlice[float32] = []float32&#123;1.0, 2.0, 3.0, 4.0&#125;fmt.Println(s2.Sum()) // 输出：10.0 用类型实参 int 实例化了泛型类型 MySlice[T]，所以泛型类型定义中的所有 T 都被替换为 int，最终我们可以把代码看作下面这样 12345678910type MySlice[int] []int // 实例化后的类型名叫 MyIntSlice[int]// 方法中所有类型形参 T 都被替换为类型实参 intfunc (s MySlice[int]) Sum() int &#123; var sum int for _, value := range s &#123; sum += value &#125; return sum&#125; 基于泛型的队列 1234567891011121314151617181920212223242526// 这里类型约束使用了空接口，代表的意思是所有类型都可以用来实例化泛型类型 Queue[T] (关于接口在后半部分会详细介绍）type Queue[T interface&#123;&#125;] struct &#123; elements []T&#125;// 将数据放入队列尾部func (q *Queue[T]) Put(value T) &#123; q.elements = append(q.elements, value)&#125;// 从队列头部取出并从头部删除对应数据func (q *Queue[T]) Pop() (T, bool) &#123; var value T if len(q.elements) == 0 &#123; return value, true &#125; value = q.elements[0] q.elements = q.elements[1:] return value, len(q.elements) == 0&#125;// 队列大小func (q Queue[T]) Size() int &#123; return len(q.elements)&#125; Queue[T] 因为是泛型类型，所以要使用的话必须实例化，实例化与使用方法如下所示： 123456789101112131415161718192021var q1 Queue[int] // 可存放int类型数据的队列q1.Put(1)q1.Put(2)q1.Put(3)q1.Pop() // 1q1.Pop() // 2q1.Pop() // 3var q2 Queue[string] // 可存放string类型数据的队列q2.Put(\"A\")q2.Put(\"B\")q2.Put(\"C\")q2.Pop() // \"A\"q2.Pop() // \"B\"q2.Pop() // \"C\"var q3 Queue[struct&#123;Name string&#125;] var q4 Queue[[]int] // 可存放[]int切片的队列var q5 Queue[chan int] // 可存放int通道的队列var q6 Queue[io.Reader] // 可存放接口的队列// ...... 动态判断变量的类型 使用接口的时候经常会用到类型断言或 type swith 来确定接口具体的类型，然后对不同类型做出不同的处理 12345678910111213var i interface&#123;&#125; = 123i.(int) // 类型断言// type switchswitch i.(type) &#123; case int: // do something case string: // do something default: // do something &#125;&#125; 那么对于 valut T 这样通过类型形参定义的变量，能不能判断具体类型然后对不同类型做出不同处理呢？答案是不允许的!!! 1234567891011121314151617// invalid operation: cannot use type assertion on type parameter value value (variable of type T constrained by interface&#123;&#125;)func (q *Queue[T]) Put(value T) &#123; value.(int) // 错误。泛型类型定义的变量不能使用类型断言 // 错误。不允许使用type switch 来判断 value 的具体类型 // cannot use type switch on type parameter value value (variable of type T constrained by interface&#123;&#125;) switch value.(type) &#123; case int: // do something case string: // do something default: // do something &#125; // ...&#125; 虽然type switch和类型断言不能用，但我们可通过反射机制达到目的： 12345678910111213141516func (receiver Queue[T]) Put(value T) &#123; // Printf() 可输出变量value的类型(底层就是通过反射实现的) fmt.Printf(\"%T\", value) // 通过反射可以动态获得变量value的类型从而分情况处理 v := reflect.ValueOf(value) switch v.Kind() &#123; case reflect.Int: // do something case reflect.String: // do something &#125; // ...&#125; 为了避免使用反射而选择了泛型，结果到头来又为了一些功能在在泛型中使用反射，当出现这种情况的时候你可能需要重新思考一下，自己的需求是不是真的需要用泛型(毕竟泛型机制本身就很复杂了，再加上反射的复杂度，增加的复杂度并不一定值得) 泛型函数 带类型形参的函数被称为泛型函数，匿名函数不支持泛型 12345fn := func(a, b int) int &#123; return a + b &#125; // 定义了一个匿名函数并赋值给 fn fmt.Println(fn(1, 2)) // 输出: 3 那么Go支不支持匿名泛型函数呢？答案是不能——匿名函数不能自己定义类型形参 1234567// 错误，匿名函数不能自己定义类型实参// function type must have no type parametersfnGeneric := func[T int | float32](a, b T) T &#123; return a + b&#125; fmt.Println(fnGeneric(1, 2)) 但是匿名函数可以使用别处定义好的类型实参，如 123456789func MyFunc[T int | float32 | float64](a, b T) &#123; // 匿名函数可使用已经定义好的类型形参 fn2 := func(i T, j T) T &#123; return i*2 - j*2 &#125; fn2(a, b)&#125; 泛型方法 Go的方法并不支持泛型 12345678type A struct &#123;&#125;// 不支持泛型方法// method must have no type parametersfunc (receiver A) Add[T int | float32 | float64](a T, b T) T &#123; return a + b&#125; 但是因为receiver支持泛型， 所以如果想在方法中使用泛型的话，目前唯一的办法就是曲线救国，迂回地通过receiver使用类型形参 1234567891011121314type A[T int | float32 | float64] struct &#123;&#125;// 方法可以使用类型定义中的形参 T func (receiver A[T]) Add(a T, b T) T &#123; return a + b&#125;// 用法：var a A[int]a.Add(1, 2)var aa A[float32]aa.Add(1.0, 2.0) 泛型使用进阶 复杂的接口 有时候使用泛型编程时，会书写长长的类型约束，如下 12// 一个可以容纳所有int,uint以及浮点类型的泛型切片type Slice[T int | int8 | int16 | int32 | int64 | uint | uint8 | uint16 | uint32 | uint64 | float32 | float64] []T 这种写法是无法忍受也难以维护的，而Go支持将类型约束单独拿出来定义到接口中，从而让代码更容易维护 12345type IntUintFloat interface &#123; int | int8 | int16 | int32 | int64 | uint | uint8 | uint16 | uint32 | uint64 | float32 | float64&#125;type Slice[T IntUintFloat] []T 不过这样的代码依旧不好维护，而接口和接口、接口和普通类型之间也是可以通过 | 进行组合 12345678910111213type Int interface &#123; int | int8 | int16 | int32 | int64&#125;type Uint interface &#123; uint | uint8 | uint16 | uint32&#125;type Float interface &#123; float32 | float64&#125;type Slice[T Int | Uint | Float] []T // 使用 '|' 将多个接口类型组合 在接口里也能直接组合其他接口 12345type SliceElement interface &#123; Int | Uint | Float | string // 组合了三个接口类型并额外增加了一个 string 类型&#125;type Slice[T SliceElement] []T ~ 符号 上面定义的 Slie[T] 虽然可以达到目的，但是有一个缺点： 12345var s1 Slice[int] // 正确 type MyInt int// MyInt does not implement int|string|float32|float64 (possibly missing ~ for int in constraint int|string|float32|float64)var s2 Slice[MyInt] // ✗ 错误。MyInt类型底层类型是int但并不是int类型，不符合 Slice[T] 的类型约束 错误原因：泛型类型 Slice[T] 允许的是 int 作为类型实参，而不是 MyInt （虽然 MyInt 类型底层类型是 int ，但它依旧不是 int 类型）。 为了从根本上解决这个问题，Go新增了一个符号 ~ ，在类型约束中使用类似 ~int 这种写法的话，就代表着不光是 int ，所有以 int 为底层类型的类型也都可用于实例化 1234567891011121314151617181920212223type Int interface &#123; ~int | ~int8 | ~int16 | ~int32 | ~int64&#125;type Uint interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32&#125;type Float interface &#123; ~float32 | ~float64&#125;type Slice[T Int | Uint | Float] []T var s Slice[int] // 正确type MyInt intvar s2 Slice[MyInt] // MyInt底层类型是int，所以可以用于实例化type MyMyInt MyIntvar s3 Slice[MyMyInt] // 正确。MyMyInt 虽然基于 MyInt ，但底层类型也是int，所以也能用于实例化type MyFloat32 float32 // 正确var s4 Slice[MyFloat32] 限制：使用 ~ 时有一定的限制： ~后面的类型不能为接口 ~后面的类型必须为基本类型 1234567type MyInt inttype _ interface &#123; ~[]byte // 正确 ~MyInt // 错误，~后的类型必须为基本类型 invalid use of ~ (underlying type of MyInt is int) ~error // 错误，~后的类型不能为接口 invalid use of ~ (error is an interface)&#125; 接口的变化 从方法集(Method set)到类型集(Type set) 在Go1.18之前，Go官方对 接口(interface) 的定义是：接口是一个方法集(method set) ReadWriter 接口定义了一个接口(方法集)，这个集合中包含了 Read() 和 Write() 这两个方法。所有同时定义了这两种方法的类型被视为实现了这一接口 An interface type specifies a method set called its interface 1234type ReadWriter interface &#123; Read(p []byte) (n int, err error) Write(p []byte) (n int, err error)&#125; 换个角度重新理解 把 ReaderWriter 接口看成代表了一个 类型的集合，所有实现了 Read() Writer() 这两个方法的类型都在接口代表的类型集合当中 接口的定义就从 方法集(method set) 变为了 类型集(type set)。而Go1.18开始就是依据这一点将接口的定义正式更改为了 类型集(Type set) An interface type defines a *type set* (一个接口类型定义了一个类型集) 12345type Float interface &#123; ~float32 | ~float64&#125;type Slice[T Float] []T 接口类型 Float 代表了一个 类型集合， 所有以 float32 或 float64 为底层类型的类型，都在这一类型集之中 而 type Slice[T Float] []T 中， 类型约束 的真正意思是：指定了类型形参可接受的类型集合，只有属于这个集合中的类型才能替换形参用于实例化 123var s Slice[int] // int 属于类型集 Float ，所以int可以作为类型实参// chan int does not implement int|string|float32|float64var s Slice[chan int] // chan int 类型不在类型集 Float 中，所以错误 接口实现(implement)定义的变化 当满足以下条件时，可以说 类型 T 实现了接口 I ( type T implements interface I)： T 不是接口时：类型 T 是接口 I 代表的类型集中的一个成员 (T is an element of the type set of I) T 是接口时： T 接口代表的类型集是 I 代表的类型集的子集(Type set of T is a subset of the type set of I) 类型的并集 一直使用的 | 符号就是求类型的并集( union ) 123type Uint interface &#123; // 类型集 Uint 是 ~uint 和 ~uint8 等类型的并集 ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125; 类型的交集 接口可以不止书写一行，如果一个接口有多行类型定义，那么取它们之间的 交集 12345678910111213141516171819202122type AllInt interface &#123; ~int | ~int8 | ~int16 | ~int32 | ~int64 | ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint32&#125;type Uint interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125;type A interface &#123; // 接口A代表的类型集是 AllInt 和 Uint 的交集 AllInt Uint&#125;type B interface &#123; // 接口B代表的类型集是 AllInt 和 ~int 的交集 AllInt ~int&#125;type C interface &#123; // 接口C代表的类型集是 int 和 ~int 的交集 int ~int&#125; 接口 A 代表的是 AllInt 与 Uint 的 交集，即 ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 接口 B 代表的则是 AllInt 和 ~int 的交集，即 ~int 很显然，~int 和 int 的交集只有int一种类型，所以接口C代表的类型集中只有int一种类型 空集 当多个类型的交集如下面 Bad 这样为空的时候， Bad 这个接口代表的类型集为一个空集： 1234type Bad interface &#123; int float32 &#125; // 类型 int 和 float32 没有相交的类型，所以接口 Bad 代表的类型集为空 没有任何一种类型属于空集。虽然 Bad 这样的写法是可以编译的，但实际上并没有什么意义 空接口和 any 接下来说一个特殊的类型集——空接口 interface{} 。因为，Go1.18开始接口的定义发生了改变，所以 interface{} 的定义也发生了一些变更： 空接口代表了所有类型的集合 所以，对于Go1.18之后的空接口应该这样理解： 虽然空接口内没有写入任何的类型，但它代表的是所有类型的集合，而非一个 空集 类型约束中指定 空接口 的意思是指定了一个包含所有类型的类型集，并不是类型约束限定了只能使用 空接口 来做类型形参 1234567// 空接口代表所有类型的集合。写入类型约束意味着所有类型都可拿来做类型实参type Slice[T interface&#123;&#125;] []Tvar s1 Slice[int] // 正确var s2 Slice[map[string]string] // 正确var s3 Slice[chan int] // 正确var s4 Slice[interface&#123;&#125;] // 正确 因为空接口是一个包含了所有类型的类型集，所以我们经常会用到它。于是，Go1.18开始提供了一个和空接口 interface{} 等价的新关键词 any ，用来使代码更简单： 1type Slice[T any] []T // 代码等价于 type Slice[T interface&#123;&#125;] []T 实际上 any 的定义就位于Go语言的 builtin.go 文件中（参考如下）， any 实际上就是 interaface{}的别名(alias)，两者完全等价 所以从 Go 1.18 开始，所有可以用到空接口的地方其实都可以直接替换为any，如： 123456var s []any // 等价于 var s []interface&#123;&#125;var m map[string]any // 等价于 var m map[string]interface&#123;&#125;func MyPrint(value any)&#123; fmt.Println(value)&#125; comparable(可比较) 和 可排序(ordered) golang中类型的比较情况 Boolean(布尔值)、Integer(整型)、Floating-point(浮点数)、Complex(复数)、String(字符)这些类型毫无疑问可以比较。 Poniter (指针) 可以比较：如果两个指针指向同一个变量，或者两个指针类型相同且值都为 nil，则它们相等。注意，指向不同的零大小变量的指针可能相等，也可能不相等。 Channel (通道)具有可比性：如果两个通道值是由同一个 make 调用创建的，则它们相等 123456789c1 := make(chan int, 2) c2 := make(chan int, 2) c3 := c1 fmt.Println(c3 == c1) // true fmt.Println(c2 == c1) // false Interface (接口值)具有可比性：如果两个接口值具有相同的动态类型和相等的动态值，则它们相等。 当类型 X 的值具有可比性且 X 实现 T 时，非接口类型 X 的值 x 和接口类型 T 的值 t 具有可比性。如果 t 的动态类型与 X 相同且 t 的动态值等于 x，则它们相等。 如果所有字段都具有可比性，则 struct (结构体值)具有可比性：如果它们对应的非空字段相等，则两个结构体值相等。 如果 array(数组)元素类型的值是可比较的，则数组值是可比较的：如果它们对应的元素相等，则两个数组值相等 slice、map、function 这些是不可以比较的，但是也有特殊情况，那就是当他们值是 nil 时，可以与 nil 进行比较 对于一些数据类型，需要在类型约束中限制只接受能 != 和 == 对比的类型，如map： 123// 错误。因为 map 中键的类型必须是可进行 != 和 == 比较的类型// incomparable map key type KEY (missing comparable constraint)type MyMap[KEY any, VALUE any] map[KEY]VALUE 所以Go直接内置了一个叫 comparable 的接口，它代表了所有可用 != 以及 == 对比的类型： 1type MyMap[KEY comparable, VALUE any] map[KEY]VALUE // 正确 comparable 比较容易引起误解的一点是很多人容易把他与可排序搞混淆。可比较指的是 可以执行 != == 操作的类型，并没确保这个类型可以执行大小比较（ &gt;,&lt;,&lt;=,&gt;= ）。如下 1234567891011121314151617type OhMyStruct struct &#123; a int&#125;var a, b OhMyStructa == b // 正确。结构体可使用 == 进行比较a != b // 正确a &gt; b // 错误。结构体不可比大小func Index[E comparable](s []E, v E) int &#123; for i, vs := range s &#123; if v == vs &#123; return i &#125; &#125; return -1&#125; 而可进行大小比较的类型被称为 Orderd 。目前Go语言并没有像 comparable 这样直接内置对应的关键词，所以想要的话需要自己来定义相关接口，比如我们可以参考Go官方包golang.org/x/exp/constraints 如何定义： 123456789101112131415161718192021222324252627// Ordered 代表所有可比大小排序的类型type Ordered interface &#123; Integer | Float | ~string&#125;type Integer interface &#123; Signed | Unsigned&#125;type Signed interface &#123; ~int | ~int8 | ~int16 | ~int32 | ~int64&#125;type Unsigned interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 | ~uintptr&#125;type Float interface &#123; ~float32 | ~float64&#125;func IsSorted[E constraints.Ordered](x []E) bool &#123; for i := len(x) - 1; i &gt; 0; i-- &#123; if x[i] &lt; x[i-1] &#123; return false &#125; &#125; return true&#125; 这里虽然可以直接使用官方包 golang.org/x/exp/constraints ，但因为这个包属于实验性质的 x 包，今后可能会发生非常大变动，所以并不推荐直接使用 接口两种类型 123456type ReadWriter interface &#123; ~string | ~[]rune Read(p []byte) (n int, err error) Write(p []byte) (n int, err error)&#125; 接口类型 ReadWriter 代表了一个类型集合，所有以 string 或 []rune 为底层类型，并且实现了 Read() Write() 这两个方法的类型都在 ReadWriter 代表的类型集当中，例如： StringReadWriter 存在于接口 ReadWriter 代表的类型集中，而 BytesReadWriter 因为底层类型是 []byte（既不是string也是不[]rune） ，所以它不属于 ReadWriter 代表的类型集 123456789101112131415161718192021// 类型 StringReadWriter 实现了接口 Readwritertype StringReadWriter string func (s StringReadWriter) Read(p []byte) (n int, err error) &#123; // ...&#125;func (s StringReadWriter) Write(p []byte) (n int, err error) &#123; // ...&#125;// 类型BytesReadWriter 没有实现接口 Readwritertype BytesReadWriter []byte func (s BytesReadWriter) Read(p []byte) (n int, err error) &#123; ...&#125;func (s BytesReadWriter) Write(p []byte) (n int, err error) &#123; ...&#125; 定义一个 ReadWriter 类型的接口变量，然后接口变量赋值的时候不光要考虑到方法的实现，还必须考虑到具体底层类型？心智负担也太大了吧。是的，为了解决这个问题也为了保持Go语言的兼容性，Go1.18开始将接口分为了两种类型 基本接口(Basic interface) 一般接口(General interface) 基本接口 接口定义中如果只有方法的话，那么这种接口被称为基本接口(Basic interface)。这种接口就是Go1.18之前的接口，用法也基本和Go1.18之前保持一致。基本接口大致可以用于如下几个地方 最常用的，定义接口变量并赋值 123456type MyError interface &#123; // 接口中只有方法，所以是基本接口 Error() string&#125;// 用法和 Go1.18之前保持一致var err MyError = fmt.Errorf(\"hello world\") 基本接口因为也代表了一个类型集，所以也可用在类型约束中 12// io.Reader 和 io.Writer 都是基本接口，也可以用在类型约束中type MySlice[T io.Reader | io.Writer] []Slice 一般接口(General interface) 如果接口内不光只有方法，还有类型的话，这种接口被称为 一般接口(General interface) 12345678910type Uint interface &#123; // 接口 Uint 中有类型，所以是一般接口 ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125;type ReadWriter interface &#123; // ReadWriter 接口既有方法也有类型，所以是一般接口 ~string | ~[]rune Read(p []byte) (n int, err error) Write(p []byte) (n int, err error)&#125; 一般接口类型不能用来定义变量，只能用于泛型的类型约束中。所以以下的用法是错误的： 12345type Uint interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125;var uintInf Uint // 错误。Uint是一般接口，只能用于类型约束，不得用于变量定义 这一限制保证了一般接口的使用被限定在了泛型之中，不会影响到Go1.18之前的代码，同时也极大减少了书写代码时的心智负担 泛型接口 所有类型的定义中都可以使用类型形参，所以接口定义自然也可以使用类型形参，观察下面这两个例子 1234567891011type DataProcessor[T any] interface &#123; Process(oriData T) (newData T) Save(data T) error&#125;type DataProcessor2[T any] interface &#123; int | ~struct&#123; Data interface&#123;&#125; &#125; Process(data T) (newData T) Save(data T) error&#125; 因为引入了类型形参，所以这两个接口是泛型类型。而泛型类型要使用的话必须传入类型实参实例化才有意义。所以尝试实例化一下这两个接口。因为 T 的类型约束是 any，所以可以随便挑一个类型来当实参(比如string) 1234567DataProcessor[string]// 实例化之后的接口定义相当于如下所示：type DataProcessor[string] interface &#123; Process(oriData string) (newData string) Save(data string) error&#125; 经过实例化之后就好理解了， DataProcessor[string] 因为只有方法，所以它实际上就是个 基本接口(Basic interface)，这个接口包含两个能处理string类型的方法。像下面这样实现了这两个能处理string类型的方法就算实现了这个接口 12345678910111213141516171819type CSVProcessor struct &#123;&#125;// 注意，方法中 oriData 等的类型是 stringfunc (c CSVProcessor) Process(oriData string) (newData string) &#123; ....&#125;func (c CSVProcessor) Save(oriData string) error &#123; ...&#125;// CSVProcessor实现了接口 DataProcessor[string] ，所以可赋值var processor DataProcessor[string] = CSVProcessor&#123;&#125; processor.Process(\"name,age\\nbob,12\\njack,30\")processor.Save(\"name,age\\nbob,13\\njack,31\")// 错误。CSVProcessor没有实现接口 DataProcessor[int]var processor2 DataProcessor[int] = CSVProcessor&#123;&#125; 再用同样的方法实例化 DataProcessor2[T] 123456789DataProcessor2[string]// 实例化后的接口定义可视为type DataProcessor2[T string] interface &#123; int | ~struct&#123; Data interface&#123;&#125; &#125; Process(data string) (newData string) Save(data string) error&#125; DataProcessor2[string] 因为带有类型并集所以它是 一般接口(General interface)，所以实例化之后的这个接口代表的意思是： 只有实现了 Process(string) string 和 Save(string) error 这两个方法，并且以 int 或 struct{ Data interface{} } 为底层类型的类型才算实现了这个接口 一般接口(General interface) 不能用于变量定义只能用于类型约束，所以接口 DataProcessor2[string] 只是定义了一个用于类型约束的类型集 12345678910111213141516171819202122232425262728293031323334353637383940414243// XMLProcessor 虽然实现了接口 DataProcessor2[string] 的两个方法，但是因为它的底层类型是 []byte，所以依旧是未实现 DataProcessor2[string]type XMLProcessor []bytefunc (c XMLProcessor) Process(oriData string) (newData string) &#123;&#125;func (c XMLProcessor) Save(oriData string) error &#123;&#125;// JsonProcessor 实现了接口 DataProcessor2[string] 的两个方法，同时底层类型是 struct&#123; Data interface&#123;&#125; &#125;。所以实现了接口 DataProcessor2[string]type JsonProcessor struct &#123; Data interface&#123;&#125;&#125;func (c JsonProcessor) Process(oriData string) (newData string) &#123;&#125;func (c JsonProcessor) Save(oriData string) error &#123;&#125;// 错误。DataProcessor2[string]是一般接口不能用于创建变量var processor DataProcessor2[string]// 正确，实例化之后的 DataProcessor2[string] 可用于泛型的类型约束type ProcessorList[T DataProcessor2[string]] []T// 正确，接口可以并入其他接口type StringProcessor interface &#123; DataProcessor2[string] PrintString()&#125;// 错误，带方法的一般接口不能作为类型并集的成员type StringProcessor interface &#123; DataProcessor2[string] | DataProcessor2[[]byte] PrintString()&#125; 接口定义的种种限制规则 Go1.18从开始，在定义类型集(接口)的时候增加了非常多十分琐碎的限制规则，其中很多规则都在之前的内容中介绍过了，但剩下还有一些规则因为找不到好的地方介绍，所以在这里统一介绍下 用 | 连接多个类型的时候，类型之间不能有相交的部分(即必须是不交集): 123456type MyInt int// 错误，MyInt的底层类型是int,和 ~int 有相交的部分type _ interface &#123; ~int | MyInt&#125; 但是相交的类型中是接口的话，则不受这一限制： 12345678910111213type MyInt inttype _ interface &#123; ~int | interface&#123; MyInt &#125; // 正确&#125;type _ interface &#123; interface&#123; ~int &#125; | MyInt // 也正确&#125;type _ interface &#123; interface&#123; ~int &#125; | interface&#123; MyInt &#125; // 也正确&#125; 类型的并集中不能有类型形参 1234567type MyInf[T ~int | ~string] interface &#123; ~float32 | T // 错误。T是类型形参&#125;type MyInf2[T ~int | ~string] interface &#123; T // 错误&#125; 接口不能直接或间接地并入自己 1234567891011121314type Bad interface &#123; Bad // 错误，接口不能直接并入自己&#125;type Bad2 interface &#123; Bad1&#125;type Bad1 interface &#123; Bad2 // 错误，接口Bad1通过Bad2间接并入了自己&#125;type Bad3 interface &#123; ~int | ~string | Bad3 // 错误，通过类型的并集并入了自己&#125; 接口的并集成员个数大于一的时候不能直接或间接并入 comparable 接口 1234567891011121314151617type OK interface &#123; comparable // 正确。只有一个类型的时候可以使用 comparable&#125;type Bad1 interface &#123; []int | comparable // 错误，类型并集不能直接并入 comparable 接口&#125;type CmpInf interface &#123; comparable&#125;type Bad2 interface &#123; chan int | CmpInf // 错误，类型并集通过 CmpInf 间接并入了comparable&#125;type Bad3 interface &#123; chan int | interface&#123;comparable&#125; // 理所当然，这样也是不行的&#125; 带方法的接口(无论是基本接口还是一般接口)，都不能写入接口的并集中： 12345678910111213141516171819type _ interface &#123; ~int | ~string | error // 错误，error是带方法的接口(一般接口) 不能写入并集中&#125;type DataProcessor[T any] interface &#123; ~string | ~[]byte Process(data T) (newData T) Save(data T) error&#125;// 错误，实例化之后的 DataProcessor[string] 是带方法的一般接口，不能写入类型并集type _ interface &#123; ~int | ~string | DataProcessor[string] &#125;type Bad[T any] interface &#123; ~int | ~string | DataProcessor[T] // 也不行&#125; 泛型并不取代Go1.18之前用接口+反射实现的动态类型，在下面情景的时候非常适合使用泛型：当你需要针对不同类型书写同样的逻辑，使用泛型来简化代码是最好的 (比如你想写个队列，写个链表、栈、堆之类的数据结构） 参考链接 https://talkgo.org/t/topic/3582 https://go.dev/doc/tutorial/generics https://segmentfault.com/a/1190000041634906 https://go.dev/blog/go1.18 https://go.googlesource.com/proposal/+/refs/heads/master/design/43651-type-parameters.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门17-init","slug":"Go/Go入门17-init","date":"2022-09-12T10:22:10.000Z","updated":"2022-09-12T10:22:10.000Z","comments":true,"path":"2022/09/12/Go/Go入门17-init/","link":"","permalink":"http://xboom.github.io/2022/09/12/Go/Go%E5%85%A5%E9%97%A817-init/","excerpt":"","text":"见下面的代码输出 1234567891011121314151617var WhatIsThe = AnswerToLife()func AnswerToLife() int &#123; // 1 return 42&#125;func init() &#123; // 2 WhatIsThe = 0&#125;func main() &#123; // 3 if WhatIsThe == 0 &#123; fmt.Println(\"It's all a lie.\") &#125;&#125;//It's all a lie. Golang 中 init 的执行顺序 如果一个包导入了其他包，则首先初始化导入的包。 然后初始化当前包的常量。 接下来初始化当前包的变量。 最后，调用当前包的 init() 函数 参考链接 https://stackoverflow.com/questions/24790175/when-is-the-init-function-run https://learnku.com/go/t/47135","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门16-工作区","slug":"Go/Go入门16-工作区","date":"2022-09-12T09:45:46.000Z","updated":"2022-09-12T09:45:46.000Z","comments":true,"path":"2022/09/12/Go/Go入门16-工作区/","link":"","permalink":"http://xboom.github.io/2022/09/12/Go/Go%E5%85%A5%E9%97%A816-%E5%B7%A5%E4%BD%9C%E5%8C%BA/","excerpt":"","text":"问题背景 Golang 在1.18 推出了新特性 workspace 工作区模式 为什么要有 workspace 什么是 workspace 怎么使用 workspace 在工作过程中发现，模块的go.mod中添加了很多 replace，其实是为了解决依赖的问题。例如有2个module处于开发阶段，一个是example.com/web，一个是example.com/util。example.com/web依赖example.com/util的函数，当两个模块同时开发过程中，为了 web 模块能够时候用到最新的 util 模块代码，有两种方案 方案1：就是将模块及时提交代码到代码仓库 如果 util 有修改，那么就需要将代码提交到代码仓库，然后打上 tag 然后使用 go get -u 更新 main 中依赖的 util 的版本号(tag) 缺点：每次需要提交代码并更新到最新 方案2：使用go.mod中使用replace指令，见代码 目录结构: 1234567module|-- web| |-- go.mod |-- main.go|-- util |-- go.mod |-- util.go 文件web/web.go : 123456789101112package mainimport ( \"fmt\" \"example.com/util\")func main() &#123; result := util.Add(1, 2) fmt.Println(result)&#125; 文件web/go.mod: 1234567module example.com/webgo 1.18require \"example.com/util\" v0.0.0replace \"example.com/util\" =&gt; ../util 文件util/util.go: 12345package utilfunc Add(a int, b int) int &#123; return a + b&#125; 文件util/go.mod 123module example.com/utilgo 1.18 通过replace指令，使用go命令编译代码的时候，会找到本地的util目录，这样example.com/web就可以使用到本地最新的example.com/util代码 缺点：提交example.com/web这个module的代码到代码仓库时，需要删除最后的replace指令，否则其他开发者下载后会编译报错，因为他们本地可能没有util目录，或者util目录的路径和你的不一样 工作区 为了解决方案2的痛点，Go1.18 新增了工作区模式(workspace mode) 去掉 web/go.mod 中的 replace命令 12345module example.com/webgo 1.18require \"example.com/util\" v0.0.0 在 web 模块中执行 go run hello.go，提示找不到依赖模块 12hello.go:6:2: missing go.sum entry for module providing package example.com/util; to add: go mod download example.com/util 使用 go work init web util指定工作模块 web 与 util，会生成 go.work 文件 123456go 1.18use ( ./util ./web) 其中 use 用于表示需要指定的模块，再次执行 go run hello.go，则运行成功 目录结构 12345678module|-- go.work|-- web| |-- go.mod |-- main.go|-- util |-- go.mod |-- util.go 命令 1234567891011root@13ce5bc74ac3:/code/module-workspace# go help workUsage: go work &lt;command&gt; [arguments]The commands are: edit edit go.work from tools or scripts init initialize workspace file sync sync workspace build list to modules use add modules to workspace fileUse \"go help work &lt;command&gt;\" for more information about a command. 通常情况下，建议不要提交 go.work 文件到 git上，因为它主要用于本地代码开发 推荐在 $GOPATH 路径下执行，生成 go.work 文件 go work init 初始化工作区文件，用于生成go.work工作区文件 go work use 添加新的模块到工作区间 123go work use ./example 添加一个模块到工作区go work use ./example ./example1 添加多个模块到工作区go work use -r ./example 递归 ./example 目录到当前工作区 go work edit 用于编辑 go.work文件 123456//可以使用 edit 命令编辑和手动编辑 go.work 文件效果是相同的 示例:go work edit -fmt go.work 重新格式化 go.work 文件go work edit -replace=github.com/link1st/example=./example go.work 替换代码模块go work edit -dropreplace=github.com/link1st/example 删除替换代码模块go work edit -use=./example go.work 添加新的模块到工作区go work edit -dropuse=./example go.work 从工作区中删除模块 go work sync 将工作区的构建列表同步到工作区的模块 go env GOWORK 查看环境变量，查看当前工作区文件路径 可以排查工作区文件是否设置正确，go.work 路径找不到可以使用 GOWORK 指定，生成多个go.work则指向最后一次生成的路径 replace 指令的语法与 go.mod 中的 replace 指令相同，并优先于 go.mod 文件中的替换。这是主要用于覆盖不同工作区中的冲突替换模块 参考链接 https://go.dev/blog/go1.18 https://go.dev/doc/tutorial/workspaces https://tonybai.com/2021/11/12/go-workspace-mode-in-go-1-18/ https://go.dev/ref/mod#workspaces","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门15-Sync","slug":"Go/Go入门15-Sync","date":"2022-09-10T15:06:20.000Z","updated":"2022-09-10T15:06:20.000Z","comments":true,"path":"2022/09/10/Go/Go入门15-Sync/","link":"","permalink":"http://xboom.github.io/2022/09/10/Go/Go%E5%85%A5%E9%97%A815-Sync/","excerpt":"","text":"Golang 源码 sync 包提供了同步操作 互斥锁 sync.Mutext 读写锁 sync.RWMutex 等待组 sync.WaitGroup 单次操作 sync.Once 内存池 sync.Pool 安全map sync.Map 同步条件 sync.Cond Mutex Mutex是一把公平锁（Mutex fairness） Mutex 有两种模式：正常模式 和 饥饿模式 正常模式：在正常模式，请求锁的 goroutines 是按照先进先出的顺序进行排队行程 waiters 的。 调用 Unlock() 方法释放锁资源时，如果发现有等待唤起的 waiter 时，则会将队头的 waiter 唤起。被唤起后调用CAS方法去尝试 修改锁的状态，如果修改成功则表示占有锁的资源成功。 锁一共有三个状态：锁住状态、唤起状态、饥饿状态 饥饿模式：当调用 Unlock()方法释放锁资源的时候，唤起的 waiter G1 需要通过 CAS 操作获取锁资源，如果此时有新请求锁资源的 goroutine G2，那么它们会一起通过 CAS 方法竞争获取锁资源。当不断有G2来进行锁资源争夺，就有可能导致 G1 一直无法获取到 锁资源而饿死，所以Go采用饥饿模式 当 G1 在超过一定时间获取不到资源之后，会在 Unlock 释放锁资源时，直接将锁的资源交给 G1，并且将当前状态改为 饥饿模式 当 G1 获取到锁的所有权时，发现自己是队列中最后一个waiter或者自己等待时间小于1ms，那么锁将切换回正常模式 正常模式拥有非常好的性能表现，因为即使存在阻塞的 waiter，一个goroutine也能够多次获取锁。 饥饿模式对于预防极端的长尾时延(tail latency) 实现原理 代码路径：/go/src/sync/mutex.go 1234567891011type Locker interface &#123; Lock() Unlock()&#125;// Mutex 是互斥锁。// 互斥锁的零值是未锁定的互斥锁。type Mutex struct &#123; state int32 //锁的状态 sema uint32 //信号&#125; Mutex 结构体有两个字段： state: 表示当前互斥锁的状态 sema: 是个信号量变量，用来控制等待 goroutine 的阻塞休眠和唤醒 注意，首次使用后不要进行Mutex的值拷贝，否则 Mutex 会失效 state 状态字段代表多个意思，mutexWaiterShift = 3低三位记录三种状态，剩下的位置，用来表示可以有1&lt;&lt;(32-3)个 Goroutine 等待互斥锁的释放 1234567const ( mutexLocked = 1 &lt;&lt; iota // 表示当前对象锁的状态 0-未锁住，1-已锁住 mutexWoken //表示当前对象是否被唤醒 0-唤醒，1-未唤醒 mutexStarving //表示当前对象是否为饥饿模式 0-正常模式，1为饥饿模式 mutexWaiterShift = iota //从倒数第四位往前的bit位表示在排队等待的goroutine starvationThresholdNs = 1e6 //1ms) 自旋 互斥锁中提到了比较重要的自旋操作 runtime_canSpin：比较保守的自旋，golang中自旋锁并不会一直自旋下去，在runtime包中runtime_canSpin方法做了一些限制 1234567891011121314//判断能否自旋func sync_runtime_canSpin(i int) bool &#123; // 1. i 大于 active_spin = 4 的时候不能自旋 // 2. 机器是单核 的时候不能自旋 // 3. gomaxprocs 小于 (闲置的p + 自旋的m + 1) 的时候不能自旋 // 4. 本地队列不为空 的时候不能自旋 if i &gt;= active_spin || ncpu &lt;= 1 || gomaxprocs &lt;= int32(sched.npidle+sched.nmspinning)+1 &#123; return false &#125; if p := getg().m.p.ptr(); !runqempty(p) &#123; //本地可运行G队列不为空 return false &#125; return true&#125; **runtime_doSpin：**会调用procyield函数，该函数也是汇编语言实现。函数内部循环调用PAUSE指令。PAUSE指令什么都不做，但是会消耗CPU时间，在执行PAUSE指令时，CPU不会对它做不必要的优化 1234567891011func sync_runtime_doSpin() &#123; procyield(active_spin_cnt) //自旋操作&#125;TEXT runtime·procyield(SB),NOSPLIT,$0-0 MOVL cycles+0(FP), AXagain: PAUSE //空命令 SUBL $1, AX JNZ again RET Lock 123456789func (m *Mutex) Lock() &#123; // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) &#123; //...竞争检测 return &#125; // Slow path (outlined so that the fast path can be inlined) m.lockSlow()&#125; 加锁首先会使用 CompareAndSwapInt32 看能不能拿到锁，否则进入到 lockSlow 流程 12//如果*addr中的值 与 old 相等，则将 *addr与 new进行交换，并返回true,否则返回falsefunc CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) lockSlow 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121func (m *Mutex) lockSlow() &#123; var waitStartTime int64 //当前groutine的等待时间 starving := false //当前goroutine是否是饥饿标记 awoke := false //当前goroutine是否是唤醒标记 iter := 0 //当前goroutine自旋次数自旋次数 old := m.state //copy锁的状态为历史状态 for &#123; // 在饥饿模式不进行自旋，锁的所有权会直接移交给waiters。 // 当锁是locked状态并且当前goroutine可以自旋时，开始自旋。 // 当锁是starving状态，就直接false，不自旋 if old&amp;(mutexLocked|mutexStarving) == mutexLocked &amp;&amp; runtime_canSpin(iter) &#123; // 触发自旋是有意义的。 // 尝试设置woken标志来通知unlock，以便不唤起其他阻塞的goroutines if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 &amp;&amp; atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) &#123; // 如果当前goroutine是未唤醒状态，互斥锁也是未唤醒状态，并且互斥锁的waiter数量不等于0， // 就比较锁的最新状态（m.state）和历史状态（old），如果未发生改变，将锁的状态更新为woken。 // 并且设置当前goroutine为awoke状态。 awoke = true &#125; //自旋 runtime_doSpin() //自旋次数增加 iter++ // copy锁的状态为历史状态，自旋期间其他goroutine可能修改了state，所以要更新 old = m.state // 继续尝试自旋 continue &#125; //此时，到这里有两个可能： //1. 锁的状态发生了变化(唤起状态、饥饿状态) //2. 无法继续进行自旋 new := old // copy锁的历史状态为new状态(为什么不是直接获取m.state) // 如果锁的历史状态（old）不是starving状态，将锁的新状态（new）更新为locked状态 if old&amp;mutexStarving == 0 &#123; new |= mutexLocked &#125; // 如果锁的历史状态（old）是locked状态或者是starving状态，将锁的waiter数量加1 if old&amp;(mutexLocked|mutexStarving) != 0 &#123; new += 1 &lt;&lt; mutexWaiterShift //注意 waiter数量怎样记录数量的 &#125; // 如果当前goroutine是starving状态且锁的历史状态（old）是locked状态，将锁的新状态(new）更新为starving状态 if starving &amp;&amp; old&amp;mutexLocked != 0 &#123; new |= mutexStarving &#125; // 如果当前goroutine是awoke状态 if awoke &#123; // 锁的状态是非唤醒状态则直接报错 if new&amp;mutexWoken == 0 &#123; throw(\"sync: inconsistent mutex state\") &#125; // &amp;^ 是 bit clear (AND NOT) // 取消锁的新状态（new）的woken状态标志。 new &amp;^= mutexWoken &#125; // 比较锁的最新状态（m.state）和历史状态（old），如果未发生改变，那么更新为new。 if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; //如果cas更新成功，并且锁的历史状态（old）即不是locked也不是starving，那么结束循环，通过CAS加锁成功。 if old&amp;(mutexLocked|mutexStarving) == 0 &#123; break // locked the mutex with CAS &#125; // 如果之前已经等待，将排在队列前面。 // 当前goroutine是否等待过。 queueLifo := waitStartTime != 0 // 如果开始等待时间为0，更新为当前时间为开始等待时间。 if waitStartTime == 0 &#123; waitStartTime = runtime_nanotime() &#125; // 通过信号量获取锁 // runtime实现代码：https://github.com/golang/go/blob/go1.15.5/src/runtime/sema.go#L69-L72 // runtime信号量获取：https://github.com/golang/go/blob/go1.15.5/src/runtime/sema.go#L98-L153 runtime_SemacquireMutex(&amp;m.sema, queueLifo, 1) // 如果当前goroutine是starving状态或者等待时间大于1ms，更新当前goroutine为starving状态。 starving = starving || runtime_nanotime()-waitStartTime &gt; starvationThresholdNs // 更新锁的历史状态（old） old = m.state //锁是饥饿状态 if old&amp;mutexStarving != 0 &#123; // 如果当前goroutine是唤醒状态并且锁在饥饿模式， // 锁的所有权转移给当前goroutine，但是锁处于不一致的状态中：mutexLocked没有设置 // 并且我们将任然被认为是waiter。这个状态需要被修复。 // 如果锁的历史状态（old）是locked或者woken的，或者waiters的数量不为0，触发锁状态异常。 if old&amp;(mutexLocked|mutexWoken) != 0 || old&gt;&gt;mutexWaiterShift == 0 &#123; throw(\"sync: inconsistent mutex state\") &#125; // 当前goroutine获取锁，waiter数量-1 delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift) // 如果当前goroutine不是starving状态或者锁的历史状态（old）的waiter数量是1，delta减去3。 if !starving || old&gt;&gt;mutexWaiterShift == 1 &#123; // 退出饥饿模式 // 在这里这么做至关重要，还要考虑等待时间。 // 饥饿模式是非常低效率的，一旦两个goroutine将互斥锁切换为饥饿模式，它们便可以无限锁。 delta -= mutexStarving &#125; // 更新锁的状态 atomic.AddInt32(&amp;m.state, delta) break //退出自旋 &#125; // 当前goroutine更新为awoke状态 awoke = true // 当前goroutine自旋次数清零 iter = 0 &#125; else &#123; // 更新锁的历史状态（old） //当在处理 new 状态的时候 锁的状态发生了变化，那么重新复制 old 再进行逻辑判断 old = m.state &#125; &#125;//for end //...竞争判断&#125; Unlock 解锁的前提是加锁 123456789func (m *Mutex) Unlock() &#123; //竞争.. // 如果waiter数量为0，三个标志位去除locked后也为0，那么可以直接解锁了。 new := atomic.AddInt32(&amp;m.state, -mutexLocked) if new != 0 &#123; m.unlockSlow(new) &#125;&#125; unlockSlow 123456789101112131415161718192021222324252627282930func (m *Mutex) unlockSlow(new int32) &#123; if (new+mutexLocked)&amp;mutexLocked == 0 &#123; // 当new不是锁住状态 throw(\"sync: unlock of unlocked mutex\") &#125; //如果不是饥饿模式 if new&amp;mutexStarving == 0 &#123; old := new for &#123; // 如果waiter数量为0，锁的三个标志位任一非0，直接返回 if old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken|mutexStarving) != 0 &#123; return &#125; // 尝试将锁更新为woken状态，如果成功了，就通过信号量去唤醒goroutine new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; runtime_Semrelease(&amp;m.sema, false, 1) return &#125; old = m.state &#125; &#125; else &#123; // Starving mode: handoff mutex ownership to the next waiter, and yield // our time slice so that the next waiter can start to run immediately. // Note: mutexLocked is not set, the waiter will set it after wakeup. // But mutex is still considered locked if mutexStarving is set, // so new coming goroutines won't acquire it. // 饥饿模式直接手把手交接锁的控制权 runtime_Semrelease(&amp;m.sema, true, 1) &#125;&#125; 总结 互斥锁只有在普通模式才能进入自旋 能够自旋的条件：a. 需要运行在多 CPU 的机器上；b. 当前的Goroutine 为了获取该锁进入自旋的次数小于四次；c. 当前机器上至少存在一个正在运行的处理器 P 并且处理的运行队列为空 一旦当前 Goroutine 能够进入自旋就会调用runtime.sync_runtime_doSpin 和 runtime.procyield 并执行 30 次的 PAUSE 指令，该指令只会占用 CPU 并消耗 CPU 时间 RWMutex 读写互斥锁 sync.RWMutex 是细粒度的互斥锁，它不限制资源的并发读，但是读写、写写操作无法并行执行 实现原理 代码路径：/go/src/sync/rwmutex.go 123456789type RWMutex struct &#123; w Mutex // 复制原子锁的能力 writerSem uint32 // 写等待读完成信号量 readerSem uint32 // 读等待写完成信号量 readerCount int32 // 当前正在执行的读操作的数量 readerWait int32 // 当写操作被阻塞时等待的读操作个数&#125;const rwmutexMaxReaders = 1 &lt;&lt; 30 同理，首次使用后不要进行值拷贝，否则会失效 写锁 12345678910111213func (rw *RWMutex) Lock() &#123; //竞争检查 // 首先调用锁的能力，阻塞后续的写 rw.w.Lock() // Announce to readers there is a pending writer. r := atomic.AddInt32(&amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // Wait for active readers. if r != 0 &amp;&amp; atomic.AddInt32(&amp;rw.readerWait, r) != 0 &#123; runtime_SemacquireMutex(&amp;rw.writerSem, false, 0) &#125; //竞争检查&#125; 获取写锁时会先阻塞写锁的获取，后阻塞读锁的获取，这种策略能够保证读操作不会被连续的写操作饿死 1234567891011121314151617func (rw *RWMutex) Unlock() &#123; //竞争检查... // Announce to readers there is no active writer. r := atomic.AddInt32(&amp;rw.readerCount, rwmutexMaxReaders) if r &gt;= rwmutexMaxReaders &#123; race.Enable() throw(\"sync: Unlock of unlocked RWMutex\") &#125; // Unblock blocked readers, if any. for i := 0; i &lt; int(r); i++ &#123; runtime_Semrelease(&amp;rw.readerSem, false, 0) &#125; // Allow other writers to proceed. rw.w.Unlock() //竞争检查&#125; 读锁 12345678func (rw *RWMutex) RLock() &#123; //竞争检查... if atomic.AddInt32(&amp;rw.readerCount, 1) &lt; 0 &#123; // A writer is pending, wait for it. runtime_SemacquireMutex(&amp;rw.readerSem, false, 0) &#125; //竞争检查...&#125; 1234567891011121314151617181920func (rw *RWMutex) RUnlock() &#123; //竞争检查... if r := atomic.AddInt32(&amp;rw.readerCount, -1); r &lt; 0 &#123; // Outlined slow-path to allow the fast-path to be inlined rw.rUnlockSlow(r) &#125; //竞争检查...&#125;func (rw *RWMutex) rUnlockSlow(r int32) &#123; if r+1 == 0 || r+1 == -rwmutexMaxReaders &#123; race.Enable() throw(\"sync: RUnlock of unlocked RWMutex\") &#125; // A writer is pending. if atomic.AddInt32(&amp;rw.readerWait, -1) == 0 &#123; // The last reader unblocks the writer. runtime_Semrelease(&amp;rw.writerSem, false, 1) &#125;&#125; WaitGroup WaitGroup 等待一组 goroutine 完成，主 goroutine 调用 Add 设置数量要等待的 goroutine。然后每个 goroutine 完成后运行并调用 Done。同时，等待可用于阻塞，直到所有 goroutine 完成。 代码路径：go/src/sync/waitgroup.go 实现原理 123456789type WaitGroup struct &#123; noCopy noCopy // 64 位值：高 32 位是计数器，低 32 位是等待者的数量。 // 64位原子操作需要64位对齐，但是32位编译器只保证 64 位字段是 32 位对齐的。 // 出于这个原因，在 32 位架构上，需要检查 state()，判断 state1 是否对齐并自动“交换”字段顺序 state1 uint64 state2 uint32&#125; 同理，首次使用后不要进行值拷贝，否则会失效 state 1234567891011func (wg *WaitGroup) state() (statep *uint64, semap *uint32) &#123; if unsafe.Alignof(wg.state1) == 8 || uintptr(unsafe.Pointer(&amp;wg.state1))%8 == 0 &#123; // state1 is 64-bit aligned: nothing to do. return &amp;wg.state1, &amp;wg.state2 &#125; else &#123; // state1 is 32-bit aligned but not 64-bit aligned: this means that // (&amp;state1)+4 is 64-bit aligned. state := (*[3]uint32)(unsafe.Pointer(&amp;wg.state1)) return (*uint64)(unsafe.Pointer(&amp;state[1])), &amp;state[0] &#125;&#125; Add/Done Add(n) 表示添加一个等待 123456789101112131415161718192021222324252627282930func (wg *WaitGroup) Add(delta int) &#123; statep, semap := wg.state() //竞争检查... state := atomic.AddUint64(statep, uint64(delta)&lt;&lt;32) v := int32(state &gt;&gt; 32) w := uint32(state) //竞争检查... if v &lt; 0 &#123; panic(\"sync: negative WaitGroup counter\") &#125; if w != 0 &amp;&amp; delta &gt; 0 &amp;&amp; v == int32(delta) &#123; panic(\"sync: WaitGroup misuse: Add called concurrently with Wait\") &#125; if v &gt; 0 || w == 0 &#123; return &#125; // This goroutine has set counter to 0 when waiters &gt; 0. // Now there can't be concurrent mutations of state: // - Adds must not happen concurrently with Wait, // - Wait does not increment waiters if it sees counter == 0. // Still do a cheap sanity check to detect WaitGroup misuse. if *statep != state &#123; panic(\"sync: WaitGroup misuse: Add called concurrently with Wait\") &#125; // Reset waiters count to 0. *statep = 0 for ; w != 0; w-- &#123; runtime_Semrelease(semap, false, 0) &#125;&#125; Done() 操作其实是 减1 123func (wg *WaitGroup) Done() &#123; wg.Add(-1)&#125; Wait 1234567891011121314151617181920212223242526func (wg *WaitGroup) Wait() &#123; statep, semap := wg.state() //竞争检查... for &#123; state := atomic.LoadUint64(statep) v := int32(state &gt;&gt; 32) w := uint32(state) if v == 0 &#123; // Counter is 0, no need to wait. //竞争检查... return &#125; // Increment waiters count. if atomic.CompareAndSwapUint64(statep, state, state+1) &#123; //竞争检查... runtime_Semacquire(semap) if *statep != 0 &#123; panic(\"sync: WaitGroup is reused before previous Wait has returned\") &#125; //竞争检查... return &#125; &#125;&#125; Once Once 是一个仅执行一次操作的对象 使用例子 12345678var d sync.Oncefor i := 0; i &lt; 5; i++ &#123; d.Do(func() &#123; fmt.Println(\"hello\") &#125;)&#125;//hello 循环执行打印输出操作，不同的是使用 sync.Once 对象执行打印方法，仅执行一次。并发执行效果一致 实现原理 代码路径：/go/src/sync/once.go 1234type Once struct &#123; done uint32 //done 表示动作是否已经执行 m Mutex&#125; 执行函数逻辑 123456789101112131415func (o *Once) Do(f func()) &#123; if atomic.LoadUint32(&amp;o.done) == 0 &#123; //如果不等于0直接退出 // Outlined slow-path to allow inlining of the fast-path. o.doSlow(f) &#125;&#125;func (o *Once) doSlow(f func()) &#123; o.m.Lock() defer o.m.Unlock() //通过获取锁来进行done状态修改 if o.done == 0 &#123; //这里直接使用o.done defer atomic.StoreUint32(&amp;o.done, 1) f() &#125;&#125; 直接使用 o.done == 0 判断是否执行过的原因是上面获取到了写锁 将 atomic.StoreUint32(&amp;o.done, 1)放置在 f() 的后面从而保证当 done 变化，f() 已经执行完毕 在注释中，它提到了另外一种错误的实现方式 12345func (o *Once) Do(f func()) &#123; if atomic.CompareAndSwapUint32(&amp;o.done, 0, 1) &#123; f() &#125;&#125; Do() 保证当它返回时，f 已经完成。此实现不会实现该保证：给定两个同时调用，cas 的获胜者将调用 f，第二个会立即返回，没有等待第一个对 f 的调用完成。 Cond sync.Cond 条件变量用来协调想要访问共享资源的那些 goroutine，当共享资源的状态发生变化的时候，它可以用来通知被互斥锁阻塞的 goroutine 它和互斥锁的区别是：互斥锁 sync.Mutex 通常用来保护临界区和共享资源，条件变量 sync.Cond 用来协调想要访问共享资源的 goroutine 注意点：每个Cond都会关联一个 Lock(*sync.Mutex or *sync.RWMutex)，当修改条件或者调用Wait方法时，必须加锁保护condition 使用案例 123456789101112131415161718192021222324252627282930313233var done = falsefunc read(name string, c *sync.Cond) &#123; c.L.Lock() for !done &#123; log.Println(name, \"starts wait\") c.Wait() log.Println(name, \"end wait\") &#125; c.L.Unlock() log.Println(name, \"end reading\")&#125;func write(name string, c *sync.Cond) &#123; log.Println(name, \"starts writing\") time.Sleep(time.Second) c.L.Lock() done = true c.L.Unlock() log.Println(name, \"wakes all\") c.Broadcast()&#125;func main() &#123; cond := sync.NewCond(&amp;sync.Mutex&#123;&#125;) go read(\"reader1\", cond) go read(\"reader2\", cond) go read(\"reader3\", cond) write(\"writer\", cond) time.Sleep(time.Second * 3)&#125; done 即互斥锁需要保护的条件变量。 read() 调用 Wait() 等待通知，直到 done 为 true。 write() 接收数据，接收完成后，将 done 置为 true，调用 Broadcast() 通知所有等待的协程。 write() 中的暂停了 1s，一方面是模拟耗时，另一方面是确保前面的 3 个 read 协程都执行到 Wait()，处于等待状态。main 函数最后暂停了 3s，确保所有操作执行完毕 运行结果如下； 12345678910112022/09/10 22:40:00 writer starts writing2022/09/10 22:40:00 reader1 starts wait2022/09/10 22:40:00 reader2 starts wait2022/09/10 22:40:00 reader3 starts wait2022/09/10 22:40:01 writer wakes all2022/09/10 22:40:01 reader3 end wait2022/09/10 22:40:01 reader3 end reading2022/09/10 22:40:01 reader1 end wait2022/09/10 22:40:01 reader1 end reading2022/09/10 22:40:01 reader2 end wait2022/09/10 22:40:01 reader2 end reading 注意点： 每次都是 end reading 下一个才会开始执行，原因其实是 c.L.Unlock()，只有释放锁然后其他Wait()中才能获取写锁继续执行 因为 done = false 所以所有的 reader 都进入 wait，但是在构建条件 sync.Cond的时候传入的是锁，那么它又是怎么跟 done 以及 Broadcast 联系在一起的呢？其实没有关系，done 只是用于判断是否重复进入 Wait() 当去掉 done 的变换 123456789func write(name string, c *sync.Cond) &#123; log.Println(name, \"starts writing\") time.Sleep(time.Second) // c.L.Lock() // done = true // c.L.Unlock() log.Println(name, \"wakes all\") c.Broadcast()&#125; 结果变成了 12345678910112022/09/10 22:46:14 writer starts writing2022/09/10 22:46:14 reader3 starts wait2022/09/10 22:46:14 reader1 starts wait2022/09/10 22:46:14 reader2 starts wait2022/09/10 22:46:15 writer wakes all2022/09/10 22:46:15 reader2 end wait2022/09/10 22:46:15 reader2 starts wait2022/09/10 22:46:15 reader3 end wait2022/09/10 22:46:15 reader3 starts wait2022/09/10 22:46:15 reader1 end wait2022/09/10 22:46:15 reader1 starts wait 可以看出来，当 Broadcast的时候 wait 才会返回，由于 done 没有变化，所以reader重新进入了Wait状态 实现原理 1234567891011121314151617181920212223242526272829303132type Cond struct &#123; noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker&#125;type notifyList struct &#123; wait uint32 notify uint32 lock uintptr // key field of the mutex head unsafe.Pointer tail unsafe.Pointer&#125;type copyChecker uintptr//判断是否复制过func (c *copyChecker) check() &#123; if uintptr(*c) != uintptr(unsafe.Pointer(c)) &amp;&amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) &amp;&amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) &#123; panic(\"sync.Cond is copied\") &#125;&#125;func NewCond(l Locker) *Cond &#123; return &amp;Cond&#123;L: l&#125;&#125; 注意： noCopy 只在 go vet 语法检测的时候有效。即使发生拷贝，编译与运行都能正常的运行 Wait Wait()会自动释放c.L，并挂起调用者的goroutine。之后恢复执行，Wait()会在返回时对c.L加锁。除非被Signal或者Broadcast唤醒，否则Wait()不会返回。由于Wait()第一次恢复时，C.L并没有加锁，所以当Wait返回时，调用者通常并不能假设条件为真。取而代之的是, 调用者应该在循环中调用Wait。(简单来说，只要想使用condition，就必须加锁) 1234567func (c *Cond) Wait() &#123; c.checker.check() t := runtime_notifyListAdd(&amp;c.notify) //添加通知 c.L.Unlock() //释放锁 runtime_notifyListWait(&amp;c.notify, t) //等待通知 c.L.Lock() //加锁&#125; 对条件的检查，使用了 for !condition() 而非 if，是因为当前协程被唤醒时，条件不一定符合要求，需要再次 Wait 等待下次被唤醒。为了保险起见，使用 for 能够确保条件符合要求后，再执行后续的代码 123456c.L.Lock()for !condition() &#123; c.Wait()&#125;... make use of condition ...c.L.Unlock() Singal Signal只唤醒1个等待c的goroutine。 调用Signal的时候，可以加锁，也可以不加锁。 1234func (c *Cond) Signal() &#123; c.checker.check() runtime_notifyListNotifyOne(&amp;c.notify)&#125; Boardcase Broadcast会唤醒所有等待c 的 goroutine 1234func (c *Cond) Broadcast() &#123; c.checker.check() runtime_notifyListNotifyAll(&amp;c.notify)&#125; 总结 这里一直出现的 race 到底是干嘛的后续文章说明 这里一直出现的 runtime_Semacquire 是干嘛的后续文章说明 参考链接 https://www.modb.pro/db/170131 https://vearne.cc/archives/680","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门13-style","slug":"Go/Go入门13-style","date":"2022-09-10T15:05:52.000Z","updated":"2022-09-10T15:05:52.000Z","comments":true,"path":"2022/09/10/Go/Go入门13-style/","link":"","permalink":"http://xboom.github.io/2022/09/10/Go/Go%E5%85%A5%E9%97%A813-style/","excerpt":"","text":"所有代码都应该通过golint和go vet的检查并无错误。建议将编辑器设置为： 保存时运行 goimports 运行 golint 和 go vet 检查错误 指导原则 指向 interface 的指针 应该将接口作为值进行传递，在这样的传递过程中，实质上传递的底层数据仍然可以是指针。接口实质上在底层用两个字段表示： 一个指向某些特定类型信息的指针。可以将其视为&quot;type&quot;。 数据指针。如果存储的数据是指针，则直接存储。如果存储的数据是一个值，则存储指向该值的指针。 如果希望接口方法修改基础数据，则必须使用指针传递 (将对象指针赋值给接口变量)。 12345678910111213141516type F interface &#123; f()&#125;type S1 struct&#123;&#125;func (s S1) f() &#123;&#125;type S2 struct&#123;&#125;func (s *S2) f() &#123;&#125;// f1.f() 无法修改底层数据// f2.f() 可以修改底层数据，给接口变量 f2 赋值时使用的是对象指针var f1 F = S1&#123;&#125;var f2 F = &amp;S2&#123;&#125; Interface 合理性验证 在编译时验证接口的符合性。这包括： 将实现特定接口的导出类型作为接口 API 的一部分进行检查 实现同一接口的 (导出和非导出) 类型属于实现类型的集合 任何违反接口合理性检查的场景，都会终止编译，并通知给用户 上面 3 条是编译器对接口的检查机制，错误使用接口会在编译期报错。所以可以利用这个机制让部分问题在编译期暴露&gt; BadGood 12345678910// 如果 Handler 没有实现 http.Handler，会在运行时报错type Handler struct &#123; // ...&#125;func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request,) &#123; ...&#125; 123456789101112type Handler struct &#123; // ...&#125;// 用于触发编译期的接口的合理性检查机制// 如果 Handler 没有实现 http.Handler，会在编译期报错var _ http.Handler = (*Handler)(nil)func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request,) &#123; // ...&#125; 如果 *Handler 与 http.Handler 的接口不匹配，那么语句 var _ http.Handler = (*Handler)(nil) 将无法编译通过。 赋值的右边应该是断言类型的零值。对于指针类型（如 *Handler）、切片和映射，这是 nil；对于结构类型，这是空结构。 1234567891011type LogHandler struct &#123; h http.Handler log *zap.Logger&#125;var _ http.Handler = LogHandler&#123;&#125;func (h LogHandler) ServeHTTP( w http.ResponseWriter, r *http.Request,) &#123; // ...&#125; 接收器 (receiver) 与接口 使用值接收器的方法既可以通过值调用，也可以通过指针调用。带指针接收器的方法只能通过指针或 addressable values 调用。 例如， 12345678910111213141516171819202122232425type S struct &#123; data string&#125;func (s S) Read() string &#123; return s.data&#125;func (s *S) Write(str string) &#123; s.data = str&#125;sVals := map[int]S&#123;1: &#123;\"A\"&#125;&#125;// 通过值只能调用 ReadsVals[1].Read()// 这不能编译通过：Cannot call pointer method on 'sVals[1]'// sVals[1].Write(\"test\")sPtrs := map[int]*S&#123;1: &#123;\"A\"&#125;&#125;// 通过指针既可以调用 Read，也可以调用 Write 方法sPtrs[1].Read()sPtrs[1].Write(\"test\") 注意：map[int]*S{1: {&quot;A&quot;}} 与 map[int]s{1: {&quot;A&quot;}} 都是用 {&quot;A&quot;} 进行初始化 类似的，即使方法有了值接收器，也同样可以用指针接收器来满足接口。 123456789101112131415161718192021222324type F interface &#123; f()&#125;type S1 struct&#123;&#125;func (s S1) f() &#123;&#125;type S2 struct&#123;&#125;func (s *S2) f() &#123;&#125;s1Val := S1&#123;&#125;s1Ptr := &amp;S1&#123;&#125;s2Val := S2&#123;&#125;s2Ptr := &amp;S2&#123;&#125;var i Fi = s1Vali = s1Ptri = s2Ptr// 下面代码无法通过编译。因为 s2Val 是一个值，而 S2 的 f 方法中没有使用值接收器// i = s2Val Effective Go 中有一段关于 pointers vs. values 的精彩讲解。 补充： 一个类型可以有值接收器方法集和指针接收器方法集 值接收器方法集是指针接收器方法集的子集，反之不是 规则 值对象只可以使用值接收器方法集 指针对象可以使用 值接收器方法集 + 指针接收器方法集 接口的匹配 (或者叫实现) 类型实现了接口的所有方法，叫匹配 具体的讲，要么是类型的值方法集匹配接口，要么是指针方法集匹配接口 具体的匹配分两种： 值方法集和接口匹配 给接口变量赋值的不管是值还是指针对象，都 ok，因为都包含值方法集 指针方法集和接口匹配 只能将指针对象赋值给接口变量，因为只有指针方法集和接口匹配 如果将值对象赋值给接口变量，会在编译期报错 (会触发接口合理性检查机制) 为啥 i = s2Val 会报错，因为值方法集和接口不匹配。 零值 Mutex 是有效的 零值 sync.Mutex 和 sync.RWMutex 是有效的。所以指向 mutex 的指针基本是不必要的。 BadGood 12mu := new(sync.Mutex)mu.Lock() 12var mu sync.Mutexmu.Lock() 如果使用结构体指针，mutex 应该作为结构体的非指针字段。即使该结构体不被导出，也不要直接把 mutex 嵌入到结构体中。 BadGood 123456789101112131415161718type SMap struct &#123; sync.Mutex data map[string]string&#125;func NewSMap() *SMap &#123; return &amp;SMap&#123; data: make(map[string]string), &#125;&#125;func (m *SMap) Get(k string) string &#123; m.Lock() defer m.Unlock() return m.data[k]&#125; 123456789101112131415161718type SMap struct &#123; mu sync.Mutex data map[string]string&#125;func NewSMap() *SMap &#123; return &amp;SMap&#123; data: make(map[string]string), &#125;&#125;func (m *SMap) Get(k string) string &#123; m.mu.Lock() defer m.mu.Unlock() return m.data[k]&#125; Mutex 字段， Lock 和 Unlock 方法是 SMap 导出的 API 中不刻意说明的一部分。 mutex 及其方法是 SMap 的实现细节，对其调用者不可见。 在边界处拷贝 Slices 和 Maps slices 和 maps 包含了指向底层数据的指针，因此在需要复制它们时要特别注意。 接收 Slices 和 Maps 请记住，当 map 或 slice 作为函数参数传入时，如果存储了对它们的引用，则用户可以对其进行修改。 Bad Good 123456789func (d *Driver) SetTrips(trips []Trip) &#123; d.trips = trips&#125;trips := ...d1.SetTrips(trips)// 要修改 d1.trips 吗？trips[0] = ... 12345678910func (d *Driver) SetTrips(trips []Trip) &#123; d.trips = make([]Trip, len(trips)) copy(d.trips, trips)&#125;trips := ...d1.SetTrips(trips)// 这里修改 trips[0]，但不会影响到 d1.tripstrips[0] = ... 返回 slices 或 maps 同样，请注意用户对暴露内部状态的 map 或 slice 的修改。 BadGood 123456789101112131415161718type Stats struct &#123; mu sync.Mutex counters map[string]int&#125;// Snapshot 返回当前状态。func (s *Stats) Snapshot() map[string]int &#123; s.mu.Lock() defer s.mu.Unlock() return s.counters&#125;// snapshot 不再受互斥锁保护// 因此对 snapshot 的任何访问都将受到数据竞争的影响// 影响 stats.counterssnapshot := stats.Snapshot() 12345678910111213141516171819type Stats struct &#123; mu sync.Mutex counters map[string]int&#125;func (s *Stats) Snapshot() map[string]int &#123; s.mu.Lock() defer s.mu.Unlock() result := make(map[string]int, len(s.counters)) for k, v := range s.counters &#123; result[k] = v &#125; return result&#125;// snapshot 现在是一个拷贝snapshot := stats.Snapshot() 使用 defer 释放资源 使用 defer 释放资源，诸如文件和锁。 BadGood 12345678910111213p.Lock()if p.count &lt; 10 &#123; p.Unlock() return p.count&#125;p.count++newCount := p.countp.Unlock()return newCount// 当有多个 return 分支时，很容易遗忘 unlock 1234567891011p.Lock()defer p.Unlock()if p.count &lt; 10 &#123; return p.count&#125;p.count++return p.count// 更可读 Defer 的开销非常小，只有可以证明函数执行时间处于纳秒级的程度时，才应避免这样做。使用 defer 提升可读性是值得的，因为使用它们的成本微不足道。尤其适用于那些不仅仅是简单内存访问的较大的方法，在这些方法中其他计算的资源消耗远超过 defer。 Channel 的 size 要么是 1，要么是无缓冲的 channel 通常 size 应为 1 或是无缓冲的。默认情况下，channel 是无缓冲的，其 size 为零。任何其他尺寸都必须经过严格的审查。需要考虑如何确定大小，考虑是什么阻止了 channel 在高负载下和阻塞写时的写入，以及当这种情况发生时系统逻辑有哪些变化。(翻译解释：按照原文意思是需要界定通道边界，竞态条件，以及逻辑上下文梳理) BadGood 12// 应该足以满足任何情况！c := make(chan int, 64) 1234// 大小：1c := make(chan int, 1) // 或者// 无缓冲 channel，大小为 0c := make(chan int) 枚举从 1 开始 在 Go 中引入枚举的标准方法是声明一个自定义类型和一个使用了 iota 的 const 组。由于变量的默认值为 0，因此通常应以非零值开头枚举。 BadGood 123456789type Operation intconst ( Add Operation = iota Subtract Multiply)// Add=0, Subtract=1, Multiply=2 123456789type Operation intconst ( Add Operation = iota + 1 Subtract Multiply)// Add=1, Subtract=2, Multiply=3 在某些情况下，使用零值是有意义的（枚举从零开始），例如，当零值是理想的默认行为时。 123456789type LogOutput intconst ( LogToStdout LogOutput = iota LogToFile LogToRemote)// LogToStdout=0, LogToFile=1, LogToRemote=2 使用 time 处理时间 时间处理很复杂。关于时间的错误假设通常包括以下几点。 一天有 24 小时 一小时有 60 分钟 一周有七天 一年 365 天 还有更多 例如，1 表示在一个时间点上加上 24 小时并不总是产生一个新的日历日。 因此，在处理时间时始终使用 &quot;time&quot; 包，因为它有助于以更安全、更准确的方式处理这些不正确的假设。 使用 time.Time 表达瞬时时间 在处理时间的瞬间时使用 time.Time，在比较、添加或减去时间时使用 time.Time 中的方法。 BadGood 123func isActive(now, start, stop int) bool &#123; return start &lt;= now &amp;&amp; now &lt; stop&#125; 123func isActive(now, start, stop time.Time) bool &#123; return (start.Before(now) || start.Equal(now)) &amp;&amp; now.Before(stop)&#125; 使用 time.Duration 表达时间段 在处理时间段时使用 time.Duration . BadGood 1234567func poll(delay int) &#123; for &#123; // ... time.Sleep(time.Duration(delay) * time.Millisecond) &#125;&#125;poll(10) // 是几秒钟还是几毫秒？ 1234567func poll(delay time.Duration) &#123; for &#123; // ... time.Sleep(delay) &#125;&#125;poll(10*time.Second) 回到第一个例子，在一个时间瞬间加上 24 小时，用于添加时间的方法取决于意图。如果想要下一个日历日 (当前天的下一天) 的同一个时间点，应该使用 Time.AddDate。但是，如果想保证某一时刻比前一时刻晚 24 小时，应该使用 Time.Add。 12newDay := t.AddDate(0 /* years */, 0 /* months */, 1 /* days */)maybeNewDay := t.Add(24 * time.Hour) 对外部系统使用 time.Time 和 time.Duration 尽可能在与外部系统的交互中使用 time.Duration 和 time.Time 例如 : Command-line 标志: flag 通过 time.ParseDuration 支持 time.Duration JSON: encoding/json 通过其 UnmarshalJSON method 方法支持将 time.Time 编码为 RFC 3339 字符串 SQL: database/sql 支持将 DATETIME 或 TIMESTAMP 列转换为 time.Time，如果底层驱动程序支持则返回 YAML: gopkg.in/yaml.v2 支持将 time.Time 作为 RFC 3339 字符串，并通过 time.ParseDuration 支持 time.Duration。 当不能在这些交互中使用 time.Duration 时，请使用 int 或 float64，并在字段名称中包含单位。 例如，由于 encoding/json 不支持 time.Duration，因此该单位包含在字段的名称中。 BadGood 1234// &#123;\"interval\": 2&#125;type Config struct &#123; Interval int `json:\"interval\"`&#125; 1234// &#123;\"intervalMillis\": 2000&#125;type Config struct &#123; IntervalMillis int `json:\"intervalMillis\"`&#125; 当在这些交互中不能使用 time.Time 时，除非达成一致，否则使用 string 和 RFC 3339 中定义的格式时间戳。默认情况下，Time.UnmarshalText 使用此格式，并可通过 time.RFC3339 在 Time.Format 和 time.Parse 中使用。 尽管这在实践中并不成问题，但请记住，&quot;time&quot; 包不支持解析闰秒时间戳（8728），也不在计算中考虑闰秒（15190）。如果比较两个时间瞬间，则差异将不包括这两个瞬间之间可能发生的闰秒。 Errors 错误类型 声明错误的选项很少。在选择最适合的用例的选项之前，请考虑以下事项。 调用者是否需要匹配错误以便他们可以处理它？ 如果是，必须通过声明顶级错误变量或自定义类型来支持 errors.Is 或 errors.As 函数。 错误消息是否为静态字符串，还是需要上下文信息的动态字符串？ 如果是静态字符串，可以使用 errors.New，但对于后者，必须使用 fmt.Errorf 或自定义错误类型。 是否正在传递由下游函数返回的新错误？ 如果是这样，请参阅错误包装部分。 错误匹配？ 错误消息 指导 No static errors.New No dynamic fmt.Errorf Yes static top-level var with errors.New Yes dynamic custom error type 例如，使用 errors.New 表示带有静态字符串的错误。 如果调用者需要匹配并处理此错误，则将此错误导出为变量以支持将其与 errors.Is 匹配。 无错误匹配错误匹配 123456789101112// package foofunc Open() error &#123; return errors.New(\"could not open\")&#125;// package barif err := foo.Open(); err != nil &#123; // Can't handle the error. panic(\"unknown error\")&#125; 1234567891011121314151617// package foovar ErrCouldNotOpen = errors.New(\"could not open\")func Open() error &#123; return ErrCouldNotOpen&#125;// package barif err := foo.Open(); err != nil &#123; if errors.Is(err, foo.ErrCouldNotOpen) &#123; // handle the error &#125; else &#123; panic(\"unknown error\") &#125;&#125; 对于动态字符串的错误， 如果调用者不需要匹配它，则使用 fmt.Errorf， 如果调用者确实需要匹配它，则自定义 error。 无错误匹配错误匹配 123456789101112// package foofunc Open(file string) error &#123; return fmt.Errorf(\"file %q not found\", file)&#125;// package barif err := foo.Open(\"testfile.txt\"); err != nil &#123; // Can't handle the error. panic(\"unknown error\")&#125; 12345678910111213141516171819202122232425// package footype NotFoundError struct &#123; File string&#125;func (e *NotFoundError) Error() string &#123; return fmt.Sprintf(\"file %q not found\", e.File)&#125;func Open(file string) error &#123; return &amp;NotFoundError&#123;File: file&#125;&#125;// package barif err := foo.Open(\"testfile.txt\"); err != nil &#123; var notFound *NotFoundError if errors.As(err, &amp;notFound) &#123; // handle the error &#125; else &#123; panic(\"unknown error\") &#125;&#125; 请注意，如果从包中导出错误变量或类型， 它们将成为包的公共 API 的一部分。 错误包装 如果调用其他方法时出现错误, 通常有三种处理方式可以选择： 将原始错误原样返回 使用 fmt.Errorf 搭配 %w 将错误添加进上下文后返回 使用 fmt.Errorf 搭配 %v 将错误添加进上下文后返回 如果没有要添加的其他上下文，则按原样返回原始错误。 这将保留原始错误类型和消息。 这非常适合底层错误消息有足够的信息来追踪它来自哪里的错误。 否则，尽可能在错误消息中添加上下文 这样就不会出现诸如“连接被拒绝”之类的模糊错误， 会收到更多有用的错误，例如“调用服务 foo：连接被拒绝”。 使用 fmt.Errorf 为错误添加上下文，根据调用者是否应该能够匹配和提取根本原因，在 %w 或 %v 动词之间进行选择。 如果调用者应该可以访问底层错误，请使用 %w。 对于大多数包装错误，这是一个很好的默认值， 但请注意，调用者可能会开始依赖此行为。因此，对于包装错误是已知var或类型的情况，请将其作为函数契约的一部分进行记录和测试。 使用 %v 来混淆底层错误。 调用者将无法匹配它，但如果需要，可以在将来切换到 %w。 在为返回的错误添加上下文时，通过避免使用&quot;failed to&quot;之类的短语来保持上下文简洁，当错误通过堆栈向上渗透时，它会一层一层被堆积起来： BadGood 12345s, err := store.New()if err != nil &#123; return fmt.Errorf( \"failed to create new store: %w\", err)&#125; 12345s, err := store.New()if err != nil &#123; return fmt.Errorf( \"new store: %w\", err)&#125; 1failed to x: failed to y: failed to create new store: the error 1x: y: new store: the error 然而，一旦错误被发送到另一个系统，应该清楚消息是一个错误（例如err 标签或日志中的&quot;Failed&quot;前缀）。 另见 不要只检查错误，优雅地处理它们。 错误命名 对于存储为全局变量的错误值， 根据是否导出，使用前缀 Err 或 err。 请看指南 对于未导出的顶层常量和变量，使用_作为前缀。 12345678910var ( // 导出以下两个错误，以便此包的用户可以将它们与 errors.Is 进行匹配。 ErrBrokenLink = errors.New(\"link is broken\") ErrCouldNotOpen = errors.New(\"could not open\") // 这个错误没有被导出，因为不想让它成为公共 API 的一部分。 可能仍然在带有错误的包内使用它。 errNotFound = errors.New(\"not found\")) 对于自定义错误类型，请改用后缀 Error。 123456789101112131415161718// 同样，这个错误被导出，以便这个包的用户可以将它与 errors.As 匹配。type NotFoundError struct &#123; File string&#125;func (e *NotFoundError) Error() string &#123; return fmt.Sprintf(\"file %q not found\", e.File)&#125;// 并且这个错误没有被导出，因为不想让它成为公共 API 的一部分。 仍然可以在带有 errors.As 的包中使用它。type resolveError struct &#123; Path string&#125;func (e *resolveError) Error() string &#123; return fmt.Sprintf(\"resolve %q\", e.Path)&#125; 处理断言失败 类型断言 将会在检测到不正确的类型时，以单一返回值形式返回 panic。 因此，请始终使用“逗号 ok”习语。 BadGood 1t := i.(string) 1234t, ok := i.(string)if !ok &#123; // 优雅地处理错误&#125; 不要使用 panic 在生产环境中运行的代码必须避免出现 panic。panic 是 级联失败 的主要根源 。如果发生错误，该函数必须返回错误，并允许调用方决定如何处理它。 BadGood 12345678910func run(args []string) &#123; if len(args) == 0 &#123; panic(\"an argument is required\") &#125; // ...&#125;func main() &#123; run(os.Args[1:])&#125; 1234567891011121314func run(args []string) error &#123; if len(args) == 0 &#123; return errors.New(\"an argument is required\") &#125; // ... return nil&#125;func main() &#123; if err := run(os.Args[1:]); err != nil &#123; fmt.Fprintln(os.Stderr, err) os.Exit(1) &#125;&#125; panic/recover 不是错误处理策略。仅当发生不可恢复的事情（例如：nil 引用）时，程序才必须 panic。程序初始化是一个例外：程序启动时应使程序中止的不良情况可能会引起 panic。 1var _statusTemplate = template.Must(template.New(\"name\").Parse(\"_statusHTML\")) 即使在测试代码中，也优先使用t.Fatal或者t.FailNow而不是 panic 来确保失败被标记。 BadGood 12345// func TestFoo(t *testing.T)f, err := ioutil.TempFile(\"\", \"test\")if err != nil &#123; panic(\"failed to set up test\")&#125; 12345// func TestFoo(t *testing.T)f, err := ioutil.TempFile(\"\", \"test\")if err != nil &#123; t.Fatal(\"failed to set up test\")&#125; 使用 go.uber.org/atomic 使用 [sync/atomic] 包的原子操作对原始类型 (int32, int64等）进行操作，因为很容易忘记使用原子操作来读取或修改变量。 [go.uber.org/atomic] 通过隐藏基础类型为这些操作增加了类型安全性。此外，它包括一个方便的atomic.Bool类型。 BadGood 123456789101112131415type foo struct &#123; running int32 // atomic&#125;func (f* foo) start() &#123; if atomic.SwapInt32(&amp;f.running, 1) == 1 &#123; // already running… return &#125; // start the Foo&#125;func (f *foo) isRunning() bool &#123; return f.running == 1 // race!&#125; 123456789101112131415type foo struct &#123; running atomic.Bool&#125;func (f *foo) start() &#123; if f.running.Swap(true) &#123; // already running… return &#125; // start the Foo&#125;func (f *foo) isRunning() bool &#123; return f.running.Load()&#125; 避免可变全局变量 使用选择依赖注入方式避免改变全局变量。既适用于函数指针又适用于其他值类型 BadGood 123456// sign.govar _timeNow = time.Nowfunc sign(msg string) string &#123; now := _timeNow() return signWithTime(msg, now)&#125; 12345678910111213// sign.gotype signer struct &#123; now func() time.Time&#125;func newSigner() *signer &#123; return &amp;signer&#123; now: time.Now, &#125;&#125;func (s *signer) Sign(msg string) string &#123; now := s.now() return signWithTime(msg, now)&#125; 123456789// sign_test.gofunc TestSign(t *testing.T) &#123; oldTimeNow := _timeNow _timeNow = func() time.Time &#123; return someFixedTime &#125; defer func() &#123; _timeNow = oldTimeNow &#125;() assert.Equal(t, want, sign(give))&#125; 12345678// sign_test.gofunc TestSigner(t *testing.T) &#123; s := newSigner() s.now = func() time.Time &#123; return someFixedTime &#125; assert.Equal(t, want, s.Sign(give))&#125; 避免在公共结构中嵌入类型 这些嵌入的类型泄漏实现细节、禁止类型演化和模糊的文档。 假设使用共享的 AbstractList 实现了多种列表类型，请避免在具体的列表实现中嵌入 AbstractList。 相反，只需手动将方法写入具体的列表，该列表将委托给抽象列表。 123456789type AbstractList struct &#123;&#125;// 添加将实体添加到列表中。func (l *AbstractList) Add(e Entity) &#123; // ...&#125;// 移除从列表中移除实体。func (l *AbstractList) Remove(e Entity) &#123; // ...&#125; BadGood 1234// ConcreteList 是一个实体列表。type ConcreteList struct &#123; *AbstractList&#125; 123456789101112// ConcreteList 是一个实体列表。type ConcreteList struct &#123; list *AbstractList&#125;// 添加将实体添加到列表中。func (l *ConcreteList) Add(e Entity) &#123; l.list.Add(e)&#125;// 移除从列表中移除实体。func (l *ConcreteList) Remove(e Entity) &#123; l.list.Remove(e)&#125; Go 允许 类型嵌入 作为继承和组合之间的折衷。外部类型获取嵌入类型的方法的隐式副本。默认情况下，这些方法委托给嵌入实例的同一方法。 结构还获得与类型同名的字段。所以，如果嵌入的类型是 public，那么字段是 public。为了保持向后兼容性，外部类型的每个未来版本都必须保留嵌入类型。 很少需要嵌入类型。 这是一种方便，可以帮助避免编写冗长的委托方法。 即使嵌入兼容的抽象列表 interface，而不是结构体，这将为开发人员提供更大的灵活性来改变未来，但仍然泄露了具体列表使用抽象实现的细节。 BadGood 123456789// AbstractList 是各种实体列表的通用实现。type AbstractList interface &#123; Add(Entity) Remove(Entity)&#125;// ConcreteList 是一个实体列表。type ConcreteList struct &#123; AbstractList&#125; 123456789101112// ConcreteList 是一个实体列表。type ConcreteList struct &#123; list AbstractList&#125;// 添加将实体添加到列表中。func (l *ConcreteList) Add(e Entity) &#123; l.list.Add(e)&#125;// 移除从列表中移除实体。func (l *ConcreteList) Remove(e Entity) &#123; l.list.Remove(e)&#125; 无论是使用嵌入结构还是嵌入接口，都会限制类型的演化。 向嵌入接口添加方法是一个破坏性的改变。 从嵌入结构体删除方法是一个破坏性改变。 删除嵌入类型是一个破坏性的改变。 即使使用满足相同接口的类型替换嵌入类型，也是一个破坏性的改变。 尽管编写这些委托方法是乏味的，但是额外的工作隐藏了实现细节，留下了更多的更改机会，还消除了在文档中发现完整列表接口的间接性操作。 避免使用内置名称 Go 语言规范 概述了几个内置的，不应在 Go 项目中使用的 预先声明的标识符。 根据上下文的不同，将这些标识符作为名称重复使用，将在当前作用域（或任何嵌套作用域）中隐藏原始标识符，或者混淆代码。 在最好的情况下，编译器会报错；在最坏的情况下，这样的代码可能会引入潜在的、难以恢复的错误。 BadGood 12345678var error string// `error` 作用域隐式覆盖// orfunc handleErrorMessage(error string) &#123; // `error` 作用域隐式覆盖&#125; 12345678var errorMessage string// `error` 指向内置的非覆盖// orfunc handleErrorMessage(msg string) &#123; // `error` 指向内置的非覆盖&#125; 123456789101112131415type Foo struct &#123; // 虽然这些字段在技术上不构成阴影，但`error`或`string`字符串的重映射现在是不明确的。 error error string string&#125;func (f Foo) Error() error &#123; // `error` 和 `f.error` 在视觉上是相似的 return f.error&#125;func (f Foo) String() string &#123; // `string` and `f.string` 在视觉上是相似的 return f.string&#125; 12345678910111213type Foo struct &#123; // `error` and `string` 现在是明确的。 err error str string&#125;func (f Foo) Error() error &#123; return f.err&#125;func (f Foo) String() string &#123; return f.str&#125; 注意，编译器在使用预先分隔的标识符时不会生成错误， 但是诸如go vet之类的工具会正确地指出这些和其他情况下的隐式问题。 避免使用 init() 尽可能避免使用init()。当init()是不可避免或可取的，代码应先尝试： 无论程序环境或调用如何，都要完全确定。 避免依赖于其他init()函数的顺序或副作用。虽然init()顺序是明确的，但代码可以更改， 因此init()函数之间的关系可能会使代码变得脆弱和容易出错。 避免访问或操作全局或环境状态，如机器信息、环境变量、工作目录、程序参数/输入等。 避免I/O，包括文件系统、网络和系统调用。 init 调用顺序 在同一个 package 中，可以多个文件中定义 init 方法，按照文件名先后执行各个文件中的 init 方法 在同一个 go 文件中，可以重复定义 init 方法，按照在代码中编写顺序依次执行不同的 init 方法 对于不同的 package， 如果不相互依赖的话，按照 main 包中 import 的顺序调用其包中的 init() 函数 如果 package 存在依赖，调用顺序为最后被依赖的最先被初始化，例如：导入顺序 main –&gt; A –&gt; B –&gt; C，则初始化顺序为 C –&gt; B –&gt; A –&gt; main，一次执行对应的 init 方法 不能满足这些要求的代码可能属于要作为main()调用的一部分(或程序生命周期中的其他地方)，或者作为main()本身的一部分写入。特别是，打算由其他程序使用的库应该特别注意完全确定性，而不是执行“init magic” BadGood 123456789type Foo struct &#123; // ...&#125;var _defaultFoo Foofunc init() &#123; _defaultFoo = Foo&#123; // ... &#125;&#125; 12345678910var _defaultFoo = Foo&#123; // ...&#125;// or，为了更好的可测试性：var _defaultFoo = defaultFoo()func defaultFoo() Foo &#123; return Foo&#123; // ... &#125;&#125; 12345678910111213type Config struct &#123; // ...&#125;var _config Configfunc init() &#123; // Bad: 基于当前目录 cwd, _ := os.Getwd() // Bad: I/O raw, _ := ioutil.ReadFile( path.Join(cwd, \"config\", \"config.yaml\"), ) yaml.Unmarshal(raw, &amp;_config)&#125; 1234567891011121314type Config struct &#123; // ...&#125;func loadConfig() Config &#123; cwd, err := os.Getwd() // handle err raw, err := ioutil.ReadFile( path.Join(cwd, \"config\", \"config.yaml\"), ) // handle err var config Config yaml.Unmarshal(raw, &amp;config) return config&#125; 考虑到上述情况，在某些情况下，init()可能更可取或是必要的，可能包括： 不能表示为单个赋值的复杂表达式。 可插入的钩子，如database/sql、编码类型注册表等。 对 Google Cloud Functions 和其他形式的确定性预计算的优化。 追加时优先指定切片容量 追加时优先指定切片容量，在尽可能的情况下，在初始化要追加的切片时为make()提供一个容量值。 BadGood 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0, size) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 1BenchmarkBad-4 100000000 2.48s 1BenchmarkGood-4 100000000 0.21s 主函数退出方式 (Exit) Go 程序使用 os.Exit 或者 log.Fatal* 立即退出 (使用panic不是退出程序的好方法，请 不要使用 panic。) 仅在main() 中调用其中一个 os.Exit 或者 log.Fatal*。所有其他函数应将错误返回到信号失败中。 BadGood 123456789101112131415func main() &#123; body := readFile(path) fmt.Println(body)&#125;func readFile(path string) string &#123; f, err := os.Open(path) if err != nil &#123; log.Fatal(err) &#125; b, err := ioutil.ReadAll(f) if err != nil &#123; log.Fatal(err) &#125; return string(b)&#125; 123456789101112131415161718func main() &#123; body, err := readFile(path) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(body)&#125;func readFile(path string) (string, error) &#123; f, err := os.Open(path) if err != nil &#123; return \"\", err &#125; b, err := ioutil.ReadAll(f) if err != nil &#123; return \"\", err &#125; return string(b), nil&#125; 原则上：退出的具有多种功能的程序存在一些问题： 不明显的控制流：任何函数都可以退出程序，因此很难对控制流进行推理。 难以测试：退出程序的函数也将退出调用它的测试。这使得函数很难测试，并引入了跳过 go test 尚未运行的其他测试的风险。 跳过清理：当函数退出程序时，会跳过已经进入defer队列里的函数调用。这增加了跳过重要清理任务的风险。 一次性退出 如果可能的话，main（）函数中 最多一次 调用 os.Exit或者log.Fatal。如果有多个错误场景停止程序执行，请将该逻辑放在单独的函数下并从中返回错误。 这会缩短 main() 函数，并将所有关键业务逻辑放入一个单独的、可测试的函数中。 BadGood 1234567891011121314151617181920package mainfunc main() &#123; args := os.Args[1:] if len(args) != 1 &#123; log.Fatal(\"missing file\") &#125; name := args[0] f, err := os.Open(name) if err != nil &#123; log.Fatal(err) &#125; defer f.Close() // 如果调用 log.Fatal 在这条线之后 // f.Close 将会被执行。 b, err := ioutil.ReadAll(f) if err != nil &#123; log.Fatal(err) &#125; // ...&#125; 1234567891011121314151617181920212223package mainfunc main() &#123; if err := run(); err != nil &#123; log.Fatal(err) &#125;&#125;func run() error &#123; args := os.Args[1:] if len(args) != 1 &#123; return errors.New(\"missing file\") &#125; name := args[0] f, err := os.Open(name) if err != nil &#123; return err &#125; defer f.Close() b, err := ioutil.ReadAll(f) if err != nil &#123; return err &#125; // ...&#125; 在序列化结构中使用字段标记 任何序列化到JSON、YAML，或其他支持基于标记的字段命名的格式应使用相关标记进行注释。 BadGood 12345678type Stock struct &#123; Price int Name string&#125;bytes, err := json.Marshal(Stock&#123; Price: 137, Name: \"UBER\",&#125;) 123456789type Stock struct &#123; Price int `json:\"price\"` Name string `json:\"name\"` // Safe to rename Name to Symbol.&#125;bytes, err := json.Marshal(Stock&#123; Price: 137, Name: \"UBER\",&#125;) 理论上： 结构的序列化形式是不同系统之间的契约。 对序列化表单结构（包括字段名）的更改会破坏此约定。在标记中指定字段名使约定明确， 它还可以通过重构或重命名字段来防止意外违反约定。 性能 性能方面的特定准则只适用于高频场景。 优先使用 strconv 而不是 fmt 将原语转换为字符串或从字符串转换时，strconv速度比fmt快。 BadGood 123for i := 0; i &lt; b.N; i++ &#123; s := fmt.Sprint(rand.Int())&#125; 123for i := 0; i &lt; b.N; i++ &#123; s := strconv.Itoa(rand.Int())&#125; 1BenchmarkFmtSprint-4 143 ns&#x2F;op 2 allocs&#x2F;op 1BenchmarkStrconv-4 64.2 ns&#x2F;op 1 allocs&#x2F;op 避免字符串到字节的转换 不要反复从固定字符串创建字节 slice。相反，请执行一次转换并捕获结果。 BadGood 123for i := 0; i &lt; b.N; i++ &#123; w.Write([]byte(\"Hello world\"))&#125; 1234data := []byte(\"Hello world\")for i := 0; i &lt; b.N; i++ &#123; w.Write(data)&#125; 1BenchmarkBad-4 50000000 22.2 ns&#x2F;op 1BenchmarkGood-4 500000000 3.25 ns&#x2F;op 即使是内容相同的字符串，它们都会单独在分配一块内存 另外在测试的时候发现：字符串常量无法获取地址！！！，因为取值符号&amp; 仅适用于堆栈内存或堆内存中的值，即可以实际写入的值。表达式和常量(它们本身是程序本身)实际上存储在内存中，但无法写入该内存。因此，能够引入该内存就没有意义 指定容器容量 尽可能指定容器容量，以便为容器预先分配内存。这将在添加元素时最小化后续分配（通过复制和调整容器大小）。 指定 Map 容量提示 在尽可能的情况下，在使用 make() 初始化的时候提供容量信息 1make(map[T1]T2, hint) 向make()提供容量提示会在初始化时尝试调整 map 的大小，这将减少在将元素添加到 map 时为 map 重新分配内存。 注意，与 slices 不同。map capacity 提示并不保证完全的抢占式分配，而是用于估计所需的 hashmap bucket 的数量。因此，在将元素添加到 map 时，甚至在指定 map 容量时，仍可能发生分配。 BadGood 123456m := make(map[string]os.FileInfo)files, _ := ioutil.ReadDir(\"./files\")for _, f := range files &#123; m[f.Name()] = f&#125; 123456files, _ := ioutil.ReadDir(\"./files\")m := make(map[string]os.FileInfo, len(files))for _, f := range files &#123; m[f.Name()] = f&#125; m 是在没有大小提示的情况下创建的； 在运行时可能会有更多分配。 m 是有大小提示创建的；在运行时可能会有更少的分配。 指定切片容量 在尽可能的情况下，在使用make()初始化切片时提供容量信息，特别是在追加切片时。 1make([]T, length, capacity) 与 maps 不同，slice capacity 不是一个提示：编译器将为提供给make()的 slice 的容量分配足够的内存， 这意味着后续的 append()`操作将导致零分配（直到 slice 的长度与容量匹配，在此之后，任何 append 都可能调整大小以容纳其他元素）。 BadGood 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0, size) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 1BenchmarkBad-4 100000000 2.48s 1BenchmarkGood-4 100000000 0.21s 规范 避免过长的行 避免使用需要读者水平滚动或过度转动头部的代码行。建议将行长度限制为 99 characters (99 个字符)。但这不是硬性限制。允许代码超过此限制。 一致性 本文中概述的一些标准都是客观性的评估，是根据场景、上下文、或者主观性的判断； 但是最重要的是，保持一致. 一致性的代码更容易维护、是更合理的、需要更少的学习成本、并且随着新的约定出现或者出现错误后更容易迁移、更新、修复 bug 相反，在一个代码库中包含多个完全不同或冲突的代码风格会导致维护成本开销、不确定性和认知偏差。所有这些都会直接导致速度降低、代码审查痛苦、而且增加 bug 数量。 将这些标准应用于代码库时，建议在 package（或更大）级别进行更改，子包级别的应用程序通过将多个样式引入到同一代码中，违反了上述关注点。 相似的声明放在一组 Go 语言支持将相似的声明放在一个组内。 BadGood 12import \"a\"import \"b\" 1234import ( \"a\" \"b\") 这同样适用于常量、变量和类型声明： BadGood 12345678const a = 1const b = 2var a = 1var b = 2type Area float64type Volume float64 1234567891011121314const ( a = 1 b = 2)var ( a = 1 b = 2)type ( Area float64 Volume float64) 仅将相关的声明放在一组。不要将不相关的声明放在一组。 BadGood 12345678type Operation intconst ( Add Operation = iota + 1 Subtract Multiply EnvVar = \"MY_ENV\") 123456789type Operation intconst ( Add Operation = iota + 1 Subtract Multiply)const EnvVar = \"MY_ENV\" 分组使用的位置没有限制，例如：可以在函数内部使用它们： BadGood 1234567func f() string &#123; red := color.New(0xff0000) green := color.New(0x00ff00) blue := color.New(0x0000ff) ...&#125; 123456789func f() string &#123; var ( red = color.New(0xff0000) green = color.New(0x00ff00) blue = color.New(0x0000ff) ) ...&#125; 例外：如果变量声明与其他变量相邻，则应将变量声明（尤其是函数内部的声明）分组在一起。对一起声明的变量执行此操作，即使它们不相关。 BadGood 1234567func (c *client) request() &#123; caller := c.name format := \"json\" timeout := 5*time.Second var err error // ...&#125; 123456789func (c *client) request() &#123; var ( caller = c.name format = \"json\" timeout = 5*time.Second err error ) // ...&#125; import 分组 导入应该分为两组： 标准库 其他库 默认情况下，这是 goimports 应用的分组。 BadGood 123456import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\") 1234567import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\") 包名 当命名包时，请按下面规则选择一个名称： 全部小写。没有大写或下划线。 大多数使用命名导入的情况下，不需要重命名。 简短而简洁。请记住，在每个使用的地方都完整标识了该名称。 不用复数。例如net/url，而不是net/urls。 不要用“common”，“util”，“shared”或“lib”。这些是不好的，信息量不足的名称。 另请参阅 Go 包命名规则 和 Go 包样式指南. 函数名 遵循 Go 社区关于使用 MixedCaps 作为函数名 的约定。有一个例外，为了对相关的测试用例进行分组，函数名可能包含下划线，如：TestMyFunction_WhatIsBeingTested. 导入别名 如果程序包名称与导入路径的最后一个元素不匹配，则必须使用导入别名。 123456import ( \"net/http\" client \"example.com/client-go\" trace \"example.com/trace/v2\") 在所有其他情况下，除非导入之间有直接冲突，否则应避免导入别名。 BadGood 123456import ( \"fmt\" \"os\" nettrace \"golang.net/x/trace\") 1234567import ( \"fmt\" \"os\" \"runtime/trace\" nettrace \"golang.net/x/trace\") 函数分组与顺序 函数应按粗略的调用顺序排序。 同一文件中的函数应按接收者分组。 因此，导出的函数应先出现在文件中，放在struct, const, var定义的后面。 在定义类型之后，但在接收者的其余方法之前，可能会出现一个 newXYZ()/NewXYZ() 由于函数是按接收者分组的，因此普通工具函数应在文件末尾出现。 BadGood 12345678910111213func (s *something) Cost() &#123; return calcCost(s.weights)&#125;type something struct&#123; ... &#125;func calcCost(n []int) int &#123;...&#125;func (s *something) Stop() &#123;...&#125;func newSomething() *something &#123; return &amp;something&#123;&#125;&#125; 12345678910111213type something struct&#123; ... &#125;func newSomething() *something &#123; return &amp;something&#123;&#125;&#125;func (s *something) Cost() &#123; return calcCost(s.weights)&#125;func (s *something) Stop() &#123;...&#125;func calcCost(n []int) int &#123;...&#125; 减少嵌套 代码应通过尽可能先处理错误情况/特殊情况并尽早返回或继续循环来减少嵌套。减少嵌套多个级别的代码的代码量。 BadGood 123456789101112for _, v := range data &#123; if v.F1 == 1 &#123; v = process(v) if err := v.Call(); err == nil &#123; v.Send() &#125; else &#123; return err &#125; &#125; else &#123; log.Printf(\"Invalid v: %v\", v) &#125;&#125; 123456789101112for _, v := range data &#123; if v.F1 != 1 &#123; log.Printf(\"Invalid v: %v\", v) continue &#125; v = process(v) if err := v.Call(); err != nil &#123; return err &#125; v.Send()&#125; 不必要的 else 如果在 if 的两个分支中都设置了变量，则可以将其替换为单个 if。 BadGood 123456var a intif b &#123; a = 100&#125; else &#123; a = 10&#125; 1234a := 10if b &#123; a = 100&#125; 顶层变量声明 在顶层，使用标准var关键字。请勿指定类型，除非它与表达式的类型不同。 BadGood 123var _s string = F()func F() string &#123; return \"A\" &#125; 12345var _s = F()// 由于 F 已经明确了返回一个字符串类型，因此没有必要显式指定_s 的类型// 还是那种类型func F() string &#123; return \"A\" &#125; 如果表达式的类型与所需的类型不完全匹配，请指定类型。 12345678type myError struct&#123;&#125;func (myError) Error() string &#123; return \"error\" &#125;func F() myError &#123; return myError&#123;&#125; &#125;var _e error = F()// F 返回一个 myError 类型的实例，但是要 error 类型 对于未导出的顶层常量和变量，使用_作为前缀 在未导出的顶级vars和consts， 前面加上前缀_，以使它们在使用时明确表示它们是全局符号。 基本依据：顶级变量和常量具有包范围作用域。使用通用名称可能很容易在其他文件中意外使用错误的值。 BadGood 1234567891011121314151617// foo.goconst ( defaultPort = 8080 defaultUser = \"user\")// bar.gofunc Bar() &#123; defaultPort := 9090 ... fmt.Println(\"Default port\", defaultPort) // We will not see a compile error if the first line of // Bar() is deleted.&#125; 123456// foo.goconst ( _defaultPort = 8080 _defaultUser = \"user\") 例外：未导出的错误值可以使用不带下划线的前缀 err。 参见错误命名。 结构体中的嵌入 嵌入式类型（例如 mutex）应位于结构体内的字段列表的顶部，并且必须有一个空行将嵌入式字段与常规字段分隔开。 BadGood 1234type Client struct &#123; version int http.Client&#125; 12345type Client struct &#123; http.Client version int&#125; 内嵌应该提供切实的好处，比如以语义上合适的方式添加或增强功能。它应该在对用户没有任何不利影响的情况下使用。（另请参见：避免在公共结构中嵌入类型）。例外：即使在未导出类型中，Mutex 也不应该作为内嵌字段。另请参见：零值 Mutex 是有效的。 嵌入 不应该: 纯粹是为了美观或方便。 使外部类型更难构造或使用。 影响外部类型的零值。如果外部类型有一个有用的零值，则在嵌入内部类型之后应该仍然有一个有用的零值。 作为嵌入内部类型的副作用，从外部类型公开不相关的函数或字段。 公开未导出的类型。 影响外部类型的复制形式。 更改外部类型的 API 或类型语义。 嵌入内部类型的非规范形式。 公开外部类型的实现详细信息。 允许用户观察或控制类型内部。 通过包装的方式改变内部函数的一般行为，这种包装方式会给用户带来一些意料之外情况。 简单地说，有意识地和有目的地嵌入。一种很好的测试体验是， “是否所有这些导出的内部方法/字段都将直接添加到外部类型” 如果答案是some或no，不要嵌入内部类型 - 而是使用字段。 BadGood 12345type A struct &#123; // Bad: A.Lock() and A.Unlock() 现在可用 // 不提供任何功能性好处，并允许用户控制有关 A 的内部细节。 sync.Mutex&#125; 12345678910type countingWriteCloser struct &#123; // Good: Write() 在外层提供用于特定目的， // 并且委托工作到内部类型的 Write() 中。 io.WriteCloser count int&#125;func (w *countingWriteCloser) Write(bs []byte) (int, error) &#123; w.count += len(bs) return w.WriteCloser.Write(bs)&#125; 12345678910type Book struct &#123; // Bad: 指针更改零值的有用性 io.ReadWriter // other fields&#125;// latervar b Bookb.Read(...) // panic: nil pointerb.String() // panic: nil pointerb.Write(...) // panic: nil pointer 12345678910type Book struct &#123; // Good: 有用的零值 bytes.Buffer // other fields&#125;// latervar b Bookb.Read(...) // okb.String() // okb.Write(...) // ok 123456type Client struct &#123; sync.Mutex sync.WaitGroup bytes.Buffer url.URL&#125; 123456type Client struct &#123; mtx sync.Mutex wg sync.WaitGroup buf bytes.Buffer url url.URL&#125; 本地变量声明 如果将变量明确设置为某个值，则应使用短变量声明形式 (:=)。 BadGood 1var s = \"foo\" 1s := \"foo\" 但是，在某些情况下，var 使用关键字时默认值会更清晰。例如，声明空切片。 BadGood 12345678func f(list []int) &#123; filtered := []int&#123;&#125; for _, v := range list &#123; if v &gt; 10 &#123; filtered = append(filtered, v) &#125; &#125;&#125; 12345678func f(list []int) &#123; var filtered []int for _, v := range list &#123; if v &gt; 10 &#123; filtered = append(filtered, v) &#125; &#125;&#125; nil 是一个有效的 slice nil 是一个有效的长度为 0 的 slice，这意味着， 不应明确返回长度为零的切片。应该返回nil 来代替。 BadGood 123if x == \"\" &#123; return []int&#123;&#125;&#125; 123if x == \"\" &#123; return nil&#125; 要检查切片是否为空，请始终使用len(s) == 0。而非 nil。 BadGood 123func isEmpty(s []string) bool &#123; return s == nil&#125; 123func isEmpty(s []string) bool &#123; return len(s) == 0&#125; 零值切片（用var声明的切片）可立即使用，无需调用make()创建。 BadGood 12345678910nums := []int&#123;&#125;// or, nums := make([]int)if add1 &#123; nums = append(nums, 1)&#125;if add2 &#123; nums = append(nums, 2)&#125; 123456789var nums []intif add1 &#123; nums = append(nums, 1)&#125;if add2 &#123; nums = append(nums, 2)&#125; 记住，虽然 nil 切片是有效的切片，但它不等于长度为 0 的切片（一个为 nil，另一个不是），并且在不同的情况下（例如序列化），这两个切片的处理方式可能不同。 缩小变量作用域 如果有可能，尽量缩小变量作用范围。除非它与 减少嵌套的规则冲突。 BadGood 1234err := ioutil.WriteFile(name, data, 0644)if err != nil &#123; return err&#125; 123if err := ioutil.WriteFile(name, data, 0644); err != nil &#123; return err&#125; 如果需要在 if 之外使用函数调用的结果，则不应尝试缩小范围。 BadGood 1234567891011if data, err := ioutil.ReadFile(name); err == nil &#123; err = cfg.Decode(data) if err != nil &#123; return err &#125; fmt.Println(cfg) return nil&#125; else &#123; return err //注意这个err的范围&#125; 1234567891011data, err := ioutil.ReadFile(name)if err != nil &#123; return err&#125;if err := cfg.Decode(data); err != nil &#123; return err&#125;fmt.Println(cfg)return nil 注意 if 中变量的范围 12345678910111213func f1() (int, error) &#123; return 0, errors.New(\"err test\")&#125;func main() &#123; if v, err := f1(); err == nil &#123; //为了在 else中打印 err 这里使用 == a := 1 // &#125; else &#123; fmt.Printf(\"%v %v\", err, v) fmt.Printf(\"%v\", a) //编译错误，undefined: a &#125; &#125; 避免参数语义不明确 (Avoid Naked Parameters) 函数调用中的意义不明确的参数可能会损害可读性。当参数名称的含义不明显时，请为参数添加 C 样式注释 (/* ... */) BadGood 123// func printInfo(name string, isLocal, done bool)printInfo(\"foo\", true, true) 123// func printInfo(name string, isLocal, done bool)printInfo(\"foo\", true /* isLocal */, true /* done */) 对于上面的示例代码，还有一种更好的处理方式是将上面的 bool 类型换成自定义类型。将来，该参数可以支持不仅仅局限于两个状态（true/false）。 12345678910111213141516type Region intconst ( UnknownRegion Region = iota Local)type Status intconst ( StatusReady Status= iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future.)func printInfo(name string, region Region, status Status) 使用原始字符串字面值，避免转义 Go 支持使用 原始字符串字面值，也就是 &quot; ` &quot; 来表示原生字符串，在需要转义的场景下，应该尽量使用这种方案来替换。 可以跨越多行并包含引号。使用这些字符串可以避免更难阅读的手工转义的字符串。 BadGood 1wantError := \"unknown name:\\\"test\\\"\" 1wantError := `unknown error:\"test\"` 初始化结构体 使用字段名初始化结构 初始化结构时，几乎应该始终指定字段名。目前由 go vet 强制执行。 BadGood 1k := User&#123;\"John\", \"Doe\", true&#125; 12345k := User&#123; FirstName: \"John\", LastName: \"Doe\", Admin: true,&#125; 例外：当有 3 个或更少的字段时，测试表中的字段名may可以省略。 1234567tests := []struct&#123; op Operation want string&#125;&#123; &#123;Add, \"add\"&#125;, &#123;Subtract, \"subtract\"&#125;,&#125; 省略结构中的零值字段 初始化具有字段名的结构时，除非提供有意义的上下文，否则忽略值为零的字段。也就是，让自动将这些设置为零值 BadGood 123456user := User&#123; FirstName: \"John\", LastName: \"Doe\", MiddleName: \"\", Admin: false,&#125; 1234user := User&#123; FirstName: \"John\", LastName: \"Doe\",&#125; 这有助于通过省略该上下文中的默认值来减少阅读的障碍。只指定有意义的值。 在字段名提供有意义上下文的地方包含零值。例如，表驱动测试 中的测试用例可以受益于字段的名称，即使它们是零值的。 1234567tests := []struct&#123; give string want int&#125;&#123; &#123;give: \"0\", want: 0&#125;, // ...&#125; 对零值结构使用 var 如果在声明中省略了结构的所有字段，请使用 var 声明结构。 BadGood 1user := User&#123;&#125; 1var user User 这将零值结构与那些具有类似于为 初始化 Maps 创建的，区别于非零值字段的结构区分开来， 并与更喜欢的 声明空切片 方式相匹配。 初始化 Struct 引用 在初始化结构引用时，请使用&amp;T{}代替new(T)，以使其与结构体初始化一致。 BadGood 12345sval := T&#123;Name: \"foo\"&#125;// inconsistentsptr := new(T)sptr.Name = \"bar\" 123sval := T&#123;Name: \"foo\"&#125;sptr := &amp;T&#123;Name: \"bar\"&#125; 初始化 Maps 对于空 map 请使用 make(..) 初始化， 并且 map 是通过编程方式填充的。 这使得 map 初始化在表现上不同于声明，并且它还可以方便地在 make 后添加大小提示。 BadGood 123456var ( // m1 读写安全; // m2 在写入时会 panic m1 = map[T1]T2&#123;&#125; m2 map[T1]T2) 123456var ( // m1 读写安全; // m2 在写入时会 panic m1 = make(map[T1]T2) m2 map[T1]T2) 声明和初始化看起来非常相似的。 声明和初始化看起来差别非常大。 在尽可能的情况下，请在初始化时提供 map 容量大小，详细请看 指定 Map 容量提示。另外，如果 map 包含固定的元素列表，则使用 map literals(map 初始化列表) 初始化映射。 BadGood 1234m := make(map[T1]T2, 3)m[k1] = v1m[k2] = v2m[k3] = v3 12345m := map[T1]T2&#123; k1: v1, k2: v2, k3: v3,&#125; 基本准则是：在初始化时使用 map 初始化列表 来添加一组固定的元素。否则使用 make (如果可以，请尽量指定 map 容量)。 字符串 string format 如果在函数外声明Printf-style 函数的格式字符串，请将其设置为const常量。这有助于go vet对格式字符串执行静态分析。 BadGood 12msg := \"unexpected values %v, %v\\n\"fmt.Printf(msg, 1, 2) 12const msg = \"unexpected values %v, %v\\n\"fmt.Printf(msg, 1, 2) 命名 Printf 样式的函数 声明Printf-style 函数时，请确保go vet可以检测到它并检查格式字符串。 这意味着应尽可能使用预定义的Printf-style 函数名称。go vet将默认检查这些。有关更多信息，请参见 Printf 系列。 如果不能使用预定义的名称，请以 f 结束选择的名称：Wrapf，而不是Wrap。go vet可以要求检查特定的 Printf 样式名称，但名称必须以f结尾。 1$ go vet -printfuncs=wrapf,statusf 另请参阅 go vet: Printf family check. 编程模式 表驱动测试 当测试逻辑是重复的时候，通过 subtests 使用 table 驱动的方式编写 case 代码看上去会更简洁。 BadGood 123456789101112131415161718192021// func TestSplitHostPort(t *testing.T)host, port, err := net.SplitHostPort(\"192.0.2.0:8000\")require.NoError(t, err)assert.Equal(t, \"192.0.2.0\", host)assert.Equal(t, \"8000\", port)host, port, err = net.SplitHostPort(\"192.0.2.0:http\")require.NoError(t, err)assert.Equal(t, \"192.0.2.0\", host)assert.Equal(t, \"http\", port)host, port, err = net.SplitHostPort(\":8000\")require.NoError(t, err)assert.Equal(t, \"\", host)assert.Equal(t, \"8000\", port)host, port, err = net.SplitHostPort(\"1:8\")require.NoError(t, err)assert.Equal(t, \"1\", host)assert.Equal(t, \"8\", port) 12345678910111213141516171819202122232425262728293031323334353637// func TestSplitHostPort(t *testing.T)tests := []struct&#123; give string wantHost string wantPort string&#125;&#123; &#123; give: \"192.0.2.0:8000\", wantHost: \"192.0.2.0\", wantPort: \"8000\", &#125;, &#123; give: \"192.0.2.0:http\", wantHost: \"192.0.2.0\", wantPort: \"http\", &#125;, &#123; give: \":8000\", wantHost: \"\", wantPort: \"8000\", &#125;, &#123; give: \"1:8\", wantHost: \"1\", wantPort: \"8\", &#125;,&#125;for _, tt := range tests &#123; t.Run(tt.give, func(t *testing.T) &#123; host, port, err := net.SplitHostPort(tt.give) require.NoError(t, err) assert.Equal(t, tt.wantHost, host) assert.Equal(t, tt.wantPort, port) &#125;)&#125; 很明显，使用 test table 的方式在代码逻辑扩展的时候，比如新增 test case，都会显得更加的清晰。 遵循这样的约定：将结构体切片称为tests。 每个测试用例称为tt。此外，鼓励使用give和want前缀说明每个测试用例的输入和输出值。 1234567891011tests := []struct&#123; give string wantHost string wantPort string&#125;&#123; // ...&#125;for _, tt := range tests &#123; // ...&#125; 并行测试，比如一些专门的循环（例如，生成goroutine或捕获引用作为循环体的一部分的那些循环） 必须注意在循环的范围内显式地分配循环变量，以确保它们保持预期的值。 12345678910111213tests := []struct&#123; give string // ...&#125;&#123; // ...&#125;for _, tt := range tests &#123; tt := tt // for t.Parallel t.Run(tt.give, func(t *testing.T) &#123; t.Parallel() // ... &#125;)&#125; 在上面的例子中，由于下面使用了t.Parallel()，必须声明一个作用域为循环迭代的tt变量。 如果不这样做，大多数或所有测试都会收到一个意外的tt值，或者一个在运行时发生变化的值。 功能选项 功能选项是一种模式，可以在其中声明一个不透明 Option 类型，该类型在某些内部结构中记录信息。接受这些选项的可变编号，并根据内部结构上的选项记录的全部信息采取行动。 将此模式用于需要扩展的构造函数和其他公共 API 中的可选参数，尤其是在这些功能上已经具有三个或更多参数的情况下。 BadGood 123456789// package dbfunc Open( addr string, cache bool, logger *zap.Logger) (*Connection, error) &#123; // ...&#125; 123456789101112131415161718192021// package dbtype Option interface &#123; // ...&#125;func WithCache(c bool) Option &#123; // ...&#125;func WithLogger(log *zap.Logger) Option &#123; // ...&#125;// Open creates a connection.func Open( addr string, opts ...Option,) (*Connection, error) &#123; // ...&#125; 必须始终提供缓存和记录器参数，即使用户希望使用默认值。 1234db.Open(addr, db.DefaultCache, zap.NewNop())db.Open(addr, db.DefaultCache, log)db.Open(addr, false /* cache */, zap.NewNop())db.Open(addr, false /* cache */, log) 只有在需要时才提供选项。 12345678db.Open(addr)db.Open(addr, db.WithLogger(log))db.Open(addr, db.WithCache(false))db.Open( addr, db.WithCache(false), db.WithLogger(log),) 我建议实现此模式的方法是使用一个 Option 接口，该接口保存一个未导出的方法，在一个未导出的 options 结构上记录选项。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647type options struct &#123; cache bool logger *zap.Logger&#125;type Option interface &#123; apply(*options)&#125;type cacheOption boolfunc (c cacheOption) apply(opts *options) &#123; opts.cache = bool(c)&#125;func WithCache(c bool) Option &#123; return cacheOption(c)&#125;type loggerOption struct &#123; Log *zap.Logger&#125;func (l loggerOption) apply(opts *options) &#123; opts.logger = l.Log&#125;func WithLogger(log *zap.Logger) Option &#123; return loggerOption&#123;Log: log&#125;&#125;// Open creates a connection.func Open( addr string, opts ...Option,) (*Connection, error) &#123; options := options&#123; cache: defaultCache, logger: zap.NewNop(), &#125; for _, o := range opts &#123; o.apply(&amp;options) &#125; // ...&#125; 注意：还有一种使用闭包实现这个模式的方法，但是相信上面的模式为作者提供了更多的灵活性，并且更容易对用户进行调试和测试。特别是，在不可能进行比较的情况下它允许在测试和模拟中对选项进行比较。此外，它还允许选项实现其他接口，包括 fmt.Stringer，允许用户读取选项的字符串表示形式。 还可以参考下面资料： Self-referential functions and the design of options Functional options for friendly APIs Linting 比任何 “blessed” linter 集更重要的是，lint 在一个代码库中始终保持一致。 建议至少使用以下 linters，因为我认为它们有助于发现最常见的问题，并在不需要规定的情况下为代码质量建立一个高标准： errcheck 以确保错误得到处理 goimports 格式化代码和管理 imports golint 指出常见的文体错误 govet 分析代码中的常见错误 staticcheck 各种静态分析检查 Lint Runners 推荐 golangci-lint 作为 go-to lint 的运行程序，这主要是因为它在较大的代码库中的性能以及能够同时配置和使用许多规范。这个 repo 有一个示例配置文件 .golangci.yml 和推荐的 linter 设置。 golangci-lint 有 various-linters 可供使用。建议将上述 linters 作为基本 set，鼓励团队添加对他们的项目有意义的任何附加 linters。 Stargazers over time 参考链接 https://github.com/xxjwxc/uber_go_guide_cn https://golang.org/doc/effective_go.html https://github.com/golang/go/wiki/CommonMistakes https://github.com/golang/go/wiki/CodeReviewComments https://go.dev/doc/effective_go#pointers_vs_values https://go.dev/ref/spec#Method_values","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"设计模式-概述","slug":"Design Patterns/设计模式-概述","date":"2022-08-14T12:00:54.000Z","updated":"2022-08-14T12:00:54.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-概述/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A6%82%E8%BF%B0/","excerpt":"","text":"设计模式是软件设计中常见问题的典型解决方案。 它们就像能根据需求进行调整的预制蓝图， 可用于解决代码中反复出现的设计问题 模式根据其目的来分类： 创建型模式提供创建对象的机制， 增加已有代码的灵活性和可复用性 结构型模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效 行为模式负责对象间的高效沟通和职责委派 创建型模式 单例模式(Singleton Design Pattern) 简单工厂模式(Simple Factory Pattern) 工厂方法模式(Factory Method Pattern) 抽象工厂模式(Abstract Factory) 建造者模式(Builder Design Pattern) 原型模式(Prototype Design Pattern) 结构性模式 代理模式(Proxy Design Pattern) 桥接模式(Bridge Design Pattern) 装饰器模式(Decorator Design Pattern) 适配器模式(Adapter Design Pattern) 门面模式(Facade Design Pattern) 组合模式(Composite Design Pattern) 享元模式(Flyweight Design Pattern) 行为模式 观察者模式(Observer Design Pattern) 模板模式(Template Design Pattern) 策略模式(Strategy Method Design Pattern) 职责链模式(Chain Of Responsibility Design Pattern) 状态模式(State Design Pattern) 迭代器模式(Iterator Design Pattern) 访问者模式(Visitor Design Pattern) 备忘录模式(Memento Design Pattern) 命令模式(Command Design Pattern) 解释器模式(Interpreter Design Pattern) 中介模式(Mediator Design Pattern) 参考文档 https://lailin.xyz/post/go-design-pattern.html https://docs.microsoft.com/zh-cn/azure/architecture/patterns/ https://github.com/senghoo/golang-design-pattern https://github.com/mohuishou/go-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-单例模式","slug":"Design Patterns/设计模式-单例模式","date":"2022-08-14T11:11:29.000Z","updated":"2022-08-14T11:11:29.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-单例模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一个类只允许创建一个对象(或实例)，那这个类就是一个单例类，这种设计模式就叫做单例设计模式(Singleton Design Pattern) 单例模式分为 饿汉式 和 懒汉式 两种实现 饿汉式：初始化的时候已经创建好实例 懒汉式：只有在调用的时候才会初始化 构建的时候注意： 构造函数是私有访问权限(防止被其他地方重新构建) 构建的时候考虑并发情况 考虑是否支持延迟加载 饿汉式 代码实现 123456789101112131415package singleton// Singleton 饿汉式单例type Singleton struct&#123;&#125;var singleton *Singletonfunc init() &#123; singleton = &amp;Singleton&#123;&#125;&#125;// GetInstance 获取实例func GetInstance() *Singleton &#123; return singleton&#125; 单元测试 12345678910111213func TestGetInstance(t *testing.T) &#123; assert.Equal(t, singleton.GetInstance(), singleton.GetInstance()) //true&#125;func BenchmarkGetInstanceParallel(b *testing.B) &#123; b.RunParallel(func(pb *testing.PB) &#123; for pb.Next() &#123; if singleton.GetInstance() != singleton.GetInstance() &#123; b.Errorf(\"test fail\") &#125; &#125; &#125;)&#125; 性能测试结果 1BenchmarkGetInstanceParallel-8 1000000000 0.234 ns/op 懒汉式 代码实现 123456789101112131415161718192021222324// Singleton 是单例模式接口，导出的// 通过该接口可以避免 GetInstance 返回一个包私有类型的指针type Singleton interface &#123; foo()&#125;// singleton 是单例模式类，包私有的type singleton struct&#123;&#125;func (s singleton) foo() &#123;&#125;var ( instance *singleton once sync.Once)//GetInstance 用于获取单例模式对象func GetInstance() Singleton &#123; once.Do(func() &#123; instance = &amp;singleton&#123;&#125; &#125;) return instance&#125; 单元测试 12345678910111213func TestGetLazyInstance(t *testing.T) &#123; assert.Equal(t, singleton.GetLazyInstance(), singleton.GetLazyInstance())&#125;func BenchmarkGetLazyInstanceParallel(b *testing.B) &#123; b.RunParallel(func(pb *testing.PB) &#123; for pb.Next() &#123; if singleton.GetLazyInstance() != singleton.GetLazyInstance() &#123; b.Errorf(\"test fail\") &#125; &#125; &#125;)&#125; 性能测试结果 1BenchmarkGetLazyInstanceParallel-8 1000000000 0.816 ns/op 总结 可拓展性差，如果需要多实例对象可能比较麻烦，适用于单实例对象 可测试性差，因为是唯一实例，进行多场景修改实例进行测试可能会比较麻烦 参考文档 https://lailin.xyz/post/singleton.html https://github.com/senghoo/golang-design-pattern/blob/master/03_singleton/singleton.go","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-组合模式","slug":"Design Patterns/设计模式-组合模式","date":"2022-08-14T10:02:10.000Z","updated":"2022-08-14T10:02:10.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-组合模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"组合模式是一种结构型设计模式， 你可以使用它将对象组合成树状结构， 并且能像使用独立对象一样使用它们 实现代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package compositeimport \"fmt\"type Component interface &#123; Parent() Component SetParent(Component) Name() string SetName(string) AddChild(Component) Print(string)&#125;const ( LeafNode = iota CompositeNode)func NewComponent(kind int, name string) Component &#123; var c Component switch kind &#123; case LeafNode: c = NewLeaf() case CompositeNode: c = NewComposite() &#125; c.SetName(name) return c&#125;type component struct &#123; parent Component name string&#125;func (c *component) Parent() Component &#123; return c.parent&#125;func (c *component) SetParent(parent Component) &#123; c.parent = parent&#125;func (c *component) Name() string &#123; return c.name&#125;func (c *component) SetName(name string) &#123; c.name = name&#125;func (c *component) AddChild(Component) &#123;&#125;func (c *component) Print(string) &#123;&#125;type Leaf struct &#123; component&#125;func NewLeaf() *Leaf &#123; return &amp;Leaf&#123;&#125;&#125;func (c *Leaf) Print(pre string) &#123; fmt.Printf(\"%s-%s\\n\", pre, c.Name())&#125;type Composite struct &#123; component childs []Component&#125;func NewComposite() *Composite &#123; return &amp;Composite&#123; childs: make([]Component, 0), &#125;&#125;func (c *Composite) AddChild(child Component) &#123; child.SetParent(c) c.childs = append(c.childs, child)&#125;func (c *Composite) Print(pre string) &#123; fmt.Printf(\"%s+%s\\n\", pre, c.Name()) pre += \" \" for _, comp := range c.childs &#123; comp.Print(pre) &#125;&#125; 单元测试 1234567891011121314151617181920212223242526272829package compositefunc ExampleComposite() &#123; root := NewComponent(CompositeNode, \"root\") c1 := NewComponent(CompositeNode, \"c1\") c2 := NewComponent(CompositeNode, \"c2\") c3 := NewComponent(CompositeNode, \"c3\") l1 := NewComponent(LeafNode, \"l1\") l2 := NewComponent(LeafNode, \"l2\") l3 := NewComponent(LeafNode, \"l3\") root.AddChild(c1) root.AddChild(c2) c1.AddChild(c3) c1.AddChild(l1) c2.AddChild(l2) c2.AddChild(l3) root.Print(\"\") // Output: // +root // +c1 // +c3 // -l1 // +c2 // -l2 // -l3&#125; 总结 可以利用多态和递归机制更方便地使用复杂树结构。 开闭原则。 无需更改现有代码， 就可以在应用中添加新元素， 使其成为对象树的一部分 参考链接 https://refactoringguru.cn/design-patterns/composite https://lailin.xyz/post/composite.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-解释器模式","slug":"Design Patterns/设计模式-解释器模式","date":"2022-08-14T09:58:57.000Z","updated":"2022-08-14T09:58:57.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-解释器模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%A3%E9%87%8A%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package interpreterimport ( \"strconv\" \"strings\")//节点解释 接口type Node interface &#123; Interpret() int&#125;//节点type ValNode struct &#123; val int&#125;//节点的值func (n *ValNode) Interpret() int &#123; return n.val&#125;//左右节点type AddNode struct &#123; left, right Node&#125;//左右节点 解释器func (n *AddNode) Interpret() int &#123; return n.left.Interpret() + n.right.Interpret()&#125;//最小节点type MinNode struct &#123; left, right Node&#125;//最小节点解释器func (n *MinNode) Interpret() int &#123; return n.left.Interpret() - n.right.Interpret()&#125;//解释器type Parser struct &#123; exp []string index int prev Node&#125;//解析func (p *Parser) Parse(exp string) &#123; p.exp = strings.Split(exp, \" \") for &#123; if p.index &gt;= len(p.exp) &#123; return &#125; switch p.exp[p.index] &#123; case \"+\": p.prev = p.newAddNode() case \"-\": p.prev = p.newMinNode() default: p.prev = p.newValNode() &#125; &#125;&#125;func (p *Parser) newAddNode() Node &#123; p.index++ return &amp;AddNode&#123; left: p.prev, right: p.newValNode(), &#125;&#125;func (p *Parser) newMinNode() Node &#123; p.index++ return &amp;MinNode&#123; left: p.prev, right: p.newValNode(), &#125;&#125;func (p *Parser) newValNode() Node &#123; v, _ := strconv.Atoi(p.exp[p.index]) p.index++ return &amp;ValNode&#123; val: v, &#125;&#125;func (p *Parser) Result() Node &#123; return p.prev&#125; 单元测试 12345678910111213package interpreterimport \"testing\"func TestInterpreter(t *testing.T) &#123; p := &amp;Parser&#123;&#125; p.Parse(\"1 + 2 + 3 - 4 + 5 - 6\") res := p.Result().Interpret() expect := 1 if res != expect &#123; t.Fatalf(\"expect %d got %d\", expect, res) &#125;&#125; 应用场景 自定义实现一个自定义接口告警规则功能 参考链接 https://lailin.xyz/post/interpreter.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-中介模式","slug":"Design Patterns/设计模式-中介模式","date":"2022-08-14T09:46:21.000Z","updated":"2022-08-14T09:46:21.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-中介模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%B8%AD%E4%BB%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"中介者模式是一种行为设计模式， 能让你减少对象之间混乱无序的依赖关系。 该模式会限制对象之间的直接交互， 迫使它们通过一个中介者对象进行合作。 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package mediatorimport ( \"fmt\" \"strings\")//CD驱动 对象type CDDriver struct &#123; Data string&#125;//CDDriver 读取数据func (c *CDDriver) ReadData() &#123; c.Data = \"music,image\" fmt.Printf(\"CDDriver: reading data %s\\n\", c.Data) GetMediatorInstance().changed(c)&#125;//CPU 对象type CPU struct &#123; Video string Sound string&#125;//CPU 处理数据func (c *CPU) Process(data string) &#123; sp := strings.Split(data, \",\") c.Sound = sp[0] c.Video = sp[1] fmt.Printf(\"CPU: split data with Sound %s, Video %s\\n\", c.Sound, c.Video) GetMediatorInstance().changed(c)&#125;//VideoCard 驱动type VideoCard struct &#123; Data string&#125;//VideoCard 播放func (v *VideoCard) Display(data string) &#123; v.Data = data fmt.Printf(\"VideoCard: display %s\\n\", v.Data) GetMediatorInstance().changed(v)&#125;//SoundCard 对象type SoundCard struct &#123; Data string&#125;//SoundCard 播放func (s *SoundCard) Play(data string) &#123; s.Data = data fmt.Printf(\"SoundCard: play %s\\n\", s.Data) GetMediatorInstance().changed(s)&#125;//中介type Mediator struct &#123; CD *CDDriver CPU *CPU Video *VideoCard Sound *SoundCard&#125;var mediator *Mediatorfunc GetMediatorInstance() *Mediator &#123; if mediator == nil &#123; //单例模式-懒汉模式 mediator = &amp;Mediator&#123;&#125; &#125; return mediator&#125;func (m *Mediator) changed(i interface&#123;&#125;) &#123; switch inst := i.(type) &#123; case *CDDriver: m.CPU.Process(inst.Data) case *CPU: m.Sound.Play(inst.Sound) m.Video.Display(inst.Video) &#125;&#125; 单元测试 12345678910111213141516171819202122232425262728293031323334package mediatorimport \"testing\"func TestMediator(t *testing.T) &#123; mediator := GetMediatorInstance() mediator.CD = &amp;CDDriver&#123;&#125; mediator.CPU = &amp;CPU&#123;&#125; mediator.Video = &amp;VideoCard&#123;&#125; mediator.Sound = &amp;SoundCard&#123;&#125; //Tiggle mediator.CD.ReadData() if mediator.CD.Data != \"music,image\" &#123; t.Fatalf(\"CD unexpect data %s\", mediator.CD.Data) &#125; if mediator.CPU.Sound != \"music\" &#123; t.Fatalf(\"CPU unexpect sound data %s\", mediator.CPU.Sound) &#125; if mediator.CPU.Video != \"image\" &#123; t.Fatalf(\"CPU unexpect video data %s\", mediator.CPU.Video) &#125; if mediator.Video.Data != \"image\" &#123; t.Fatalf(\"VidoeCard unexpect data %s\", mediator.Video.Data) &#125; if mediator.Sound.Data != \"music\" &#123; t.Fatalf(\"SoundCard unexpect data %s\", mediator.Sound.Data) &#125;&#125; 适用场景 当一些对象和其他对象紧密耦合以致难以对其进行修改时 当组件因过于依赖其他组件而无法在不同应用中复用时，可使用中介者模式 如果为了能在不同情景下复用一些基本行为，导致需要被迫创建大量组件子类时 总结 单一职责原则。 可以将多个组件间的交流抽取到同一位置， 使其更易于理解和维护。 开闭原则。 无需修改实际组件就能增加新的中介者 可以减轻应用中多个组件间的耦合情况。 可以更方便地复用各个组件。 参考链接 https://refactoringguru.cn/design-patterns/mediator https://lailin.xyz/post/mediator.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-状态模式","slug":"Design Patterns/设计模式-状态模式","date":"2022-08-14T09:45:54.000Z","updated":"2022-08-14T09:45:54.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-状态模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"状态模式是一种行为设计模式， 让你能在一个对象的内部状态变化时改变其行为， 使其看上去就像改变了自身所属的类一样。 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596package stateimport \"fmt\"type Week interface &#123; Today() Next(*DayContext)&#125;type DayContext struct &#123; today Week&#125;func NewDayContext() *DayContext &#123; return &amp;DayContext&#123; today: &amp;Sunday&#123;&#125;, &#125;&#125;func (d *DayContext) Today() &#123; d.today.Today()&#125;func (d *DayContext) Next() &#123; d.today.Next(d)&#125;type Sunday struct&#123;&#125;func (*Sunday) Today() &#123; fmt.Printf(\"Sunday\\n\")&#125;func (*Sunday) Next(ctx *DayContext) &#123; ctx.today = &amp;Monday&#123;&#125;&#125;type Monday struct&#123;&#125;func (*Monday) Today() &#123; fmt.Printf(\"Monday\\n\")&#125;func (*Monday) Next(ctx *DayContext) &#123; ctx.today = &amp;Tuesday&#123;&#125;&#125;type Tuesday struct&#123;&#125;func (*Tuesday) Today() &#123; fmt.Printf(\"Tuesday\\n\")&#125;func (*Tuesday) Next(ctx *DayContext) &#123; ctx.today = &amp;Wednesday&#123;&#125;&#125;type Wednesday struct&#123;&#125;func (*Wednesday) Today() &#123; fmt.Printf(\"Wednesday\\n\")&#125;func (*Wednesday) Next(ctx *DayContext) &#123; ctx.today = &amp;Thursday&#123;&#125;&#125;type Thursday struct&#123;&#125;func (*Thursday) Today() &#123; fmt.Printf(\"Thursday\\n\")&#125;func (*Thursday) Next(ctx *DayContext) &#123; ctx.today = &amp;Friday&#123;&#125;&#125;type Friday struct&#123;&#125;func (*Friday) Today() &#123; fmt.Printf(\"Friday\\n\")&#125;func (*Friday) Next(ctx *DayContext) &#123; ctx.today = &amp;Saturday&#123;&#125;&#125;type Saturday struct&#123;&#125;func (*Saturday) Today() &#123; fmt.Printf(\"Saturday\\n\")&#125;func (*Saturday) Next(ctx *DayContext) &#123; ctx.today = &amp;Sunday&#123;&#125;&#125; 单元测试 12345678910111213141516171819202122package statefunc ExampleWeek() &#123; ctx := NewDayContext() todayAndNext := func() &#123; ctx.Today() ctx.Next() &#125; for i := 0; i &lt; 8; i++ &#123; todayAndNext() &#125; // Output: // Sunday // Monday // Tuesday // Wednesday // Thursday // Friday // Saturday // Sunday&#125; 适用场景 模式建议将所有特定于状态的代码抽取到一组独立的类中。 这样一来可以在独立于其他状态的情况下添加新状态或修改已有状态， 从而减少维护成本 状态模式会将这些条件语句的分支抽取到相应状态类的方法中。 同时还可以清除主要类中与特定状态相关的临时成员变量和帮手方法代码 状态模式让你能够生成状态类层次结构， 通过将公用代码抽取到抽象基类中来减少重复 总结 单一职责原则。 将与特定状态相关的代码放在单独的类中。 开闭原则。 无需修改已有状态类和上下文就能引入新状态。 通过消除臃肿的状态机条件语句简化上下文代码。 参考链接 https://refactoringguru.cn/design-patterns/state https://lailin.xyz/post/state.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-模板方法模式","slug":"Design Patterns/设计模式-模板方法模式","date":"2022-08-14T09:41:09.000Z","updated":"2022-08-14T09:41:09.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-模板方法模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"模板方法模式是一种行为设计模式， 它在超类中定义了一个算法的框架， 允许子类在不修改结构的情况下重写算法的特定步骤 举个 例子，假设现在要做一个短信推送的系统，那么需要 检查短信字数是否超过限制 检查手机号是否正确 发送短信 返回状态 可以发现，在发送短信的时候由于不同的供应商调用的接口不同，所以会有一些实现上的差异，但短信推送的算法（业务逻辑）是固定的 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package templatemethodimport \"fmt\"type Downloader interface &#123; Download(uri string)&#125;type template struct &#123; implement uri string&#125;type implement interface &#123; download() save()&#125;func newTemplate(impl implement) *template &#123; return &amp;template&#123; implement: impl, &#125;&#125;func (t *template) Download(uri string) &#123; t.uri = uri fmt.Print(\"prepare downloading\\n\") t.implement.download() t.implement.save() fmt.Print(\"finish downloading\\n\")&#125;func (t *template) save() &#123; fmt.Print(\"default save\\n\")&#125;type HTTPDownloader struct &#123; *template&#125;func NewHTTPDownloader() Downloader &#123; downloader := &amp;HTTPDownloader&#123;&#125; template := newTemplate(downloader) downloader.template = template return downloader&#125;func (d *HTTPDownloader) download() &#123; fmt.Printf(\"download %s via http\\n\", d.uri)&#125;func (*HTTPDownloader) save() &#123; fmt.Printf(\"http save\\n\")&#125;type FTPDownloader struct &#123; *template&#125;func NewFTPDownloader() Downloader &#123; downloader := &amp;FTPDownloader&#123;&#125; template := newTemplate(downloader) downloader.template = template return downloader&#125;func (d *FTPDownloader) download() &#123; fmt.Printf(\"download %s via ftp\\n\", d.uri)&#125; 单元测试 1234567891011121314151617181920212223package templatemethodfunc ExampleHTTPDownloader() &#123; var downloader Downloader = NewHTTPDownloader() downloader.Download(\"http://example.com/abc.zip\") // Output: // prepare downloading // download http://example.com/abc.zip via http // http save // finish downloading&#125;func ExampleFTPDownloader() &#123; var downloader Downloader = NewFTPDownloader() downloader.Download(\"ftp://example.com/abc.zip\") // Output: // prepare downloading // download ftp://example.com/abc.zip via ftp // default save // finish downloading&#125; 使用场景 模板方法将整个算法转换为一系列独立的步骤， 以便子类能对其进行扩展， 同时还可让超类中所定义的结构保持完整 在将算法转换为模板方法时， 你可将相似的实现步骤提取到超类中以去除重复代码。 子类间各不同的代码可继续保留在子类中 总结 可仅允许客户端重写一个大型算法中的特定部分， 使得算法其他部分修改对其所造成的影响减小。 可将重复代码提取到一个超类中","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-访问者模式","slug":"Design Patterns/设计模式-访问者模式","date":"2022-08-14T09:32:12.000Z","updated":"2022-08-14T09:32:12.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-访问者模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"访问者模式是一种行为设计模式， 它能将算法与其所作用的对象隔离开来。 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package visitorimport \"fmt\"//消费者 接口type Customer interface &#123; Accept(Visitor)&#125;//访问者 接口type Visitor interface &#123; Visit(Customer)&#125;//企业消费者type EnterpriseCustomer struct &#123; name string&#125;//消费者池type CustomerCol struct &#123; customers []Customer&#125;//添加消费者func (c *CustomerCol) Add(customer Customer) &#123; c.customers = append(c.customers, customer)&#125;//接收访问者func (c *CustomerCol) Accept(visitor Visitor) &#123; for _, customer := range c.customers &#123; customer.Accept(visitor) &#125;&#125;func NewEnterpriseCustomer(name string) *EnterpriseCustomer &#123; return &amp;EnterpriseCustomer&#123; name: name, &#125;&#125;func (c *EnterpriseCustomer) Accept(visitor Visitor) &#123; visitor.Visit(c)&#125;type IndividualCustomer struct &#123; name string&#125;func NewIndividualCustomer(name string) *IndividualCustomer &#123; return &amp;IndividualCustomer&#123; name: name, &#125;&#125;func (c *IndividualCustomer) Accept(visitor Visitor) &#123; visitor.Visit(c)&#125;type ServiceRequestVisitor struct&#123;&#125;func (*ServiceRequestVisitor) Visit(customer Customer) &#123; switch c := customer.(type) &#123; case *EnterpriseCustomer: fmt.Printf(\"serving enterprise customer %s\\n\", c.name) case *IndividualCustomer: fmt.Printf(\"serving individual customer %s\\n\", c.name) &#125;&#125;// only for enterprisetype AnalysisVisitor struct&#123;&#125;func (*AnalysisVisitor) Visit(customer Customer) &#123; switch c := customer.(type) &#123; case *EnterpriseCustomer: fmt.Printf(\"analysis enterprise customer %s\\n\", c.name) &#125;&#125; 单元测试 123456789101112131415161718192021222324package visitorfunc ExampleRequestVisitor() &#123; c := &amp;CustomerCol&#123;&#125; c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Accept(&amp;ServiceRequestVisitor&#123;&#125;) // Output: // serving enterprise customer A company // serving enterprise customer B company // serving individual customer bob&#125;func ExampleAnalysis() &#123; c := &amp;CustomerCol&#123;&#125; c.Add(NewEnterpriseCustomer(\"A company\")) c.Add(NewIndividualCustomer(\"bob\")) c.Add(NewEnterpriseCustomer(\"B company\")) c.Accept(&amp;AnalysisVisitor&#123;&#125;) // Output: // analysis enterprise customer A company // analysis enterprise customer B company&#125; 适用场景 访问者模式通过在访问者对象中为多个目标类提供相同操作的变体， 让你能在属于不同类的一组对象上执行同一操作 该模式会将所有非主要的行为抽取到一组访问者类中， 使得程序的主要类能更专注于主要的工作 可将该行为抽取到单独的访问者类中， 只需实现接收相关类的对象作为参数的访问者方法并将其他方法留空即可。 总结 开闭原则。 可以引入在不同类对象上执行的新行为， 且无需对这些类做出修改。 单一职责原则。 可将同一行为的不同版本移到同一个类中。 访问者对象可以在与各种对象交互时收集一些有用的信息。 当你想要遍历一些复杂的对象结构 （例如对象树）， 并在结构中的每个对象上应用访问者时， 这些信息可能会有所帮助 参考链接 https://refactoringguru.cn/design-patterns/visitor https://lailin.xyz/post/visitor.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-策略模式","slug":"Design Patterns/设计模式-策略模式","date":"2022-08-14T09:14:10.000Z","updated":"2022-08-14T09:14:10.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-策略模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"策略模式是一种行为设计模式， 它能让你定义一系列算法， 并将每种算法分别放入独立的类中， 以使算法的对象能够相互替换 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package strategyimport \"fmt\"//支付结构体type Payment struct &#123; context *PaymentContext strategy PaymentStrategy&#125;//支付上下文type PaymentContext struct &#123; Name, CardID string Money int&#125;//构建新的支付对象func NewPayment(name, cardid string, money int, strategy PaymentStrategy) *Payment &#123; return &amp;Payment&#123; context: &amp;PaymentContext&#123; Name: name, CardID: cardid, Money: money, &#125;, strategy: strategy, &#125;&#125;//根据策略进行支付func (p *Payment) Pay() &#123; p.strategy.Pay(p.context)&#125;type PaymentStrategy interface &#123; Pay(*PaymentContext)&#125;type Cash struct&#123;&#125;func (*Cash) Pay(ctx *PaymentContext) &#123; fmt.Printf(\"Pay $%d to %s by cash\", ctx.Money, ctx.Name)&#125;type Bank struct&#123;&#125;func (*Bank) Pay(ctx *PaymentContext) &#123; fmt.Printf(\"Pay $%d to %s by bank account %s\", ctx.Money, ctx.Name, ctx.CardID)&#125; 单元测试 123456789101112131415package strategyfunc ExamplePayByCash() &#123; payment := NewPayment(\"Ada\", \"\", 123, &amp;Cash&#123;&#125;) payment.Pay() // Output: // Pay $123 to Ada by cash&#125;func ExamplePayByBank() &#123; payment := NewPayment(\"Bob\", \"0002\", 888, &amp;Bank&#123;&#125;) payment.Pay() // Output: // Pay $888 to Bob by bank account 0002&#125; 适用场景 当想使用对象中各种不同的算法变体，并希望能在运行时切换算法时 当有许多仅在执行某些行为时略有不同的相似类时 如果算法在上下文的逻辑中不是特别重要，使用该模式能将类的业务逻辑与其算法实现细节隔离开来 当类中使用了复杂条件运算符以在同一算法的不同变体中切换时 总结 可以在运行时切换对象内的算法。 可以将算法的实现和使用算法的代码隔离开来。 可以使用组合来代替继承。 开闭原则。 无需对上下文进行修改就能够引入新的策略 参考链接 https://refactoringguru.cn/design-patterns/strategy https://lailin.xyz/post/strategy.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-观察者模式","slug":"Design Patterns/设计模式-观察者模式","date":"2022-08-14T08:44:02.000Z","updated":"2022-08-14T08:44:02.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-观察者模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"观察者模式是一种行为设计模式，允许你定义一种订阅机制，可在对象事件发生时通知多个“观察” 该对象的其他对象 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package observerimport \"fmt\"//对象type Subject struct &#123; observers []Observer //观察者 context string&#125;func NewSubject() *Subject &#123; return &amp;Subject&#123; observers: make([]Observer, 0), &#125;&#125;//增加观察者func (s *Subject) Attach(o Observer) &#123; s.observers = append(s.observers, o)&#125;//通知观察者func (s *Subject) notify() &#123; for _, o := range s.observers &#123; o.Update(s) &#125;&#125;//更新上下文func (s *Subject) UpdateContext(context string) &#123; s.context = context s.notify()&#125;//订阅接口type Observer interface &#123; Update(*Subject)&#125;type Reader struct &#123; name string&#125;func NewReader(name string) *Reader &#123; return &amp;Reader&#123; name: name, &#125;&#125;//接收func (r *Reader) Update(s *Subject) &#123; fmt.Printf(\"%s receive %s\\n\", r.name, s.context)&#125; 单元测试 1234567891011121314151617package observerfunc ExampleObserver() &#123; subject := NewSubject() reader1 := NewReader(\"reader1\") reader2 := NewReader(\"reader2\") reader3 := NewReader(\"reader3\") subject.Attach(reader1) subject.Attach(reader2) subject.Attach(reader3) subject.UpdateContext(\"observer mode\") // Output: // reader1 receive observer mode // reader2 receive observer mode // reader3 receive observer mode&#125; 适用场景 当一个对象状态的改变需要改变其他对象，或实际对象是事先未知的或动态变化的时 当应用中的一些对象必须观察其他对象时，但仅能在有限时间内或特定情况下使用 总结 开闭原则。 无需修改发布者代码就能引入新的订阅者类 (如果是发布者接口则可轻松引入发布者类)，channel 也存在类似功能。 可以在运行时建立对象之间的联系。 参考链接 https://refactoringguru.cn/design-patterns/observer https://lailin.xyz/post/observer.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-备忘录模式","slug":"Design Patterns/设计模式-备忘录模式","date":"2022-08-14T08:23:49.000Z","updated":"2022-08-14T08:23:49.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-备忘录模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%87%E5%BF%98%E5%BD%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"备忘录模式是一种行为设计模式，允许在不暴露对象实现细节的情况下保存和恢复对象之前的状态。 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940package mementoimport \"fmt\"//备忘录 接口type Memento interface&#123;&#125;//Game 结构type Game struct &#123; hp, mp int&#125;// game 备忘录(拥有相同的接口)type gameMemento struct &#123; hp, mp int&#125;func (g *Game) Play(mpDelta, hpDelta int) &#123; g.mp += mpDelta g.hp += hpDelta&#125;//返回 Game 内部成员func (g *Game) Save() Memento &#123; return &amp;gameMemento&#123; hp: g.hp, mp: g.mp, &#125;&#125;//保存 备忘录内容func (g *Game) Load(m Memento) &#123; gm := m.(*gameMemento) g.mp = gm.mp g.hp = gm.hp&#125;func (g *Game) Status() &#123; fmt.Printf(\"Current HP:%d, MP:%d\\n\", g.hp, g.mp)&#125; 单元测试 12345678910111213141516171819202122package mementofunc ExampleGame() &#123; game := &amp;Game&#123; hp: 10, mp: 10, &#125; game.Status() progress := game.Save() game.Play(-2, -3) game.Status() game.Load(progress) game.Status() // Output: // Current HP:10, MP:10 // Current HP:7, MP:8 // Current HP:10, MP:10&#125; 适用场景 当需要创建对象状态快照来恢复其之前的状态时 当直接访问对象的成员变量、获取器或设置器将导致封装被突破时 总结 可以在不破坏对象封装情况的前提下创建对象状态快照 可以通过让负责人维护原发器状态历史记录来简化原发器代码 参考链接 https://refactoringguru.cn/design-patterns/memento https://lailin.xyz/post/memento.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-迭代器模式","slug":"Design Patterns/设计模式-迭代器模式","date":"2022-08-14T07:51:26.000Z","updated":"2022-08-14T07:51:26.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-迭代器模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"迭代器模式是一种行为设计模式， 让你能在不暴露集合底层表现形式 （列表、 栈和树等） 的情况下遍历集合中所有的元素 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package iteratorimport \"fmt\"//聚合type Aggregate interface &#123; Iterator() Iterator&#125;//迭代器type Iterator interface &#123; First() //第一个 IsDone() bool //是否结束 Next() interface&#123;&#125; //下一个&#125;//数量type Numbers struct &#123; start, end int //开始与结束&#125;func NewNumbers(start, end int) *Numbers &#123; return &amp;Numbers&#123; start: start, end: end, &#125;&#125;type NumbersIterator struct &#123; numbers *Numbers next int&#125;//迭代器func (n *Numbers) Iterator() Iterator &#123; return &amp;NumbersIterator&#123; numbers: n, next: n.start, &#125;&#125;//第一个func (i *NumbersIterator) First() &#123; i.next = i.numbers.start //第一个&#125;//是否结束func (i *NumbersIterator) IsDone() bool &#123; return i.next &gt; i.numbers.end&#125;//下一个func (i *NumbersIterator) Next() interface&#123;&#125; &#123; if !i.IsDone() &#123; next := i.next i.next++ return next &#125; return nil&#125;func IteratorPrint(i Iterator) &#123; for i.First(); !i.IsDone(); &#123; c := i.Next() fmt.Printf(\"%#v\\n\", c) &#125;&#125; 单元测试 12345678910111213141516171819package iteratorfunc ExampleIterator() &#123; var aggregate Aggregate aggregate = NewNumbers(1, 10) IteratorPrint(aggregate.Iterator()) // Output: // 1 // 2 // 3 // 4 // 5 // 6 // 7 // 8 // 9 // 10&#125; 适用场景 集合背后为复杂的数据结构且希望对客户端隐藏其复杂性时 使用该模式可以减少程序中重复的遍历代码 希望代码能够遍历不同的甚至是无法预知的数据结构 总结 单一职责原则。 通过将体积庞大的遍历算法代码抽取为独立的类， 对客户端代码和集合进行整理。 开闭原则。 可实现新型的集合和迭代器并将其传递给现有代码， 无需修改现有代码。 可以并行遍历同一集合， 因为每个迭代器对象都包含其自身的遍历状态。 相似的， 可以暂停遍历并在需要时继续。 参考链接 https://refactoringguru.cn/design-patterns/iterator https://lailin.xyz/post/iterator.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-命令模式","slug":"Design Patterns/设计模式-命令模式","date":"2022-08-14T07:39:18.000Z","updated":"2022-08-14T07:39:18.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-命令模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"命令模式是一种行为设计模式， 它可将请求转换为一个包含与请求相关的所有信息的独立对象。 该转换让你能根据不同的请求将方法参数化、 延迟请求执行或将其放入队列中， 且能实现可撤销操作 实现代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package commandimport \"fmt\"//命令接口type Command interface &#123; Execute()&#125;type StartCommand struct &#123; mb *MotherBoard&#125;func NewStartCommand(mb *MotherBoard) *StartCommand &#123; return &amp;StartCommand&#123; mb: mb, &#125;&#125;func (c *StartCommand) Execute() &#123; c.mb.Start()&#125;//重启命令type RebootCommand struct &#123; mb *MotherBoard&#125;func NewRebootCommand(mb *MotherBoard) *RebootCommand &#123; return &amp;RebootCommand&#123; mb: mb, &#125;&#125;func (c *RebootCommand) Execute() &#123; c.mb.Reboot()&#125;type MotherBoard struct&#123;&#125;func (*MotherBoard) Start() &#123; fmt.Print(\"system starting\\n\")&#125;func (*MotherBoard) Reboot() &#123; fmt.Print(\"system rebooting\\n\")&#125;//Box 包含两个命令type Box struct &#123; button1 Command button2 Command&#125;func NewBox(button1, button2 Command) *Box &#123; return &amp;Box&#123; button1: button1, button2: button2, &#125;&#125;func (b *Box) PressButton1() &#123; b.button1.Execute()&#125;func (b *Box) PressButton2() &#123; b.button2.Execute()&#125; 单元测试 1234567891011121314151617181920package commandfunc ExampleCommand() &#123; mb := &amp;MotherBoard&#123;&#125; startCommand := NewStartCommand(mb) rebootCommand := NewRebootCommand(mb) box1 := NewBox(startCommand, rebootCommand) box1.PressButton1() box1.PressButton2() box2 := NewBox(rebootCommand, startCommand) box2.PressButton1() box2.PressButton2() // Output: // system starting // system rebooting // system rebooting // system starting&#125; 适用场景 通过操作来参数化对象 想要将操作放入队列中、操作的执行或者远程执行操作 想要实现操作回滚功能 总结 单一职责原则。 可以解耦触发和执行操作的类 开闭原则。 可以在不修改已有客户端代码的情况下在程序中创建新的命令 可以实现撤销和恢复功能 可以实现操作的延迟执行 可以将一组简单命令组合成一个复杂命令 参考链接 https://refactoringguru.cn/design-patterns/command https://lailin.xyz/post/command.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-职责链模式","slug":"Design Patterns/设计模式-职责链模式","date":"2022-08-14T07:26:20.000Z","updated":"2022-08-14T07:26:20.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-职责链模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"责任链模式是一种行为设计模式， 允许你将请求沿着处理者链进行发送。 收到请求后， 每个处理者均可对请求进行处理， 或将其传递给链上的下个处理者 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package chainimport \"fmt\"type Manager interface &#123; HaveRight(money int) bool HandleFeeRequest(name string, money int) bool&#125;type RequestChain struct &#123; Manager successor *RequestChain&#125;func (r *RequestChain) SetSuccessor(m *RequestChain) &#123; r.successor = m&#125;func (r *RequestChain) HandleFeeRequest(name string, money int) bool &#123; if r.Manager.HaveRight(money) &#123; return r.Manager.HandleFeeRequest(name, money) &#125; if r.successor != nil &#123; return r.successor.HandleFeeRequest(name, money) &#125; return false&#125;func (r *RequestChain) HaveRight(money int) bool &#123; return true&#125;//----------- ProjectManager -------------type ProjectManager struct&#123;&#125;func NewProjectManagerChain() *RequestChain &#123; return &amp;RequestChain&#123; Manager: &amp;ProjectManager&#123;&#125;, &#125;&#125;func (*ProjectManager) HaveRight(money int) bool &#123; return money &lt; 500&#125;func (*ProjectManager) HandleFeeRequest(name string, money int) bool &#123; if name == \"bob\" &#123; fmt.Printf(\"Project manager permit %s %d fee request\\n\", name, money) return true &#125; fmt.Printf(\"Project manager don't permit %s %d fee request\\n\", name, money) return false&#125;//---------- DepManager ------------------type DepManager struct&#123;&#125;func NewDepManagerChain() *RequestChain &#123; return &amp;RequestChain&#123; Manager: &amp;DepManager&#123;&#125;, &#125;&#125;func (*DepManager) HaveRight(money int) bool &#123; return money &lt; 5000&#125;func (*DepManager) HandleFeeRequest(name string, money int) bool &#123; if name == \"tom\" &#123; fmt.Printf(\"Dep manager permit %s %d fee request\\n\", name, money) return true &#125; fmt.Printf(\"Dep manager don't permit %s %d fee request\\n\", name, money) return false&#125;//---------- GeneralManager ------------------type GeneralManager struct&#123;&#125;func NewGeneralManagerChain() *RequestChain &#123; return &amp;RequestChain&#123; Manager: &amp;GeneralManager&#123;&#125;, &#125;&#125;func (*GeneralManager) HaveRight(money int) bool &#123; return true&#125;func (*GeneralManager) HandleFeeRequest(name string, money int) bool &#123; if name == \"ada\" &#123; fmt.Printf(\"General manager permit %s %d fee request\\n\", name, money) return true &#125; fmt.Printf(\"General manager don't permit %s %d fee request\\n\", name, money) return false&#125; 单元测试 1234567891011121314151617181920212223package chainfunc ExampleChain() &#123; c1 := NewProjectManagerChain() c2 := NewDepManagerChain() c3 := NewGeneralManagerChain() c1.SetSuccessor(c2) c2.SetSuccessor(c3) var c Manager = c1 c.HandleFeeRequest(\"bob\", 400) c.HandleFeeRequest(\"tom\", 1400) c.HandleFeeRequest(\"ada\", 10000) c.HandleFeeRequest(\"floar\", 400) // Output: // Project manager permit bob 400 fee request // Dep manager permit tom 1400 fee request // General manager permit ada 10000 fee request // Project manager don't permit floar 400 fee request&#125; 还有一种责任链的方式 12345678910111213141516171819202122232425262728293031type check struct &#123; err error&#125;func (c check) checkName(name string) check &#123; if c.err != nil &#123; if len(name) &lt; 0 &#123; c.err = errors.New(\"name length wrong\") return c &#125; &#125; return check&#123;&#125;&#125;func (c check) checkAge(age uint8) check &#123; if c.err != nil &#123; if age &gt; 200 &#123; c.err = errors.New(\"age wrong\") return c &#125; &#125; return check&#123;&#125;&#125;func main() &#123; c := check&#123;&#125; c := c.checkName(\"hello\").checkAge(18) if c.err != nil &#123; //.... &#125;&#125; 适用场景 当程序需要使用不同方式处理不同种类请求，而且请求类型和顺序预先未知时 当必须按顺序执行多个处理者时 如果所需处理者及其顺序必须在运行时进行改变 总结 可以控制请求处理的顺序 单一职责原则。 可对发起操作和执行操作的类进行解耦 开闭原则。 可以在不更改现有代码的情况下在程序中新增处理者 参考链接 https://refactoringguru.cn/design-patterns/chain-of-responsibility https://lailin.xyz/post/chain.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-享元模式","slug":"Design Patterns/设计模式-享元模式","date":"2022-08-14T06:51:43.000Z","updated":"2022-08-14T06:51:43.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-享元模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"享元模式是一种结构型设计模式， 它摒弃了在每个对象中保存所有数据的方式， 通过共享多个对象所共有的相同状态， 让你能在有限的内存容量中载入更多对象 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package flyweightimport \"fmt\"//图片享元工厂type ImageFlyweightFactory struct &#123; maps map[string]*ImageFlyweight&#125;var imageFactory *ImageFlyweightFactory//获取图片工厂func GetImageFlyweightFactory() *ImageFlyweightFactory &#123; if imageFactory == nil &#123; imageFactory = &amp;ImageFlyweightFactory&#123; maps: make(map[string]*ImageFlyweight), &#125; &#125; return imageFactory&#125;//获取具体对象func (f *ImageFlyweightFactory) Get(filename string) *ImageFlyweight &#123; image := f.maps[filename] if image == nil &#123; image = NewImageFlyweight(filename) f.maps[filename] = image &#125; return image&#125;//type ImageFlyweight struct &#123; data string&#125;func NewImageFlyweight(filename string) *ImageFlyweight &#123; // Load image file data := fmt.Sprintf(\"image data %s\", filename) return &amp;ImageFlyweight&#123; data: data, &#125;&#125;func (i *ImageFlyweight) Data() string &#123; return i.data&#125;type ImageViewer struct &#123; *ImageFlyweight&#125;func NewImageViewer(filename string) *ImageViewer &#123; image := GetImageFlyweightFactory().Get(filename) return &amp;ImageViewer&#123; ImageFlyweight: image, &#125;&#125;func (i *ImageViewer) Display() &#123; fmt.Printf(\"Display: %s\\n\", i.Data())&#125; 单元测试 12345678910111213141516171819package flyweightimport \"testing\"func ExampleFlyweight() &#123; viewer := NewImageViewer(\"image1.png\") viewer.Display() // Output: // Display: image data image1.png&#125;func TestFlyweight(t *testing.T) &#123; viewer1 := NewImageViewer(\"image1.png\") viewer2 := NewImageViewer(\"image1.png\") if viewer1.ImageFlyweight != viewer2.ImageFlyweight &#123; t.Fail() &#125;&#125; 总结 仅在程序必须支持大量对象且没有足够的内存容量时使用享元模式(类似与内存池，但是这个重复利用的不是分配的内存而是对象) 参考链接 https://refactoringguru.cn/design-patterns/flyweight https://lailin.xyz/post/flyweight.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-门面模式","slug":"Design Patterns/设计模式-门面模式","date":"2022-08-14T06:40:52.000Z","updated":"2022-08-14T06:40:52.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-门面模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%97%A8%E9%9D%A2%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"**门面模式(外观模式)**是一种结构型设计模式， 能为程序库、 框架或其他复杂类提供一个简单的接口。 问题描述：假设必须在代码中使用某个复杂的库或框架中的众多对象。 正常情况下，需要负责所有对象的初始化工作、 管理其依赖关系并按正确的顺序执行方法等。最终， 程序中类的业务逻辑将与第三方类的实现细节紧密耦合， 使得理解和维护代码的工作很难进行。 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package facadeimport \"fmt\"func NewAPI() API &#123; return &amp;apiImpl&#123; a: NewAModuleAPI(), b: NewBModuleAPI(), &#125;&#125;//API is facade interface of facade packagetype API interface &#123; Test() string&#125;//facade implementtype apiImpl struct &#123; a AModuleAPI b BModuleAPI&#125;func (a *apiImpl) Test() string &#123; aRet := a.a.TestA() bRet := a.b.TestB() return fmt.Sprintf(\"%s\\n%s\", aRet, bRet)&#125;//NewAModuleAPI return new AModuleAPIfunc NewAModuleAPI() AModuleAPI &#123; return &amp;aModuleImpl&#123;&#125;&#125;//AModuleAPI ...type AModuleAPI interface &#123; TestA() string&#125;type aModuleImpl struct&#123;&#125;func (*aModuleImpl) TestA() string &#123; return \"A module running\"&#125;//NewBModuleAPI return new BModuleAPIfunc NewBModuleAPI() BModuleAPI &#123; return &amp;bModuleImpl&#123;&#125;&#125;//BModuleAPI ...type BModuleAPI interface &#123; TestB() string&#125;type bModuleImpl struct&#123;&#125;func (*bModuleImpl) TestB() string &#123; return \"B module running\"&#125; 单元测试 1234567891011121314package facadeimport \"testing\"var expect = \"A module running\\nB module running\"// TestFacadeAPI ...func TestFacadeAPI(t *testing.T) &#123; api := NewAPI() ret := api.Test() if ret != expect &#123; t.Fatalf(\"expect %s, return %s\", expect, ret) &#125;&#125; 适用场景 如果需要一个指向复杂子系统的直接接口，且该接口的功能有限，则可以使用外观模式 如果需要将子系统组织为多层结构，可以使用外观 参考链接 https://refactoringguru.cn/design-patterns/facade https://lailin.xyz/post/facade.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-适配器模式","slug":"Design Patterns/设计模式-适配器模式","date":"2022-08-14T06:15:31.000Z","updated":"2022-08-14T06:15:31.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-适配器模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"适配器模式是一种结构型设计模式， 它能使接口不兼容的对象能够相互合作。 适配器模式通过封装对象将复杂的转换过程隐藏于幕后， 被封装的对象甚至察觉不到适配器的存在。 其还有助于采用不同接口的对象之间的合作。 它的运作方式如下： 适配器实现与其中一个现有对象兼容的接口。 现有对象可以使用该接口安全地调用适配器方法。 适配器方法被调用后将以另一个对象兼容的格式和顺序将请求传递给该对象 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041package adapter//Target 是适配的目标接口type Target interface &#123; Request() string&#125;//Adaptee 是被适配的目标接口type Adaptee interface &#123; SpecificRequest() string&#125;//NewAdaptee 是被适配接口的工厂函数func NewAdaptee() Adaptee &#123; return &amp;adapteeImpl&#123;&#125;&#125;//AdapteeImpl 是被适配的目标类type adapteeImpl struct&#123;&#125;//SpecificRequest 是目标类的一个方法func (*adapteeImpl) SpecificRequest() string &#123; return \"adaptee method\"&#125;//NewAdapter 是Adapter的工厂函数func NewAdapter(adaptee Adaptee) Target &#123; return &amp;adapter&#123; Adaptee: adaptee, &#125;&#125;//Adapter 是转换Adaptee为Target接口的适配器type adapter struct &#123; Adaptee&#125;//Request 实现Target接口func (a *adapter) Request() string &#123; return a.SpecificRequest()&#125; 单元测试 1234567891011121314package adapterimport \"testing\"var expect = \"adaptee method\"func TestAdapter(t *testing.T) &#123; adaptee := NewAdaptee() target := NewAdapter(adaptee) res := target.Request() if res != expect &#123; t.Fatalf(\"expect: %s, actual: %s\", expect, res) &#125;&#125; 适用场景 当你希望使用某个类，但是其接口与其他代码不兼容时，可以使用适配器类 如果您需要复用这样一些类，他们处于同一个继承体系，并且他们又有了额外的一些共同的方法，但是这些共同的方法不是所有在这一继承体系中的子类所具有的共性 总结 单一职责原则你可以将接口或数据转换代码从程序主要业务逻辑中分离。 开闭原则。 只要客户端代码通过客户端接口与适配器进行交互， 你就能在不修改现有客户端代码的情况下在程序中添加新类型的适配器 参考链接 https://lailin.xyz/post/adapter.html https://refactoringguru.cn/design-patterns/adapter","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-装饰模式","slug":"Design Patterns/设计模式-装饰模式","date":"2022-08-14T06:04:06.000Z","updated":"2022-08-14T06:04:06.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-装饰模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"装饰模式是一种结构型设计模式， 允许你通过将对象放入包含行为的特殊封装对象中来为原对象绑定新的行为。 问题描述 更改一个对象的行为时，不能忽视继承可能引发的几个严重问题 继承是静态的。 无法在运行时更改已有对象的行为， 只能使用由不同子类创建的对象替代当前的整个对象。 子类只能有一个父类。 大部分编程语言不允许一个类同时继承多个类的行为 装饰模式使用对象组合的方式动态改变或增加对象行为，Go语言借助于匿名组合和非入侵式接口可以很方便实现装饰模式。使用匿名组合，在装饰器中不必显式定义转调原对象方法 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package decorator//组件type Component interface &#123; Calc() int&#125;//具体的组件type ConcreteComponent struct&#123;&#125;func (*ConcreteComponent) Calc() int &#123; return 0&#125;//乘法装饰type MulDecorator struct &#123; Component num int&#125;func WarpMulDecorator(c Component, num int) Component &#123; return &amp;MulDecorator&#123; Component: c, num: num, &#125;&#125;//乘法装饰实现接口func (d *MulDecorator) Calc() int &#123; return d.Component.Calc() * d.num&#125;//加法装饰type AddDecorator struct &#123; Component num int&#125;func WarpAddDecorator(c Component, num int) Component &#123; return &amp;AddDecorator&#123; Component: c, num: num, &#125;&#125;//实现相同的接口func (d *AddDecorator) Calc() int &#123; return d.Component.Calc() + d.num&#125; 单元测试 1234567891011121314package decoratorimport \"fmt\"func ExampleDecorator() &#123; var c Component = &amp;ConcreteComponent&#123;&#125; c = WarpAddDecorator(c, 10) c = WarpMulDecorator(c, 8) res := c.Calc() fmt.Printf(\"res %d\\n\", res) // Output: // res 80&#125; 适用场景 如果希望在无需修改代码的情况下即可使用对象，且希望在运行时为对象新增额外的行为，可以使用装饰模式 如果用继承来扩展对象行为的方案难以实现或者根本不可行，你可以使用该模式 总结 无需创建新子类即可扩展对象的行为。 可以在运行时添加或删除对象的功能。 可以用多个装饰封装对象来组合几种行为。 单一职责原则。 可以将实现了许多不同行为的一个大类拆分为多个较小的类 参考链接 https://refactoringguru.cn/design-patterns/decorator https://lailin.xyz/post/decorator.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-桥接模式","slug":"Design Patterns/设计模式-桥接模式","date":"2022-08-14T05:24:17.000Z","updated":"2022-08-14T05:24:17.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-桥接模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"桥接模式是一种结构型设计模式， 可将一个大类或一系列紧密相关的类拆分为抽象和实现两个独立的层次结构，从而能在开发时分别使用。桥接模式类似于策略模式，区别在于策略模式封装一系列算法使得算法可以互相替换。 策略模式使抽象部分和实现部分分离，可以独立变化。 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package bridgeimport \"fmt\"//抽象消息接口type AbstractMessage interface &#123; SendMessage(text, to string)&#125;//实现消息接口type MessageImplementer interface &#123; Send(text, to string)&#125;//短信消息type MessageSMS struct&#123;&#125;//虚拟短信消息接口func ViaSMS() MessageImplementer &#123; return &amp;MessageSMS&#123;&#125;&#125;//短信消息实现func (*MessageSMS) Send(text, to string) &#123; fmt.Printf(\"send %s to %s via SMS\", text, to)&#125;//邮件消息type MessageEmail struct&#123;&#125;//虚拟邮件消息接口func ViaEmail() MessageImplementer &#123; return &amp;MessageEmail&#123;&#125;&#125;//邮件消息实现短信接口func (*MessageEmail) Send(text, to string) &#123; fmt.Printf(\"send %s to %s via Email\", text, to)&#125;//公共消息type CommonMessage struct &#123; method MessageImplementer&#125;func NewCommonMessage(method MessageImplementer) *CommonMessage &#123; return &amp;CommonMessage&#123; method: method, &#125;&#125;func (m *CommonMessage) SendMessage(text, to string) &#123; m.method.Send(text, to)&#125;//紧急消息type UrgencyMessage struct &#123; method MessageImplementer //消息接口&#125;func NewUrgencyMessage(method MessageImplementer) *UrgencyMessage &#123; return &amp;UrgencyMessage&#123; method: method, &#125;&#125;func (m *UrgencyMessage) SendMessage(text, to string) &#123; m.method.Send(fmt.Sprintf(\"[Urgency] %s\", text), to)&#125; 单元测试 1234567891011121314151617181920212223242526272829package bridgefunc ExampleCommonSMS() &#123; m := NewCommonMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via SMS&#125;func ExampleCommonEmail() &#123; m := NewCommonMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send have a drink? to bob via Email&#125;func ExampleUrgencySMS() &#123; m := NewUrgencyMessage(ViaSMS()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via SMS&#125;func ExampleUrgencyEmail() &#123; m := NewUrgencyMessage(ViaEmail()) m.SendMessage(\"have a drink?\", \"bob\") // Output: // send [Urgency] have a drink? to bob via Email&#125; 适用场景 想要拆分或重组一个具有多重功能的庞杂类(例如能与多个数据库服务器进行交互的类)，可以使用桥接模式 希望在几个独立维度上扩展一个类 需要在运行时切换不同实现方法 总结 开闭原则。 你可以新增抽象部分和实现部分， 且它们之间不会相互影响。 单一职责原则。 抽象部分专注于处理高层逻辑， 实现部分处理平台细节 对高内聚的类使用该模式可能会让代码更加复杂。 参考链接 https://refactoringguru.cn/design-patterns/bridge https://lailin.xyz/post/bridge.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-代理模式","slug":"Design Patterns/设计模式-代理模式","date":"2022-08-14T04:40:41.000Z","updated":"2022-08-14T04:40:41.000Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-代理模式/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"代理模式 建议新建一个与原服务对象接口相同的代理类， 然后更新应用以将代理对象传递给所有原始对象客户端。 代理类接收到客户端请求后会创建实际的服务对象， 并将所有工作委派给它。 代码实现 123456789101112131415161718192021222324252627282930package proxytype Subject interface &#123; Do() string&#125;type RealSubject struct&#123;&#125;func (RealSubject) Do() string &#123; return \"real\"&#125;type Proxy struct &#123; real RealSubject&#125;func (p Proxy) Do() string &#123; var res string // 在调用真实对象之前的工作，检查缓存，判断权限，实例化真实对象等。。 res += \"pre:\" // 调用真实对象 res += p.real.Do() // 调用之后的操作，如缓存结果，对结果进行处理等。。 res += \":after\" return res&#125; 单元测试 1234567891011121314package proxyimport \"testing\"func TestProxy(t *testing.T) &#123; var sub Subject sub = &amp;Proxy&#123;&#125; res := sub.Do() if res != \"pre:real:after\" &#123; t.Fail() &#125;&#125; 适用场景 延迟初始化， 如果有一个偶尔使用的重量级服务对象，一直保持该对象运行会消耗系统资源时使用代理模式(类似单例模式，但单例模式强调的是单实例) 访问控制(保护代理)，如果你只希望特定客户端使用服务对象，这里的对象可以是操作系统中非常重要的部分，而客户端则是各种已启动的程序(包括恶意程序)，此时可使用代理模式 总结 可以在客户端毫无察觉的情况下控制服务对象 如果客户端对服务对象的生命周期没有特殊要求， 你可以对生命周期进行管理 即使服务对象还未准备好或不存在， 代理也可以正常工作 可以在不对服务或客户端做出修改的情况下创建新代理 代码可能会变得复杂， 因为需要新建许多类。 服务响应可能会延迟(经过了一层搭理) 参考链接 https://lailin.xyz/post/factory.html https://refactoringguru.cn/design-patterns/proxy","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-建造者模式","slug":"Design Patterns/设计模式-建造者模式","date":"2022-08-13T14:07:54.000Z","updated":"2022-08-13T14:07:54.000Z","comments":true,"path":"2022/08/13/Design Patterns/设计模式-建造者模式/","link":"","permalink":"http://xboom.github.io/2022/08/13/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"建造者模式 该模式允许你使用相同的创建代码生成不同类型和形式的对象 假设有这样一个复杂对象， 在对其进行构造时需要对诸多成员变量和嵌套对象进行繁复的初始化工作。 这些初始化代码通常深藏于一个包含众多参数且让人基本看不懂的构造函数中；甚至还有更糟糕的情况， 那就是这些代码散落在客户端代码的多个位置 代码实现 普通负载对象构建方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596package builderimport \"fmt\"const ( defaultMaxTotal = 10 defaultMaxIdle = 9 defaultMinIdle = 1)// ResourcePoolConfig resource pooltype ResourcePoolConfig struct &#123; name string maxTotal int maxIdle int minIdle int&#125;// ResourcePoolConfigBuilder 用于构建 ResourcePoolConfigtype ResourcePoolConfigBuilder struct &#123; name string maxTotal int maxIdle int minIdle int&#125;// SetName SetNamefunc (b *ResourcePoolConfigBuilder) SetName(name string) error &#123; if name == \"\" &#123; return fmt.Errorf(\"name can not be empty\") &#125; b.name = name return nil&#125;// SetMinIdle SetMinIdlefunc (b *ResourcePoolConfigBuilder) SetMinIdle(minIdle int) error &#123; if minIdle &lt; 0 &#123; return fmt.Errorf(\"max tatal cannot &lt; 0, input: %d\", minIdle) &#125; b.minIdle = minIdle return nil&#125;// SetMaxIdle SetMaxIdlefunc (b *ResourcePoolConfigBuilder) SetMaxIdle(maxIdle int) error &#123; if maxIdle &lt; 0 &#123; return fmt.Errorf(\"max tatal cannot &lt; 0, input: %d\", maxIdle) &#125; b.maxIdle = maxIdle return nil&#125;// SetMaxTotal SetMaxTotalfunc (b *ResourcePoolConfigBuilder) SetMaxTotal(maxTotal int) error &#123; if maxTotal &lt;= 0 &#123; return fmt.Errorf(\"max tatal cannot &lt;= 0, input: %d\", maxTotal) &#125; b.maxTotal = maxTotal return nil&#125;// Build Buildfunc (b *ResourcePoolConfigBuilder) Build() (*ResourcePoolConfig, error) &#123; if b.name == \"\" &#123; return nil, fmt.Errorf(\"name can not be empty\") &#125; // 设置默认值 if b.minIdle == 0 &#123; b.minIdle = defaultMinIdle &#125; if b.maxIdle == 0 &#123; b.maxIdle = defaultMaxIdle &#125; if b.maxTotal == 0 &#123; b.maxTotal = defaultMaxTotal &#125; if b.maxTotal &lt; b.maxIdle &#123; return nil, fmt.Errorf(\"max total(%d) cannot &lt; max idle(%d)\", b.maxTotal, b.maxIdle) &#125; if b.minIdle &gt; b.maxIdle &#123; return nil, fmt.Errorf(\"max idle(%d) cannot &lt; min idle(%d)\", b.maxIdle, b.minIdle) &#125; return &amp;ResourcePoolConfig&#123; name: b.name, maxTotal: b.maxTotal, maxIdle: b.maxIdle, minIdle: b.minIdle, &#125;, nil&#125; 使用可选参数 123456789101112131415161718192021222324252627282930313233343536373839404142434445package builderimport \"fmt\"// ResourcePoolConfigOption optiontype ResourcePoolConfigOption struct &#123; maxTotal int maxIdle int minIdle int&#125;// ResourcePoolConfigOptFunc to set optiontype ResourcePoolConfigOptFunc func(option *ResourcePoolConfigOption)// NewResourcePoolConfig NewResourcePoolConfigfunc NewResourcePoolConfig(name string, opts ...ResourcePoolConfigOptFunc) (*ResourcePoolConfig, error) &#123; if name == \"\" &#123; return nil, fmt.Errorf(\"name can not be empty\") &#125; option := &amp;ResourcePoolConfigOption&#123; maxTotal: 10, maxIdle: 9, minIdle: 1, &#125; for _, opt := range opts &#123; opt(option) &#125; if option.maxTotal &lt; 0 || option.maxIdle &lt; 0 || option.minIdle &lt; 0 &#123; return nil, fmt.Errorf(\"args err, option: %v\", option) &#125; if option.maxTotal &lt; option.maxIdle || option.minIdle &gt; option.maxIdle &#123; return nil, fmt.Errorf(\"args err, option: %v\", option) &#125; return &amp;ResourcePoolConfig&#123; name: name, maxTotal: option.maxTotal, maxIdle: option.maxIdle, minIdle: option.minIdle, &#125;, nil&#125; 单元测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package builderimport ( \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\")func TestResourcePoolConfigBuilder_Build(t *testing.T) &#123; tests := []struct &#123; name string builder *ResourcePoolConfigBuilder want *ResourcePoolConfig wantErr bool &#125;&#123; &#123; name: \"name empty\", builder: &amp;ResourcePoolConfigBuilder&#123; name: \"\", maxTotal: 0, &#125;, want: nil, wantErr: true, &#125;, &#123; name: \"maxIdle &lt; minIdle\", builder: &amp;ResourcePoolConfigBuilder&#123; name: \"test\", maxTotal: 0, maxIdle: 10, minIdle: 20, &#125;, want: nil, wantErr: true, &#125;, &#123; name: \"success\", builder: &amp;ResourcePoolConfigBuilder&#123; name: \"test\", &#125;, want: &amp;ResourcePoolConfig&#123; name: \"test\", maxTotal: defaultMaxTotal, maxIdle: defaultMaxIdle, minIdle: defaultMinIdle, &#125;, wantErr: false, &#125;, &#125; for _, tt := range tests &#123; t.Run(tt.name, func(t *testing.T) &#123; got, err := tt.builder.Build() require.Equalf(t, tt.wantErr, err != nil, \"Build() error = %v, wantErr %v\", err, tt.wantErr) assert.Equal(t, tt.want, got) &#125;) &#125;&#125; 可选参数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package builderimport ( \"testing\" \"github.com/stretchr/testify/assert\" \"github.com/stretchr/testify/require\")func TestNewResourcePoolConfig(t *testing.T) &#123; type args struct &#123; name string opts []ResourcePoolConfigOptFunc &#125; tests := []struct &#123; name string args args want *ResourcePoolConfig wantErr bool &#125;&#123; &#123; name: \"name empty\", args: args&#123; name: \"\", &#125;, want: nil, wantErr: true, &#125;, &#123; name: \"success\", args: args&#123; name: \"test\", opts: []ResourcePoolConfigOptFunc&#123; func(option *ResourcePoolConfigOption) &#123; option.minIdle = 2 &#125;, &#125;, &#125;, want: &amp;ResourcePoolConfig&#123; name: \"test\", maxTotal: 10, maxIdle: 9, minIdle: 2, &#125;, wantErr: false, &#125;, &#125; for _, tt := range tests &#123; t.Run(tt.name, func(t *testing.T) &#123; got, err := NewResourcePoolConfig(tt.args.name, tt.args.opts...) require.Equalf(t, tt.wantErr, err != nil, \"error = %v, wantErr %v\", err, tt.wantErr) assert.Equal(t, tt.want, got) &#125;) &#125;&#125; 总结 建造者模式适用于对象创建成本比较大需要经过复杂计算的情况 参考文档 https://lailin.xyz/post/builder.html https://refactoringguru.cn/design-patterns/catalog https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-原型模式","slug":"Design Patterns/设计模式-原型模式","date":"2022-08-13T13:52:53.000Z","updated":"2022-08-13T13:52:53.000Z","comments":true,"path":"2022/08/13/Design Patterns/设计模式-原型模式/","link":"","permalink":"http://xboom.github.io/2022/08/13/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"原型模式是一种创建型设计模式， 使你能够复制已有对象， 而又无需使代码依赖它们所属的类(亦称： 克隆、Clone、Prototype) 如果你有一个对象， 并希望生成与其完全相同的一个复制品， 你该如何实现呢？ 首先， 你必须新建一个属于相同类的对象。 然后， 你必须遍历原始对象的所有成员变量， 并将成员变量值复制到新对象中。 可能存在的问题 并非所有对象都能通过这种方式进行复制， 因为有些对象可能拥有私有成员变量， 它们在对象本身以外是不可见的 有时只知道对象所实现的接口， 而不知道其所属的具体类， 比如可向方法的某个参数传入实现了某个接口的任何对象 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142package prototypeimport ( \"encoding/json\" \"time\")// Keyword 搜索关键字type Keyword struct &#123; word string //私有 visit int //私有 UpdatedAt *time.Time //公有&#125;// Clone 这里使用序列化与反序列化的方式深拷贝func (k *Keyword) Clone() *Keyword &#123; var newKeyword Keyword b, _ := json.Marshal(k) json.Unmarshal(b, &amp;newKeyword) return &amp;newKeyword&#125;// Keywords 关键字 maptype Keywords map[string]*Keyword// Clone 复制一个新的 keywords// updatedWords: 需要更新的关键词列表，由于从数据库中获取数据常常是数组的方式func (words Keywords) Clone(updatedWords []*Keyword) Keywords &#123; newKeywords := Keywords&#123;&#125; for k, v := range words &#123; // 这里是浅拷贝，直接拷贝了地址 newKeywords[k] = v &#125; // 替换掉需要更新的字段，这里用的是深拷贝 for _, word := range updatedWords &#123; newKeywords[word.word] = word.Clone() &#125; return newKeywords&#125; 单元测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445package prototypeimport ( \"testing\" \"time\" \"github.com/stretchr/testify/assert\")func TestKeywords_Clone(t *testing.T) &#123; updateAt, _ := time.Parse(\"2006\", \"2020\") words := Keywords&#123; \"testA\": &amp;Keyword&#123; word: \"testA\", visit: 1, UpdatedAt: &amp;updateAt, &#125;, \"testB\": &amp;Keyword&#123; word: \"testB\", visit: 2, UpdatedAt: &amp;updateAt, &#125;, \"testC\": &amp;Keyword&#123; word: \"testC\", visit: 3, UpdatedAt: &amp;updateAt, &#125;, &#125; now := time.Now() updatedWords := []*Keyword&#123; &#123; word: \"testB\", visit: 10, UpdatedAt: &amp;now, &#125;, &#125; got := words.Clone(updatedWords) assert.Equal(t, words[\"testA\"], got[\"testA\"]) assert.NotEqual(t, words[\"testB\"], got[\"testB\"]) assert.NotEqual(t, updatedWords[0], got[\"testB\"]) assert.Equal(t, words[\"testC\"], got[\"testC\"])&#125; 总结 适用于对象创建成本比较大需要经过复杂计算的情况 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-抽象工厂模式","slug":"Design Patterns/设计模式-抽象工厂","date":"2022-08-13T13:38:52.000Z","updated":"2022-08-13T13:38:52.000Z","comments":true,"path":"2022/08/13/Design Patterns/设计模式-抽象工厂/","link":"","permalink":"http://xboom.github.io/2022/08/13/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82/","excerpt":"","text":"工厂模式分为三种更加细分的类型 简单工厂(Simple Factory) 工厂方法(Factory Method) 抽象工厂(Abstract Factory) 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445package abstractfactory// 配置文件解析 接口type IRuleConfigParser interface &#123; Parse(data []byte)&#125;// json文件解析器 type jsonRuleConfigParser struct&#123;&#125;// jsonRuleConfigParser 实现解析接口func (j jsonRuleConfigParser) Parse(data []byte) &#123; panic(\"implement me\")&#125;// 系统配置解析器 接口type ISystemConfigParser interface &#123; ParseSystem(data []byte)&#125;// json 系统配置解析器type jsonSystemConfigParser struct&#123;&#125;// jsonSystemConfigParser 实现系统配置解析接口func (j jsonSystemConfigParser) ParseSystem(data []byte) &#123; panic(\"implement me\")&#125;// IConfigParserFactory 解析抽象工厂type IConfigParserFactory interface &#123; CreateRuleParser() IRuleConfigParser CreateSystemParser() ISystemConfigParser&#125;// 解析工厂type jsonConfigParserFactory struct&#123;&#125;//配置文件解析工厂func (j jsonConfigParserFactory) CreateRuleParser() IRuleConfigParser &#123; return jsonRuleConfigParser&#123;&#125;&#125;//系统文件解析工厂func (j jsonConfigParserFactory) CreateSystemParser() ISystemConfigParser &#123; return jsonSystemConfigParser&#123;&#125;&#125; 单元测试 123456789101112131415161718192021222324252627282930313233343536373839func Test_jsonConfigParserFactory_CreateRuleParser(t *testing.T) &#123; tests := []struct &#123; name string want IRuleConfigParser &#125;&#123; &#123; name: \"json\", want: jsonRuleConfigParser&#123;&#125;, &#125;, &#125; for _, tt := range tests &#123; t.Run(tt.name, func(t *testing.T) &#123; j := jsonConfigParserFactory&#123;&#125; if got := j.CreateRuleParser(); !reflect.DeepEqual(got, tt.want) &#123; t.Errorf(\"CreateRuleParser() = %v, want %v\", got, tt.want) &#125; &#125;) &#125;&#125;func Test_jsonConfigParserFactory_CreateSystemParser(t *testing.T) &#123; tests := []struct &#123; name string want ISystemConfigParser &#125;&#123; &#123; name: \"json\", want: jsonSystemConfigParser&#123;&#125;, &#125;, &#125; for _, tt := range tests &#123; t.Run(tt.name, func(t *testing.T) &#123; j := jsonConfigParserFactory&#123;&#125; if got := j.CreateSystemParser(); !reflect.DeepEqual(got, tt.want) &#123; t.Errorf(\"CreateSystemParser() = %v, want %v\", got, tt.want) &#125; &#125;) &#125;&#125; 总结 使用一个工厂常见多个不同类型的对象，而不是只创建一种 parser 解析工厂对象，有效减少工厂数目 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-工厂方法模式","slug":"Design Patterns/设计模式-工厂方法","date":"2022-08-13T12:57:55.000Z","updated":"2022-08-13T12:57:55.000Z","comments":true,"path":"2022/08/13/Design Patterns/设计模式-工厂方法/","link":"","permalink":"http://xboom.github.io/2022/08/13/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95/","excerpt":"","text":"工厂模式分为三种更加细分的类型 简单工厂(Simple Factory) 工厂方法(Factory Method) 抽象工厂(Abstract Factory) 工厂方法中在父类中提供一个创建对象的方法，允许子类决定实例化对象的类型 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package factory// 配置文件解析 接口type IRuleConfigParser interface &#123; Parse(data []byte)&#125;// json 文件解析器type jsonRuleConfigParser struct &#123;&#125;// jsonRuleConfigParser 实现解析接口func (J jsonRuleConfigParser) Parse(data []byte) &#123; panic(\"implement me\")&#125;// yaml 文件解析器type yamlRuleConfigParser struct &#123;&#125;// yamlRuleConfigParser 实现解析接口func (Y yamlRuleConfigParser) Parse(data []byte) &#123; panic(\"implement me\")&#125;// IRuleConfigParserFactory 工厂方法接口type IRuleConfigParserFactory interface &#123; CreateParser() IRuleConfigParser&#125;// yaml 解析工厂type yamlRuleConfigParserFactory struct &#123;&#125;// yamlRuleConfigParserFactory yaml文件解析器构建器func (y yamlRuleConfigParserFactory) CreateParser() IRuleConfigParser &#123; return yamlRuleConfigParser&#123;&#125;&#125;// json 解析工厂type jsonRuleConfigParserFactory struct &#123;&#125;// jsonRuleConfigParserFactory json文件解析器构建器func (j jsonRuleConfigParserFactory) CreateParser() IRuleConfigParser &#123; return jsonRuleConfigParser&#123;&#125;&#125;// NewIRuleConfigParserFactory 根据类型返回对应的构建工厂func NewIRuleConfigParserFactory(t string) IRuleConfigParserFactory &#123; switch t &#123; case \"json\": return jsonRuleConfigParserFactory&#123;&#125; case \"yaml\": return yamlRuleConfigParserFactory&#123;&#125; &#125; return nil&#125; 单元测试 12345678910111213141516171819202122232425262728func TestNewIRuleConfigParserFactory(t *testing.T) &#123; type args struct &#123; t string &#125; tests := []struct &#123; name string args args want IRuleConfigParserFactory &#125;&#123; &#123; name: \"json\", args: args&#123;t: \"json\"&#125;, want: jsonRuleConfigParserFactory&#123;&#125;, &#125;, &#123; name: \"yaml\", args: args&#123;t: \"yaml\"&#125;, want: yamlRuleConfigParserFactory&#123;&#125;, &#125;, &#125; for _, tt := range tests &#123; t.Run(tt.name, func(t *testing.T) &#123; if got := NewIRuleConfigParserFactory(tt.args.t); !reflect.DeepEqual(got, tt.want) &#123; t.Errorf(\"NewIRuleConfigParserFactory() = %v, want %v\", got, tt.want) &#125; &#125;) &#125;&#125; 总结 和简单工厂模式一样，在工厂方法构建工厂中包含较多if else ，实用工厂类型不那么多的场景 工厂方法模式适用于类型创建比较复杂不是简单的New，将复杂的创建逻辑拆分到多个工厂类中 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-简单工厂模式","slug":"Design Patterns/设计模式-简单工厂","date":"2022-08-13T12:39:08.000Z","updated":"2022-08-13T12:39:08.000Z","comments":true,"path":"2022/08/13/Design Patterns/设计模式-简单工厂/","link":"","permalink":"http://xboom.github.io/2022/08/13/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82/","excerpt":"","text":"工厂模式分为三种更加细分的类型 简单工厂(Simple Factory) 工厂方法(Factory Method) 抽象工厂(Abstract Factory) 由于 Go 本身是没有构造函数的，一般采用 NewName 的方式创建对象/接口，当它返回的是接口的时候，其实就是简单工厂模式 代码实现 1234567891011121314151617181920212223242526272829303132333435package factory////配置文件解析接口type IRuleConfigParser interface &#123; Parse(data []byte)&#125;//json 文件解析器type jsonRuleConfigParser struct &#123;&#125;//jsonRuleConfigParser 实现接口func (J jsonRuleConfigParser) Parse(data []byte) &#123; panic(\"implement me\")&#125;//yaml 文件解析器type yamlRuleConfigParser struct &#123;&#125;// yamlRuleConfigParser 实现解析接口func (Y yamlRuleConfigParser) Parse(data []byte) &#123; panic(\"implement me\")&#125;// 根据类型返回解析对象func NewIRuleConfigParser(t string) IRuleConfigParser &#123; switch t &#123; case \"json\": return jsonRuleConfigParser&#123;&#125; case \"yaml\": return yamlRuleConfigParser&#123;&#125; &#125; return nil&#125; 单元测试 12345678910111213141516171819202122232425262728func TestNewIRuleConfigParser(t *testing.T) &#123; type args struct &#123; t string &#125; tests := []struct &#123; name string args args want IRuleConfigParser &#125;&#123; &#123; name: \"json\", args: args&#123;t: \"json\"&#125;, want: jsonRuleConfigParser&#123;&#125;, &#125;, &#123; name: \"yaml\", args: args&#123;t: \"yaml\"&#125;, want: yamlRuleConfigParser&#123;&#125;, &#125;, &#125; for _, tt := range tests &#123; t.Run(tt.name, func(t *testing.T) &#123; if got := NewIRuleConfigParser(tt.args.t); !reflect.DeepEqual(got, tt.want) &#123; t.Errorf(\"NewIRuleConfigParser() = %v, want %v\", got, tt.want) &#125; &#125;) &#125;&#125; 总结 NewIRuleConfigParser 中包含多个if else，适用于类型不多，不经常修改的情况 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"go-zero10-自适应降载","slug":"GoZero/GoZero10-自适应降载","date":"2022-08-13T07:07:36.000Z","updated":"2022-08-13T07:07:36.000Z","comments":true,"path":"2022/08/13/GoZero/GoZero10-自适应降载/","link":"","permalink":"http://xboom.github.io/2022/08/13/GoZero/GoZero10-%E8%87%AA%E9%80%82%E5%BA%94%E9%99%8D%E8%BD%BD/","excerpt":"","text":"问题背景 调用链路错综复杂，做为服务的提供者需要有一种保护自己的机制，防止调用方无脑调用压垮自己，保证自身服务的高可用。自适应降载能根据服务自身的系统负载动态判断是否需要降载，它的目标： 保证系统不被拖垮 在系统稳定的前提下，保持系统的吞吐量 问题：服务怎么知道自己需要降载？ 通过CPU负载与并发数判断往往存在较大波动，这种被称为毛刺的现象可能导致系统一致频繁的自动进行降载操作。所以如果能通过统计最近一段时间内的指标均值使均值更加平滑 实现原理 统计学上有一种算法：滑动平均（exponential moving average），用来估算变量的局部均值，使得变量的更新与历史一段时间的历史取值有关，无需记录所有的历史局部变量就可以实现平均值估算 变量 V 在 t 时刻记为 Vt，θt 为变量 V 在 t 时刻的取值，即在不使用滑动平均模型时 Vt=θt，在使用滑动平均模型后，Vt 的更新公式如下： Vt=β⋅Vt−1+(1−β)⋅θtVt=β⋅Vt−1+(1−β)⋅θt Vt=β⋅Vt−1+(1−β)⋅θt β = 0 时 Vt = θt β = 0.9 时,大致相当于过去 10 个 θt 值的平均 β = 0.99 时,大致相当于过去 100 个 θt 值的平均 而统计最近一段时间内的数据则可以使用 滑动窗口算法，接下来看看如何进行自适应降载判断 技术内幕 来看看 go-zero 的自适应降载的实现 代码：core/load/adaptiveshedder.go 使用案例 1234567891011121314151617181920212223242526272829func UnarySheddingInterceptor(shedder load.Shedder, metrics *stat.Metrics) grpc.UnaryServerInterceptor &#123; ensureSheddingStat() return func(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (val interface&#123;&#125;, err error) &#123; sheddingStat.IncrementTotal() var promise load.Promise // 检查是否被降载 promise, err = shedder.Allow() // 降载，记录相关日志与指标 if err != nil &#123; metrics.AddDrop() sheddingStat.IncrementDrop() return &#125; // 最后回调执行结果 defer func() &#123; // 执行失败 if err == context.DeadlineExceeded &#123; promise.Fail() // 执行成功 &#125; else &#123; sheddingStat.IncrementPass() promise.Pass() &#125; &#125;() // 执行业务方法 return handler(ctx, req) &#125;&#125; 仅需要调用 Allow 接口进行降载逻辑初始化 如果降载 shedder.Allow()，那么直接记录信息并返回 否则执行业务逻辑 成功则执行 promise.Pass 失败则执行 promise.Fail，业务调用错误这种算执行成功，这里使用业务超时(DeadlineExceeded)表示执行失败需要降载。 这里的promise变量就是上图中的返回结果句柄，用于将业务逻辑结果更新到降载器中 接口定义 12345678910111213141516171819202122232425262728293031323334353637383940414243444546const ( defaultBuckets = 50 //默认滑动窗口槽 defaultWindow = time.Second * 5 //默认滑动窗口大小 defaultCpuThreshold = 900 //CPU阈值 defaultMinRt = float64(time.Second / time.Millisecond) //最小速率 flyingBeta = 0.9 //平均请求 系数 coolOffDuration = time.Second //冷静时间)type ( //回到函数结果处理 Promise interface &#123; // 请求成功时回调此函数 Pass() // 请求失败时回调此函数 Fail() &#125; //降载接口 Shedder interface &#123; // 降载检查 // 1. 允许调用，需手动执行 Promise.accept()/reject()上报实际执行任务结构 // 2. 拒绝调用，将会直接返回err：服务过载错误 ErrServiceOverloaded Allow() (Promise, error) &#125; // ShedderOption lets caller customize the Shedder. ShedderOption func(opts *shedderOptions) shedderOptions struct &#123; window time.Duration buckets int cpuThreshold int64 &#125; adaptiveShedder struct &#123; cpuThreshold int64 //CPU阈值 windows int64 //滑动窗口大小 flying int64 //调度统计 avgFlying float64 //平均调度 avgFlyingLock syncx.SpinLock //调度锁 dropTime *syncx.AtomicDuration //降载时间 droppedRecently *syncx.AtomicBool //降载标识 passCounter *collection.RollingWindow //滑动窗口 通过统计 rtCounter *collection.RollingWindow //滑动窗口 速率统计 &#125;) 初始化 1234567891011121314151617181920//构建一个自适应熔断调度器func NewAdaptiveShedder(opts ...ShedderOption) Shedder &#123; if !enabled.True() &#123; return newNopShedder() &#125; //... 可选参数执行 bucketDuration := options.window / time.Duration(options.buckets) //滑动窗口槽的大小 return &amp;adaptiveShedder&#123; cpuThreshold: options.cpuThreshold, //CPU阈值 windows: int64(time.Second / bucketDuration), //1s滑动窗口个数 dropTime: syncx.NewAtomicDuration(), //熔断时间 droppedRecently: syncx.NewAtomicBool(), //最近是否熔断 passCounter: collection.NewRollingWindow(options.buckets, bucketDuration, collection.IgnoreCurrentBucket()), //滑动窗口通过统计 rtCounter: collection.NewRollingWindow(options.buckets, bucketDuration, collection.IgnoreCurrentBucket()), //滑动窗口速率统计 &#125;&#125; 逻辑判断 12345678910111213141516//降载入口func (as *adaptiveShedder) Allow() (Promise, error) &#123; if as.shouldDrop() &#123; as.dropTime.Set(timex.Now()) as.droppedRecently.Set(true) return nil, ErrServiceOverloaded &#125; as.addFlying(1) return &amp;promise&#123; start: timex.Now(), shedder: as, &#125;, nil&#125; 结果句柄操作 123456789101112131415type promise struct &#123; start time.Duration shedder *adaptiveShedder&#125;func (p *promise) Fail() &#123; p.shedder.addFlying(-1)&#125;func (p *promise) Pass() &#123; rt := float64(timex.Since(p.start)) / float64(time.Millisecond) //花费的毫秒数 p.shedder.addFlying(-1) p.shedder.rtCounter.Add(math.Ceil(rt)) p.shedder.passCounter.Add(1) &#125; 注意 其中的 p.shedder.addFlying(-1) 也就是说 flying 变量用于更新调度请求数量的 失败并不会记录到调度统计中，因为计算平均请求不需要失败 1234567891011121314func (as *adaptiveShedder) addFlying(delta int64) &#123; flying := atomic.AddInt64(&amp;as.flying, delta) //请求数量更新 // 当请求完成时更新 avgFlying。 // 这个策略使得 avgFlying 相对 flying 有一点延迟，并且更平滑。 // 当 flying 请求快速增加时，avgFlying 增加较慢，接受更多请求。 // 当 flying 请求快速下降时，avgFlying 下降较慢，接受较少的请求。 // 它使服务尽可能多地处理请求。 if delta &lt; 0 &#123; //当 &lt; 0 表示 请求完成，计算平均请求 as.avgFlyingLock.Lock() as.avgFlying = as.avgFlying*flyingBeta + float64(flying)*(1-flyingBeta) //滑动平均算法 as.avgFlyingLock.Unlock() &#125;&#125; CPU超过限制 这里的CPU也是经过定时统计得出的最近一段时间CPU负载，防止毛刺 123systemOverloadChecker = func(cpuThreshold int64) bool &#123; return stat.CpuUsage() &gt;= cpuThreshold&#125; 过载中判断 如果是正在过载中则，超过一段时间冷静期就恢复正常 过载标识/时间 是当初过载时候设置的 dropTime 与droppedRecently 123456789101112131415161718//过载中判断func (as *adaptiveShedder) stillHot() bool &#123; if !as.droppedRecently.True() &#123; //是否过热中 return false &#125; dropTime := as.dropTime.Load() //加载降载时间 if dropTime == 0 &#123; return false &#125; hot := timex.Since(dropTime) &lt; coolOffDuration //是否超过冷静期 if !hot &#123; as.droppedRecently.Set(false) //更新降载标识 &#125; return hot&#125; 过载判断 过载判断 的逻辑是 12//平均请求数大于 且 当前未完成请求的数量超过了最大请求数avgFlying &gt; maxFlight &amp;&amp; flying &gt; maxFlight 这个 最大并发数 又是怎样计算的呢？ 当前系统的最大并发数 = 窗口单位时间内的最大通过数量 * 窗口单位时间内的最小响应时间 12345678910111213141516171819202122232425262728293031323334353637383940func (as *adaptiveShedder) maxFlight() int64 &#123; // windows = buckets per second // maxQPS = maxPASS * windows // minRT = min average response time in milliseconds // maxQPS * minRT / milliseconds_per_second // as.maxPass()*as.windows - 每个桶最大的qps * 1s内包含桶的数量 // as.minRt()/1e3 - 窗口所有桶中最小的平均响应时间 / 1000ms这里是为了转换成秒 return int64(math.Max(1, float64(as.maxPass()*as.windows)*(as.minRt()/1e3)))&#125;//当前滑动窗口中的最大请求的统计func (as *adaptiveShedder) maxPass() int64 &#123; var result float64 = 1 as.passCounter.Reduce(func(b *collection.Bucket) &#123; if b.Sum &gt; result &#123; result = b.Sum &#125; &#125;) return int64(result)&#125;//当前滑动窗口中最小速率的统计func (as *adaptiveShedder) minRt() float64 &#123; result := defaultMinRt as.rtCounter.Reduce(func(b *collection.Bucket) &#123; if b.Count &lt;= 0 &#123; return &#125; avg := math.Round(b.Sum / float64(b.Count)) if avg &lt; result &#123; result = avg &#125; &#125;) return result&#125; 总结 自适应降载逻辑处理 当请求突然增大的时候，虽然没有达到服务能够承受的极限，也有可能出现降载。因为平均请求数量以及最大请求数量 都超过了最近一段时间能承载的最大水平 按照第一条逻辑，如果服务刚启动那会请求确实比较多，是不是就会出现降载了。不会，这里在计算 最大并发数的时候，给定了一个最小最大并发数 1 * defaultMinRt / milliseconds_per_second 。也就是并发数低于 1000的时候也不会触发降载 参考文档 https://talkgo.org/t/topic/3058 https://www.cnblogs.com/wuliytTaotao/p/9479958.html","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero9-自适应熔断","slug":"GoZero/GoZero9-自适应熔断","date":"2022-08-01T10:07:02.000Z","updated":"2022-08-01T10:07:02.000Z","comments":true,"path":"2022/08/01/GoZero/GoZero9-自适应熔断/","link":"","permalink":"http://xboom.github.io/2022/08/01/GoZero/GoZero9-%E8%87%AA%E9%80%82%E5%BA%94%E7%86%94%E6%96%AD/","excerpt":"","text":"问题背景 使用负载均衡策略是一种避免超负载的处理方式，但服务的容量是有限的。部分服务还是会出现超载的情况，如果优雅的处理过载则对可靠的服务至关重要。 在高并发场景下，为了应对依赖服务过载，服务不可用等情况，提出了熔断、限流与降级方案。这里主要描述熔断的原理，这里存在几个问题： 都有哪些熔断的解决方案 熔断器的实现原理是什么 使用较多的熔断组件： hystrix circuit breaker（不再维护） hystrix-go resilience4j（推荐） sentinel（推荐） 熔断器原理 熔断器一般具有三个状态： 关闭: 默认状态，请求能被到达目标服务，同时统计在窗口时间成功和失败次数，如果达到错误率阈值将会进入断开状态。 断开: 此状态下将会直接返回错误，如果有 fallback 配置则直接调用 fallback 方法。 半断开: 进行断开状态会维护一个超时时间，到达超时时间开始进入 半断开 状态，尝试允许一部分请求正常通过并统计成功数量，如果请求正常则认为此时目标服务已恢复进入 关闭 状态，否则进入 断开 状态 基于熔断器的原理，通常熔断器主要关注以下参数： 错误比例阈值: 达到该阈值进入 断开 状态 断开状态超时时间: 超时后进入 半断开 状态 半断开状态允许请求数量 窗口时间大小 这里有更将详细可参考的参考以及算法说明 https://resilience4j.readme.io/docs/circuitbreaker https://sre.google/sre-book/handling-overload/ 由于go-zero 的熔断器是基于google文章实现，来看下基本算法 技术内幕 无论什么熔断器都得依靠指标统计来转换状态，而统计指标一般要求是最近的一段时间内的数据，所以通常采用一个 滑动时间窗口 数据结构 来存储统计数据。同时熔断器的状态也需要依靠指标统计来实现可观测性。 外部服务请求结果各式各样，所以需要提供一个自定义的判断方法，判断请求是否成功。熔断器需要实时收集此数据。 当外部服务被熔断时使用者往往需要自定义快速失败的逻辑，考虑提供自定义的 fallback() 功能。 接口定义 代码路径：core/breaker/breaker.go 12345678910111213141516171819202122232425262728293031323334353637383940// 250ms for bucket durationwindow = time.Second * 10buckets = 40k = 1.5protection = 5//判断断路器是否通过Acceptable func(err error) bool//Breaker 断路器 Breaker interface &#123; // 熔断器名称 Name() string //检查请求是否允许。调用成功则使用 Promise.Accept()，失败则调用 Promise.Reject()，否则表示不许云 Allow() (Promise, error) // 如果 Breaker 接受，Do 运行给定的请求。 // 如果 Breaker 拒绝请求，Do 立即返回错误。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 Do(req func() error) error // 如果 Breaker 接受，DoWithAcceptable 运行给定的请求。 // 如果 Breaker 拒绝请求，DoWithAcceptable 会立即返回错误。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 // 可接受的检查它是否是一个成功的调用，即使错误不是零。 DoWithAcceptable(req func() error, acceptable Acceptable) error // 如果 Breaker 接受，DoWithFallback 运行给定的请求。 // 如果 Breaker 拒绝请求，DoWithFallback 运行回退。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 DoWithFallback(req func() error, fallback func(err error) error) error // 如果 Breaker 接受，DoWithFallbackAcceptable 运行给定的请求。 // DoWithFallbackAcceptable 如果 Breaker 拒绝请求，则运行回退。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 // 可接受的检查它是否是一个成功的调用，即使错误不是零。 DoWithFallbackAcceptable(req func() error, fallback func(err error) error, acceptable Acceptable) error &#125; 断路器 12345678910111213141516171819202122232425262728//断路器circuitBreaker struct &#123; name string throttle&#125;//断路器内部则通过两个接口实现throttle interface &#123; allow() (Promise, error) doReq(req func() error, fallback func(err error) error, acceptable Acceptable) error&#125;//自定义断路器对象type googleBreaker struct &#123; k float64 stat *collection.RollingWindow //使用滑动窗口统计最近一段时间数据 proba *mathx.Proba&#125;func newGoogleBreaker() *googleBreaker &#123; bucketDuration := time.Duration(int64(window) / int64(buckets)) st := collection.NewRollingWindow(buckets, bucketDuration) return &amp;googleBreaker&#123; stat: st, k: k, proba: mathx.NewProba(), &#125;&#125; 数据记录 其实是将成功或者失败记录到滑动窗口中 1234567func (b *googleBreaker) markSuccess() &#123; b.stat.Add(1)&#125;func (b *googleBreaker) markFailure() &#123; b.stat.Add(0)&#125; 请求操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//统计滑动窗口中所代表的最近一段时间的数据func (b *googleBreaker) history() (accepts, total int64) &#123; b.stat.Reduce(func(b *collection.Bucket) &#123; accepts += int64(b.Sum) total += b.Count &#125;) return&#125;//检查是否允许func (b *googleBreaker) accept() error &#123; accepts, total := b.history() weightedAccepts := b.k * float64(accepts) // https://landing.google.com/sre/sre-book/chapters/handling-overload/#eq2101 dropRatio := math.Max(0, (float64(total-protection)-weightedAccepts)/float64(total+1)) if dropRatio &lt;= 0 &#123; return nil &#125; if b.proba.TrueOnProba(dropRatio) &#123; return ErrServiceUnavailable &#125; return nil&#125;//返回断路器权限func (b *googleBreaker) allow() (internalPromise, error) &#123; if err := b.accept(); err != nil &#123; return nil, err &#125; return googlePromise&#123; b: b, &#125;, nil&#125;func (b *googleBreaker) doReq(req func() error, fallback func(err error) error, acceptable Acceptable) error &#123; if err := b.accept(); err != nil &#123; if fallback != nil &#123; return fallback(err) &#125; return err &#125; defer func() &#123; if e := recover(); e != nil &#123; b.markFailure() panic(e) &#125; &#125;() err := req() if acceptable(err) &#123; //结果的处理 b.markSuccess() &#125; else &#123; b.markFailure() &#125; return err&#125; 总结 通过滑动窗口进行最近一段数据(成功失败次数)的统计 是否断路则是通过指定的公式计算 失败率=总数−可接收误差−k∗成功/失败失败率 = 总数 - 可接收误差 - k * 成功 / 失败 失败率=总数−可接收误差−k∗成功/失败 参考文档 https://talkgo.org/t/topic/3035","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"算法之美-滑动窗口","slug":"Algorithms To Live By/算法之美-滑动窗口","date":"2022-08-01T09:42:45.000Z","updated":"2022-08-01T09:42:45.000Z","comments":true,"path":"2022/08/01/Algorithms To Live By/算法之美-滑动窗口/","link":"","permalink":"http://xboom.github.io/2022/08/01/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"","text":"在TCP原理中使用滑动窗口进行流量控制，平时的算法中也会使用双指针的滑动窗口进行问题处理。这里看看滑动窗口的具体原理 实现原理 滑动窗口不仅仅可以通过长度来限制流量，还能通过在窗口中保存最近一段时间数据，以此来进行更加复杂的实时调整 为什么不使用链表：使用切片构建类似环的结构，有效节省空间 技术内幕 算法实现中的类似滑动窗口框架为 1234567891011121314151617string s, t;// 在 s 中寻找 t 的「最小覆盖子串」int left = 0, right = 0;string res = s;while(right &lt; s.size()) &#123; window.add(s[right]); right++; // 如果符合要求，移动 left 缩小窗口 while (window 符合要求) &#123; // 如果这个窗口的子串更短，则更新 res res = minLen(res, window); window.remove(s[left]); left++; &#125;&#125;return res; 看看go-zero中滑动窗口的实现 代码路径：core/collection/rollingwindow.go 对象定义 123456789101112131415type ( //自定义可选参数 RollingWindowOption func(rollingWindow *RollingWindow) // 滑动窗口结构 RollingWindow struct &#123; lock sync.RWMutex //窗口读写锁 size int //窗口大小 win *window //窗口对洗那个 interval time.Duration //时间间隔 offset int //当前位置 ignoreCurrent bool //忽略当前 lastTime time.Duration // start time of the last bucket &#125;) 槽 窗口中的每个槽可用于存储更加复杂的数据进行最近一段时间数据统计 123456789101112131415//槽type Bucket struct &#123; Sum float64 Count int64&#125;func (b *Bucket) add(v float64) &#123; b.Sum += v b.Count++&#125;func (b *Bucket) reset() &#123; b.Sum = 0 b.Count = 0&#125; 窗口操作 使用切片与取余进行类似环形操作 1234567891011121314//增加 % 类型唤醒操作func (w *window) add(offset int, v float64) &#123; w.buckets[offset%w.size].add(v) &#125;func (w *window) reduce(start, count int, fn func(b *Bucket)) &#123; for i := 0; i &lt; count; i++ &#123; fn(w.buckets[(start+i)%w.size]) &#125;&#125;func (w *window) resetBucket(offset int) &#123; w.buckets[offset%w.size].reset()&#125; 滑动窗口操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//窗口移动func (rw *RollingWindow) span() int &#123; offset := int(timex.Since(rw.lastTime) / rw.interval) //移动格子 if 0 &lt;= offset &amp;&amp; offset &lt; rw.size &#123; return offset &#125; return rw.size //当偏移量超过窗口大小，那么直接返回窗口大小(不需要移动offset的原因抛弃旧窗口即可)&#125;//更新偏移量func (rw *RollingWindow) updateOffset() &#123; span := rw.span() //窗口移动位置 if span &lt;= 0 &#123; return &#125; offset := rw.offset //当前窗口起始起始 for i := 0; i &lt; span; i++ &#123; //重置过期的槽 rw.win.resetBucket((offset + i + 1) % rw.size) &#125; rw.offset = (offset + span) % rw.size //更新起始位置 now := timex.Now() //当前时间 // align to interval time boundary rw.lastTime = now - (now-rw.lastTime)%rw.interval //最新移动时间&#125;//滑动窗口更新槽func (rw *RollingWindow) Add(v float64) &#123; rw.lock.Lock() defer rw.lock.Unlock() rw.updateOffset() //更新索引 rw.win.add(rw.offset, v) //增加值&#125;//对窗口进行 fn 操作func (rw *RollingWindow) Reduce(fn func(b *Bucket)) &#123; rw.lock.RLock() defer rw.lock.RUnlock() var diff int span := rw.span() // ignore current bucket, because of partial data if span == 0 &amp;&amp; rw.ignoreCurrent &#123; diff = rw.size - 1 &#125; else &#123; diff = rw.size - span &#125; if diff &gt; 0 &#123; offset := (rw.offset + span + 1) % rw.size rw.win.reduce(offset, diff, fn) &#125;&#125;","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"微服务-缓存一致性","slug":"Microservices/微服务-缓存一致性","date":"2022-07-31T03:01:41.000Z","updated":"2022-07-31T03:01:41.000Z","comments":true,"path":"2022/07/31/Microservices/微服务-缓存一致性/","link":"","permalink":"http://xboom.github.io/2022/07/31/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/","excerpt":"","text":"问题背景 一致性有很多种 强一致性：保证写入后立即可以读取 弱一致性：在系统写入后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态 最终一致性：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态 缓存可以提升性能、缓解数据库压力，使用缓存也会导致数据不一致性的问题 缓存系统的数据一致性通常包括持久化层和缓存层的一致性、以及多级缓存之间的一致性，这里讨论是前者。持久化层和缓存层的一致性问题也通常被称为双写一致性问题。 实现原理 引入 Cache 之后，延迟或程序失败等都会导致缓存和实际存储层数据不一致，下面几种模式减少不一致风险 Cache-Aside Pattern，即旁路缓存模式 Read-Through/Write-Through，读写穿透模式 Write behind，异步缓存写入模式 Cache-Aside 读模式 当缓存命中则直接返回，否则从数据库读取数据并更新缓存 写模式 首先更新数据库，然后删除缓存 问题1：为什么是删除缓存，而不是更新缓存 如果缓存需要通过大量的计算(联表查询更新)，那么更新缓存会是一笔不小的开销 另外如果写操作比较多，可能存在刚更新的缓存还没有读取就又要更新的情况(称为缓存扰动)，所以此模式适用于读多写少的模式 等到读请求未命中再去更新，符合懒加载思路 并发更新可能导致缓存落后与数据库，读请求读到的仍然是旧缓存 问题2：为什么是先更新数据库，而不是先删除缓存 数据库查询请求往往比更新请求更快，可能这种异常更容易出现 Read/Write Through 读模式 当缓存命中则直接返回，否则从数据库读取数据并更新缓存 Read/Write Through模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过抽象缓存层完成的 写模式 Write Through模式在发生Cache Miss的时候，只会在读请求中更新缓存。 写请求在发生Cache Miss的时候不会更新缓存，而是直接写入数据库； 如果命中缓存则先更新缓存，由缓存自己再将数据写回到数据库中 注意这个时候如果命中缓存，是先更新缓存的。也就说和 Cache-Aside一样存在并发场景下的一致性问题 这个策略的核心原则：用户只与缓存打交道，由缓存组件和DB通信，写入或者读取数据。在一些本地进程缓存组件可以考虑这种策略 Write-Through 存在的缺陷：写数据时缓存和数据库同步，但是我们知道这两块存储介质的速度差几个数量级，对写入性能是有很大影响。那我们是否异步更新数据库 Write behind Write behind 跟有相似的地方，都是由Cache Provider来负责缓存和数据库的读写。它两又有个很大的不同：Read/Write Through是同步更新缓存和数据的，Write Behind则是只更新缓存，不直接更新数据库，通过批量异步的方式来更新数据库 缓存和数据库的一致性不强，对一致性要求高的系统要谨慎使用。但是它适合频繁写的场景，MySQL的InnoDB Buffer Pool机制就使用到这种模式 延时双删 延时双删主要用于 Redis主从节点的场景，延时的原因是，mysql 和 redis 主从节点数据不是实时同步的，同步数据需要时间。 服务节点删除 redis 主库数据 服务节点修改 mysql 主库数据 服务节点使得当前业务处理 等待一段时间，等 redis 和 mysql 主从节点数据同步成功。 服务节点从 redis 主库删除数据。 当前或其它服务节点读取 redis 从库数据，发现 redis 从库没有数据，从 mysql 从库读取数据，并写入 redis 主库 注意： 延时双删，有等待环节，如果系统要求低延时，这种场景就不合适了。 延时双删，不适合“秒杀”这种频繁修改数据和要求数据强一致的场景。 延时双删，延时时间是一个预估值，不能确保 mysql 和 redis 数据在这个时间段内都实时同步或持久化成功了 重试保障 方案1：服务自行订阅删除缓存消息 更新数据库数据； 缓存因为种种问题删除失败； 将需要删除的key发送至消息队列； 自己消费消息，获得需要删除的key； 继续重试删除操作，直到成功 方案2：利用第三方服务删除缓存 更新数据库数据； 数据库会将操作信息写入binlog日志当中； 订阅程序提取出所需要的数据以及key； 另起一段非业务代码，获得该信息； 尝试删除缓存操作，发现删除失败； 将这些信息发送至消息队列； 重新从消息队列中获得该数据，重试操作 注意： 删除缓存也可能存储缓存击穿的问题 在 GoZero8-数据库缓存中中使用共享调用的方式(类似自旋锁)进行数据查询 使用方案1进行消息订阅的时候可能出现消息队列也失败的情况 强一致性肯定会有性能影响(比如 raft协议需要等待超过半数节点做出响应)，另外强一致性的异常处理 技术内幕 来看看 rockscache 如何解决缓存一致性的， 地址：https://github.com/dtm-labs/rockscache The First Redis Cache Library To Ensure Eventual Consistency And Strong Consistency With DB. 变量定义 1234567891011121314151617181920212223//rockscache client 可选参数type Options struct &#123; //标记删除key的延时删除时间 默认10s Delay time.Duration //EmptyExpire 是空结果的过期时间。默认为 60 秒 EmptyExpire time.Duration // LockExpire 是更新缓存时分配的锁的过期时间。默认为 3s LockExpire time.Duration //锁失败后的重试等待时间 100ms LockSleep time.Duration // 等待副本数量 WaitReplicas int // 副本等待超时时间 默认300ms WaitReplicasTimeout time.Duration //随机过期时间，0.1的偏移(缓存雪崩) RandomExpireAdjustment float64 // 标识缓存禁止读，默认关闭。用于缓存宕机时候的降级 DisableCacheRead bool // 标识缓存删除，默认关闭。用于缓存宕机时候的降级 DisableCacheDelete bool // 强一致性，默认关闭 StrongConsistency bool&#125; lua脚本 使用脚本进行redis操作，lua的好处是一次性执行，执行过程其他脚本或命令无法执行(注意不确定参数)。 这里使用hash进行数据存储，同时保存 key/value 与 key/lock 12345678910111213141516171819202122232425262728293031func (c *Client) luaGet(key string, owner string) ([]interface&#123;&#125;, error) &#123; res, err := callLua(c.rdb.Context(), c.rdb, ` -- luaGet local v = redis.call('HGET', KEYS[1], 'value') //获取值 local lu = redis.call('HGET', KEYS[1], 'lockUtil') //获取过期时间 if lu ~= false and tonumber(lu) &lt; tonumber(ARGV[1]) or lu == false and v == false then redis.call('HSET', KEYS[1], 'lockUtil', ARGV[2]) //如果锁已经过期或者不存在，则更新锁 redis.call('HSET', KEYS[1], 'lockOwner', ARGV[3]) return &#123; v, 'LOCKED' &#125; end return &#123;v, lu&#125; `, []string&#123;key&#125;, []interface&#123;&#125;&#123;now(), now() + int64(c.Options.LockExpire/time.Second), owner&#125;) debugf(\"luaGet return: %v, %v\", res, err) if err != nil &#123; return nil, err &#125; return res.([]interface&#123;&#125;), nil&#125;func (c *Client) luaSet(key string, value string, expire int, owner string) error &#123; _, err := callLua(c.rdb.Context(), c.rdb, `-- luaSet local o = redis.call('HGET', KEYS[1], 'lockOwner') if o ~= ARGV[2] then return end redis.call('HSET', KEYS[1], 'value', ARGV[1]) redis.call('HDEL', KEYS[1], 'lockUtil') redis.call('HDEL', KEYS[1], 'lockOwner') redis.call('EXPIRE', KEYS[1], ARGV[3]) `, []string&#123;key&#125;, []interface&#123;&#125;&#123;value, owner, expire&#125;) return err&#125; 加锁和解锁 123456789101112131415161718192021222324252627282930//加锁func (c *Client) LockForUpdate(key string, owner string) error &#123; lockUtil := math.Pow10(10) res, err := callLua(c.rdb.Context(), c.rdb, ` -- luaLock local lu = redis.call('HGET', KEYS[1], 'lockUtil') local lo = redis.call('HGET', KEYS[1], 'lockOwner') if lu == false or tonumber(lu) &lt; tonumber(ARGV[2]) or lo == ARGV[1] then redis.call('HSET', KEYS[1], 'lockUtil', ARGV[2]) redis.call('HSET', KEYS[1], 'lockOwner', ARGV[1]) return 'LOCKED' end return lo `, []string&#123;key&#125;, []interface&#123;&#125;&#123;owner, lockUtil&#125;) if err == nil &amp;&amp; res != \"LOCKED\" &#123; return fmt.Errorf(\"%s has been locked by %s\", key, res) &#125; return err&#125;//解锁func (c *Client) UnlockForUpdate(key string, owner string) error &#123; _, err := callLua(c.rdb.Context(), c.rdb, ` -- luaUnlock local lo = redis.call('HGET', KEYS[1], 'lockOwner') if lo == ARGV[1] then redis.call('HSET', KEYS[1], 'lockUtil', 0) redis.call('HDEL', KEYS[1], 'lockOwner') end `, []string&#123;key&#125;, []interface&#123;&#125;&#123;owner&#125;) return err&#125; 读取缓存 1234567891011121314151617181920// new a client for rockscache using the default optionsrc := rockscache.NewClient(redisClient, NewDefaultOptions())v, err := rc.Fetch(\"key1\", 300, func()(string, error) &#123; // fetch data from database or other sources return \"value1\", nil&#125;)func (c *Client) Fetch(key string, expire time.Duration, fn func() (string, error)) (string, error) &#123; ex := expire - c.Options.Delay - time.Duration(rand.Float64()*c.Options.RandomExpireAdjustment*float64(expire)) v, err, _ := c.group.Do(key, func() (interface&#123;&#125;, error) &#123; //同样使用共享调用进行操作 if c.Options.DisableCacheRead &#123; //缓存崩溃直接读数据库 return fn() &#125; else if c.Options.StrongConsistency &#123; //强一致性 return c.strongFetch(key, ex, fn) &#125; return c.weakFetch(key, ex, fn) &#125;) return v.(string), err&#125; 这里也提供了忽略锁的操作 12345678910111213func (c *Client) RawGet(key string) (string, error) &#123; return c.rdb.HGet(c.rdb.Context(), key, \"value\").Result()&#125;func (c *Client) RawSet(key string, value string, expire time.Duration) error &#123; err := c.rdb.HSet(c.rdb.Context(), key, \"value\", value).Err() if err == nil &#123; //如果过期操作失败了，那么缓存可能永远不过期(根据AOF策略，默认每秒) //操作失败可能是网络或者redis宕机，如果是宕机，那么key可能都还没有落盘。所以这里得考虑网络异常情况 err = c.rdb.Expire(c.rdb.Context(), key, expire).Err() &#125; return err&#125; 强一致性获取 1234567891011121314151617181920212223242526272829303132333435363738394041func (c *Client) weakFetch(key string, expire time.Duration, fn func() (string, error)) (string, error) &#123; debugf(\"weakFetch: key=%s\", key) owner := shortuuid.New() r, err := c.luaGet(key, owner) for err == nil &amp;&amp; r[0] == nil &amp;&amp; r[1].(string) != locked &#123; debugf(\"empty result for %s locked by other, so sleep %s\", key, c.Options.LockSleep.String()) time.Sleep(c.Options.LockSleep) r, err = c.luaGet(key, owner) &#125; if err != nil &#123; return \"\", err &#125; if r[1] != locked &#123; return r[0].(string), nil &#125; if r[0] == nil &#123; return c.fetchNew(key, expire, owner, fn) &#125; go withRecover(func() &#123; _, _ = c.fetchNew(key, expire, owner, fn) &#125;) return r[0].(string), nil&#125;func (c *Client) strongFetch(key string, expire time.Duration, fn func() (string, error)) (string, error) &#123; debugf(\"strongFetch: key=%s\", key) owner := shortuuid.New() r, err := c.luaGet(key, owner) for err == nil &amp;&amp; r[1] != nil &amp;&amp; r[1] != locked &#123; // locked by other debugf(\"locked by other, so sleep %s\", c.Options.LockSleep) time.Sleep(c.Options.LockSleep) r, err = c.luaGet(key, owner) &#125; if err != nil &#123; return \"\", err &#125; if r[1] != locked &#123; // normal value return r[0].(string), nil &#125; return c.fetchNew(key, expire, owner, fn)&#125; 12345678910111213141516func (c *Client) fetchNew(key string, expire time.Duration, owner string, fn func() (string, error)) (string, error) &#123; result, err := fn() //自定义读取数据 if err != nil &#123; //成功则删除锁 _ = c.UnlockForUpdate(key, owner) return \"\", err &#125; if result == \"\" &#123; //如果结果为空 if c.Options.EmptyExpire == 0 &#123; // if empty expire is 0, then delete the key err = c.rdb.Del(c.rdb.Context(), key).Err() return \"\", err &#125; expire = c.Options.EmptyExpire &#125; err = c.luaSet(key, result, int(expire/time.Second), owner) //更新缓存 return result, err&#125; 总结 应该根据场景来设计合适的方案解决缓存一致性问题 读多写少的场景下，可以选择采用 Cache-Aside 结合消费数据库日志做补偿 的方案 写多的场景下，可以选择采用 Write-Through 结合分布式锁的方案 写多的极端场景下，可以选择采用 Write-Behind 的方案 可以通过读取 binlog (阿里云canal)异步删除缓存缓存 参考文档 https://blog.csdn.net/qq_34827674/article/details/123463175 https://learn.lianglianglee.com/专栏/300分钟吃透分布式缓存-完 分布式之数据库和缓存双写一致性方式解析 Cache-Aside Pattern Scaling Memcache at Facebook https://www.w3cschool.cn/architectroad/architectroad-cache-architecture-design.html https://cloud.tencent.com/developer/article/1932934 https://segmentfault.com/a/1190000040976439 https://talkgo.org/t/topic/1505 https://github.com/dtm-labs/rockscache/blob/main/helper/README-cn.md","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"算法-A071-前缀异或和","slug":"Algorithms To Live By/算法-A071-前缀异或和","date":"2022-07-24T06:48:35.000Z","updated":"2023-03-12T14:42:13.600Z","comments":true,"path":"2022/07/24/Algorithms To Live By/算法-A071-前缀异或和/","link":"","permalink":"http://xboom.github.io/2022/07/24/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95-A071-%E5%89%8D%E7%BC%80%E5%BC%82%E6%88%96%E5%92%8C/","excerpt":"","text":"前缀异或和问题是通过 前缀和 + 状态压缩 + 哈希表来处理连续 xxx 问题，发现是有规律的 通过前缀异或和获取每个位置的状态 使用哈希表记录每个状态第一次出现的位置 如果状态小于一定数量，可以通过状态压缩(BitMask)记录 问题 问题1：统计美丽子数组数目 12345678910111213141516171819class Solution &#123;public: long long beautifulSubarrays(vector&lt;int&gt;&amp; nums) &#123; long long res = 0; //计算前缀异或和！！！ vector&lt;int&gt; temp(nums.size() + 1); for(int i = 0; i &lt; nums.size(); i++) &#123; temp[i + 1] = temp[i] ^ nums[i]; //temp[i] 表示包含了 nums[i] 的异或和 &#125; //计算数据对总数！！！ unordered_map&lt;int, int&gt;dict; for(int x : temp) &#123; //printf(\"%d %d \\n\", x, dict[x]); res += dict[x]++; //这样算，每次 加1 类似 1 + 2 + 3 + n -&gt; 计算 n个数字一共有多少个数据对 &#125; return res; &#125;&#125;; 这里有两个知识点： 子数组的异或和等于前缀和的异或和 AB ^ A = B 注意 res += dict[mod]++;,可以理解为动态计算 1 + 2 + 3 + .... + n 问题2：和可被 K 整除的子数组 12345678910111213141516class Solution &#123;public: int subarraysDivByK(vector&lt;int&gt;&amp; nums, int k) &#123; unordered_map&lt;int, int&gt; dict; dict[0] = 1; int res = 0; int s = 0; for(int i = 0; i &lt; nums.size(); i++) &#123; s += nums[i]; //当被除数为负数时取模结果为负数，需要纠正(保证取余一定为正) int mod = (s % k + k) % k; res += dict[mod]++; &#125; return res; &#125;&#125;; 这里的知识点表示： C++取余是向0取整，所以就有 1234567/4= 17/(-4)= -17%4= 37%(-4)= 3(-7)/4= -1(-7)%4= -3 一个负数取余可能出现模为负数的情况，可以通过 mod = (s % k + k) % k 解决 问题3：找出最长的超赞子字符串 123456789101112131415161718192021222324252627class Solution &#123;public: int longestAwesome(string s) &#123; int temp = 0; unordered_map&lt;int, int&gt; dict; int l = 0; dict[0] = -1; for(int i = 0; i &lt; s.size(); i++) &#123; temp ^= (1 &lt;&lt; (s[i] - '0')); //status1 与 status2 两个状态相同，那么所有的都出现了偶数次 if(dict.count(temp) != 0) &#123; l = max(l, i - dict[temp]); &#125; else &#123; dict[temp] = i; &#125; //status1 与 status2 只差一位不同，找出这一位 for(int k = 0; k &lt; 10; k++) &#123; if(dict.count(temp ^ (1 &lt;&lt; k))) &#123; l = max(l, i - dict[temp ^ (1 &lt;&lt; k)]); &#125; &#125; &#125; return l; &#125;&#125;; 注意点： 为什么在查找 只差一位的情况时没有进行存储，因为当前前缀异或和的值是 temp 总结 遇到奇偶个数校验，想到 XOR 遇到有限的参数(小于20个)表状态， 想到状态压缩 （bitmask） 遇到求最长的连续子串使得和为k 想到 前缀和 + 哈希表 记录第一次出现某一状态的位置。 顺序练习 两数之和 和为K的子数组 和可被K整除的子数组 使数组和能被 P 整除 连续的子数组和 连续数组 面试题 17.05. 字母与数字 最美子字符串的数目 和相同的二元子数组 每个元音包含偶数次的最长子字符串 找出最长的超赞子字符串","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"go-zero8-数据库缓存","slug":"GoZero/GoZero8-数据库缓存","date":"2022-07-22T13:54:57.000Z","updated":"2022-07-22T13:54:57.000Z","comments":true,"path":"2022/07/22/GoZero/GoZero8-数据库缓存/","link":"","permalink":"http://xboom.github.io/2022/07/22/GoZero/GoZero8-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%AD%98/","excerpt":"","text":"问题背景 在 进程内缓存中说到的是直接在进程进行缓存的自管理，一般应用于业务生成的自定义数据(多副本情况下可能存在缓存一致性问题)。而随着业务量的增加，利用Redis构建专门的数据缓存，加快数据库访问。 由于数据库数据的特点，那么这里就有几个实现需要注意的点 数据一致性 缓存击穿、穿透、雪崩 缓存访问量、缓存命中率 实现原理 源码注释：core/stores/cache 构建缓存对象 根据缓存节点配置构建一个缓存对象用于业务层进行缓存处理 缓存支持通过多节点构建的缓存集群，也支持单个节点结构 多节点缓存集群通过一致性Hash进行访问，这里的多节点是指多个无关的缓存节点。而每个缓存节点可能都是一个Redis集群 共享调用防止缓存击穿 统计将环中命中，DB查询等情况上报给外部 12345//缓存集群对象cacheCluster struct &#123; dispatcher *hash.ConsistentHash errNotFound error&#125; 关于一致性Hash 在 《go-zero-一致性Hash》有说明 集群缓存操作 集群的缓存操作都是根据一致性Hash算法得出对应节点，然后演变成单节点的缓存操作 与单点不同的是，缓存集群批量删除Key,而Key有可能存在于多个节点上 当keys数量大于1的时候，首先会 for Hash(Key) 找到所有的缓存节点，然后再通过 for del 单次映射Hash节点或者删除Key并不会影响后续操作，而是通过 BatchError记录每一次错误 通过 make(map[interface{}][]string)保存节点与Key的关系 单点缓存操作 查询缓存 查询缓存的过程其实就是从Redis中获取数据的过程 这里有两点需要注意 当加载数据完毕，有一个将结果与空占位符比较的过程 value = &quot;*&quot;，是为了防止缓存穿透而故意设置的占位符。那么它是什么时候怎么插入的？ 将结果反序列如果失败，那么会去Redis删除这个 Key，表示存储的缓存异常 设置缓存 设置缓存是直接按照redis语法设置 k/v 与 expire 删除缓存 删除缓存时候，如果本身就是一个缓存集群，当对 keys进行批量删除的时候，需要依次删除每一个Key，而不是直接 del(keys...) 删除。 删除失败这里添加延时任务进行重试，但只会重试一次，失败后直接退出 获取缓存 获取缓存当缓存数据库没有的时候，就会直接从数据库加载并将数据保存到缓存数据库 当缓存中没有且数据库中也没有的时候，那么这个时候就会设置占位符(防止缓存穿透)，占位符的过期时间与普通的Key一致 关于查询数据库操作，这里仅仅是将结果实体传入，由数据层进行数据加载 当缓存没有的时候，是先查询数据库，然后更新缓存 技术内幕 代码：core/stores/cache/cachenode.go cacheNode 表示是单个缓存节点的对象 12345678910111213141516171819//cacheNode 表示单个缓存节点type cacheNode struct &#123; rds *redis.Redis //redis句柄 expiry time.Duration //缓存过期时间 notFoundExpiry time.Duration // barrier syncx.SingleFlight //共享调用 r *rand.Rand lock *sync.Mutex //原子锁 unstableExpiry mathx.Unstable stat *Stat //统计(单个节点的统计) errNotFound error&#125;const ( notFoundPlaceholder = \"*\" //避免缓存雪崩，这里加上随机过期时间随机值 [0.95, 1.05] * seconds expiryDeviation = 0.05) 这里几个过期分别有什么作用 expiry: notFoundExpiry: unstableExpiry: 使用共享调用 barrier减少缓存调用 rand.Rand 的随机数 新建缓存节点 123456789101112131415func NewNode(rds *redis.Redis, barrier syncx.SingleFlight, st *Stat, errNotFound error, opts ...Option) Cache &#123; o := newOptions(opts...) return cacheNode&#123; rds: rds, //redis句柄 expiry: o.Expiry, //过期时间 notFoundExpiry: o.NotFoundExpiry, //设置占位符过期时间 barrier: barrier, //共享调用 r: rand.New(rand.NewSource(time.Now().UnixNano())), lock: new(sync.Mutex), unstableExpiry: mathx.NewUnstable(expiryDeviation), //一定范围内过期时间 stat: st, //统计逻辑 errNotFound: errNotFound, //找不到缓存 &#125;&#125; 查询缓存 12345678910111213141516171819202122232425262728293031323334353637func (c cacheNode) doGetCache(ctx context.Context, key string, v interface&#123;&#125;) error &#123; c.stat.IncrementTotal() data, err := c.rds.GetCtx(ctx, key) if err != nil &#123; c.stat.IncrementMiss() return err &#125; if len(data) == 0 &#123; //数据为空 c.stat.IncrementMiss() return c.errNotFound &#125; c.stat.IncrementHit() if data == notFoundPlaceholder &#123; //防止缓存穿透 return errPlaceholder &#125; return c.processCache(ctx, key, data, v) //缓存处理&#125;func (c cacheNode) processCache(ctx context.Context, key, data string, v interface&#123;&#125;) error &#123; err := jsonx.Unmarshal([]byte(data), v) //反序列化 if err == nil &#123; return nil &#125; //... 日志记录 if _, e := c.rds.DelCtx(ctx, key); e != nil &#123; //删除缓存 logger.Errorf(\"delete invalid cache, node: %s, key: %s, value: %s, error: %v\", c.rds.Addr, key, data, e) &#125; // returns errNotFound to reload the value by the given queryFn return c.errNotFound&#125; 设置缓存 123456789func (c cacheNode) SetWithExpireCtx(ctx context.Context, key string, val interface&#123;&#125;, expire time.Duration) error &#123; data, err := jsonx.Marshal(val) //序列化 if err != nil &#123; return err &#125; return c.rds.SetexCtx(ctx, key, string(data), int(expire.Seconds()))&#125; 获取缓存 获取缓存直接加载加载redis，并根据加载结果进行不同的处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (c cacheNode) doTake(ctx context.Context, v interface&#123;&#125;, key string, query func(v interface&#123;&#125;) error, cacheVal func(v interface&#123;&#125;) error) error &#123; logger := logx.WithContext(ctx) val, fresh, err := c.barrier.DoEx(key, func() (interface&#123;&#125;, error) &#123; //共享调用加载key if err := c.doGetCache(ctx, key, v); err != nil &#123; if err == errPlaceholder &#123; //如果是占位符直接返回找不到 return nil, c.errNotFound &#125; else if err != c.errNotFound &#123; //如果是其他错误，直接返回而不要继续将错误蔓延到dbs //如果不这样，可能高并发导致redis奔溃之后，dbs也会跟着崩溃 return nil, err &#125; //数据库查询 if err = query(v); err == c.errNotFound &#123; //没有找到就设置占位符 if err = c.setCacheWithNotFound(ctx, key); err != nil &#123; logger.Error(err) &#125; return nil, c.errNotFound &#125; else if err != nil &#123; c.stat.IncrementDbFails() return nil, err &#125; if err = cacheVal(v); err != nil &#123; //缓存找到的缓存 logger.Error(err) &#125; &#125; return jsonx.Marshal(v) &#125;) if err != nil &#123; return err &#125; if fresh &#123; return nil &#125; //放在最后的原因是共享调用，这里共享调用不记录到总数中 c.stat.IncrementTotal() c.stat.IncrementHit() return jsonx.Unmarshal(val.([]byte), v) //反序列化&#125; 删除缓存 1234567891011121314151617181920func (c cacheNode) DelCtx(ctx context.Context, keys ...string) error &#123; if len(keys) == 0 &#123; return nil &#125; logger := logx.WithContext(ctx) if len(keys) &gt; 1 &amp;&amp; c.rds.Type == redis.ClusterType &#123; //如果是集群则for循环批量删除 for _, key := range keys &#123; if _, err := c.rds.DelCtx(ctx, key); err != nil &#123; logger.Errorf(\"failed to clear cache with key: %q, error: %v\", key, err) c.asyncRetryDelCache(key) //失败添加重试任务 &#125; &#125; &#125; else if _, err := c.rds.DelCtx(ctx, keys...); err != nil &#123; logger.Errorf(\"failed to clear cache with keys: %q, error: %v\", formatKeys(keys), err) c.asyncRetryDelCache(keys...) &#125; return nil&#125; 缓存统计 代码：core/stores/cache/cachestat.go 用于统计缓存情况 新建统计对象 12345678910111213141516171819202122232425262728293031// NewStat returns a Stat.func NewStat(name string) *Stat &#123; ret := &amp;Stat&#123; name: name, &#125; go ret.statLoop() return ret&#125;//statLoop 开启统计func (s *Stat) statLoop() &#123; ticker := time.NewTicker(statInterval) defer ticker.Stop() for range ticker.C &#123; total := atomic.SwapUint64(&amp;s.Total, 0) if total == 0 &#123; continue &#125; //计算缓存情况 hit := atomic.SwapUint64(&amp;s.Hit, 0) percent := 100 * float32(hit) / float32(total) //命中率 miss := atomic.SwapUint64(&amp;s.Miss, 0) //未命中 dbf := atomic.SwapUint64(&amp;s.DbFails, 0) //数据库调用事变 //发送统计信息 logx.Statf(\"dbcache(%s) - qpm: %d, hit_ratio: %.1f%%, hit: %d, miss: %d, db_fails: %d\", s.name, total, percent, hit, miss, dbf) &#125;&#125; 注意： 协程是一个常驻协程，缺少退出 SwapUint64的作用是：将新的值写入 addr，而返回addr中旧的值 缓存清理 当缓存删除失败，这里添加一个重试机制 初始化 1234567891011121314151617181920212223242526272829303132func init() &#123; var err error //时间轮 timingWheel, err = collection.NewTimingWheel(time.Second, timingWheelSlots, clean) logx.Must(err) //监听系统异常退出 proc.AddShutdownListener(func() &#123; timingWheel.Drain(clean) &#125;)&#125;//clean 时间轮操作func clean(key, value interface&#123;&#125;) &#123; taskRunner.Schedule(func() &#123; dt := value.(delayTask) err := dt.task() //执行任务 if err == nil &#123; return &#125; next, ok := nextDelay(dt.delay) //重复任务设置 if ok &#123; dt.delay = next timingWheel.SetTimer(key, dt, next) &#125; else &#123; msg := fmt.Sprintf(\"retried but failed to clear cache with keys: %q, error: %v\", formatKeys(dt.keys), err) logx.Error(msg) stat.Report(msg) //暂未实现 &#125; &#125;)&#125; 这里做了一个动态清理 1234567891011121314func nextDelay(delay time.Duration) (time.Duration, bool) &#123; switch delay &#123; case time.Second: return time.Second * 5, true case time.Second * 5: return time.Minute, true case time.Minute: return time.Minute * 5, true case time.Minute * 5: return time.Hour, true default: return 0, false &#125;&#125; 添加清理任务 12345678// AddCleanTask adds a clean task on given keys.func AddCleanTask(task func() error, keys ...string) &#123; timingWheel.SetTimer(stringx.Randn(taskKeyLen), delayTask&#123; delay: time.Second, task: task, keys: keys, &#125;, time.Second)&#125; 总结 采用共享调用的方式防止缓存击穿 采用占位符方式缓存床头 设置范围过期时间防止缓存雪崩 增加重试删除机制(时间轮) 参考链接 https://talkgo.org/t/topic/1716 https://talkgo.org/t/topic/1505","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"有趣的算法-老鼠毒药问题","slug":"Interesting Algorithm/有趣的算法-老鼠毒药","date":"2022-07-21T02:00:35.000Z","updated":"2022-07-21T02:00:35.000Z","comments":true,"path":"2022/07/21/Interesting Algorithm/有趣的算法-老鼠毒药/","link":"","permalink":"http://xboom.github.io/2022/07/21/Interesting%20Algorithm/%E6%9C%89%E8%B6%A3%E7%9A%84%E7%AE%97%E6%B3%95-%E8%80%81%E9%BC%A0%E6%AF%92%E8%8D%AF/","excerpt":"","text":"问题描述 有 1000 个一模一样的瓶子，其中有 999 瓶是普通的水，有一瓶是毒药。任何喝下毒药的生物都会在一星期之后死亡。现在，你只有 10 只小白鼠和一星期的时间，如何检验出哪个瓶子里有毒药？ 问题分析 这个题目需要面对几个问题 因为毒药会在一个星期后生效，而只有一星期的时间，那么只能让老鼠多喝几瓶最后根据老鼠中毒情况判断出哪一瓶有毒 要喝完这一千瓶，那么每只老鼠要喝很多，如果老鼠们喝的不交叉，那么即使老鼠中毒，也无法区分它喝的哪一瓶有毒 如果它们喝的有交叉，就需要每一瓶都存在部分老鼠喝了部分老鼠没有喝，且每一瓶喝与不喝的老鼠都不一样 使用二进制思想(当时就是没想到) 10个老鼠相当于10位的二进制位，可以表达的最大数量为1024 老鼠 1 2 3 4 5 6 7 8 9 10 0 0 0 0 0 0 0 0 0 0 如果给瓶子从1开始编号到1000，那么根据数字的二进制位，如果位数上是1的对应的老鼠就要喝掉这一瓶。根据老鼠的生存情况，就可以推断出哪一瓶有毒，假如：500是有毒的，那么它的二进制为 0111110100那么， 老鼠 1 2 3 4 5 6 7 8 9 10 500 0 1 1 1 1 1 0 1 0 0 508 0 1 1 1 1 1 1 1 0 0 510 0 1 1 1 1 1 1 1 1 0 512 0 1 1 1 1 1 1 1 1 1 也就是说：第2、3、4、5、6、8只老鼠会死掉。而喝了508，510、512的老鼠都没有事情 参考文档 https://blog.csdn.net/qq_43827595/article/details/104154716","categories":[{"name":"Interesting Algorithm","slug":"Interesting-Algorithm","permalink":"http://xboom.github.io/categories/Interesting-Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"go-zero6-服务管理","slug":"GoZero/GoZero6-服务管理","date":"2022-07-20T16:55:50.000Z","updated":"2022-07-20T16:55:50.000Z","comments":true,"path":"2022/07/21/GoZero/GoZero6-服务管理/","link":"","permalink":"http://xboom.github.io/2022/07/21/GoZero/GoZero6-%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/","excerpt":"","text":"问题背景 之前云平台是基于MQ实现的通信机制，后来需要将通信方式切换为Grpc但又不想修改老的接口和自定义逻辑。所以就需要修改原本的库。因为内部包含了配置加载、MQ适配、Grpc功能、服务注册发现、链路追踪、自定义功能(限速、大包处理、调试、过滤、统计、存储…)。这些子服务是相互独立且可选(不启用也可以)，部分存在依赖关系。所以就想着使用服务管理的方式管理这些小功能，并按照期望的方式进行启动运行 实现方案 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package mainimport \"context\"type ServerNum uint8const ( DemoA ServerNum = iota DemoB MaxServerNum)var serverList = []Server&#123; DemoA: &amp;Server1&#123;&#125;, DemoB: &amp;Server2&#123;&#125;,&#125;type Server interface &#123; Start(ctx context.Context) (Server, error) //服务启动 ID() int //返回服务标志，用于循序启动 Close() error //关闭服务 IsOpen() bool //是否开启&#125;//开启服务func Start(ctx context.Context) &#123; for _, v := range serverList &#123; v.Start(ctx) &#125;&#125;//服务管理type Manager struct &#123;&#125;//服务启动入口func (m *Manager)Start(ctx context.Context, s Server) error &#123; if s == nil || !s.IsOpen() &#123; //未设置对象或未开启则直接退出 return nil &#125; v, err := s.Start(ctx) if err != nil &#123; panic(err) &#125; key := v.ID() //构建k/v进行服务句柄存储 //TODO 这里使用 封装的context return nil&#125;func (m *Manager)Stop(ctx context.Context) &#123; for i := MaxServerNum - 1; i &gt;=0; i++ &#123; v := serverList[i] if v != nil &#123; v.Close() &#125; &#125;&#125; 单个服务内的子模块是否相互依赖的，所以这里使用切片进行存储，关闭时按照启动顺序的反方向 构建 IsOpen 方便根据配置加载服务需要的模块 服务句柄的存储暂时保存，需要使用服务句柄也许直接使用大写暴露会更好，或者使用不需要句柄的外部函数 注意服务的常驻等待(例如HTTP服务就是Listen之后不做后续处理，这个时候就需要使用协程拉起)，而所有服务加载完成之后，需要有一个常驻等待逻辑 12345678func Loop() &#123; exit := make(chan os.Signal, 1) signal.Notify(exit, syscall.SIGINT, syscall.SIGTERM) select &#123; case sig := &lt;- exit: log.Infof(context.Background(), \"recv signal %s\", sig.String()) &#125;&#125; 技术内幕 代码： core/service/servicegroup.go 1234567891011import \"github.com/zeromicro/go-zero/core/service\"// more codefunc main() &#123; group := service.NewServiceGroup() //初始化管理对象 defer group.Stop() //停止 group.Add(Morning&#123;&#125;) //添加服务morning group.Add(Evening&#123;&#125;) //添加服务evening group.Start() //服务开启&#125; 数据结构 123456789101112131415161718192021type ( Starter interface &#123; //开启接口 Start() &#125; Stopper interface &#123; //停止接口 Stop() &#125; //服务接口，开启与停止 Service interface &#123; Starter Stopper &#125; //服务批量管理接口 ServiceGroup struct &#123; services []Service //切片保存服务 stopOnce func() //停止一次接口 &#125;) 初始化对象 123456// NewServiceGroup returns a ServiceGroup.func NewServiceGroup() *ServiceGroup &#123; sg := new(ServiceGroup) sg.stopOnce = syncx.Once(sg.doStop) //停止逻辑使用sync.Once 处理 return sg&#125; 增加对象 12345//Add 服务管理中添加自定义服务func (sg *ServiceGroup) Add(service Service) &#123; // push front, stop with reverse order. sg.services = append([]Service&#123;service&#125;, sg.services...)&#125; 开启服务 1234567891011121314151617181920212223//Start 服务管理启动func (sg *ServiceGroup) Start() &#123; proc.AddShutdownListener(func() &#123; //将服务添加到退出监听中 log.Println(\"Shutting down...\") sg.stopOnce() &#125;) sg.doStart()&#125;//doStart 开始func (sg *ServiceGroup) doStart() &#123; routineGroup := threading.NewRoutineGroup() for i := range sg.services &#123; service := sg.services[i] routineGroup.RunSafe(func() &#123; //使用每个服务使用协程开启(包含panic处理) service.Start() &#125;) &#125; routineGroup.Wait() //等待退出&#125; 使用协程池进行每个服务的启动，所以每个服务启动的顺序是不一定的 使用RunSafe进行服务启动，所以一个服务panic，另外的服务也能启动 服务关闭 1234567891011// Stop stops the ServiceGroup.func (sg *ServiceGroup) Stop() &#123;NewCache sg.stopOnce()&#125;//停止服务管理(服务初始化的时候已经设置好了)func (sg *ServiceGroup) doStop() &#123; for _, service := range sg.services &#123; service.Stop() &#125;&#125; 仅有开启服务 12345678910111213141516171819202122232425262728293031323334// WithStart wraps a start func as a Service.func WithStart(start func()) Service &#123; return startOnlyService&#123; start: start, &#125;&#125;// WithStarter wraps a Starter as a Service.func WithStarter(start Starter) Service &#123; return starterOnlyService&#123; Starter: start, &#125;&#125;type ( stopper struct&#123;&#125; startOnlyService struct &#123; start func() stopper &#125; starterOnlyService struct &#123; Starter stopper &#125;)func (s stopper) Stop() &#123;&#125;func (s startOnlyService) Start() &#123; s.start()&#125; 总结 由于子服务无法保证启动顺序，所以各服务之间不能有相互依赖或关联 使用 stopOnce 防止出现多次调用stop的情况 使用 proc.AddShutdownListener做退出监听 优化： 如果能控制服务启动和调用顺序是否会更好(一个服务内的多个子模块必然存在先后启动关系)，那么停止服务的时候也注意先后关系 如果使用协程池拉起服务，那么单个服务的异常无法让所有子服务都退出。缺少一个同步退出机制 使用 startOnlyService是否有点多余，在服务停止中不做任何处理，那么退出时也能正常退出 参考文档 https://mp.weixin.qq.com/s/G6WG_-C6d-raoRmH4hBjoQ","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero7-进程内缓存","slug":"GoZero/GoZero7-进程内缓存","date":"2022-07-20T16:53:11.000Z","updated":"2022-07-20T16:53:11.000Z","comments":true,"path":"2022/07/21/GoZero/GoZero7-进程内缓存/","link":"","permalink":"http://xboom.github.io/2022/07/21/GoZero/GoZero7-%E8%BF%9B%E7%A8%8B%E5%86%85%E7%BC%93%E5%AD%98/","excerpt":"","text":"问题背景 缓存，目的是减少繁重的IO操作，增加系统并发能力。之前做的内存方案 同一类数据，设计一个双链表，然后添加一个定时器，动态删除过期内存。 实现一个简单的LRU缓存淘汰算法。 设计内进程内缓存主要需要考虑的是 可拓展性(缓存不同类型的数据) 过期处理，怎样才能友好的删除过期数据，不能无限存储 定时删除，不断循环所有key。缺点：需要遍历所有key，空耗CPU 惰性删除，访问的时候判断该键是否。缺点：如果不在访问，那么就会一直存在 如果保障进程内缓存一致性 单节点通知：一个节点完成修改，通知其他节点(类似raft，但raft有主)。缺点：时效性、可靠性 MQ订阅通知：一个节点完成修改，通知其他节点进行修改。缺点：缓存维护更复杂 定时更新：单个节点定时器，定时拉去新数据，更新内存数据。缺点：时效性 实现原理 这里看看go-zero是如何实现进程缓存设计的 从图中可以看出 缓存被存放在map LRU(使用双向链表+Map实现)用于对缓存的过期删除 limit 进行缓存数量限制 定时逻辑则是通过时间轮进行管理 还包含一个统计逻辑，进行缓存命中情况的上报 接下来看看它们是如何运转起来的 更新缓存 设置缓存要做三件事情 更新缓存 将Key到LRU管理中 添加或更新定时器 问题1：图中还有一个获取过期时间的过程，设置缓存的时候不是应该已经设置了过期时间了吗，为什么这个地方还要再次获取？ 其实这里获取的与过期时间近似的时间，目的是防止缓存雪崩 deviation:表示一个正负差值 base:给定的过期时间 v=(1+deviation−2∗deviation∗random)∗basev = (1 + deviation - 2 * deviation * random) * base v=(1+deviation−2∗deviation∗random)∗base 问题2：原理图中还有一个设置limit的过程，为什么在添加的过程中没有体现？ 其实这是LRU内部实现的时候使用的，当LRU双向链表长度大于limit的时候，会直接删除尾部节点 查询缓存 当缓存存在的时候，需要更新LRU。这里还添加了一个命中的逻辑 获取缓存 获取缓存与查询缓存不同的是，如果缓存中不存在，那么需要从DB获取数据并插入到缓存中 注意： 使用共享调用防止缓存穿透 查询DB操作其实是自定义操作(所以缓存不关心数据来源) 当缓存不存在，启动共享调用逻辑的时候它再次查询了一次缓存。因为DB操作是IO操作，而查询缓存是 O(1)的map内存操作 删除缓存 删除分为定时删除以及主动删除，当未更新定时器的时候，那么到期就会主动删除 K/V 如果是主动删除，则直接删除缓存，LRU 和 定时器 技术内幕 代码路径：core/collection/cache.go 123456789101112131415// 自定义缓存可选参数CacheOption func(cache *Cache)// 进程内缓存对象Cache struct &#123; name string //缓存名称，默认为 defaultCacheName=\"proc\" lock sync.Mutex //map锁 data map[string]interface&#123;&#125; //存储缓存 expire time.Duration //过期时间 timingWheel *TimingWheel //时间轮 lruCache lru //LRU 缓存淘汰机制 barrier syncx.SingleFlight //共享调用 unstableExpiry mathx.Unstable //随机过期时间差值(防止雪崩) stats *cacheStat //缓存统计&#125; 初始化缓存 1234567891011121314151617181920212223242526272829303132333435func NewCache(expire time.Duration, opts ...CacheOption) (*Cache, error) &#123; cache := &amp;Cache&#123; data: make(map[string]interface&#123;&#125;), expire: expire, lruCache: emptyLruCache, barrier: syncx.NewSingleFlight(), //共享调用 unstableExpiry: mathx.NewUnstable(expiryDeviation), &#125; //自定义操作包含 limit设置、缓存过期时间范围 expiryDeviation for _, opt := range opts &#123; opt(cache) &#125; if len(cache.name) == 0 &#123; cache.name = defaultCacheName //默认名称可以忽略 &#125; cache.stats = newCacheStat(cache.name, cache.size) //时间轮已经订好了1s时间间隔 timingWheel, err := NewTimingWheel(time.Second, slots, func(k, v interface&#123;&#125;) &#123; key, ok := k.(string) if !ok &#123; return &#125; cache.Del(key) //到期删除缓存 &#125;) if err != nil &#123; return nil, err &#125; cache.timingWheel = timingWheel return cache, nil&#125; 查询缓存 查询缓存很简单 1234567891011121314151617181920212223// Get returns the item with the given key from c.func (c *Cache) Get(key string) (interface&#123;&#125;, bool) &#123; value, ok := c.doGet(key) if ok &#123; c.stats.IncrementHit() &#125; else &#123; c.stats.IncrementMiss() &#125; return value, ok&#125;func (c *Cache) doGet(key string) (interface&#123;&#125;, bool) &#123; c.lock.Lock() defer c.lock.Unlock() value, ok := c.data[key] if ok &#123; c.lruCache.add(key) &#125; return value, ok&#125; 设置缓存 1234567891011121314151617181920//Set 设置缓存，默认使用缓存过期时间func (c *Cache) Set(key string, value interface&#123;&#125;) &#123; c.SetWithExpire(key, value, c.expire) &#125;// SetWithExpire 设置缓存 自定义过期时间func (c *Cache) SetWithExpire(key string, value interface&#123;&#125;, expire time.Duration) &#123; c.lock.Lock() _, ok := c.data[key] c.data[key] = value c.lruCache.add(key) c.lock.Unlock() expiry := c.unstableExpiry.AroundDuration(expire) //获取过期时间 if ok &#123; c.timingWheel.MoveTimer(key, expiry) &#125; else &#123; c.timingWheel.SetTimer(key, value, expiry) &#125;&#125; 获取缓存 123456789101112131415161718192021222324252627282930313233343536func (c *Cache) Take(key string, fetch func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; if val, ok := c.doGet(key); ok &#123; //查询缓存 c.stats.IncrementHit() return val, nil &#125; var fresh bool val, err := c.barrier.Do(key, func() (interface&#123;&#125;, error) &#123; // because O(1) on map search in memory, and fetch is an IO query // so we do double check, cache might be taken by another call // 再进行一次缓存操作 if val, ok := c.doGet(key); ok &#123; return val, nil &#125; v, e := fetch() if e != nil &#123; return nil, e &#125; fresh = true c.Set(key, v) return v, nil &#125;) if err != nil &#123; return nil, err &#125; if fresh &#123; c.stats.IncrementMiss() return val, nil &#125; // got the result from previous ongoing query c.stats.IncrementHit() return val, nil&#125; 删除缓存 1234567func (c *Cache) Del(key string) &#123; c.lock.Lock() delete(c.data, key) //删除k/v c.lruCache.remove(key) //删除LRU c.lock.Unlock() c.timingWheel.RemoveTimer(key) //删除定时器&#125; 总结 设置缓存过期时间的时候，设置一个范围时间防止缓存雪崩 使用共享调用去请求数据防止缓存击穿 通过LRU进行缓存淘汰，当多副本存在可能存在缓存一致性问题 统计逻辑用于上报，并没有一个开关(可能存在性能问题) 参考链接 https://talkgo.org/t/topic/2263","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero5-时间轮","slug":"GoZero/GoZero5-时间轮","date":"2022-07-17T08:41:58.000Z","updated":"2022-07-17T08:41:58.000Z","comments":true,"path":"2022/07/17/GoZero/GoZero5-时间轮/","link":"","permalink":"http://xboom.github.io/2022/07/17/GoZero/GoZero5-%E6%97%B6%E9%97%B4%E8%BD%AE/","excerpt":"","text":"问题背景 一个系统中存在着大量的延迟/定时任务： 在一个间隔时间之后做某事: 例如在最后一次消息发送的5分钟之后, 断开连接 在一个间隔时间之后不停的做某事: 例如每隔5分钟之后去发送心跳，检测连接是否正常 如果每个任务都使用自己的调度器来管理任务声明周期的话，浪费CPU的资源而且很低效，比如： 在定时器的数量增长到百万级之后, 基于最小堆实现的定时器的性能会显著降低 客户端会定时发送心跳以此来确保连接的可用性。导致每个连接都需要新建一些协程去维护 解决方案 延迟操作，通常可以采用两个方案： Timer：定时器维护一个优先队列，到时间点执行，然后把需要执行的 task 存储在 map 中 collection 中的 timingWheel ，维护一个存放任务组的数组，每一个槽都维护一个存储 task 的双向链表。开始执行时，计时器每隔指定时间执行一个槽里面的 tasks 时间轮是一种高效来利用线程资源来进行批量化调度的一种调度模型。把大批量的调度任务全部都绑定到同一个的调度器上面，使用这一个调度器来进行所有任务的管理(manager)，触发(trigger)以及运行(runnable) 实现原理 使用一个 类Map 的结构进行时间轮而构建，其中： 定时器的间隔就是从第n个槽进入到第n+1个槽的时间 每个槽中的任务通过双向链表进行存储，定时器到达槽的位置之后，并发处理槽的任务(指定时间相同) 问题1：新建的任务如何添加到对应的位置 如上图，槽位(numSlots)为12的时间轮，时间轮的定时间隔(interval)为1s，当前正在执行槽(tickedPos)为1位置的任务。 当添加一个延时时间(delay)为5s的任务的时候，那么： 需要等待的间隔数step：step = delay / interval 为 5。 需要放入的槽的位置position:position = (step + tickedPos) % numSlots 为 6。 问题2：新建任务超过槽的数目怎么办？ 如上图，当添加一个延时时间(delay)为18s的时候，根据上面的计算公司 需要等待的间隔数step = delay / interval = 18 需要放入的槽的位置 postion = (step + tickedPos) % numSlots = 6。 这个时候就出现一个问题，延迟5s的任务和延迟18s的任务会一起在5s后执行，这个时候就出现了多层环的概念 当第一层环的时间无法满足任务的延时的时候，可以将任务放置到第n层环上。时间轮还是会按照环的顺序进行执行 环的位置circle: circle = (steps - 1) / numSlots 所以延时5s的任务它的环 circle = (5 - 1) / 12 = 0，延时18s的任务它的环 circle = (18 -1)/12 = 1 随着时间的推移第一层环完成之后再去执行第二层环 问题3：不同环的任务是否都需要构建一个新的环来保存任务 不需要，不同环但是槽相同的任务放入到同一个双向链表中，当执行该槽任务的时候，只需要判断当前的任务if circle == 0，否则就不是当前环的任务，将circle -= 1 这样也有一个缺点：就是在遍历槽中任务的时候，虽然不是相同时间执行，但是槽中所有任务都需要遍历。不过一个微服务中定时任务也不会太多，所以缺点基本可以忽略 问题4：定时任务时间轮都是是如何处理的 根据任务类型可以分为延时任务与定时任务。根据任务执行一次可以分为执行一次和重复执行。这里有两点需要解决 不同类型任务的开始时间是怎么计算开始时间的 需要重复的任务是如何存储的 延时任务 执行一次：根据延时时间计算在时间轮中的位置，定时触发即可 重复执行：每次执行任务完成，判断任务是否重复，然后重新计算任务位置重新插入即可 定时任务： 执行一次： 问题5：如果一个时间间隔内时间任务处理不过来怎么办 通过下一节技术内幕来查看处理逻辑。不过可以预想到的是每个槽都会拉起协程来进行任务处理，如果是顺序执行有可能导致部分任务超出执行时间，所以每个任务都会使用一个协程处理，那么这里就可以考虑协程池以及对象池了。 问题6：如果一个任务的延时时间小于时间轮的时间片间隔，那么任务何时执行 这里有两个解决方案，go-zero使用第一种方案： 由于小于延时时间小于时间间隔，那么就认为任务不需要等到下一个时间轮的时间片执行，而是立即执行 利用多层环的原理处理更细粒度的任务。这样就不能重用circle的概念，而是需要 sub circle子环概念 circle用来解决延时时间超过当前环的问题 sub circle用来解决更细粒度的时间片需求。缺点就是逻辑结构更复杂，每个sub circle都需要有一个更细的时间粒度 技术内幕 代码路径：core/collection/timingwheel.go，前一节就是 go-zero 中时间轮的时间原理 时间轮对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// A TimingWheel is a timing wheel object to schedule tasks.TimingWheel struct &#123; interval time.Duration //时间划分刻度 ticker timex.Ticker slots []*list.List //数组内为双向链表指针 timers *SafeMap // tickedPos int //环的位置 numSlots int //时间槽 execute Execute //时间点执行任务的方法 //以下为不同任务的隧道 setChannel chan timingEntry //设置任务隧道 moveChannel chan baseEntry //移动任务隧道 removeChannel chan interface&#123;&#125; //删除任务隧道 drainChannel chan func(key, value interface&#123;&#125;) //并发执行任务隧道 stopChannel chan lang.PlaceholderType //时间轮停止通知隧道&#125;//Execute 执行任务的方法Execute func(key, value interface&#123;&#125;)//timingEntry timingEntry struct &#123; //时间轮实体对象 baseEntry value interface&#123;&#125; circle int diff int removed bool //是否删除&#125;//baseEntry 基础属性baseEntry struct &#123; delay time.Duration key interface&#123;&#125;&#125;//positionEntry 位置对象positionEntry struct &#123; pos int item *timingEntry&#125;//timingTask 时间任务timingTask struct &#123; key interface&#123;&#125; value interface&#123;&#125;&#125; 其中： 每个槽中因为是链表，所以并没有数量限制 时间轮实体timingEntry中有两个值需要关注 diff 和 removed 时间轮结构中 timers 作用往下看 timers 使用的 SafeMap 不仅仅是为了处理并发安全，还有就是原生map内存泄露问题的临时替代品(详见《GoZero-SafeMap》) 初始化时间轮 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// NewTimingWheel 初始化并返回一个时间轮func NewTimingWheel(interval time.Duration, numSlots int, execute Execute) (*TimingWheel, error) &#123; if interval &lt;= 0 || numSlots &lt;= 0 || execute == nil &#123; return nil, fmt.Errorf(\"interval: %v, slots: %d, execute: %p\", interval, numSlots, execute) &#125; return newTimingWheelWithClock(interval, numSlots, execute, timex.NewTicker(interval))&#125;//newTimingWheelWithClock 初始化时间轮//interval：时间划分刻度//numSlots：时间槽//execute：时间点执行函数func newTimingWheelWithClock(interval time.Duration, numSlots int, execute Execute, ticker timex.Ticker) (*TimingWheel, error) &#123; tw := &amp;TimingWheel&#123; interval: interval, // 单个时间格时间间隔 ticker: ticker, // 定时器，做时间推动，以interval为单位推进 slots: make([]*list.List, numSlots), // 时间轮 timers: NewSafeMap(), // 存储task&#123;key, value&#125;的map [执行execute所需要的参数] tickedPos: numSlots - 1, // at previous virtual circle execute: execute, // 执行函数 numSlots: numSlots, // 初始化 slots num setChannel: make(chan timingEntry), // 以下几个channel是做task传递的 moveChannel: make(chan baseEntry), removeChannel: make(chan interface&#123;&#125;), drainChannel: make(chan func(key, value interface&#123;&#125;)), stopChannel: make(chan lang.PlaceholderType), &#125; tw.initSlots() //使用list.New()初始化每个槽，构建双向链表保存任务 go tw.run() //开启协程，使用channel来做task时间任务接收与处理 return tw, nil&#125;//根据隧道类型等待不同的任务执行func (tw *TimingWheel) run() &#123; for &#123; select &#123; case &lt;-tw.ticker.Chan(): tw.onTick() case task := &lt;-tw.setChannel: //收到设置任务 tw.setTask(&amp;task) case key := &lt;-tw.removeChannel: //删除隧道 tw.removeTask(key) case task := &lt;-tw.moveChannel: //重复任务隧道 tw.moveTask(task) case fn := &lt;-tw.drainChannel: //并发任务 tw.drainAll(fn) case &lt;-tw.stopChannel: //停止隧道 tw.ticker.Stop() return &#125; &#125;&#125; 在时间轮拉起协程进行隧道的监听与处理，这里需要注意的是： 除了时间轮的定时器隧道，其他隧道都可以通过外部接口将消息传入处理 其中moveChannel和drainChannel需要解释： moveChannel：更新任务延时时间 drainChannel：使用自定义函数并发执行所有任务 添加任务 12345678910111213141516171819202122232425262728293031323334353637// SetTimer 设置过期时间func (tw *TimingWheel) SetTimer(key, value interface&#123;&#125;, delay time.Duration) error &#123; if delay &lt;= 0 || key == nil &#123; return ErrArgument &#125; select &#123; case tw.setChannel &lt;- timingEntry&#123; //向设置隧道中添加一个基础的任务对象 baseEntry: baseEntry&#123; delay: delay, key: key, &#125;, value: value, &#125;: return nil //设置完直接结束 case &lt;-tw.stopChannel: //当设置过程中收到退出通知则直接退出 return ErrClosed &#125;&#125;//setTask 时间轮收到设置任务任务之后进行任务添加func (tw *TimingWheel) setTask(task *timingEntry) &#123; if task.delay &lt; tw.interval &#123; //如果任务延迟时间小于时间轮的刻度，那么延迟时间等于刻度 task.delay = tw.interval &#125; if val, ok := tw.timers.Get(task.key); ok &#123; //如果已经存在这个任务，更新任务 entry := val.(*positionEntry) entry.item.value = task.value tw.moveTask(task.baseEntry) &#125; else &#123; pos, circle := tw.getPositionAndCircle(task.delay) //根据延时获取任务位置 task.circle = circle //设置任务所在的环 tw.slots[pos].PushBack(task) //将任务添加到对应槽中 tw.setTimerPosition(pos, task) //更新任务最新位置 &#125;&#125; 注意： 在添加任务的时候这里的 value 类型是 interface 类型，而不是指定的任务对象 注意这里使用 setTimerPosition 又缓存任务，它的作用往下看 如 实现原理 中介绍的那样计算任务所在的槽以及环 1234567891011121314//getPositionAndCircle 获取位置与环//@param: d 任务延时时间//@return:// pos 任务所在槽// circle 任务环func (tw *TimingWheel) getPositionAndCircle(d time.Duration) (pos, circle int) &#123; //如果将任务的延迟时间按照时间轮刻度划分，那么 steps 就是在第几个刻度内 steps := int(d / tw.interval) // 时间轮的相对位置 + 延迟相对时间刻度的位置 % 槽的数量 = 任务在时间轮中的位置 pos = (tw.tickedPos + steps) % tw.numSlots circle = (steps - 1) / tw.numSlots return&#125; 设置任务的位置 12345678910111213//setTimerPosition 设置任务位置func (tw *TimingWheel) setTimerPosition(pos int, task *timingEntry) &#123; if val, ok := tw.timers.Get(task.key); ok &#123; //如果任务缓存已经存在，那么更新槽的位置 timer := val.(*positionEntry) timer.item = task timer.pos = pos &#125; else &#123; tw.timers.Set(task.key, &amp;positionEntry&#123; //如果任务缓存不存在，则设置任务缓存 pos: pos, item: task, &#125;) &#125;&#125; 所以 时间轮中 timers其实是通过key保存任务，并把完成任务和位置都保存 更新任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//MoveTimer移动任务func (tw *TimingWheel) MoveTimer(key interface&#123;&#125;, delay time.Duration) error &#123; if delay &lt;= 0 || key == nil &#123; return ErrArgument &#125; select &#123; case tw.moveChannel &lt;- baseEntry&#123; delay: delay, key: key, &#125;: return nil case &lt;-tw.stopChannel: return ErrClosed &#125;&#125;//moveTask 更新任务执行位置func (tw *TimingWheel) moveTask(task baseEntry) &#123; val, ok := tw.timers.Get(task.key) //通过key获取任务(因为是移动，所以当任务不存在，那么直接结束。) if !ok &#123; return &#125; timer := val.(*positionEntry) if task.delay &lt; tw.interval &#123; //如果任务的delay小于时间片，那么立即执行 threading.GoSafe(func() &#123; //拉起协程进行执行 tw.execute(timer.item.key, timer.item.value) &#125;) return &#125; pos, circle := tw.getPositionAndCircle(task.delay) //计算任务位置 if pos &gt;= timer.pos &#123; timer.item.circle = circle timer.item.diff = pos - timer.pos //计算任务位置与缓存中任务位置的不同 &#125; else if circle &gt; 0 &#123; //不是当前环的任务，那么 circl - 1 circle-- timer.item.circle = circle timer.item.diff = tw.numSlots + pos - timer.pos //不同 加上一个环的槽数，判断位置是否相同 &#125; else &#123; timer.item.removed = true newItem := &amp;timingEntry&#123; baseEntry: task, value: timer.item.value, &#125; tw.slots[pos].PushBack(newItem) tw.setTimerPosition(pos, newItem) &#125;&#125; 注意： 移动任务MoveTimer(key interface{}, delay time.Duration) error {... }并没有参数 Value，用来移动更新已有的任务 执行任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//定时器触发任务func (tw *TimingWheel) onTick() &#123; //获取需要执行任务的槽位置 tw.tickedPos = (tw.tickedPos + 1) % tw.numSlots l := tw.slots[tw.tickedPos] //获取槽的对应链表 tw.scanAndRunTasks(l)&#125;func (tw *TimingWheel) scanAndRunTasks(l *list.List) &#123; var tasks []timingTask for e := l.Front(); e != nil; &#123; //遍历链表 task := e.Value.(*timingEntry) if task.removed &#123; //如果任务状态为删除，则从任务链表中删除任务 next := e.Next() l.Remove(e) e = next continue &#125; else if task.circle &gt; 0 &#123; //如果任务所在的环大于0 task.circle-- e = e.Next() continue &#125; else if task.diff &gt; 0 &#123; //如果任务位置发生了变化 next := e.Next() l.Remove(e) //先删掉任务 // (tw.tickedPos+task.diff)%tw.numSlots // cannot be the same value of tw.tickedPos pos := (tw.tickedPos + task.diff) % tw.numSlots //根据任务最新的位置计算位置重新放入任务 tw.slots[pos].PushBack(task) tw.setTimerPosition(pos, task) task.diff = 0 e = next continue &#125; tasks = append(tasks, timingTask&#123; key: task.key, value: task.value, &#125;) next := e.Next() l.Remove(e) tw.timers.Del(task.key) e = next &#125; tw.runTasks(tasks)&#125; 具体执行 12345678910111213func (tw *TimingWheel) runTasks(tasks []timingTask) &#123; if len(tasks) == 0 &#123; return &#125; go func() &#123; for i := range tasks &#123; //拉起一个协程，不断执行任务(多任务非并发) threading.RunSafe(func() &#123; tw.execute(tasks[i].key, tasks[i].value) &#125;) &#125; &#125;()&#125; 注意：每个任务都使用一个协程进行执行，这里使用的是 go-zero封装的协程池，这个go 会等待所有任务执行完毕才会结束 删除任务 12345678910111213141516171819202122232425// RemoveTimer removes the task with the given key.func (tw *TimingWheel) RemoveTimer(key interface&#123;&#125;) error &#123; if key == nil &#123; return ErrArgument &#125; select &#123; case tw.removeChannel &lt;- key: return nil case &lt;-tw.stopChannel: return ErrClosed &#125;&#125;//removeTaskfunc (tw *TimingWheel) removeTask(key interface&#123;&#125;) &#123; val, ok := tw.timers.Get(key) //任务不存在 if !ok &#123; return &#125; timer := val.(*positionEntry) timer.item.removed = true //增加一个状态表示已经被删除 tw.timers.Del(key) &#125; 排空任务 123456789101112131415161718192021222324252627//Drain 使用 fn 排空任务func (tw *TimingWheel) Drain(fn func(key, value interface&#123;&#125;)) error &#123; select &#123; case tw.drainChannel &lt;- fn: return nil case &lt;-tw.stopChannel: return ErrClosed &#125;&#125;//drainAll 排空任务func (tw *TimingWheel) drainAll(fn func(key, value interface&#123;&#125;)) &#123; runner := threading.NewTaskRunner(drainWorkers) for _, slot := range tw.slots &#123; for e := slot.Front(); e != nil; &#123; task := e.Value.(*timingEntry) next := e.Next() slot.Remove(e) e = next if !task.removed &#123; runner.Schedule(func() &#123; fn(task.key, task.value) &#125;) &#125; &#125; &#125;&#125; 排空任务并不是直接删除，而是提供一个自定义函数接口用于处理剩下的任务，有利于安全退出。但需要注意的是每个任务都会拉一个协程进行处理，也就是不能立即执行 时间轮的 Stop 停止时间轮 1234// Stop stops tw. No more actions after stopping a TimingWheel.func (tw *TimingWheel) Stop() &#123; close(tw.stopChannel)&#125; 总结 task从 优先队列 O(nlog(n)) 降到 双向链表 O(1)，而执行task也只要轮询一个时间点的tasks O(N)，不需要像优先队列，放入和删除元素 O(nlog(n))。 时间轮中的多层环是一种虚拟概念，用来记录超出范围(nusSlots * interval)的任务 使用chan接收外部接口调用的好处是并发安全，当然内部逻辑实现还是需要注意map安全 补充： 可以添加一个在协程池中讲到的时间轮状态，保证不会有新的任务在退出的时候加进来 可以使用协程池和内存池的节省内存空间，但如果协程池处理不过来使后续任务阻塞，可能会导致时间轮功能异常 分布式系统中系统的定时调用则需要使用分布式定时器，这在另外的章节中学习 技术应用 go-zero用于缓存的定时删除 12345678910111213timingWheel, err := NewTimingWheel(time.Second, slots, func(k, v interface&#123;&#125;) &#123; key, ok := k.(string) if !ok &#123; return &#125; cache.Del(key) //到达则删除缓存&#125;)if err != nil &#123; return nil, err&#125;cache.timingWheel = timingWheel 参考链接 https://www.ericcai.fun/detail/16 https://juejin.cn/post/6844904110399946766 https://xiaorui.cc/archives/6160 https://zhuanlan.zhihu.com/p/264826698 https://lk668.github.io/2021/04/05/2021-04-05-手把手教你如何用golang实现一个timewheel/ https://go-zero.dev/cn/docs/blog/principle/timing-wheel","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"有趣的算法-赛马问题","slug":"Interesting Algorithm/有趣的算法-赛马问题","date":"2022-07-09T12:06:06.000Z","updated":"2022-07-09T12:06:06.000Z","comments":true,"path":"2022/07/09/Interesting Algorithm/有趣的算法-赛马问题/","link":"","permalink":"http://xboom.github.io/2022/07/09/Interesting%20Algorithm/%E6%9C%89%E8%B6%A3%E7%9A%84%E7%AE%97%E6%B3%95-%E8%B5%9B%E9%A9%AC%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题描述 64匹马，8个跑道，问最少比赛多少场，可以选出跑得最快的4匹马。每场比赛每个跑道只允许一匹马，且不存在并列情形 问题分析 第一步：先让马儿跑起来，首先将马儿分批次赛跑，一共需要进行8次赛跑。假设结果如下： 分组 第一名 第二名 第三名 第四名 第五名 第六名 第七名 第八名 A A1 A2 A3 A4 A5 A6 A7 A8 B B1 B2 B3 B4 B5 B6 B7 B8 C C1 C2 C3 C4 C5 C6 C7 C8 D D1 D2 D3 D4 D5 D6 D7 D8 E E1 E2 E3 E4 E5 E6 E7 E8 F F1 F2 F3 F4 F5 F6 F7 F8 G G1 G2 G3 G4 G5 G6 G7 G8 H H1 H2 H3 H4 H5 H6 H7 H8 可直接排除各组最后四名赛马，剩余64-4*8=32匹赛马待定 第二步：将每一组中的第一名进行赛跑(如果每一组选多个参加赛马，那样就存在重复比赛)，需要进行1次赛跑。假设结果如下： 分组 第一名 第二名 第三名 第四名 第五名 第六名 第七名 第八名 1 A1 B1 C1 D1 E1 F1 G1 H2 可直接排除各组最后四名赛马，也就是后四组全部淘汰，剩余 32 - 4 * 4 = 16，其中第一名已经知道就是A1 需要注意的是：这里还可以继续排除 因为在头名争夺中 D1只能排第四，所以D1最快也是第四，D组剩余被淘汰 同理C组最多只有2名在前4 同理B组最多只有1名在前4 分组 第一名 第二名 第三名 第四名 A A1 A2 A3 A4 B B1 B2 B3 B4 C C1 C2 C3 C4 D D1 D2 D3 D4 所以剩余需要确认的数量为 16 - 1 - 3 - 2 - 1 = 9。 第三步：剩余的9匹赛马中需要选出8匹马再次进行一次赛马 这里是否有一匹特殊的马，不需要参与赛跑进行这一次赛马就能得出结果？ 排除A组中的3匹马中的一匹，那么除非B1都输或者都赢，否则B1以及后面的排名不确定。也就是排除前面的对后面影响较大 排除D1/C2，那么可能无法确认D1与C2谁是第四 排除C1与排除其他一样，可能无法确认自己和它身后的排名 所以，这里最好从D1与C2中排除一个进行赛跑，如果这一轮得出结果，那么就不用跑。如果没有得出结论就再跑一次 因为所有赛马的第一名已经确认是第一，所以剩下的比赛就是确认 2 - 4 名次，C1要是所有赛马的前四名，这次必须跑入前三。 第一种可能：C1第三名或者第三名之后，那么比赛结束，一共经过了 8 + 1 + 1 = 10 赛出前四名 第二种可能：C1排在第二名，也就是说 C2 和 D1 无法确认谁是第四个，那么就需要加赛一场。排除前三，剩余的马再比一场。一共经过了 8 + 1 + 1 + 1 = 11 参考文档 https://zhuanlan.zhihu.com/p/103572219","categories":[{"name":"Interesting Algorithm","slug":"Interesting-Algorithm","permalink":"http://xboom.github.io/categories/Interesting-Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"go-zero4-协程池1","slug":"GoZero/GoZero4-协程池1","date":"2022-07-06T14:55:05.000Z","updated":"2023-03-03T15:38:13.085Z","comments":true,"path":"2022/07/06/GoZero/GoZero4-协程池1/","link":"","permalink":"http://xboom.github.io/2022/07/06/GoZero/GoZero4-%E5%8D%8F%E7%A8%8B%E6%B1%A01/","excerpt":"","text":"问题背景 需要协程池吗？ 虽然协程非常轻量级的，一般用不上协程池。协程池的作用 无休止地创建大量goroutine，势必会因为对大量go 程的创建、调度和销毁带来性能损耗。 为了解决这个问题，可以引入协程池 解决方案 协程池需要什么？ 协程如何重用、任务如何执行 协程池支持自定义协程池大小 如果当前任务数量超过协程池大小，那么当前任务需要等待，等待时间支持超时退出 协程支持自定义退出 异常捕获，防止因为单个协程的异常处理导致整个协程池无法使用 协程池大致的逻辑如下图所示： 方案1 这是一个网上能搜到的 “100行实现一个协程池”， 第一步：定义一个任务 1234type Task struct &#123; Handler func(v ...interface&#123;&#125;) Params []interface&#123;&#125;&#125; 其实是将需要执行的协程方法，使用结构体封装起来 第二步：定义一个协程池 12345678910111213type Pool struct &#123; capacity uint64 //容量 runningWorkers uint64 //正在运行的协程树 status int64 //状态(防止在退出过程中，仍然在新建协程) chTask chan *Task //任务队列 PanicHandler func(interface&#123;&#125;)//panic处理函数 sync.Mutex //原子锁,用于保证runningWorkers原子数据变化&#125;const ( RUNNING = 1 STOPED = 0) 第三步：利用协程池启动一个协程执行任务 12345678910111213141516171819202122232425262728293031323334353637func (p *Pool) run() &#123; p.incRunning() //增加正在运行的协程数目 go func() &#123; defer func() &#123; p.decRunning() //结束任务 if r := recover(); r != nil &#123; if p.PanicHandler != nil &#123; p.PanicHandler(r) &#125; else &#123; log.Printf(\"Worker panic: %s\\n\", r) &#125; &#125; p.checkWorker() // check worker avoid no worker running &#125;() for &#123; select &#123; case task, ok := &lt;-p.chTask: if !ok &#123; return &#125; task.Handler(task.Params...) //任务处理 &#125; &#125; &#125;()&#125;//如果任务数量大于0，但是这个时候协程都退出，则再次构建一个协程func (p *Pool) checkWorker() &#123; p.Lock() defer p.Unlock() if p.runningWorkers == 0 &amp;&amp; len(p.chTask) &gt; 0 &#123; p.run() &#125;&#125; 协程池的协程不是常驻协程吗，为什么会出现协程数量为0，但是任务大于0的情况呢？ 答：工作协程可能因为 panic 都退出了，那么这个时候就需要有一个重新拉起协程去执行任务 第四步：生产任务 1234567891011121314151617181920func (p *Pool) Put(task *Task) error &#123; p.Lock() defer p.Unlock() if p.status == STOPED &#123; return ErrPoolAlreadyClosed &#125; // run worker if p.GetRunningWorkers() &lt; p.GetCap() &#123; p.run() &#125; // send task if p.status == RUNNING &#123; p.chTask &lt;- task &#125; return nil&#125; 为什么在存放任务的时候，会多一个协程池的判断呢？ 答：可能会出现协程池结束关闭的情况，如果这个时候又有新的任务，那就又会创建新的协程去执行 最后：关闭协程池 12345678910111213// Close close pool gracefulfunc (p *Pool) Close() &#123; if !p.setStatus(STOPED) &#123; // stop put task return &#125; for len(p.chTask) &gt; 0 &#123; // 等待所有的任务都被消费 time.Sleep(1e6) // 防止等待任务清空 cpu 负载突然变大, 这里小睡一下 &#125; close(p.chTask)&#125; 总结： 这里添加了协程池的状态，防止退出时候的任务增加 为什么在退出的时候，如果任务大于0，那么需要 sleep 一下？ 异常捕获之后再次检查是否有协程在执行任务，没有则添加一个协程 使用无缓冲channel进行任务执行，可能会出现添加任务阻塞的情况 任务是否可以添加一个运行超时时间，防止单个任务死锁？ 方案2 字节跳动开源的协程池，仓库地址：https://github.com/bytedance/gopkg/tree/develop/util/gopool 使用 生产者-消费者模式 设计协程池 第一步：协程池 具有的功能 12345678type Pool interface &#123; Name() string //协程池名称 SetCap(cap int32) //协程池容量 Go(f func()) //使用协程执行 f CtxGo(ctx context.Context, f func()) //使用协程执行f并支持参数 ctx SetPanicHandler(f func(context.Context, interface&#123;&#125;)) //设置协程处理函数 WorkerCount() int32 //返回正在运行的协程数量&#125; 第二步：协程池的结构 1234567891011type pool struct &#123; name string //协程池名称 cap int32 //协程池容量 config *Config //协程池配置 taskHead *task //任务头部 taskTail *task //任务尾部 taskLock sync.Mutex //任务原子锁(竞争) taskCount int32 //任务数量 workerCount int32 //正在运行的协程数量 panicHandler func(context.Context, interface&#123;&#125;) //Panic处理逻辑&#125; 第三步：看看 任务 task 的定义 123456789101112type task struct &#123; ctx context.Context f func() //执行函数 next *task //指向下一个任务的指针&#125;type taskList struct &#123; //使用双向链表将任务连接起来 sync.Mutex taskHead *task taskTail *task&#125; 第四步：查看协程池是怎么运行的 1234567891011121314151617181920212223242526272829var taskPool sync.Pool //对象池func (p *pool) CtxGo(ctx context.Context, f func()) &#123; t := taskPool.Get().(*task) //从对象池获取任务对象 t.ctx = ctx t.f = f p.taskLock.Lock() //获取任务写锁 if p.taskHead == nil &#123; //如果任务链表为空则新建，否则插入链表尾部 p.taskHead = t p.taskTail = t &#125; else &#123; p.taskTail.next = t p.taskTail = t &#125; p.taskLock.Unlock() //释放任务写锁 atomic.AddInt32(&amp;p.taskCount, 1) //增加任务数量 /* 1. 如果任务数量大于配置的数量 &amp;&amp; 正在执行的协程数量小于协程池容量，说明任务太多，还有空闲的协程 那么就开启一个新的协程处理 如果任务数量小于配置的数量 &amp;&amp; 正在执行的协程数量小于协程池容量。说明任务还不多，就让当前协程顺序执行 2. 正在执行的协程为0 */ if (atomic.LoadInt32(&amp;p.taskCount) &gt;= p.config.ScaleThreshold &amp;&amp; p.WorkerCount() &lt; atomic.LoadInt32(&amp;p.cap)) || p.WorkerCount() == 0 &#123; p.incWorkerCount() w := workerPool.Get().(*worker) w.pool = p w.run() &#125;&#125; 这里额外定义了一个 workPool，其实是消费者池 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354var workerPool sync.Pooltype worker struct &#123; //消费者其实是一个协程池 pool *pool&#125;func (w *worker) run() &#123; go func() &#123; for &#123; //这是一个常驻协程 var t *task w.pool.taskLock.Lock() //获取任务写锁 if w.pool.taskHead != nil &#123; //获取任务并将任务数量-1 t = w.pool.taskHead w.pool.taskHead = w.pool.taskHead.next atomic.AddInt32(&amp;w.pool.taskCount, -1) &#125; if t == nil &#123; //如果没有任务，那么worker销毁 // if there's no task to do, exit w.close() w.pool.taskLock.Unlock() w.Recycle() return &#125; w.pool.taskLock.Unlock() //释放任务写锁 func() &#123; defer func() &#123; if r := recover(); r != nil &#123; //异常处理 if w.pool.panicHandler != nil &#123; w.pool.panicHandler(t.ctx, r) &#125; else &#123; msg := fmt.Sprintf(\"GOPOOL: panic in pool: %s: %v: %s\", w.pool.name, r, debug.Stack()) logger.CtxErrorf(t.ctx, msg) &#125; &#125; &#125;() t.f() //执行任务 f &#125;() t.Recycle() //任务结束后，会收work &#125; &#125;()&#125;func (w *worker) close() &#123; //减少worker数目 w.pool.decWorkerCount()&#125;func (w *worker) zero() &#123; //释放缓存池 w.pool = nil&#125;func (w *worker) Recycle() &#123; //释放worker(存入缓存池) w.zero() workerPool.Put(w)&#125; 最后，它还定义了一个 poolMap 用于根据名称注册与使用 多个协程池 注意： 使用双向链表存储任务，表示它理论上支持无限个任务。后面的任务可能存在长时间等待的情况 使用任务上限的好处是，不是每来一个任务都开启一个协程，而是任务超过一定数量而又空闲的协程才开启新的协程去执行 技术内幕 go-zero是如何实现协程池的，代码路径：core/threading 第一步，定义了recover逻辑，用于 panic 之后的清理操作 12345678910// core/rescue/recover.gofunc Recover(cleanups ...func()) &#123; for _, cleanup := range cleanups &#123; cleanup() &#125; if p := recover(); p != nil &#123; //logx.ErrorStack(p) &#125;&#125; 第二步，定义了一个安全运行goroutine的方案 GoSafe，包含处理panic逻辑 123456789func GoSafe(fn func()) &#123; go RunSafe(fn)&#125;func RunSafe(fn func()) &#123; defer rescue.Recover() fn()&#125; TaskRunner： 使用 limitChan 协程池 执行协程 123456789101112131415161718192021222324// TaskRunner 用于并发控制协程数量type TaskRunner struct &#123; limitChan chan lang.PlaceholderType&#125;// 创建 TaskRunner 对象func NewTaskRunner(concurrency int) *TaskRunner &#123; return &amp;TaskRunner&#123; limitChan: make(chan lang.PlaceholderType, concurrency), &#125;&#125;// 在 任务并发控制下执行 taskfunc (rp *TaskRunner) Schedule(task func()) &#123; rp.limitChan &lt;- lang.Placeholder //limitChan 类似一个并发锁 go func() &#123; defer rescue.Recover(func() &#123; &lt;-rp.limitChan &#125;) task() &#125;()&#125; 注意：当limitChan满那么任务执行会出现超时，缺乏超时逻辑 WorkerGroup：使用 wokers 并发执行任务 job 123456789101112131415161718192021type WorkerGroup struct &#123; job func() workers int&#125;// NewWorkerGroup returns a WorkerGroup with given job and workers.func NewWorkerGroup(job func(), workers int) WorkerGroup &#123; return WorkerGroup&#123; job: job, workers: workers, &#125;&#125;// Start starts a WorkerGroup.func (wg WorkerGroup) Start() &#123; group := NewRoutineGroup() for i := 0; i &lt; wg.workers; i++ &#123; group.RunSafe(wg.job) &#125; group.Wait()&#125; RoutineGroup: 多协程等待 123456789101112131415161718192021222324252627282930313233// RoutineGroup 多协程等待type RoutineGroup struct &#123; waitGroup sync.WaitGroup&#125;func NewRoutineGroup() *RoutineGroup &#123; return new(RoutineGroup)&#125;// 不要引用外部参数，可能被其他协程修改func (g *RoutineGroup) Run(fn func()) &#123; g.waitGroup.Add(1) go func() &#123; defer g.waitGroup.Done() fn() &#125;()&#125;//不要引用外部参数，可能被其他协程修改func (g *RoutineGroup) RunSafe(fn func()) &#123; g.waitGroup.Add(1) GoSafe(func() &#123; defer g.waitGroup.Done() fn() &#125;)&#125;// 等待所有协程结束func (g *RoutineGroup) Wait() &#123; g.waitGroup.Wait()&#125; 所以 go-zero的threading 并不是真正的协程池，仅仅是提供多种并发执行 goroutine的方法 总结 所以从目前来看，实现一个协程池都有哪些值得学习的地方呢？ 将需要使用临时协程执行的函数已任务的形式 任务 -- 协程池(Pool) -- 工人执行 协程池是有容量限制的，有了容量就有正在运行的协程数 协程池有状态防止在退出的时候仍然进行任务构建与执行 协程池有异常捕获机制，保证单个异常不会影响整个协程池 任务已任务合集的形式存在，让消费者并发消费 有了异常捕获与任务合集，为了防止工人都发生异常，而还有任务没有执行，则需要有工人唤起机制 可以使用本地缓存池进行工人的重复利用 任务合集缓隧道还是双链表、每个任务都构建一个协程还是单个任务多任务执行的选择 下一节，将学习另外一个协程池 ants 的实现方式 参考文档 https://go-zero.dev/cn/docs/goctl/installation/","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero3-MapReduce","slug":"GoZero/GoZero3-MapReduce","date":"2022-07-03T08:37:40.000Z","updated":"2022-07-03T08:41:55.000Z","comments":true,"path":"2022/07/03/GoZero/GoZero3-MapReduce/","link":"","permalink":"http://xboom.github.io/2022/07/03/GoZero/GoZero3-MapReduce/","excerpt":"","text":"什么是MapReduce? MapReduce是Google提出了一个软件架构，用于大规模数据集的并行运算。 MapReduce通过把对数据集的大规模操作分发给网络上的每个节点实现可靠性；每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。 go-zero的MapReduce则借鉴其中的思想，接下来一起看下go-zero是如何应用这一思想的 问题背景 在微服务中开发中，如果多个服务串行依赖的话那么整个API的耗时将会大大增加。通过什么手段来优化？ 传输层面通过MQ的解耦特性来降低API的耗时 MQ通信效率没有grpc高(消息通过MQ服务器进行中转) 业务层面通过Go语言的WaitGroup工具来进行并发控制 自行封装Add与Done 实际业务场景中： 如果接口的多个依赖有一个出错，则期望能立即返回且不必等待所有依赖都执行完毕。已经完成的接口调用也应该回滚 多个依赖可能有部分依赖之间也存在着相互依赖，或者上下关系 go-zero的主要应用场景为：需要从不同的rpc服务中获取相应属性组装成复杂对象，比如要查询商品详情： 商品服务-查询商品属性 库存服务-查询库存属性 价格服务-查询价格属性 营销服务-查询营销属性 如果是串行调用的话响应时间会随着 rpc 调用次数呈线性增长，简单场景下使用 WaitGroup 也能够满足需求，但如果要对 rpc 调用返回的数据进行校验、数据加工转换、数据汇总呢？ go-zero通过mapreduce来处理这种对输入数据进行处理最后输出清洗数据的问题。是一种经典的模式：生产者消费者模式。将数据处理分为三个阶段： 数据生产 generate(查询，必选) 数据加工 mapper(加工，可选) 数据聚合 reducer(聚合，可选) 利用协程处理以及管道通信，实现数据的加速处理 应用场景 场景1 对数据批处理，比如对一批用户id，效验每个用户的合法性并且效验过程中有一个出错就认为效验失败，返回的结果为效验合法的用户id 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mapreduceimport ( \"errors\" \"log\" \"github.com/zeromicro/go-zero/core/mr\")func Run(uids []int) ([]int, error) &#123; r, err := mr.MapReduce(func(source chan&lt;- interface&#123;&#125;) &#123; for _, uid := range uids &#123; source &lt;- uid &#125; &#125;, func(item interface&#123;&#125;, writer mr.Writer, cancel func(error)) &#123; uid := item.(int) ok, err := check(uid) if err != nil &#123; //如果校验逻辑有问题，这里执行cancel整个校验过程停止 cancel(err) &#125; if ok &#123; //如果校验失败，那么不返回该uid writer.Write(uid) &#125; &#125;, func(pipe &lt;-chan interface&#123;&#125;, writer mr.Writer, cancel func(error)) &#123; var uids []int for p := range pipe &#123; uids = append(uids, p.(int)) &#125; writer.Write(uids) &#125;) if err != nil &#123; log.Printf(\"check error: %v\", err) return nil, err &#125; return r.([]int), nil&#125;func check(uid int) (bool, error) &#123; // do something check user legal if uid == 0 &#123; return false, errors.New(\"uid wrong\") &#125; return true, nil&#125; 其实是利用N个协程等待数据生产者的数据传输然后转交给聚合逻辑处理 场景2 某些功能的结果往往需要依赖多个服务，比如商品详情的结果往往会依赖用户服务、库存服务、订单服务等等，一般被依赖的服务都是以rpc的形式对外提供，为了降低依赖的耗时我们往往需要对依赖做并行处理 1234567891011121314151617181920func productDetail(uid, pid int64) (*ProductDetail, error) &#123; var pd ProductDetail err := mr.Finish(func() (err error) &#123; pd.User, err = userRpc.User(uid) return &#125;, func() (err error) &#123; pd.Store, err = storeRpc.Store(pid) return &#125;, func() (err error) &#123; pd.Order, err = orderRpc.Order(pid) return &#125;) if err != nil &#123; log.Printf(\"product detail error: %v\", err) return nil, err &#125; return &amp;pd, nil&#125; 技术内幕 源码目录：core/mr/mapreduce.go 其中利用到的对外函数有 12345678910111213141516171819202122// MapReduce 包含数据生产、数据处理以及数据聚合阶段并返回结果func MapReduce(generate GenerateFunc, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123;&#125;// MapReduceChan 包含数据生产、数据处理以及数据聚合阶段并返回结果。其中利用chan代替数据生产func MapReduceChan(source &lt;-chan interface&#123;&#125;, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; panicChan := &amp;onceChan&#123;channel: make(chan interface&#123;&#125;)&#125; return mapReduceWithPanicChan(source, panicChan, mapper, reducer, opts...)&#125; // ForEach 只包含数据生产和数据处理阶段，但没有任何输出func ForEach(generate GenerateFunc, mapper ForEachFunc, opts ...Option) &#123;&#125;// FinishVoid 并行运行 fnsfunc FinishVoid(fns ...func()) &#123; // Finish 并行运行 fns，在任何错误时取消func Finish(fns ...func() error) error &#123;&#125; //WithWorkers 定义一个 mapreduce 有几个协程func WithWorkers(workers int) Option &#123;&#125; 数据生产阶段 首先定义 buildSource使用协程进行数据生产 12345678910111213141516//buildSource 使用协程执行generate 并将协程的参数一个非缓冲的channel返回。如果generate发生panic，则将错误写入 onceChanfunc buildSource(generate GenerateFunc, panicChan *onceChan) chan interface&#123;&#125; &#123; source := make(chan interface&#123;&#125;) go func() &#123; defer func() &#123; if r := recover(); r != nil &#123; panicChan.write(r) &#125; close(source) &#125;() generate(source) //返回的非缓冲隧道也是数据生产的入口 &#125;() return source&#125; 其中的 onceChan则是一个非阻塞会缓冲的channel，当channel中还有数据没有处理完，则直接返回 123456789101112type onceChan struct &#123; channel chan interface&#123;&#125; wrote int32&#125;func (oc *onceChan) write(val interface&#123;&#125;) &#123; if atomic.AddInt32(&amp;oc.wrote, 1) &gt; 1 &#123; return &#125; oc.channel &lt;- val&#125; 数据处理阶段 接着利用mapReduceWithPanicChan进行数据处理mapper和数据聚合reducer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func MapReduce(generate GenerateFunc, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; panicChan := &amp;onceChan&#123;channel: make(chan interface&#123;&#125;)&#125; source := buildSource(generate, panicChan) //使用协程执行generate并返回数据生产无缓冲隧道source return mapReduceWithPanicChan(source, panicChan, mapper, reducer, opts...) //将隧道和处理函数传入&#125;//source就是数据来源隧道func mapReduceWithPanicChan(source &lt;-chan interface&#123;&#125;, panicChan *onceChan, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; options := buildOptions(opts...) // output is used to write the final result output := make(chan interface&#123;&#125;) //...defer //collector 用于从mapper收集数据，消费者是数据聚合 collector := make(chan interface&#123;&#125;, options.workers) //done表示结束，所有的mappers和reducer都要结束 done := make(chan lang.PlaceholderType) writer := newGuardedWriter(options.ctx, output, done) var closeOnce sync.Once // use atomic.Value to avoid data race //... go func() &#123; //起一个协程进行 数据聚合 //panic.wirte() reducer(collector, writer, cancel) &#125;() go executeMappers(mapperContext&#123; //进行 数据处理 ctx: options.ctx, mapper: func(item interface&#123;&#125;, w Writer) &#123; mapper(item, w, cancel) &#125;, source: source, panicChan: panicChan, collector: collector, doneChan: done, workers: options.workers, &#125;) select &#123; //等待结果 case &lt;-options.ctx.Done(): cancel(context.DeadlineExceeded) return nil, context.DeadlineExceeded case v := &lt;-panicChan.channel: panic(v) case v, ok := &lt;-output: if err := retErr.Load(); err != nil &#123; return nil, err &#125; else if ok &#123; return v, nil &#125; else &#123; return nil, ErrReduceNoOutput &#125; &#125;&#125; 其中数据处理阶段又定义了一个协程进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142func executeMappers(mCtx mapperContext) &#123; var wg sync.WaitGroup defer func() &#123; wg.Wait() close(mCtx.collector) drain(mCtx.source) &#125;() var failed int32 pool := make(chan lang.PlaceholderType, mCtx.workers) //协程池 writer := newGuardedWriter(mCtx.ctx, mCtx.collector, mCtx.doneChan) for atomic.LoadInt32(&amp;failed) == 0 &#123; select &#123; case &lt;-mCtx.ctx.Done(): return case &lt;-mCtx.doneChan: return case pool &lt;- lang.Placeholder: //这里是定义的N个works的chan，也就是会在下面创建n个协程 item, ok := &lt;-mCtx.source if !ok &#123; //如果来源关闭，那么将pool数据释放 &lt;-pool return &#125; wg.Add(1) go func() &#123; defer func() &#123; if r := recover(); r != nil &#123; atomic.AddInt32(&amp;failed, 1) mCtx.panicChan.write(r) &#125; wg.Done() &lt;-pool &#125;() //item:生产的数据 //writer: 数据处理对象 mCtx.mapper(item, writer) //执行map &#125;() &#125; &#125;&#125; 数据聚合阶段 最后来看一下是数据处理 1234567891011121314type guardedWriter struct &#123; //写入接口 ctx context.Context //保证超时退出 channel chan&lt;- interface&#123;&#125; //接收数据，这里传输的就是 长度为 N 的 collect channel done &lt;-chan lang.PlaceholderType //主动结束退出&#125;func newGuardedWriter(ctx context.Context, channel chan&lt;- interface&#123;&#125;, done &lt;-chan lang.PlaceholderType) guardedWriter &#123; return guardedWriter&#123; ctx: ctx, channel: channel, done: done, &#125;&#125; Finish Finish逻辑只进行并发处理，其实内部是将执行函数做为数据生产的生产的数据，然后又数据处理逻辑进行处理 12345678910111213141516171819202122232425262728func Finish(fns ...func() error) error &#123; if len(fns) == 0 &#123; //n个外部调用 return nil &#125; return MapReduceVoid(func(source chan&lt;- interface&#123;&#125;) &#123; for _, fn := range fns &#123; source &lt;- fn //数据生产者将函数传入 &#125; &#125;, func(item interface&#123;&#125;, writer Writer, cancel func(error)) &#123; fn := item.(func() error) if err := fn(); err != nil &#123; //数据处理逻辑执行函数 cancel(err) //这里并没有写入，所以第三个函数其实并没有执行 &#125; &#125;, func(pipe &lt;-chan interface&#123;&#125;, cancel func(error)) &#123; &#125;, WithWorkers(len(fns)))&#125;//底层还是执行的MapReduce逻辑func MapReduceVoid(generate GenerateFunc, mapper MapperFunc, reducer VoidReducerFunc, opts ...Option) error &#123; _, err := MapReduce(generate, mapper, func(input &lt;-chan interface&#123;&#125;, writer Writer, cancel func(error)) &#123; reducer(input, cancel) &#125;, opts...) if errors.Is(err, ErrReduceNoOutput) &#123; return nil &#125; return err&#125; 总结： 适用于并发无顺序依赖的并发调用，如果是多个调用具有前后依赖关系，依然需要有先后调用顺序(废话) 也不存在回滚的操作，内部只是将不在等待处理结果直接退出 select 参考链接 https://talkgo.org/t/topic/1452 https://zh.wikipedia.org/wiki/MapReduce","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero2-共享调用","slug":"GoZero/GoZero2-共享调用","date":"2022-06-27T16:55:17.000Z","updated":"2022-07-02T14:45:14.000Z","comments":true,"path":"2022/06/28/GoZero/GoZero2-共享调用/","link":"","permalink":"http://xboom.github.io/2022/06/28/GoZero/GoZero2-%E5%85%B1%E4%BA%AB%E8%B0%83%E7%94%A8/","excerpt":"","text":"问题背景 并发场景下，可能会有多个线程（协程）同时请求同一份资源，如果每个请求都要走一遍资源的请求过程，除了比较低效之外，还会对资源服务造成并发的压力。 例如： 缓存失效的同时多个请求同时到达某服务请求相同资源，这些请求会继续访问DB做查询，会引起数据库压力瞬间增大。而使用 SharedCalls 可以使得同时多个请求只需要发起一次拿结果的调用，其他请求&quot;坐享其成&quot;。有效减少了资源服务的并发压力，可以有效防止缓存击穿 云平台服务众多，使用grpc通信的时候不期望每个服务之间都建立链接，而只在向对端发送消息的时候，才在服务之间建立通信。老的逻辑是当开始建立连接的时候会将创建改为正在创建链接，后续消息会因为正在建立链接会直接返回错误或阻塞等待结果(自行实现)，而使用SharedCalls可以短时间内等待链接建立然后继续发送消息 演示代码 1234567891011121314151617181920212223242526func main() &#123; const round = 5 var wg sync.WaitGroup barrier := syncx.NewSharedCalls() wg.Add(round) for i := 0; i &lt; round; i++ &#123; // 多个线程同时执行 go func() &#123; defer wg.Done() // 可以看到，多个线程在同一个 key 上去请求资源，获取资源的实际函数只会被调用一次 val, err := barrier.Do(\"once\", func() (interface&#123;&#125;, error) &#123; // sleep 1秒，为了让多个线程同时取 once 这个 key 上的数据 time.Sleep(time.Second) // 生成了一个随机的 id return stringx.RandId(), nil &#125;) if err != nil &#123; fmt.Println(err) &#125; else &#123; fmt.Println(val) &#125; &#125;() &#125; wg.Wait()&#125; 技术内幕 文件目录：core/syncx/singleflight.go SingleFlight 通过为并发的请求根据相同的key提供相同的结果 一共提供了 Do 与 DoEx 两种接口 123456789101112131415var SingleFlight interface &#123; Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) DoEx(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, bool, error)&#125;var call struct &#123; //代指一次调用 wg sync.WaitGroup //用于等待call结束 val interface&#123;&#125; err error&#125;var flightGroup struct &#123; calls map[string]*call lock sync.Mutex&#125; 首先查看两个基础函数 123456789101112131415161718192021222324252627282930func (g *flightGroup) createCall(key string) (c *call, done bool) &#123; // 先申请加锁 g.lock.Lock() if c, ok := g.calls[key]; ok &#123; //如果key存在，那么等待 // 拿到 call 以后，释放锁，此处 call 可能还没有实际数据，只是一个空的内存占位 g.lock.Unlock() //调用 wg.Wait，判断是否有其他 goroutine 正在申请资源，如果阻塞，说明有其他 goroutine 正在获取资源 c.wg.Wait() //等待相同的call的结束 // 当 wg.Wait 不再阻塞，表示资源获取已经结束，可以直接返回结果 return c, true &#125; c = new(call) //创建一个新的call c.wg.Add(1) //并为这个call添加一个的等待 g.calls[key] = c g.lock.Unlock() return c, false&#125;func (g *flightGroup) makeCall(c *call, key string, fn func() (interface&#123;&#125;, error)) &#123; defer func() &#123; g.lock.Lock() delete(g.calls, key) //删除 g.lock.Unlock() c.wg.Done() //结束call &#125;() c.val, c.err = fn() //执行函数并返回结果&#125; 再看实现逻辑 1234567891011121314151617181920212223242526func (g *flightGroup) Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; c, done := g.createCall(key) //根据key定义一个call if done &#123; //如果是等待结束了，且等待结束了，则返回Call的value return c.val, c.err &#125; //如果是新的，那么就首次执行call，并返回结果 g.makeCall(c, key, fn) return c.val, c.err&#125;func (g *flightGroup) DoEx(key string, fn func() (interface&#123;&#125;, error)) (val interface&#123;&#125;, fresh bool, err error) &#123; c, done := g.createCall(key) if done &#123; //等待结束 return c.val, false, c.err &#125; g.makeCall(c, key, fn) return c.val, true, c.err //新的结束&#125;// NewSingleFlight returns a SingleFlight.func NewSingleFlight() SingleFlight &#123; return &amp;flightGroup&#123; calls: make(map[string]*call), &#125;&#125; DoEx 相较于 Do 中增加了一个 bool 类型的返回值，表示返回的值是共享的还是首次拿到的 参考链接 https://talkgo.org/t/topic/968","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero1-架构","slug":"GoZero/GoZero1-架构","date":"2022-06-27T16:55:05.000Z","updated":"2022-07-17T13:54:33.000Z","comments":true,"path":"2022/06/28/GoZero/GoZero1-架构/","link":"","permalink":"http://xboom.github.io/2022/06/28/GoZero/GoZero1-%E6%9E%B6%E6%9E%84/","excerpt":"","text":"go-zero 包含很多微服务治理能力，所以不一定要完整使用go-zero，通过外部引入也是可以的 Go-Zero结构 包含诸多特点： 强大的工具支持，尽可能少的代码编写 极简的接口 完全兼容 net/http 支持中间件，方便扩展 高性能 面向故障编程，弹性设计 内建服务发现、负载均衡 内建限流、熔断、降载，且自动触发，自动恢复 API 参数自动校验 超时级联控制 自动缓存控制 链路跟踪、统计报警等 高并发支撑，稳定保障了疫情期间每天的流量洪峰 如下图，我们从多个层面保障了整体服务的高可用： 框架设计 设计理念： 保持简单，第一原则 弹性设计，面向故障编程 工具大于约定和文档 高可用 高并发 易扩展 对业务开发友好，封装复杂度 约束做一件事只有一种方式 技术内幕 防止缓存击穿之共享调用 参考链接 https://github.com/zeromicro/go-zero/blob/master/readme-cn.md https://talkgo.org/c/go-zero/23 https://go-zero.dev/cn/docs/design/design","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"协议4-SMB","slug":"Protocol/协议4-SMB","date":"2022-06-25T07:00:00.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/06/25/Protocol/协议4-SMB/","link":"","permalink":"http://xboom.github.io/2022/06/25/Protocol/%E5%8D%8F%E8%AE%AE4-SMB/","excerpt":"","text":"SMB协议简介 早期使用 FTP 来传文件，但是无法直接修改主机上面的档案数据，只能先下载后才能修改 SMB（Server Message Block），SMB1.0又称CIFS(Common Internet File System),一种应用层网络传输协议，主要使网络上的机器能够共享计算机文件、打印机、串行端口和通讯等资源。也提供认证的进程间通讯技能。主要用在Windows的机器上。 Samba让UNIX与Windows的SMB/CIFS（Server Message Block/Common Internet File System）网络协议做链接的自由软件，让两者互通有。 samba是许多服务以及协议的实现，其包括TCP/IP上的NetBIOS、SMB、CIFS等协议。 Samba能够为选定的Unix目录（包括所有子目录）创建网络共享。该功能使得Windows用户可以像访问普通Windows下的文件夹那样来通过网络访问这些Unix目录。Samba服务是由两个进程组成，分别是nmbd和smbd。 nmbd：其功能是进行NetBIOS名解析，并提供浏览服务显示网络上的共享资源列表。 smbd：其主要功能就是用来管理Samba服务器上的共享目录、打印机等，主要是针对网络上的共享资源进行管理的服务。当要访问服务器时，要查找共享文件，这时我们就要依靠smbd这个进程来管理数据传输 SMB原理 下图展示了SMB2和SMB3的报文头部结构，MS-SMB2协议文档。 一个普通的SMB会话从开始到结束一般会经过以下六个生命阶段： SMB协议协商（Negotiate） 在一个SMB还没有开始的时候，由客户端率先发出一个协商请求。在请求中，客户端会列出所有它所支持协议版本以及所支持的一些特性（比如加密Encryption、持久句柄Persistent Handle、客户端缓存Leasing等等）。而服务端在回复中则会指定一个SMB版本且列出客户端与服务端共同支持的特性。 建立SMB会话（Session Setup） 客户端选择一个服务端支持的协议来进行用户认证，可以选择的认证协议一般包括NTLM、Kerberos等。按照选择的认证协议的不同，这个阶段可能会进行一次或多次SESSION_SETOP请求/回复的网络包交换。至于NTLM或Kerberos认证协议的细节，我们会另文再叙。 连接一个文件分享（Tree Connect） 在会话建立之后，客户端会发出连接文件分享的请求。源于文件系统的树形结构，该请求被命名为树连接（Tree Connect）。以SMB协议的阿里云NAS为例，一般的SMB挂载命令为： 文件系统操作 在文件分享连接成功之后，用户通过SMB客户端进行真正的对于目标文件分享的业务操作。这个阶段可以用到的指令有CREATE、CLOSE、FLUSH、READ、WRITE、SETINFO、GETINFO等等。 断开文件分享连接（Tree Disconnect） 当一个SMB会话被闲置一定时间之后，Windows会自动断开文件分享连接并随后中止SMB会话。这个闲置时间可以通过Windows注册表进行设定[9]。当然，用户也可以主动发起断开连接请求。 终止SMB会话（Logoff） 当客户端发出会话中止请求并得到服务端发回的中止成功的回复之后，这个SMB会话至此便正式结束了。 SMB报文分析 SMB1的交互过程如下 SMB2的交换过程如下 Name query &lt;NetBIOS名称&gt;: 根据以上Windows系统的名字的解析机制，如果计算机需要知道一个NetBIOS名称或者域名对应的IP地址，首先会查找本地Hosts文件和NetBIOS缓存，其次会去联系DNS服务器进行解析。如果这几种方式都无法完成解析，则计算机会发出NBNS数据包。 TCP 三次握手：由于SMB是运行在TCP/IP上，这里会先完成三次握手 Negotiate Protocol Request:客户端向服务器发起协商请求，Requested Dialects包含客户端支持的所有SMB协议版本 Negotiate Protocol Reponse:服务器收到该请求后，选择它支持的最新版本 Dialect SMB2 wildcard (0x02ff 表示SMB2及以上版本)回复给客户端 0x0202 SMB 2.002 0x0210 SMB 2.1 0x0300 SMB 3.0 0x0302 SMB 3.02 0x02FF SMB2 Negotiate Protocol Request:上一次协商已经确定使用SMB2及以上版本，所以这次Dialect中只列出了SMB2及以上版本 Negotiate Protocol Reponse:服务器收到该请求后返回具体协议 SMB 3.0 Session Setup Request(NTLMSSP_NEGOTIATE): Negotiation结束之后，客户端请求和服务器建立一个session，在客户端发送的Session Setup Request里，包含了身份验证请求 Session Setup Reponse: 提示 STATUS_MORE_PROCESSING_REQUIRED, 需要进行NTLMSSP_AUTH认证 Session Setup Request(NTLMSSP_AUTH): 进行 NTLMSSP_AUTH认证 Session Setup Reponse: 返回会话创建结果创建会话成功 Tree Connect Request Tree: 客户端发送的 Tree Connect Request 来访问具体的共享，如果前面没有指定共享名（\\服务器名），客户端访问的是命名管道$IPC, 也可以指定访问共享目录 \\服务器\\共享名 Tree Connect Response: 服务器在检查过用户对该路径的权限后，回复Tree Connect Response。检查用户权限是这样进行的：服务器从Session Setup Request中已经得到用户所属的组，再通过和该路径上的ACL对比，即可得到用户权限。至此，用户就进入了共享文件夹 接下来是在会话以及读写权限获取之后使用单独的读取和写入以及事务处理命名管道操作在命名管道上执行事务的步骤 Create Request File:客户端发送 Create Reques File 去打开一个命名管道(这里叫lsarpc) CREATE Reponse File:服务器以Create Reponse File和打开管道作为响应 Ioctl Request(Bind) Ioctl Reponse(Bind_ack) 这里还不清楚具体的功能先写在这里 Close Request File :关闭刚才打开的管道 lsarcp Close Resonse: 服务器端确认关闭 同理开启另外一个隧道执行完毕后关闭(为什么要开启两个隧道) TCP 四次挥手：最后通过TCP四次挥手结束此次会话 可能存在的其他操作可以参考微软文档以及wireshark的SMB2报文分析 TODO: SMB1 和SMB2 的报文分析 SAMBA到底是什么 Winbindd是什么 wireshark的包和自己的包有什么区别 域到底是什么 其中提到的其他协议CIFS\\NBNS\\RPC协议到底是什么 华为加域过程分析 参考链接 SMB2 Protocol SAMBA原理 SMB小专 Microsoft Document Samba SMB.CONF配置说明 Windows帮助与支持","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"SMB","slug":"SMB","permalink":"http://xboom.github.io/tags/SMB/"}]},{"title":"k8s入门5-容器网络之网络栈","slug":"K8S/k8s入门5-容器网络","date":"2022-02-20T16:35:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门5-容器网络/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A85-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/","excerpt":"","text":"容器网络 一个 Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的 网络栈包括：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。这些要素就构成了一个进程发起和响应网络请求的基本环境 回环设备是指拿一个大的镜像文件，如xxx.iso或xxx.img等，在此文件内建立一个文件系统，此文件就像一个新的磁盘或光盘设备一样使用。回环可以理解成回复重用，在已有设备上建立文件来模拟物理块设备 一个容器可以声明直接使用宿主机的网络栈（–net=host），即不开启 Network Namespace 1$ docker run –d –net=host --name nginx-host nginx 在这种情况下，容器启动后，直接监听的就是宿主机的 80 端口 优点：可以为容器提供良好的网络性能，不需要进行网络转发操作 缺点：不可避免地引入共享网络资源的问题，比如端口冲突 在大多数情况下，还是希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口 问题1：这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？ 在 Linux 中，能够起到虚拟交换机作用的网络设备是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上 Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信 问题2：如果将容器连接到网桥上? 答案是使用Veth Pair，它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里，使得 Veth Pair 被用作连接不同 Network Namespace 的“网线” 容器里 eth0 的网卡，正是一个 Veth Pair 设备在容器里的这一端 通过 route 命令查看 nginx-1 容器的路由表，可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则） 再查看Veth Pair 设备的另一端(宿主机的网络信息)，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth5d20cf2 Mac 无法查看，可以通过上面的host模型进行查看 1docker run -d --net=host --name nginx-host nginx 通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上 如果再在这台宿主机上启动另外一个容器，将会发现一个新的名字 veth7e63610 也插在docker0网桥上 如果在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址（172.17.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的，这是因为： nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机 同一个宿主机上的不同容器通过docker0网桥进行通信 要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址 ARP(Address Resolution Protocol)，是通过三层的 IP 地址找到对应的二层 MAC 地址的协议。 这个 eth0 网卡是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上，变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包，全部交给对应的网桥。 docker0 在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器。 有了目的 MAC 地址，nginx-1 容器的 eth0 网卡就 ping 包发出去。而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth5d20cf2 虚拟网卡上，然后直接流入到了 docker0 网桥里。 docker0 继续扮演二层交换机的角色。docker0 网桥根据数据包的目的 MAC 地址，在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：veth7e63610，然后把数据包发往这个端口。而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。 nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。 在实际的数据传递时，上述数据的传递过程在网络协议栈的不同层次，都有 Linux 内核 Netfilter 参与其中。通过打开 iptables 的 TRACE 功能查看到数据包的传输过程，在 系统日志 /var/log/syslog 或者 var/log/messages 里看到数据包传输的日志，具体方法如下所示 1234# 在宿主机上执行$ iptables -t raw -A OUTPUT -p icmp -j TRACE$ iptables -t raw -A PREROUTING -p icmp -j TRACE docker0 网桥的工作方式可以理解为，在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换 在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里 当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3， 请求数据包首先经过 docker0 网桥出现在宿主机上。 然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理 这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。 当遇到容器连不通“外网”的时候，先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常 问题4：如果在另外一台宿主机（比如：10.168.0.3）上，也有一个 Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢？ 在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了 创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上 构建这种容器网络的核心在于：需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络，被称为：Overlay Network（覆盖网络） 容器跨主机网络 Flannel 为了解决这个容器“跨主通信”的问题，出现了那么多的容器网络方案。Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现 Flannel 支持三种后端实现，也代表了三种容器跨主网络的主流实现方法 VXLAN host-gw UDP UDP 假设有两台宿主机： 宿主机 Node1 上有容器 container-1，它的 IP 地址是 100.96.1.2，docker0 网桥的地址是：100.96.1.1/24 宿主机 Node2 上有容器 container-2，它的 IP 地址是 100.96.2.3，docker0 网桥的地址是：100.96.2.1/24 第一步：container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。 第二步：由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。 第三步：这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示： 123456# 在Node 1上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2 可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。 flannel0 的设备类型是一个 TUN 设备，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能是在操作系统内核和用户应用程序之间传递 IP 包 当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。 反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向 当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。 那么 flannelId 又是如何知道这个IP地址对应的容器运行在Node 2上面呢？ 在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示： 1234$ etcdctl ls /coreos.com/network/subnets/coreos.com/network/subnets/100.96.1.0-24/coreos.com/network/subnets/100.96.2.0-24/coreos.com/network/subnets/100.96.3.0-24 所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3 12$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24&#123;\"PublicIP\":\"10.168.0.3\"&#125; 而对于 flanneld 来说，只要 Node 1 和 Node 2 是互通的，那么 flanneld 作为 Node 1 上的一个普通进程，就一定可以通过上述 IP 地址（10.168.0.3）访问到 Node 2，这没有任何问题 **flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。**这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。 当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。 通过这样一个普通的、宿主机之间的 UDP 通信，一个 UDP 包就从 Node 1 到达了 Node 2。而 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包。 而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。根据我前面讲解的 TUN 设备的原理，这正是一个从用户态向内核态的流动方向（Flannel 进程向 TUN 设备发送数据包），所以 Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示： 123456# 在Node 2上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.3 由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。 接下来的流程，docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里。而 container-2 返回给 container-1 的数据包，则会经过与上述过程完全相反的路径回到 container-1 中。 需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可： 12$ FLANNEL_SUBNET=100.96.1.1/24$ dockerd --bip=$FLANNEL_SUBNET ... 以上，就是基于 Flannel UDP 模式的跨主通信的基本原理了，如下所示。 可以看到，Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。 上述 UDP 模式有严重的性能问题，所以已经被废弃了。 相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示： 可以看到： 第一次，用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态； 第二次，IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程； 第三次，flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。 此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。这也正是造成 Flannel UDP 模式性能不好的主要原因 VXLAN 即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network） VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。 而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。 而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。 上述基于 VTEP 设备进行“隧道”通信的流程，如下所示： 可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。那么，与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。也就是说，来到了“隧道”的入口。为了方便叙述，我接下来会把这个 IP 包称为“原始 IP 包”。 为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1 这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。从图 3 的 Flannel VXLAN 模式的流程图中我们可以看到，10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。为了方便叙述，接下来我会把 Node 1 和 Node 2 上的 flannel.1 设备分别称为“源 VTEP 设备”和“目的 VTEP 设备”。而这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。 “源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。 目的 VTEP 设备 的 MAC 地址是什么？ 此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示： 123# 在Node 1上$ ip neigh show dev flannel.110.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT 这条记录的意思非常明确，即：IP 地址 10.1.16.0，对应的 MAC 地址是 5e:f8:4f:00:e3:37。 可以看到，最新版本的 Flannel 并不依赖 L3 MISS 事件和 ARP 学习，而会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录，直接下放到其他每台宿主机上 有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示： 可以看到，Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。需要注意的是，上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。所以图中的 Inner IP Header 字段，依然是 container-2 的 IP 地址，即 10.1.16.3。 但是，上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Outer Ethernet Frame）。 为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。 所以，跟 UDP 模式类似，在宿主机看来，它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备，发起了一次普通的 UDP 链接。它哪里会知道，这个 UDP 包里面，其实是一个完整的二层数据帧。这是不是跟特洛伊木马的故事非常像呢？不过，不要忘了，一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对应的宿主机地址是什么。也就是说，这个 UDP 包该发给哪台宿主机呢？在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。 不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示： 123# 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:375e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent 可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即：发往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node 2，UDP 包要发往的目的地就找到了。 所以接下来的流程，就是一个正常的、宿主机网络上的封包工作。 我们知道，UDP 包是一个四层数据包，所以 Linux 内核会在它前面加上一个 IP 头，即原理图中的 Outer IP Header，组成一个 IP 包。并且，在这个 IP 头里，会填上前面通过 FDB 查询出来的目的主机的 IP 地址，即 Node 2 的 IP 地址 10.168.0.3。然后，Linux 内核再在这个 IP 包前面加上二层数据帧头，即原理图中的 Outer Ethernet Header，并把 Node 2 的 MAC 地址填进去。这个 MAC 地址本身，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护。这时候，我们封装出来的“外部数据帧”的格式，如下所示： 这样，封包工作就宣告完成了。接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。以上，就是 Flannel VXLAN 模式的具体工作原理了。 Flannel 能负责保证二层网络（MAC 地址）的连通性吗？ CNI网络插件 UDP与VXLAN都需要容器连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作 网络插件是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。 Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0 以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已 Kubernetes 为 Flannel 分配的子网范围是 10.244.0.0/16。这个参数可以在部署的时候指定 1$ kubeadm init --pod-network-cidr=10.244.0.0/16 也可以在部署完成后，通过修改 kube-controller-manager 的配置文件来指定。这时候，假设 Infra-container-1 要访问 Infra-container-2（也就是 Pod-1 要访问 Pod-2），这个 IP 包的源地址就是 10.244.0.2，目的 IP 地址是 10.244.1.3。而此时，Infra-container-1 里的 eth0 设备，同样是以 Veth Pair 的方式连接在 Node 1 的 cni0 网桥上。所以这个 IP 包就会经过 cni0 网桥出现在宿主机上 此时，Node 1 上的路由表，如下所示： 12345678# 在Node 1上$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.244.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni010.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 因为我们的 IP 包的目的 IP 地址是 10.244.1.3，所以它只能匹配到第二条规则，也就是 10.244.1.0 对应的这条路由规则。可以看到，这条规则指定了本机的 flannel.1 设备进行处理。并且，flannel.1 在处理完后，要将 IP 包转发到的网关（Gateway），正是“隧道”另一端的 VTEP 设备，也就是 Node 2 的 flannel.1 设备。所以，接下来的流程，就跟上一篇文章中介绍过的 Flannel VXLAN 模式完全一样了 需要注意的是，CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。而此时，如果你用 docker run 单独启动一个容器，那么 Docker 项目还是会把这个容器连接到 docker0 网桥上。所以这个容器的 IP 地址，一定是属于 docker0 网桥的 172.17.0.0/16 网段。 Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面： 一方面，Kubernetes 项目并没有使用 Docker 的网络模型（CNM），所以它并不希望、也不具备配置 docker0 网桥的能力； 另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关。 Kubernetes 创建一个 Pod 的第一步，就是创建并启动一个 Infra 容器，用来“hold”住这个 Pod 的 Network Namespace。所以，CNI 的设计思想，就是：Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。 那么它是怎么完成这个网络栈的配置的呢？ 在部署 Kubernetes 的时候，有一个步骤是安装 kubernetes-cni 包，它的目的就是在宿主机上安装 CNI 插件所需的基础可执行文件，在安装完成后，你可以在宿主机的 /opt/cni/bin 目录下看到它们，如下所示： 1234567891011121314$ ls -al /opt/cni/bin/total 73088-rwxr-xr-x 1 root root 3890407 Aug 17 2017 bridge-rwxr-xr-x 1 root root 9921982 Aug 17 2017 dhcp-rwxr-xr-x 1 root root 2814104 Aug 17 2017 flannel-rwxr-xr-x 1 root root 2991965 Aug 17 2017 host-local-rwxr-xr-x 1 root root 3475802 Aug 17 2017 ipvlan-rwxr-xr-x 1 root root 3026388 Aug 17 2017 loopback-rwxr-xr-x 1 root root 3520724 Aug 17 2017 macvlan-rwxr-xr-x 1 root root 3470464 Aug 17 2017 portmap-rwxr-xr-x 1 root root 3877986 Aug 17 2017 ptp-rwxr-xr-x 1 root root 2605279 Aug 17 2017 sample-rwxr-xr-x 1 root root 2808402 Aug 17 2017 tuning-rwxr-xr-x 1 root root 3475750 Aug 17 2017 vlan 这些 CNI 的基础可执行文件，按照功能可以分为三类： 第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件。比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan。我在前面提到过的 Flannel、Weave 等项目，都属于“网桥”类型的 CNI 插件。所以在具体的实现中，它们往往会调用 bridge 这个二进制文件。 第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。 第三类，是由 CNI 社区维护的内置 CNI 插件。比如： flannel，就是专门为 Flannel 项目提供的 CNI 插件； tuning，是一个通过 sysctl 调整网络设备参数的二进制文件； portmap，是一个通过 iptables 配置端口映射的二进制文件； bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。 从这些二进制文件中，我们可以看到，如果要实现一个给 Kubernetes 用的容器网络方案，其实需要做两部分工作，以 Flannel 项目为例： 首先，实现这个网络方案本身。这一部分需要编写的，其实就是 flanneld 进程里的主要逻辑。比如，创建和配置 flannel.1 设备、配置宿主机路由、配置 ARP 和 FDB 表里的信息等等。然后，实现该网络方案对应的 CNI 插件。这一部分主要需要做的，就是配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。 由于 Flannel 项目对应的 CNI 插件已经被内置了，所以它无需再单独安装。而对于 Weave、Calico 等其他项目来说，我们就必须在安装插件的时候，把对应的 CNI 插件的可执行文件放在 /opt/cni/bin/ 目录下。 实际上，对于 Weave、Calico 这样的网络方案来说，它们的 DaemonSet 只需要挂载宿主机的 /opt/cni/bin/，就可以实现插件可执行文件的安装了。你可以想一下具体应该怎么做，就当作一个课后小问题留给你去实践了。 接下来，你就需要在宿主机上安装 flanneld（网络方案本身）。而在这个过程中，flanneld 启动后会在每台宿主机上生成它对应的 CNI 配置文件（它其实是一个 ConfigMap），从而告诉 Kubernetes，这个集群要使用 Flannel 作为容器网络方案。 这个 CNI 配置文件的内容如下所示： 12345678910111213141516171819$ cat /etc/cni/net.d/10-flannel.conflist &#123; \"name\": \"cbr0\", \"plugins\": [ &#123; \"type\": \"flannel\", \"delegate\": &#123; \"hairpinMode\": true, \"isDefaultGateway\": true &#125; &#125;, &#123; \"type\": \"portmap\", \"capabilities\": &#123; \"portMappings\": true &#125; &#125; ]&#125; Soft multi-tenancy 为什么说 Kubernetes 只有 soft multi-tenancy？ 参考链接 《极客时间-深入剖析 Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门13-容器网络之Service和Ingress","slug":"K8S/k8s入门13-容器网络之Service和Ingress","date":"2022-02-20T16:35:06.000Z","updated":"2022-02-20T16:36:12.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门13-容器网络之Service和Ingress/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A813-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8BService%E5%92%8CIngress/","excerpt":"","text":"Service 暴露给外界的三种方法中有一个叫作 LoadBalancer 类型的 Service，它会为你在 Cloud Provider（比如：Google Cloud 或者 OpenStack）里创建一个与该 Service 对应的负载均衡服务 由于每个 Service 都要有一个负载均衡服务，为什么没有一个内置一个全局的负载均衡器。通过访问的 URL，把请求转发给不同的后端 Service。 这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。可以说是 Service 的“Service”。 举个例子，假如我现在有这样一个站点：https://cafe.example.com。其中， https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。 https://cafe.example.com/tea，对应的则是“茶水点餐系统”。 这两个系统，分别由名叫 coffee 和 tea 这样两个 Deployment 来提供服务 如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？ 在 Kubernetes 里就需要通过 Ingress 对象来描述，如下所示： 123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: cafe-ingressspec: tls: - hosts: - cafe.example.com secretName: cafe-secret rules: - host: cafe.example.com http: paths: - path: /tea backend: serviceName: tea-svc servicePort: 80 - path: /coffee backend: serviceName: coffee-svc servicePort: 80 在 Kubernetes 里，文件的 rules 字段叫作：IngressRule IngressRule 的 Key 就叫做：host。它必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。 而 host 字段定义的值就是这个 Ingress 的入口。当用户访问 cafe.example.com 的时候，实际上访问到的是这个 Ingress 对象。这样，Kubernetes 就能使用 IngressRule 来对你的请求进行下一步转发。 接下来 IngressRule 规则的定义，则依赖于 path 字段。你可以简单地理解为，这里的每一个 path 都对应一个后端 Service。所以在我们的例子里，我定义了两个 path，它们分别对应 coffee 和 tea 这两个 Deployment 的 Service（即：coffee-svc 和 tea-svc） 所以所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。而这个代理服务对应的转发规则，就是 IngressRule。跟 Nginx、HAproxy 等项目的配置文件的写法是一致的。Kubernetes 的用户就无需关心 Ingress 的具体细节了。选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可 这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。 以最常用的 Nginx Ingress Controller 为例，在用 kubeadm 部署的 Bare-metal 环境中，实践 Ingress 机制的使用过程。 部署 Nginx Ingress Controller 的方法非常简单，如下所示： 1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml 在mandatory.yaml这个文件里，正是 Nginx 官方为你维护的 Ingress Controller 的定义。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: ... spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE - name: http valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 上述是使用 nginx-ingress-controller 镜像的 Pod。 注意: 这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数{POD_NAMESPACE}。而这个信息，当然是通过 Downward API 拿到的，即：Pod 的 env 字段里的定义（env.valueFrom.fieldRef.fieldPath） 而这个 Pod 本身，就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。 而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置 此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。可以看到，一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门12-容器网络之Service调试访问","slug":"K8S/k8s入门12-容器网络之Service调试访问","date":"2022-02-20T16:34:52.000Z","updated":"2022-02-20T16:36:17.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门12-容器网络之Service调试访问/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A812-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8BService%E8%B0%83%E8%AF%95%E8%AE%BF%E9%97%AE/","excerpt":"","text":"Service 的访问入口，其实就是每台宿主机上由 kube-proxy 生成的 iptables 规则，以及 kube-dns 生成的 DNS 记录。Service 的访问信息在 Kubernetes 集群之外是无效的 如何从外部(Kubernetes 集群之外)，访问到 Kubernetes 里创建的 Service 外部访问Service 最常用的办法：NodePort 123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: my-nginx labels: run: my-nginxspec: type: NodePort ports: - nodePort: 8080 targetPort: 80 protocol: TCP name: http - nodePort: 443 protocol: TCP name: https selector: run: my-nginx 在 Service 定义中声明它的类型是，type=NodePort。然后，在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口 如果不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，可通过 kube-apiserver 的–service-node-port-range 参数来修改它 这时候，要访问这个 Service，你只需要访问 1&lt;任何一台宿主机的IP地址&gt;:8080 就可以访问到某一个被代理的 Pod 的 80 端口了。NodePort 模式下，kube-proxy 要做的就是在每台宿主机上生成这样一条 iptables 规则： 1-A KUBE-NODEPORTS -p tcp -m comment --comment \"default/my-nginx: nodePort\" -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了。需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示： 1-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE 可以看到，这条规则设置在 POSTROUTING 检查点，也就是说，它给即将离开这台主机的 IP 包，进行了一次 SNAT 操作，将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。 当然，这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行（否则普通的 IP 包就被影响了）。而 iptables 做这个判断的依据，就是查看该 IP 包是否有一个“0x4000”的“标志”。你应该还记得，这个标志正是在 IP 包被执行 DNAT 操作之前被打上去的。 可是，为什么一定要对流出的包做 SNAT操作呢？ 这里的原理其实很简单，如下所示： 123456789 client \\ ^ \\ \\ v \\ node 1 &lt;--- node 2 | ^ SNAT | | ---&gt; v |endpoint 当一个外部的 client 通过 node 2 的地址访问一个 Service 的时候，node 2 上的负载均衡规则，就可能把这个 IP 包转发给一个在 node 1 上的 Pod。这里没有任何问题。 而当 node 1 上的这个 Pod 处理完请求之后，它就会按照这个 IP 包的源地址发出回复。 可是，如果没有做 SNAT 操作的话，这时候，被转发来的 IP 包的源地址就是 client 的 IP 地址。所以此时，Pod 就会直接将回复发给client。对于 client 来说，它的请求明明发给了 node 2，收到的回复却来自 node 1，这个 client 很可能会报错。 所以，在上图中，当 IP 包离开 node 2 之后，它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址。这样，Pod 在处理完成之后就会先回复给 node 2（而不是 client），然后再由 node 2 发送给 client。 当然，这也就意味着这个 Pod 只知道该 IP 包来自于 node 2，而不是外部的 client。对于 Pod 需要明确知道所有请求来源的场景来说，这是不可以的。 所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。 而这个机制的实现原理也非常简单：这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示： 123456789 client ^ / \\ / / \\ / v X node 1 node 2 ^ | | | | vendpoint 当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。 从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 LoadBalancer 类型的 Service，如下所示： 123456789101112---kind: ServiceapiVersion: v1metadata: name: example-servicespec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer 在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。 而第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 ExternalName。举个例子： 1234567kind: ServiceapiVersion: v1metadata: name: my-servicespec: type: ExternalName externalName: my.database.example.com 在上述 Service 的 YAML 文件中，我指定了一个 externalName=my.database.example.com 的字段。而且你应该会注意到，这个 YAML 文件里不需要指定 selector。 这时候，当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。 此外，Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址，比如下面这个例子： 1234567891011121314kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 在上述 Service 中，我为它指定的 externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。不过，在这里 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点。你可以想一想这是为什么。 实际上，在理解了 Kubernetes Service 机制的工作原理之后，很多与 Service 相关的问题，其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。 比如，当你的 Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常： 1234567# 在一个Pod里执行$ nslookup kubernetes.defaultServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local 如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。 而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints： 123$ kubectl get endpoints hostnamesNAME ENDPOINTShostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 需要注意的是，如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。 而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示： 12345678910I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container \"/kube-proxy\"I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier.I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns-tcp\" to [10.244.1.3:53]I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns\" to [10.244.1.3:53]I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for \"default/kubernetes:https\" to [10.240.0.2:443]I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from masterI1027 22:14:54.040048 5063 proxier.go:294] Adding new service \"default/kubernetes:https\" at 10.0.0.1:443/TCPI1027 22:14:54.040154 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns\" at 10.0.0.10:53/UDPI1027 22:14:54.040223 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns-tcp\" at 10.0.0.10:53/TCP 如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。而一个 iptables 模式的 Service 对应的规则，我在上一篇以及这一篇文章里已经全部介绍到了，它们包括： KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应； KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应； KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致； 如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。 通过查看这些链的数量、转发目的地址、端口、过滤条件等信息，你就能很容易发现一些异常的蛛丝马迹。 当然，还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己。这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。关于 Hairpin 的原理我在前面已经介绍过，这里就不再赘述了。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。 其中，在 hairpin-veth 模式下，你应该能看到 CNI 网桥对应的各个 VETH 设备，都将 Hairpin 模式设置为了 1，如下所示： 123$ for d in &#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;veth*&#x2F;hairpin_mode; do echo &quot;$d &#x3D; $(cat $d)&quot;; done&#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;veth4bfbfe74&#x2F;hairpin_mode &#x3D; 1&#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;vethfc2a18c5&#x2F;hairpin_mode &#x3D; 1 而如果是 promiscuous-bridge 模式的话，你应该看到 CNI 网桥的混杂模式（PROMISC）被开启，如下所示： 12$ ifconfig cni0 |grep PROMISCUP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1 总结： 从外部访问Service的三种方式 NodePort LoadBalancer External Name 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护 Kubernetes 里面的 Service 和 DNS 机制，也都不具备强多租户能力。比如，在多租户情况下，每个租户应该拥有一套独立的 Service 规则（Service 只应该看到和代理同一个租户下的 Pod）。再比如 DNS，在多租户情况下，每个租户应该拥有自己的 kube-dns（kube-dns 只应该为同一个租户下的 Service 和 Pod 创建 DNS Entry） 为什么Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点？ 因为k8s只是在集群中的每个节点上创建了一个 externalIPs 与kube-ipvs0网卡的绑定关系. 若流量都无法路由到任意的一个k8s节点,那自然无法将流量转给具体的service 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门11-容器网络之服务发现","slug":"K8S/k8s入门11-容器网络之服务发现","date":"2022-02-20T16:34:37.000Z","updated":"2022-02-20T16:36:23.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门11-容器网络之服务发现/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A811-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。 一个Service例子： 123456789101112apiVersion: v1kind: Servicemetadata: name: hostnamesspec: selector: app: hostnames ports: - name: default protocol: TCP port: 80 targetPort: 9376 使用 selector 字段来声明这个 Service 只代理携带了 app=hostnames 标签的 Pod。Service 的 80 端口，代理的是 Pod 的 9376 端口 然后应用的Deployment为： 1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: hostnamesspec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: - name: hostnames image: k8s.gcr.io/serve_hostname ports: - containerPort: 9376 protocol: TCP 这个应用的作用就是每次访问 9376 端口时，返回它自己的 hostname。 被 selector 选中的 Pod，称为 Service 的 Endpoints，可使用 kubectl get ep 命令看到它们，如下所示： 123$ kubectl get endpoints hostnamesNAME ENDPOINTShostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉 通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod ： 12345678910111213$ kubectl get svc hostnamesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhostnames ClusterIP 10.0.1.175 &lt;none&gt; 80/TCP 5s$ curl 10.0.1.175:80hostnames-0uton$ curl 10.0.1.175:80hostnames-yp2kp$ curl 10.0.1.175:80hostnames-bvc05 这个 VIP 地址是 Kubernetes 自动为 Service 分配的。 通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。这种方式称为：ClusterIP 模式的 Service Service 是由 kube-proxy 组件，加上 iptables 来共同实现的 举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示： 1-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \"default/hostnames: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。 10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。 KUBE-SVC-NWV5X2332I4OT4T3 规则，实际上是一组规则的集合，如下所示： 123-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -j KUBE-SEP-57KPRZ3JQVENLNBR 这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。 而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。 这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。 通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示 123456789-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.3.6:9376-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.1.7:9376-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.2.3:9376 可以看到，这三条链，其实是三条 DNAT 规则。但在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。而 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。 这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的 kube-proxy - IPVS模式 kube-proxy 通过 iptables 处理 Service 的过程，需要在宿主机上设置相当多的 iptables 规则，并在控制循环里不断地刷新这些规则来确保它们始终是正确的 当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。 解决办法：IPVS模式的Service IPVS 模式的工作原理：当创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示： 123456# ip addr ... 73：kube-ipvs0：&lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff inet 10.0.1.175/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示： 12345678# ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.102.128.4:80 rr -&gt; 10.244.3.6:9376 Masq 1 0 0 -&gt; 10.244.1.7:9376 Masq 1 0 0 -&gt; 10.244.2.3:9376 Masq 1 0 0 三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。 相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。 不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。所以，在大规模集群里，建议为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。 service 与 DNS 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。 对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：…svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。 而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：…svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。 此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：…pod.cluster.local。这条记录指向 Pod 的 IP 地址。而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：…svc.cluster.local。这条记录也指向 Pod 的 IP 地址。 但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain 字段，那么这时候 Pod 的 A 记录就会变成：…svc.cluster.local，比如： 12345678910111213141516171819202122232425262728apiVersion: v1kind: Servicemetadata: name: default-subdomainspec: selector: name: busybox clusterIP: None ports: - name: foo port: 1234 targetPort: 1234---apiVersion: v1kind: Podmetadata: name: busybox1 labels: name: busyboxspec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - \"3600\" name: busybox 在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。 在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。 总结： 当服务（Pod）的 IP 地址是不固定的且没办法提前获知时，该如何通过一个固定的方式访问到这个 Pod 呢？ ClusterIP 模式的 Service 为你提供的，就是一个 Pod 的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过 Label 确定的。 Headless Service 提供的是一个 Pod 的稳定的 DNS 名字，并且名字是可以通过 Pod 名字和 Service 名字拼接出来的。 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门10-容器网络之多租户","slug":"K8S/k8s入门10-容器网络之多租户","date":"2022-02-20T16:34:25.000Z","updated":"2022-02-20T16:36:30.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门10-容器网络之多租户/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A810-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E7%A7%9F%E6%88%B7/","excerpt":"","text":"什么是多租户：一种软件架构规范，运行在服务器上的单一实例，可服务多个客户或组织（租户）,一个满足多租户规范的软件应用需要对数据和配置进行隔离，每一个租户都有自己虚拟的实例 Kubernetes 的网络模型，以及前面这些网络方案的实现，都只关注网络的“连通”，却不关心“隔离”，那么 Kubernetes 的网络方案对“隔离”是如何考虑的？ NetworkPolicy 在 Kubernetes 里，网络隔离能力的定义，是依靠一种专门的 API 对象来描述的，即：NetworkPolicy 12345678910111213141516171819202122232425262728293031323334apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 Kubernetes 里的 Pod 默认都是“允许所有”（Accept All）的，即：Pod 可以接收来自任何发送方的请求；或者，向任何接收方发送请求。而如果你要对这个情况作出限制，就必须通过 NetworkPolicy 对象来指定 而在上面这个例子里，你首先会看到 podSelector 字段。它的作用，就是定义这个 NetworkPolicy 的限制范围，比如：当前 Namespace 里携带了 role=db 标签的 Pod 而如果你把 podSelector 字段留空： 12spec: podSelector: &#123;&#125; 那么这个 NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod。而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。而 NetworkPolicy 定义的规则，其实就是“白名单”。 例如，在我们上面这个例子里，我在 policyTypes 字段，定义了这个 NetworkPolicy 的类型是 ingress 和 egress，即：它既会影响流入（ingress）请求，也会影响流出（egress）请求。 然后，在 ingress 字段里，我定义了 from 和 ports，即：允许流入的“白名单”和端口。其中，这个允许流入的“白名单”里，我指定了三种并列的情况，分别是：ipBlock、namespaceSelector 和 podSelector。 而在 egress 字段里，我则定义了 to 和 ports，即：允许流出的“白名单”和端口。这里允许流出的“白名单”的定义方法与 ingress 类似。只不过，这一次 ipblock 字段指定的，是目的地址的网段。 综上所述，这个 NetworkPolicy 对象，指定的隔离规则如下所示： 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。 Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：a. default Namespace 里的，携带了 role=fronted 标签的 Pod；b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。 Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。 需要注意的是，定义一个 NetworkPolicy 对象的过程，容易犯错的是“白名单”部分（from 和 to 字段） 12345678910... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ... 像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。 而下面这个例子，虽然看起来类似，但是它定义的规则却完全不同： 12345678910... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 注意看，这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。 此外，如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用，你的 CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的。 在具体实现上，凡是支持 NetworkPolicy 的 CNI 网络插件，都维护着一个 NetworkPolicy Controller，通过控制循环的方式对 NetworkPolicy 对象的增删改查做出响应，然后在宿主机上完成 iptables 规则的配置工作。 在 Kubernetes 生态里，目前已经实现了 NetworkPolicy 的网络插件包括 Calico、Weave 和 kube-router 等多个项目，但是并不包括 Flannel 项目。 所以说，如果想要在使用 Flannel 的同时还使用 NetworkPolicy 的话，你就需要再额外安装一个网络插件，比如 Calico 项目，来负责执行 NetworkPolicy。 安装 Flannel + Calico 的流程非常简单，你直接参考这个文档一键安装即可 网络隔离 以三层网络插件为例(比如 Calico 和 kube-router)，分析一下这部分的原理。简单的NetworkPolicy 策略 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db ingress: - from: - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: tcp port: 6379 可以看到，我们指定的 ingress“白名单”，是任何 Namespace 里，携带 project=myproject 标签的 Namespace 里的 Pod；以及 default Namespace 里，携带了 role=frontend 标签的 Pod。允许被访问的端口是：6379。而被隔离的对象，是所有携带了 role=db 标签的 Pod。 那么这个时候，Kubernetes 的网络插件就会使用这个 NetworkPolicy 的定义，在宿主机上生成 iptables 规则。这个过程，我可以通过如下所示的一段 Go 语言风格的伪代码来为你描述： 1234567for dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址 for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址 for port, protocol := range ingress.ports &#123; iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT &#125; &#125;&#125; 可以看到，这是一条最基本的、通过匹配条件决定下一步动作的 iptables 规则。 这条规则的名字是 KUBE-NWPLCY-CHAIN，含义是：当 IP 包的源地址是 srcIP、目的地址是 dstIP、协议是 protocol、目的端口是 port 的时候，就允许它通过（ACCEPT）。 而正如这段伪代码所示，匹配这条规则所需的这四个参数，都是从 NetworkPolicy 对象里读取出来的 Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。 在设置好上述“隔离”规则之后，网络插件还需要想办法，将所有对被隔离 Pod 的访问请求，都转发到上述 KUBE-NWPLCY-CHAIN 规则上去进行匹配。并且，如果匹配不通过，这个请求应该被“拒绝”。 在 CNI 网络插件中，上述需求可以通过设置两组 iptables 规则来实现。 第一组规则，负责“拦截”对被隔离 Pod 的访问请求。生成这一组规则的伪代码，如下所示： 1234567for pod := range 该Node上的所有Pod &#123; if pod是networkpolicy.spec.podSelector选中的 &#123; iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN ... &#125;&#125; IPTABLE iptables 规则使用到了内置链：FORWARD。它是什么意思呢？说到这里，我就得为你稍微普及一下 iptables 的知识了。实际上，iptables 只是一个操作 Linux 内核 Netfilter 子系统的“界面”。顾名思义，Netfilter 子系统的作用，就是 Linux 内核里挡在“网卡”和“用户态进程”之间的一道“防火墙”。它们的关系，可以用如下的示意图来表示： 可以看到，这幅示意图中，IP 包“一进一出”的两条路径上，有几个关键的“检查点”，它们正是 Netfilter 设置“防火墙”的地方。在 iptables 中，这些“检查点”被称为：链（Chain）。这是因为这些“检查点”对应的 iptables 规则，是按照定义顺序依次进行匹配的。这些“检查点”的具体工作原理，可以用如下所示的示意图进行描述： 可以看到，当一个 IP 包通过网卡进入主机之后，它就进入了 Netfilter 定义的流入路径（Input Path）里。在这个路径中，IP 包要经过路由表路由来决定下一步的去向。而在这次路由之前，Netfilter 设置了一个名叫 PREROUTING 的“检查点”。在 Linux 内核的实现里，所谓“检查点”实际上就是内核网络协议栈代码里的 Hook（比如，在执行路由判断的代码之前，内核会先调用 PREROUTING 的 Hook）。 而在经过路由之后，IP 包的去向就分为了两种： 第一种，继续在本机处理； 第二种，被转发到其他目的地 我们先说一下 IP 包的第一种去向。这时候，IP 包将继续向上层协议栈流动。在它进入传输层之前，Netfilter 会设置一个名叫 INPUT 的“检查点”。到这里，IP 包流入路径（Input Path）结束。 接下来，这个 IP 包通过传输层进入用户空间，交给用户进程处理。而处理完成后，用户进程会通过本机发出返回的 IP 包。这时候，这个 IP 包就进入了流出路径（Output Path）。此时，IP 包首先还是会经过主机的路由表进行路由。 路由结束后，Netfilter 就会设置一个名叫 OUTPUT 的“检查点”。然后，在 OUTPUT 之后，再设置一个名叫 POSTROUTING“检查点”。你可能会觉得奇怪，为什么在流出路径结束后，Netfilter 会连着设置两个“检查点”呢？ 这就要说到在流入路径里，路由判断后的第二种去向了。在这种情况下，这个 IP 包不会进入传输层，而是会继续在网络层流动，从而进入到转发路径（Forward Path）。在转发路径中，Netfilter 会设置一个名叫 FORWARD 的“检查点”。而在 FORWARD“检查点”完成后，IP 包就会来到流出路径。而转发的 IP 包由于目的地已经确定，它就不会再经过路由，也自然不会经过 OUTPUT，而是会直接来到 POSTROUTING“检查点”。所以说，POSTROUTING 的作用，其实就是上述两条路径，最终汇聚在一起的“最终检查点”。 需要注意的是，在有网桥参与的情况下，上述 Netfilter 设置“检查点”的流程，实际上也会出现在链路层（二层），并且会跟我在上面讲述的网络层（三层）的流程有交互。 这些链路层的“检查点”对应的操作界面叫作 ebtables。所以，准确地说，数据包在 Linux Netfilter 子系统里完整的流动过程，其实应该如下所示（这是一幅来自Netfilter 官方的原理图，建议你点击图片以查看大图）： 可以看到，我前面为你讲述的，正是上图中绿色部分，也就是网络层的 iptables 链的工作流程。 另外，你应该还能看到，每一个白色的“检查点”上，还有一个绿色的“标签”，比如：raw、nat、filter 等等。 在 iptables 里，这些标签叫作：表。比如，同样是 OUTPUT 这个“检查点”，filter Output 和 nat Output 在 iptables 里的语法和参数，就完全不一样，实现的功能也完全不同。 所以说，iptables 表的作用，就是在某个具体的“检查点”（比如 Output）上，按顺序执行几个不同的检查动作（比如，先执行 nat，再执行 filter）。 在理解了 iptables 的工作原理之后，我们再回到 NetworkPolicy 上来。这时候，前面由网络插件设置的、负责“拦截”进入 Pod 的请求的三条 iptables 规则，就很容易读懂了： 123iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAINiptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN... 其中，第一条 FORWARD 链“拦截”的是一种特殊情况：它对应的是同一台宿主机上容器之间经过 CNI 网桥进行通信的流入数据包。其中，–physdev-is-bridged 的意思就是，这个 FORWARD 链匹配的是，通过本机上的网桥设备，发往目的地址是 podIP 的 IP 包。 kube-router 其实是一个简化版的 Calico，它也使用 BGP 来维护路由信息，但是使用 CNI bridge 插件负责跟 Kubernetes 进行交互。 而第二条 FORWARD 链“拦截”的则是最普遍的情况，即：容器跨主通信。这时候，流入容器的数据包都是经过路由转发（FORWARD 检查点）来的。 不难看到，这些规则最后都跳转（即：-j）到了名叫 KUBE-POD-SPECIFIC-FW-CHAIN 的规则上。它正是网络插件为 NetworkPolicy 设置的第二组规则。而这个 KUBE-POD-SPECIFIC-FW-CHAIN 的作用，就是做出“允许”或者“拒绝”的判断。这部分功能的实现，可以简单描述为下面这样的 iptables 规则： 12iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAINiptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable 可以看到，首先在第一条规则里，我们会把 IP 包转交给前面定义的 KUBE-NWPLCY-CHAIN 规则去进行匹配。按照我们之前的讲述，如果匹配成功，那么 IP 包就会被“允许通过”。 而如果匹配失败，IP 包就会来到第二条规则上。可以看到，它是一条 REJECT 规则。通过这条规则，不满足 NetworkPolicy 定义的请求就会被拒绝掉，从而实现了对该容器的“隔离”。 以上，就是 CNI 网络插件实现 NetworkPolicy 的基本方法了。当然，对于不同的插件来说，上述实现过程可能有不同的手段，但根本原理是不变的。 NetworkPolicy 实际上只是宿主机上的一系列 iptables 规则，Kubernetes 负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力 它使得指定的 Namespace（比如 my-namespace）里的所有 Pod，都不能接收任何 Ingress 请求。 job，cronjob这类计算型pod不需要也不应该对外提供服务，可以拒绝所有流入流量，提高系统安全。 参考链接 《极客时间-深入剖析 Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"协议5-TLS","slug":"Protocol/协议5-TLS","date":"2022-02-20T16:27:19.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/02/21/Protocol/协议5-TLS/","link":"","permalink":"http://xboom.github.io/2022/02/21/Protocol/%E5%8D%8F%E8%AE%AE5-TLS/","excerpt":"","text":"引言 HTTPS是建立在HTTP的基础上添加SSL加密层，通过TLS/SSL具有的身份验证、信息加密和完整性校验功能保证数据安全。 HTTP与HTTPS的区别： HTTPS是加密传输协议，HTTP是名文传输协议; HTTPS需要用到SSL证书，而HTTP不用; HTTPS比HTTP更加安全，对搜索引擎更友好，利于SEO HTTPS标准端口443，HTTP标准端口80; HTTPS基于传输层，HTTP基于应用层; HTTPS在浏览器显示绿色安全锁，HTTP没有显示; 握手与密钥协商过程(报文分析) 基于RSA握手和密钥交换的TLS/SSL握手过程 未完待续。。。 参考链接 SSL/TLS握手过程 SSL/TLS详解","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"TLS","slug":"TLS","permalink":"http://xboom.github.io/tags/TLS/"}]},{"title":"算法之美-令牌桶算法","slug":"Algorithms To Live By/算法之美-令牌桶算法","date":"2022-02-20T16:22:31.000Z","updated":"2022-07-21T01:53:50.000Z","comments":true,"path":"2022/02/21/Algorithms To Live By/算法之美-令牌桶算法/","link":"","permalink":"http://xboom.github.io/2022/02/21/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E4%BB%A4%E7%89%8C%E6%A1%B6%E7%AE%97%E6%B3%95/","excerpt":"","text":"在高并发系统中有三把利器用来保护系统：缓存、降级和限流 其中常用的限流算法有 **漏桶算法 **和 令牌桶算法 漏桶算法 把请求比作是水，水来了都先放进桶里，并以限定的速度出水，当水来得过猛而出水不够快时就会导致水直接溢出，即拒绝服务 漏斗有一个进水口 和 一个出水口，出水口以一定速率出水，并且有一个最大出水速率： 在漏斗中没有水的时候， 如果进水速率小于等于最大出水速率，那么，出水速率等于进水速率，此时，不会积水 如果进水速率大于最大出水速率，那么，漏斗以最大速率出水，此时，多余的水会积在漏斗中 在漏斗中有水的时候 出水口以最大速率出水 如果漏斗未满，且有进水的话，那么这些水会积在漏斗中 如果漏斗已满，且有进水的话，那么这些水会溢出到漏斗之外 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package ratelimit // import \"go.uber.org/ratelimit\"import ( \"time\" \"sync/atomic\" \"unsafe\")type state struct &#123; last time.Time sleepFor time.Duration&#125;type atomicLimiter struct &#123; state unsafe.Pointer //用于存储上一次执行的时间以及需要sleep的时间 padding [56]byte //是一个无意义的填充数据，为了提高性能，避免 cpu 缓存的 false sharing perRequest time.Duration //只单位，默认为秒 maxSlack time.Duration //松弛时间，也就是可以允许的突发流量的大小, 默认是 Pre/10 clock Clock //时钟，用于在测试的时候可以 mock 掉不使用真实的时间&#125;// newAtomicBased returns a new atomic based limiter.func newAtomicBased(rate int, opts ...Option) *atomicLimiter &#123; // TODO consider moving config building to the implementation // independent code. config := buildConfig(opts) perRequest := config.per / time.Duration(rate) l := &amp;atomicLimiter&#123; perRequest: perRequest, maxSlack: -1 * time.Duration(config.slack) * perRequest, clock: config.clock, &#125; initialState := state&#123; last: time.Time&#123;&#125;, sleepFor: 0, &#125; atomic.StorePointer(&amp;l.state, unsafe.Pointer(&amp;initialState)) return l&#125;// Take blocks to ensure that the time spent between multiple// Take calls is on average time.Second/rate.func (t *atomicLimiter) Take() time.Time &#123; var ( newState state // 状态 taken bool // 用于表示原子操作是否成功 interval time.Duration // 需要 sleep 的时间 ) for !taken &#123; // 如果 CAS 操作不成功就一直尝试 now := t.clock.Now() //获取当前时间 previousStatePointer := atomic.LoadPointer(&amp;t.state) // load 出上一次调用的时间 oldState := (*state)(previousStatePointer) newState = state&#123; last: now, sleepFor: oldState.sleepFor, &#125; // 如果 last 是零值的话，表示之前就没用过，直接保存返回即可 if oldState.last.IsZero() &#123; taken = atomic.CompareAndSwapPointer(&amp;t.state, previousStatePointer, unsafe.Pointer(&amp;newState)) continue &#125; // sleepFor 是需要睡眠的时间，由于引入了松弛时间，所以 sleepFor 可能是一个 // maxSlack ~ 0 之间的一个值，所以这里需要将现在的需要 sleep 的时间和上一次 // sleepFor 的值相加 newState.sleepFor += t.perRequest - now.Sub(oldState.last) // 如果距离上一次调用已经很久了，sleepFor 可能会是一个很小的值 // 最小值只能是 maxSlack 的大小 if newState.sleepFor &lt; t.maxSlack &#123; newState.sleepFor = t.maxSlack &#125; // 如果 sleepFor 大于 0 的话，计算出需要 sleep 的时间 // 然后将 state.sleepFor 置零 if newState.sleepFor &gt; 0 &#123; newState.last = newState.last.Add(newState.sleepFor) interval, newState.sleepFor = newState.sleepFor, 0 &#125; // 保存状态 taken = atomic.CompareAndSwapPointer(&amp;t.state, previousStatePointer, unsafe.Pointer(&amp;newState)) &#125; t.clock.Sleep(interval) return newState.last&#125; 令牌桶算法 令牌桶算法的原理是系统以恒定的速率产生令牌，然后把令牌放到令牌桶中，令牌桶有一个容量，当令牌桶满了的时候，再向其中放令牌，那么多余的令牌会被丢弃；当想要处理一个请求的时候，需要从令牌桶中取出一个令牌，如果此时令牌桶中没有令牌，那么则拒绝该请求 源码：https://github.com/beefsack/go-rate/blob/master/rate.go 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package rateimport ( \"container/list\" \"sync\" \"time\")// A RateLimiter limits the rate at which an action can be performed. It// applies neither smoothing (like one could achieve in a token bucket system)// nor does it offer any conception of warmup, wherein the rate of actions// granted are steadily increased until a steady throughput equilibrium is// reached.type RateLimiter struct &#123; limit int interval time.Duration mtx sync.Mutex times list.List //双向链表&#125;// New creates a new rate limiter for the limit and interval.func New(limit int, interval time.Duration) *RateLimiter &#123; lim := &amp;RateLimiter&#123; limit: limit, interval: interval, &#125; lim.times.Init() return lim&#125;// Wait blocks if the rate limit has been reached. Wait offers no guarantees// of fairness for multiple actors if the allowed rate has been temporarily// exhausted.func (r *RateLimiter) Wait() &#123; for &#123; ok, remaining := r.Try() if ok &#123; break &#125; time.Sleep(remaining) &#125;&#125;// Try returns true if under the rate limit, or false if over and the// remaining time before the rate limit expires.func (r *RateLimiter) Try() (ok bool, remaining time.Duration) &#123; r.mtx.Lock() defer r.mtx.Unlock() now := time.Now() if l := r.times.Len(); l &lt; r.limit &#123; r.times.PushBack(now) return true, 0 &#125; frnt := r.times.Front() if diff := now.Sub(frnt.Value.(time.Time)); diff &lt; r.interval &#123; return false, r.interval - diff &#125; frnt.Value = now r.times.MoveToBack(frnt) return true, 0&#125; 漏桶VS令牌桶 漏桶算法 能够强行限制数据的传输速率， 令牌桶算法 在能够限制数据的平均传输速率外，只要桶中存在令牌，就允许突发地传输数据直到达到用户配置的门限，所以也适合于具有突发特性的流量 参考链接 https://www.cnblogs.com/xuwc/p/9123078.html https://www.jianshu.com/p/d6250493308b https://segmentfault.com/a/1190000015967922","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"算法之美-剑指","slug":"Algorithms To Live By/算法之美-剑指","date":"2022-02-14T16:27:12.000Z","updated":"2022-07-21T01:56:08.000Z","comments":true,"path":"2022/02/15/Algorithms To Live By/算法之美-剑指/","link":"","permalink":"http://xboom.github.io/2022/02/15/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E5%89%91%E6%8C%87/","excerpt":"","text":"数据结构 链表 从尾到头打印链表 题目 1234567输入一个链表的头节点，按链表从尾到头的顺序返回每个节点的值（用数组返回）。如输入&#123;1,2,3&#125;的链表如下图:返回一个数组为[3,2,1]0 &lt;= 链表长度 &lt;= 10000 解答 123456789101112131415161718192021/*** struct ListNode &#123;* int val;* struct ListNode *next;* ListNode(int x) :* val(x), next(NULL) &#123;* &#125;* &#125;;*/class Solution &#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; vector&lt;int&gt; result; while(head) &#123; result.push_back(head-&gt;val); head = head-&gt;next; &#125; reverse(result.begin(), result.end()); return result; &#125;&#125;; 反转链表 题目 给定一个单链表的头结点pHead(该头节点是有值的，比如在下图，它的val是1)，长度为n，反转该链表后，返回新链表的表头。 数据范围： 0\\leq n\\leq10000≤n≤1000 要求：空间复杂度 O(1)O(1) ，时间复杂度 O(n)O(n) 。 如当输入链表{1,2,3}时， 经反转后，原链表变为{3,2,1}，所以对应的输出为{3,2,1}。 以上转换过程如下图所示： 解答 1234567891011121314151617181920212223/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* ReverseList(ListNode* pHead) &#123; ListNode *pre = nullptr; ListNode *cur = pHead; ListNode *nex = nullptr; // 这里可以指向nullptr，循环里面要重新指向 while (cur) &#123; nex = cur-&gt;next; cur-&gt;next = pre; pre = cur; cur = nex; &#125; return pre; &#125;&#125;; 合并两个排序的链表 题目 输入两个递增的链表，单个链表的长度为n，合并这两个链表并使新链表中的节点仍然是递增排序的。 数据范围： 0 \\le n \\le 10000≤n≤1000，-1000 \\le 节点值 \\le 1000−1000≤节点值≤1000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 如输入{1,3,5},{2,4,6}时，合并后的链表为{1,2,3,4,5,6}，所以对应的输出为{1,2,3,4,5,6} 解答 12345678910111213141516171819202122232425262728/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) &#123; ListNode * pre = new ListNode(-1); ListNode * cur = pre; while(pHead1 &amp;&amp; pHead2) &#123; if(pHead1-&gt;val &lt; pHead2-&gt;val) &#123; cur-&gt;next = pHead1; pHead1 = pHead1-&gt;next; &#125; else &#123; cur-&gt;next = pHead2; pHead2 = pHead2-&gt;next; &#125; cur = cur-&gt;next; &#125; if(!pHead1) cur-&gt;next = pHead2; if(!pHead2) cur-&gt;next = pHead1; return pre-&gt;next; &#125;&#125;; 两个链表的第一个公共结点 题目 输入两个无环的单向链表，找出它们的第一个公共结点，如果没有公共节点则返回空。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的） 数据范围： n \\le 1000n≤1000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如，输入{1,2,3},{4,5},{6,7}时，两个无环的单向链表的结构如下图所示： 输入分为是3段，第一段是第一个链表的非公共部分，第二段是第二个链表的非公共部分，第三段是第一个链表和二个链表的公共部分。 后台会将这3个参数组装为两个链表，并将这两个链表对应的头节点传入到函数FindFirstCommonNode里面，用户得到的输入只有pHead1和pHead2 解答 1234567891011121314151617181920/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) &#123; ListNode *ta = pHead1; ListNode *tb = pHead2; while (ta != tb) &#123; ta = ta ? ta-&gt;next : pHead2; tb = tb ? tb-&gt;next : pHead1; &#125; return ta; &#125;&#125;; 链表中环的入口结点 题目 给一个长度为n链表，若其中包含环，请找出该链表的环的入口结点，否则，返回null。 数据范围： n\\le10000n≤10000，1&lt;=结点值&lt;=100001&lt;=结点值&lt;=10000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如，输入{1,2},{3,4,5}时，对应的环形链表如下图所示： 12345678910111213141516171819202122232425262728/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; ListNode *fast = pHead; ListNode *slow = pHead; while(fast &amp;&amp; fast-&gt;next) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; if(fast == slow) break; &#125; if(!fast || !fast-&gt;next) return nullptr; fast = pHead; while(fast != slow) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return fast; &#125;&#125;; 链表中倒数最后k个结点 题目 输入一个长度为 n 的链表，设链表中的元素的值为 ai ，返回该链表中倒数第k个节点。 如果该链表长度小于k，请返回一个长度为 0 的链表。 数据范围：0 \\leq n \\leq 10^50≤n≤105，0 \\leq a_i \\leq 10^90≤a**i≤109，0 \\leq k \\leq 10^90≤k≤109 要求：空间复杂度 O(n)O(n)，时间复杂度 O(n)O(n) 进阶：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如输入{1,2,3,4,5},2时，对应的链表结构如下图所示： 其中蓝色部分为该链表的最后2个结点，所以返回倒数第2个结点（也即结点值为4的结点）即可，系统会打印后面所有的节点来比较。 解答 12345678910111213141516171819202122232425262728/** * struct ListNode &#123; * int val; * struct ListNode *next; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * &#125;; */class Solution &#123;public: /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param pHead ListNode类 * @param k int整型 * @return ListNode类 */ ListNode* FindKthToTail(ListNode* pHead, int k) &#123; ListNode* r = pHead; while (k-- &amp;&amp; r) r = r-&gt;next; // 移动右侧指针造成 k 的距离差 if (k &gt;= 0) return nullptr; // 此时说明 k 比链表长度长 ListNode* l = pHead; while (r) r = r-&gt;next, l = l-&gt;next; // 两个指针一起移动找到倒数第 k 个节点 return l; &#125;&#125;; 删除链表中重复的节点 题目 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 数据范围：链表长度满足 0 \\le n \\le 1000 \\0≤n≤1000 ，链表中的值满足 1 \\le val \\le 1000 \\1≤val≤1000 进阶：空间复杂度 O(n)*O*(n) ，时间复杂度 O(n) *O*(n) 例如输入{1,2,3,3,4,4,5}时，对应的输出为{1,2,5}，对应的输入输出链表如下图所示： 例如 输入：{1,2,3,3,4,4,5},输出：{1,2,5} 输入：{1,1,1,8}，输出：{8} 解答 123456789101112131415161718192021222324252627282930313233343536/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* deleteDuplication(ListNode* pHead) &#123; if (!pHead) return NULL; ListNode* slow = new ListNode(-1), *fast = new ListNode(-1), *dummy = new ListNode(-1); dummy-&gt;next = pHead; // 初始化两个指针 slow = dummy; //虚拟指针 fast = dummy-&gt;next; //头指针 while (fast) &#123; // 遇到重复则只更新fast节点 while (fast-&gt;next &amp;&amp; fast-&gt;val == fast-&gt;next-&gt;val) &#123; fast = fast-&gt;next; &#125; // 遇到重复 if (slow-&gt;next != fast) &#123; //如果不相等，说明中间slow 与 fast有相同值的节点 slow-&gt;next = fast-&gt;next; fast = slow-&gt;next; &#125; else &#123; // 没有重复 fast = fast-&gt;next; slow = slow-&gt;next; &#125; &#125; return dummy-&gt;next; &#125;&#125;; 参考链接 https://www.nowcoder.com/ta/coding-interviews","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"MySql入门9-温故而知新","slug":"MySql/MySql入门9-温故而知新","date":"2022-02-11T07:51:12.000Z","updated":"2022-02-11T08:17:48.000Z","comments":true,"path":"2022/02/11/MySql/MySql入门9-温故而知新/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql%E5%85%A5%E9%97%A89-%E6%B8%A9%E6%95%85%E8%80%8C%E7%9F%A5%E6%96%B0/","excerpt":"","text":"数据库基础知识","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门7-表","slug":"MySql/MySql入门7-表","date":"2022-02-11T07:49:55.000Z","updated":"2022-02-11T07:50:14.000Z","comments":true,"path":"2022/02/11/MySql/MySql入门7-表/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql%E5%85%A5%E9%97%A87-%E8%A1%A8/","excerpt":"","text":"","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门6-集群","slug":"MySql/MySql入门6-集群","date":"2022-02-11T07:47:24.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/02/11/MySql/MySql入门6-集群/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql%E5%85%A5%E9%97%A86-%E9%9B%86%E7%BE%A4/","excerpt":"","text":"高可用方案 共享存储：SAN/NAS 操作系统实时数据块恢复：DRBD架构(MySQL+DRBD+Heartbeat) 主从复制架构 主从复制(一主多从) MMM架构(双主多从) MHA架构(多主多从) 数据库高可用架构 GRT(MySQL Group Replication) Galera MySQL Cluster 和 PXC MySQL Cluster(ndb存储引擎比较复杂，并没有大规模使用) PXC(Percona XtraDB Cluster) 常见方案介绍 方案1 主从架构 主从复制的模式有哪些？https://zhuanlan.zhihu.com/p/307288925 怎么保证数据库一致性 方案2 MHA架构 MHA: Master Hight Availability Manager and Toolsfor MySQL 生产环境MySQL数据库集群MHA上线实施方案 MHA(Master High Availability Manager and Toolsfor MySQL)目前在Mysql高可用方面是一个相对成熟的解决方案。它是日本的一位MySQL专家采用Perl语言编写的一个脚本管理工具，该工具仅适用于MySQLReplication 环境，目的在于维持Master主库的高可用性。 MHA是基于标准的MySQL复制(异步/半同步)。 MHA是由管理节点(MHA Manager)和数据节点(MHA Node)两部分组成。 MHA Manager可以单独部署在一台独立机器,也可以部署在一台slave上。 方案3 MMM架构 MySQL-MMM实现MySQL高可用 MMM，全称为Master-Master replication manager for Mysql，是一套支持双主故障切换和双主日常管理的脚本程序，MMM使用Perl语言开发。主要用来监控和管理MySQL Master-Master(双)复制。特别适合DBA做维护等需要主从复制的场景，通过双主架构避免了重复搭建从库的麻烦。虽然叫做双主复制，但是业务上同一时刻只允许对一个主进行写入，另一台备选主上提供部分读服务，以加速在主主切换时备选主的预热 MMM优缺点 优点：高可用性，扩展性好，出现故障自动切换，对于主主同步，在同一时间只提供一台数据库写操作，保证的数据的一致性。 缺点：Monitor节点是单点，可以结合Keepalived实现高可用 方案4 DRBD架构 https://www.linbit.com/en/drbd-community/drbd-download/ 读写分离方案 客户端解决方案(应用层) 常用：DDL、 Sharding-Jdbc (常用shardding-jdbc) 优点： 程序自动完成，数据源方便管理 不需要维护，因为没用中间件 理论支持任何数据库 （sql标准） 缺点： 增加了开发成本、代码有入侵 不能做到动态增加数据源 程序员开发完成，运维参与不了。 中间件解决方案（代理层） 常用：mysql proxy、mycat、altas (常用mycat) 优点： 数据增加了都程序没用任何影响 应用层（程序）不需要管数据库方面的事情 增加数据源不需要重启程序 缺点： 程序依赖中间件，导致切换数据库变的困难 增加了proxy 性能下降 增加了维护工作、高可用问题 主从复制 Mysql为了保证数据库一致性，引入了集中复制类型，分别是 异步复制 半同步复制 全同步复制 主从复制整理分为以下三个步骤 主库将数据库的变更操作记录到Binlog日志文件(Master服务器对数据库更改操作记录在Binlog中，BinlogDump Thread接到写入请求后，读取Binlog信息推送给Slave的I/O Thread) 从库读取主库中的Binlog日志文件信息写入到从库的Relay Log中继日志中(Slave的I/O Thread将读取到的Binlog信息写入到本地Relay Log中) 从库读取中继日志信息在从库中进行Replay,更新从库数据信息(Slave的SQL Thread检测到Relay Log的变更请求，解析relay log中内容在从库上执行) 主库上并发的修改操作在从库上只能串行化执行，因为只有一个SQL线程来重放中继日志，这也是很多工作负载的性能瓶颈所在 异步复制 mysql主从复制存在的问题： 主库宕机后，数据可能丢失 从库只有一个SQL Thread，主库写压力大，复制很可能延时 半同步复制 为了提升数据安全，MySQL让Master在某一个时间点等待Slave节点的 ACK（Acknowledgecharacter）消息，接收到ACK消息后才进行事务提交，这也是半同步复制的基础，MySQL从5.5版本开始引入了半同步复制机制来降低数据丢失的概率。 介绍半同步复制之前先快速过一下 MySQL 事务写入碰到主从复制时的完整过程，主库事务写入分为 4个步骤： InnoDB Redo File Write (Prepare Write) Binlog File Flush &amp; Sync to Binlog File InnoDB Redo File Commit（Commit Write） Send Binlog to Slave 当Master不需要关注Slave是否接受到Binlog Event时，即为传统的主从复制。 当Master需要在第三步等待Slave返回ACK时，即为 after-commit（Master先提交，等Slave ACK以后再，回复客户端），半同步复制（MySQL 5.5引入）。 当Master需要在第二步等待 Slave 返回 ACK 时，即为 after-sync（Master先写binlog，等Slave ACK以后再提交），增强半同步（MySQL 5.7引入）。 下图是 MySQL 官方对于半同步复制的时序图，主库等待从库写入 relay log 并返回 ACK 后才进行Engine Commit。 MySQL Group Replication MySQL Group Replication是建立在已有MySQL复制框架的基础之上，通过新增Group Replication Protocol协议及Paxos协议的实现，形成的整体高可用解决方案 与原有复制方式相比，MGR主要增加了状态机一致性顺序复制和certify这两个环境 MySQL事务通过before_commit钩子进入MGR，before_commit位于MYSQL_BIN_LOG::commit()函数中，具体是在进入事务组提交MYSQL_BIN_LOG::ordered_commit()之前，这就意味着执行到before_commit这个钩子时，事务还未提交，产生的Binlog还未写入Binlog文件中，事务GTID还未产生 分布式数据库事务 分布式数据库实现分布式事务的主流方法还是2PC 如上图所示，当分布式事务提交时，会选择其中的一个数据分片作为协调者在所有数据分片上执行两阶段提交协议。由于所有数据分片都是通过 Paxos 复制日志实现多副本高可用的，当主副本发生宕机后，会由同一数据分片的备副本转换为新的主副本继续提供服务，所以可以认为参与者和协调者都是保证高可用不宕机的（多数派存活），绕开了协调者宕机的问题。 在参与者高可用的实现前提下，可以对协调者进行了“无状态”的优化。在标准的两阶段提交中，协调者要通过记录日志的方法持久化自己的状态，否则如果协调者和参与者同时宕机，协调者恢复后可能会导致事务提交状态不一致。但是如果我们认为参与者不会宕机，那么协调者并不需要写日志记录自己的状态。 所以在第一阶段所有参与者都回复prepare完成以后，即可以反馈事务提交成功，提升了2PC的效率 由于存在多副本，只要保证在prepare阶段，验证事务执行没有错误，协调者发出commit指令后，就可以乐观的认为，事务执行成功并反馈给事务发起者。相信commit消息会被多数副本收到，多数副本收到消息以后，剩下的就交给他们自己同步 在上图中（绿色部分表示写日志的动作），左侧为标准两阶段提交协议，用户感知到的提交时延是4次写日志耗时以及2次 RPC 的往返耗时；由于少了协调者的写日志耗时以及提前了应答客户端的时机，用户感知到的提交时延是1次写日志耗时以及1次 RPC 的往返耗时 参考链接 MySQL高可用集群方案 MySQL 主从复制","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"协议3-Quic","slug":"Protocol/协议3-Quic","date":"2022-02-09T07:20:49.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/02/09/Protocol/协议3-Quic/","link":"","permalink":"http://xboom.github.io/2022/02/09/Protocol/%E5%8D%8F%E8%AE%AE3-Quic/","excerpt":"","text":"TCP协议在创建链接前会进行三次握手。如果增加传输层协议(TLS)，则握手次数更多 QUIC协议 可以在1到2个数据包(取决于连接服务是未知还是已知)内，完成连接的创建(包括TLS) QUIC 非常类似于在 UDP 上实现的 TCP + TLS + HTTP/2。相比于 TCP，流控功能在用户空间而不在内核空间，可以不受限于 CUBIC 或是 BBR，而是可以自由选择甚至根据应用场景自由调整优化 为什么不修改TCP协议? TCP 是在操作系统内核和中间件固件中实现的 这么好为什么没有大规模使用？ 可能会被路由封杀UDP 443端口（ 这正是QUIC 部署的端口）； UDP包过多，由于QS限定，会被服务商误认为是攻击，UDP包被丢弃； 无论是路由器还是防火墙目前对QUIC都还没有做好准备。 QUIC 优点 QUIC 与现有 TCP + TLS + HTTP/2 方案相比，有以下几点主要特征： 利用缓存，显著减少连接建立时间； 改善拥塞控制，拥塞控制从内核空间到用户空间； 没有 head of line 阻塞的多路复用； 前向纠错，减少重传； 连接平滑迁移，网络状态的变更不会影响连接断线 TCP 的拥塞控制实际上包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复 QUIC 协议当前默认使用了 TCP 协议的 Cubic 拥塞控制算法，同时也支持 CubicBytes, Reno, RenoBytes, BBR, PCC 等拥塞控制算法。 从拥塞算法本身来看，QUIC 只是按照 TCP 协议重新实现了一遍，那么 QUIC 协议到底改进在哪些方面呢？ 可插拔 就是能够非常灵活地生效，变更和停止 应用程序层面就能实现不同的拥塞控制算法，不需要操作系统，不需要内核支持。而传统的 TCP 拥塞控制，必须要端到端的网络协议栈支持，才能实现控制效果。 即使是单个应用程序的不同连接也能支持配置不同的拥塞控制。能提供更加有效的拥塞控制。比如 BBR 适合，Cubic 适合； 程序不需要停机和升级就能实现拥塞控制的变更 单调递增的 Packet Number TCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达 QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 sequence number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。而 TCP 呢，重传 segment 的 sequence number 和原始的 segment 的 Sequence Number 保持不变，也正是由于这个特性，引入了 Tcp 重传的歧义问题 如上图，超时事件 RTO 发生后，客户端发起重传，然后接收到了 Ack 数据。由于序列号一样，这个 Ack 数据到底是原始请求的响应还是重传请求的响应呢？不好判断。 如果算成原始请求的响应，但实际上是重传请求的响应（上图左），会导致采样 RTT 变大。如果算成重传请求的响应，但实际上是原始请求的响应，又很容易导致采样 RTT 过小。 由于 Quic 重传的 Packet 和原始 Packet 的 Pakcet Number 是严格递增的，所以很容易就解决了这个问题 如上图所示，RTO 发生后，根据重传的 Packet Number 就能确定精确的 RTT 计算。如果 Ack 的 Packet Number 是 N+M，就根据重传请求计算采样 RTT。如果 Ack 的 Pakcet Number 是 N，就根据原始请求的时间计算采样 RTT，没有歧义性。 但是单纯依靠严格递增的 Packet Number 肯定是无法保证数据的顺序性和可靠性。QUIC 又引入了一个 Stream Offset 的概念。 即一个 Stream 可以经过多个 Packet 传输，Packet Number 严格递增，没有依赖。但是 Packet 里的 Payload 如果是 Stream 的话，就需要依靠 Stream 的 Offset 来保证应用数据的顺序。 假设 Packet N 丢失了，发起重传，重传的 Packet Number 是 N+2，但是它的 Stream 的 Offset 依然是 x，这样就算 Packet N + 2 是后到的，依然可以将 Stream x 和 Stream x+y 按照顺序组织起来，交给应用程序处理。 不允许 Reneging Reneging: 接收方丢弃已经接收并且上报给 SACK 选项的内容。TCP 协议不鼓励这种行为，但是协议层面允许这样的行为。主要是考虑到服务器资源有限，比如 Buffer 溢出，内存不够等情况。 Reneging 对数据重传会产生很大的干扰。因为 Sack 都已经表明接收到了，但是接收端事实上丢弃了该数据。 QUIC 在协议层面禁止 Reneging，一个 Packet 只要被 Ack，就认为它一定被正确接收，减少了这种干扰。 更多的ACK块 TCP 的 Sack 选项能够告诉发送方已经接收到的连续 Segment 的范围，方便发送方进行选择性重传。 由于 TCP 头部最大只有 60 个字节，标准头部占用了 20 字节，所以 Tcp Option 最大长度只有 40 字节，再加上 Tcp Timestamp option 占用了 10 个字节 [25]，所以留给 Sack 选项的只有 30 个字节。每一个 Sack Block 的长度是 8 个，加上 Sack Option 头部 2 个字节，也就意味着 Tcp Sack Option 最大只能提供 3 个 Block。 Quic Ack Frame 可同时提供 256 个 Ack Block，在丢包率比较高的网络下，更多的 Sack Block 可以提升网络的恢复速度，减少重传量。 Ack Delay 时间 Tcp 的 Timestamp 选项存在一个问题：只回显发送方的时间戳，但没有计算接收端接收到 segment 到发送 Ack 该 segment 的时间。这个时间可以简称为 Ack Delay 这样就会导致 RTT 计算误差。如下图： TCP 的 RTT 计算：RTT = timestamp2 - timestamp1 Quic 的RTT 计算：RTT = timestamp2 - timestamp1 - Ack Delay 当然RTT的具体计算需要采样，参考历史数据平滑计算 SRTT = SRTT + α(RTT - SRTT) RTO = β * SRTT + α * DevRTT 基于 stream 和 connection 级别的流量控制 QUIC 的流量控制类似 HTTP2，即在 Connection 和 Stream 级别提供了两种流量控制 Connection 可以类比一条 TCP 连接，Stream 可以认为就是一条 HTTP 请求。多路复用意味着在一条 Connetion 上会同时存在多条 Stream。既需要对单个 Stream 进行控制，又需要针对所有 Stream 进行总体控制。 QUIC 实现流量控制的原理比较简单： 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。 QUIC 的流量控制和 TCP 有点区别，TCP 为了保证可靠性，窗口左边沿向右滑动时的长度取决于已经确认的字节数。如果中间出现丢包，就算接收到了更大序号的 Segment，窗口也无法超过这个序列号。 但 QUIC 不同，就算此前有些 packet 没有接收到，它的滑动只取决于接收到的最大偏移字节数 针对Stream: 可用窗口 = 最大窗口数 - 接收到的最大偏移数 针对Connection: 可用窗口 = stream1 可用窗口 + stream2 可用窗口+ …… + streamN 可用窗口 STGW 也在连接和 Stream 级别设置了不同的窗口数。可以在内存不足或者上游处理性能出现问题时，通过流量控制来限制传输速率，保障服务可用性 没有对头阻塞的多路复用 QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势，很大程度上缓解甚至消除了队头阻塞的影响。 QUIC 一个连接的多个 stream 之间没有依赖。假如 stream2 丢了一个 udp packet，不会影响 其他stream 的处理。 多路复用是 HTTP2 最强大的特性，能够将多条请求在一条 TCP 连接上同时发出去。但也恶化了 TCP 的一个问题，队头阻塞。 HTTP2 在一个 TCP 连接上同时发送 4 个 Stream。其中 Stream1 已经正确到达，并被应用层读取。但是 Stream2 的第三个 tcp segment 丢失了，TCP 为了保证数据的可靠性，需要发送端重传第 3 个 segment 才能通知应用层读取接下去的数据，虽然这个时候 Stream3 和 Stream4 的全部数据已经到达了接收端，但都被阻塞住了 不仅如此，由于 HTTP2 强制使用 TLS，还存在一个 TLS 协议层面的队头阻塞 上面两段存在歧义TODO ??? gRPC 基于 HTTP2 但是并没有TLS Record 是 TLS 协议处理的最小单位，最大不超过 16K，Nginx 默认的大小就是 16K。由于一个 record 必须经过数据一致性校验才能进行加解密，所以一个 16K 的 record，就算丢了一个字节，也会导致已接收的 15.99K 数据无法处理，因为不完整 那 QUIC 多路复用为什么能避免上述问题呢？ QUIC 最基本的传输单元是 Packet，不会超过 MTU 的大小，整个加密和认证过程都是基于 Packet 的，不会跨越多个 Packet。这样就能避免 TLS 协议存在的队头阻塞； Stream 之间相互独立，比如 Stream2 丢了一个 Pakcet，不会影响 Stream3 和 Stream4。不存在 TCP 队头阻塞 当然，并不是所有的 QUIC 数据都不会受到队头阻塞的影响，比如 QUIC 当前也是使用 Hpack 压缩算法 [10]，由于算法的限制，丢失一个头部数据时，可能遇到队头阻塞。 总体来说，QUIC 在传输大量数据时，比如视频，受到队头阻塞的影响很小。 为什么压缩之后就出现对头阻塞了？ TODO 加密认证的报文 TCP 协议头部没有经过任何加密和认证，所以在传输过程中很容易被中间网络设备篡改，注入和窃听。比如修改序列号、滑动窗口。这些行为有可能是出于性能优化，也有可能是主动攻击。 但是 QUIC 的 packet 可以说是武装到了牙齿。除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。 这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。 如下图所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。 连接迁移 一条 TCP 连接是由四元组标识的（源 IP，源端口，目的 IP，目的端口）。什么叫连接迁移呢？就是当其中任何一个元素发生变化时，这条连接依然维持着，能够保持业务逻辑不中断。当然这里面主要关注的是客户端的变化，因为客户端不可控并且网络环境经常发生变化，而服务端的 IP 和端口一般都是固定的。 比如大家使用手机在 WIFI 和 4G 移动网络切换时，客户端的 IP 肯定会发生变化，需要重新建立和服务端的 TCP 连接。 又比如大家使用公共 NAT 出口时，有些连接竞争时需要重新绑定端口，导致客户端的端口发生变化，同样需要重新建立 TCP 连接。 那 QUIC 是如何做到连接迁移呢？任何一条 QUIC 连接不再以 IP 及端口四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。 由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。 其他 此外，QUIC 还能实现前向冗余纠错，在重要的包比如握手消息发生丢失时，能够根据冗余信息还原出握手消息。 QUIC 还能实现证书压缩，减少证书传输量，针对包头进行验证等 QUIC原理 代码实现：https://github.com/lucas-clemente/quic-go 参考链接 http://www.52im.net/thread-2816-1-1.html https://docs.google.com/document/d/1F2YfdDXKpy20WVKJueEf4abn_LVZHhMUMS5gX6Pgjl4/edit# 网络编程懒人入门(十)：一泡尿的时间，快速读懂QUIC协议 让互联网更快：新一代QUIC协议在腾讯的技术实践分享 七牛云技术分享：使用QUIC协议实现实时视频直播0卡顿！ https://hungryturbo.com/HTTP3-explained/quic/为什么需要QUIC.html#回顾http-2 https://zhuanlan.zhihu.com/p/32553477","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"Quic","slug":"Quic","permalink":"http://xboom.github.io/tags/Quic/"}]},{"title":"算法之美-LRU","slug":"Algorithms To Live By/算法之美-LRU","date":"2022-01-12T16:05:08.000Z","updated":"2022-07-24T06:48:35.000Z","comments":true,"path":"2022/01/13/Algorithms To Live By/算法之美-LRU/","link":"","permalink":"http://xboom.github.io/2022/01/13/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-LRU/","excerpt":"","text":"问题背景 LRU(Least Recently Used)，最近最少使用。根据时间维度来选择将要淘汰的元素，即删除掉最长时间没被访问的元素。 实现一个LRU算法需要解决的问题： 一个存储最近使用元素的结构，能够快速定位到元素(map) 如何计算元素被最少使用了，当添加或更新元素的时候方便查询最少使用元素(双向链表) 最近最少使用存在数量上限。(容量capacity) 实现原理 双向链表按照被使用的顺序存储了这些键值对，靠近头部的键值对是最近使用的，而靠近尾部的键值对是最久未使用的。 哈希表即为普通的哈希映射（HashMap），通过缓存数据的键映射到其在双向链表中的位置 删除节点 删除的时候链表和map中的都需要删除 新增节点 注意： list节点中也要存储Map的key 技术内幕 代码路径：core/collection/cache.go 对象定义 12345678910111213lru interface &#123; add(key string) //新增 remove(key string) //删除&#125;emptyLru struct&#123;&#125;keyLru struct &#123; limit int //容量限制 evicts *list.List //双向链表 elements map[string]*list.Element //map根据key存储双向链表位置 onEvict func(key string) //自定义触发函数&#125; 删除元素 12345678910111213141516171819func (klru *keyLru) remove(key string) &#123; if elem, ok := klru.elements[key]; ok &#123; //通过map判断元素是否存在 klru.removeElement(elem) //删除节点 &#125;&#125;func (klru *keyLru) removeOldest() &#123; elem := klru.evicts.Back() //获取尾部节点 if elem != nil &#123; klru.removeElement(elem) &#125;&#125;func (klru *keyLru) removeElement(e *list.Element) &#123; klru.evicts.Remove(e) //删除双向链表节点 key := e.Value.(string) delete(klru.elements, key) //删除map中节点 klru.onEvict(key) //触发自定义逻辑&#125; 添加元素 123456789101112131415func (klru *keyLru) add(key string) &#123; if elem, ok := klru.elements[key]; ok &#123; //如果元素存在则将节点移动到链表头 klru.evicts.MoveToFront(elem) return &#125; // Add new item elem := klru.evicts.PushFront(key) //放入链表头部 klru.elements[key] = elem //存放节点 // Verify size not exceeded if klru.evicts.Len() &gt; klru.limit &#123; //如果节点数量超过限制，则删除尾部节点 klru.removeOldest() &#125;&#125; 参考链接 https://talkgo.org/t/topic/2280","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"算法之美-链表问题","slug":"Algorithms To Live By/算法之美-链表问题","date":"2022-01-11T16:05:08.000Z","updated":"2022-07-24T05:31:45.000Z","comments":true,"path":"2022/01/12/Algorithms To Live By/算法之美-链表问题/","link":"","permalink":"http://xboom.github.io/2022/01/12/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E9%93%BE%E8%A1%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"17.从尾到头打印链表 https://www.acwing.com/problem/content/18/ 题目 12345678910输入一个链表的头结点，按照 从尾到头 的顺序返回节点的值。返回的结果用数组存储。数据范围0≤ 链表长度 ≤1000。样例输入：[2, 3, 5]返回：[5, 3, 2] 解答 12345678910111213141516171819&#x2F;** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; *&#x2F;class Solution &#123;public: vector&lt;int&gt; printListReversingly(ListNode* head) &#123; vector&lt;int&gt; result; while(head) &#123; result.push_back(head-&gt;val); head &#x3D; head-&gt;next; &#125; return vector&lt;int&gt;(result.rbegin(), result.rend()); &#125;&#125;; 相关知识 - 反向迭代器 begin() 返回一个迭代器，它指向容器的第一个元素 end() 返回一个迭代器，它指向容器的最后一个元素的下一个位置 rbegin() 返回一个逆序迭代器，它指向容器的最后一个元素 rend() 返回一个逆序迭代器，它指向容器的第一个元素前面的位置 应用场景： 1234&#x2F;&#x2F;sorts v in &quot;normal&quot; ordersort(v.begin(), v.end());&#x2F;&#x2F;sorts in reversesort(v.rbegin(), v.rend()); 28.O(1)删除链表结点 题目 https://www.acwing.com/problem/content/85/ 123456789101112给定单向链表的一个节点指针，定义一个函数在O(1)时间删除该结点。假设链表一定存在，并且该节点一定不是尾节点。数据范围链表长度 [1,500]。样例输入：链表 1-&gt;4-&gt;6-&gt;8 删掉节点：第2个节点即6（头节点为第0个节点）输出：新链表 1-&gt;4-&gt;8 解答 123456789101112131415161718&#x2F;** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; *&#x2F;class Solution &#123;public: void deleteNode(ListNode* node) &#123; ListNode* nxt &#x3D; node-&gt;next; node-&gt;val &#x3D; nxt-&gt;val; node-&gt;next &#x3D; nxt-&gt;next; delete nxt; &#125;&#125;; 相关知识 12//1. auto: 在声明变量的时候可根据变量初始值的数据类型自动为该变量选择与之匹配的数据类型//2. delete TODO 29.删除链表重复节点 题目 1234567891011121314在一个排序的链表中，存在重复的节点，请删除该链表中重复的节点，重复的节点不保留。数据范围链表中节点 val 值取值范围 [0,100]。链表长度 [0,100]。样例1输入：1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5输出：1-&gt;2-&gt;5样例2输入：1-&gt;1-&gt;1-&gt;2-&gt;3输出：2-&gt;3 注意重复的节点也不保留 解答 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplication(ListNode* head) &#123; ListNode * dummy = new ListNode(-1); //定义一个头指针(防止head为空) dummy-&gt;next = head; ListNode * pre = dummy; //双指针前指针 while(pre-&gt;next) &#123; ListNode * nxt = pre-&gt;next; //双指针尾指针 //一直循环，直到头指针与尾指针值不一致 //注意这里pre是从dummy开头的，而这题是要删除所有重复的节点，所以有时候pre—&gt;next需要一起删除 //所以这里每次都是比较 pre-&gt;next 与 nxt while(nxt &amp;&amp; pre-&gt;next-&gt;val == nxt-&gt;val) nxt = nxt-&gt;next; //经过上述流转出现 pre-&gt;next-&gt;val != nxt-&gt;val //1. 如果两者相邻，那么pre向后移动一个节点 //2. 如果两者不相邻，那么pre与nxt之间都是重复节点 if (pre-&gt;next-&gt;next == nxt) pre = pre-&gt;next; else pre-&gt;next = nxt; &#125; return dummy-&gt;next; &#125;&#125;; 重点：是每次使用 pre-&gt;next 与 nxt进行比较，那样即使删除也是删除pre-&gt;next 与 nxt之间的结点 33.链表中倒数第N个节点 题目 12345678910111213输入一个链表，输出该链表中倒数第 k 个结点。注意：k &gt;= 1;如果 k 大于链表长度，则返回 NULL;数据范围链表长度 [0,30]。样例输入：链表：1-&gt;2-&gt;3-&gt;4-&gt;5 ，k=2输出：4 解答 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* findKthToTail(ListNode* pListHead, int k) &#123; //使用快慢指针 ListNode *fast = pListHead; ListNode *slow = pListHead; //快指针先走k步，如果fast为空或者为空都没有走到k个，说明链表数目小于k for (int i = 0; i &lt; k; i++) &#123; if (fast) fast = fast-&gt;next; //这里k-1 是因为 i = 0 即为第一个节点 fast 需要走 i = k - 1 步 if (!fast &amp;&amp; i &lt; k - 1) return NULL; &#125; while (fast) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return slow; &#125;&#125;; 重点：快慢指针 34.链表中环的入口节点 题目 https://www.acwing.com/problem/content/86/ 12345678910111213141516给定一个链表，若其中包含环，则输出环的入口节点。若其中不包含环，则输出null。数据范围节点 val 值取值范围 [1,1000]。链表长度 [0,500]。样例如下图给定如上所示的链表：[1, 2, 3, 4, 5, 6]2注意，这里的2表示编号是2的节点，节点编号从0开始。所以编号是2的节点就是val等于3的节点。则输出环的入口节点3. 证明： 如上图所示，a 是起点，b 是环的入口，c 是两个指针的第一次相遇点，ab 距离 x，bc 距离是 y 存在快慢指针 first 与 second，其中second的速度是first的2倍 想法1： 当 first 走到 b 时， second 已经从 b 开始在环上走了 x 步，可能多余n圈，距离 b 还差 y 步 所以 second 走的路程 2x + y = x + n 圈 ==&gt; x + y = n 圈，那么从 c 走 x 步 也能到达b 想法2： 用 z 表示从 c 点顺时针走到 b 的距离。则第一次相遇时 second 所走的距离是 x+(y+z)∗n+y，其中n表示圈数，所以 12x+(y+z)∗n+y = 2(x+y) //second是 first路程的两倍x = (n-1) * (y+z) + z 所以其实 x == z ，相遇之后节点距离入环扣与链表头到入环扣距离相等 解答 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *entryNodeOfLoop(ListNode *head) &#123; if (!head || !head-&gt;next) return NULL; //初始节点判断 //快慢指针 ListNode * first = head; ListNode * second = head; while (first &amp;&amp; second)&#123; //外循环防止节点为空，找到第一个相遇的地方 first = first-&gt;next; second = second-&gt;next; if(second) second = second-&gt;next; else return NULL; if (first == second) &#123; //当相遇则再次相遇说走的路程就是换入口处 first = head; while (first != second) &#123; first = first-&gt;next; second = second-&gt;next; &#125; return first; &#125; &#125; return NULL; &#125;&#125;; 重点是：从第一次相遇的地方走x步就能到达入环扣 35.反转链表 题目 123456789101112定义一个函数，输入一个链表的头结点，反转该链表并输出反转后链表的头结点。思考题：请同时实现迭代版本和递归版本。数据范围链表长度 [0,30]。样例输入:1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出:5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 解答 非递归 12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; ListNode * second = head; ListNode * first = nullptr; //头部空指针 while(second) &#123; ListNode * pre = second-&gt;next; second-&gt;next = first; first = second; second = pre; &#125; return first; &#125;&#125;; 递归 12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if(!head || !head-&gt;next ) return head; //1 - 2 - 3 - 4 ListNode *tail = reverseList(head-&gt;next); //反转head-&gt;next 链表, 原链表的尾节点tail //为什么不用记录前一个节点进行转换， //因为 递归中 head-&gt;next 已经经过了反转，后面只需要反转 head 与 head-&gt;next的关系就可以了 head-&gt;next-&gt;next = head; //将 这里直接使用 head-&gt;next-&gt;next 不用再声明变量 head-&gt;next = nullptr; // nullptr &lt;- 1 &lt;- 2 return tail; //其实这里一直返回的是原链表的尾节点 &#125;&#125;; 36.合并两个排序的链表 题目 https://www.acwing.com/problem/content/34/ 123456789输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。数据范围链表长度 [0,500]。样例输入：1-&gt;3-&gt;5 , 2-&gt;4-&gt;5输出：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;5 解答 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* merge(ListNode* l1, ListNode* l2) &#123; ListNode * dummy = new ListNode(-1); //新建头部的保护结点dumm ListNode * cur = dummy; //当前指针 while(l1 &amp;&amp; l2) &#123; if(l1-&gt;val &gt; l2-&gt;val) &#123; cur-&gt;next = l2; l2 = l2-&gt;next; &#125;else&#123; cur-&gt;next = l1; l1 = l1-&gt;next; &#125; cur = cur-&gt;next; &#125; //二元判断，防止两个链表数目不一致 cur-&gt;next = (l1 != nullptr ? l1 : l2); return dummy-&gt;next; &#125;&#125;; 两个链表各遍历一次，所以时间复杂度为O(n) 48. 复杂链表的复刻 题目 https://www.acwing.com/problem/content/89/ 123456789请实现一个函数可以复制一个复杂链表。在复杂链表中，每个结点除了有一个指针指向下一个结点外，还有一个额外的指针指向链表中的任意结点或者null。注意：函数结束后原链表要与输入时保持一致。数据范围链表长度 [0,500]。 解答 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition for singly-linked list with a random pointer. * struct ListNode &#123; * int val; * ListNode *next, *random; * ListNode(int x) : val(x), next(NULL), random(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *copyRandomList(ListNode *head) &#123; //开始遍历添加cur的复制点np ListNode * p = head; while(p) &#123; //1 - 2 --&gt; 1 - 1 - 2 复制节点并添加到节点后面 auto * np = new ListNode(p-&gt;val); auto * nxt = p-&gt;next; p-&gt;next = np; np-&gt;next = nxt; p = nxt; &#125; //如果存在 cur 有 random 则 p-&gt;next 是被复制的节点 p = head; while (p) &#123; if(p-&gt;random) //p 是旧节点,则 p-&gt;next 是新节点 p-&gt;next-&gt;random = p-&gt;random-&gt;next; //这里也是随机指定，和原来的不一定相等 p = p-&gt;next-&gt;next; &#125; ListNode * dummy = new ListNode(-1); //保护头节点 ListNode * cur = dummy; p = head; while(p) &#123; //1 - 1 - 2 - 2 - 3 - 3 cur-&gt;next = p-&gt;next; //新链表 cur = cur-&gt;next; p-&gt;next = p-&gt;next-&gt;next; //恢复旧指针 p = p-&gt;next; &#125; return dummy-&gt;next; &#125;&#125;; 重点是：p-&gt;next-&gt;random = p-&gt;random-&gt;next; //如果有random，则它是原节点。将新节点指一个random 49.二叉搜索树与双向链表 https://www.acwing.com/problem/content/87/ 题目 1234567891011输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。注意：需要返回双向链表最左侧的节点。例如，输入上图中左边的二叉搜索树，则输出右边的排序双向链表。数据范围树中节点数量 [0,500]。 解答 在中序递归遍历的基础，用一个pre指针保存中序遍历的前一个结点。遍历顺序就是双线链表的建立顺序； 每一个结点访问时它的左子树肯定被访问过了，所以放心大胆的改它的left指针，不怕树断掉； 同理，pre指向的结点保存的数肯定小于当前结点，所以其左右子树肯定都访问过了，所以其right指针也可以直接改。 最后需要一直向左找到双向链表的头结点。 1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode * pre = NULL; //pre记录当前节点指针 TreeNode* convert(TreeNode* root) &#123; dfs(root); while(root &amp;&amp; root-&gt;left) root = root-&gt;left; //遍历找到第一个节点 return root; &#125; //深度优先遍历 void dfs(TreeNode* root) &#123; if(!root) return; dfs(root-&gt;left); root-&gt;left = pre; //左节点就是前节点 if(pre) pre-&gt;right = root; //前节点的后继节点就是自己 pre = root; dfs(root-&gt;right); &#125;&#125;; 重点是：利用深度优先遍历 + pre节点记录 66. 两个链表的第一个公共结点 题目 https://www.acwing.com/problem/content/62/ 12345678910111213141516输入两个链表，找出它们的第一个公共结点。当不存在公共节点时，返回空节点。数据范围链表长度 [1,2000]。样例给出两个链表如下所示：A： a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3输出第一个公共节点c1 不要用两层循环暴力破解！！！ 解答 不同部分为a和b，公共部分为c；a + c + b = b + c + a;让两个一起走，a走到头就转向b， b走到头转向a，则在公共部分相遇 12345678910111213141516171819202122232425/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *findFirstCommonNode(ListNode *headA, ListNode *headB) &#123; ListNode * A = headA; ListNode * B = headB; while(A != B) &#123; //A遍历完开始走到B if(A) A = A-&gt;next; else A = headB; //B遍历完开始走到A if(B) B = B-&gt;next; else B = headA; &#125; return A; &#125;&#125;; 826. 单链表 题目 1234567891011121314151617181920212223242526272829303132333435363738实现一个单链表，链表初始为空，支持三种操作：向链表头插入一个数；删除第 k 个插入的数后面的数；在第 k 个插入的数后插入一个数。现在要对该链表进行 M 次操作，进行完所有操作后，从头到尾输出整个链表。注意:题目中第 k 个插入的数并不是指当前链表的第 k 个数。例如操作过程中一共插入了 n 个数，则按照插入的时间顺序，这 n 个数依次为：第 1 个插入的数，第 2 个插入的数，…第 n 个插入的数。输入格式第一行包含整数 M，表示操作次数。接下来 M 行，每行包含一个操作命令，操作命令可能为以下几种：H x，表示向链表头插入一个数 x。D k，表示删除第 k 个插入的数后面的数（当 k 为 0 时，表示删除头结点）。I k x，表示在第 k 个插入的数后面插入一个数 x（此操作中 k 均大于 0）。输出格式共一行，将整个链表从头到尾输出。数据范围1≤M≤100000所有操作保证合法。输入样例：10H 9I 1 1D 1D 0H 6I 3 6I 4 5I 4 5I 3 4D 6输出样例：6 4 6 5 解答 静态链表 1451. 单链表快速排序 题目 12345678910111213给定一个单链表，请使用快速排序算法对其排序。要求：期望平均时间复杂度为 O(nlogn)，期望额外空间复杂度为 O(logn)。思考题： 如果只能改变链表结构，不能修改每个节点的val值该如何做呢？数据范围链表中的所有数大小均在 int 范围内，链表长度在 [0,10000]。输入样例：[5, 3, 2]输出样例：[2, 3, 5] 解答 首先复习以下快速排序 123456789101112131415161718192021222324252627282930313233343536//用于默写test中国暖的算法#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e6 + 1;int nums[N]; //用于存储数据int n;void quick_sort(int nums[], int l, int r)&#123; if(l &gt;= r) return; int mid = nums[l + r &gt;&gt; 1]; int i = l - 1; int j = r + 1; while(i &lt; j) &#123; do i++; while (nums[i] &lt; mid); do j--; while (nums[j] &gt; mid); if (i &lt; j) swap(nums[i], nums[j]); //表示num[i] 在左边但是数据却比 num[j] 大 &#125; quick_sort(nums, l, i); quick_sort(nums, i + 1, r); return;&#125;int main()&#123; scanf(\"%d\", &amp;n); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;nums[i]); quick_sort(nums, 0, n - 1); for(int i = 0; i &lt; n; i++) printf(\"%d \", nums[i]); return 0;&#125; 则链表排序为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e5 + 1;int n;struct ListNode&#123; int val; ListNode *next; ListNode(int x) : val(x), next(NULL) &#123;&#125;&#125;;void quickSort(ListNode *head, ListNode *tail)&#123; if (head != tail) &#123; int key = head-&gt;val; ListNode *p = head, *q = p-&gt;next; while (q != tail) &#123; if (q-&gt;val &lt; key) &#123; p = p-&gt;next; swap(p-&gt;val, q-&gt;val); &#125; q = q-&gt;next; &#125; if (p != head) swap(head-&gt;val, p-&gt;val); quickSort(head, p); quickSort(p-&gt;next, tail); &#125;&#125;ListNode *quickSortList(ListNode *head)&#123; if (!head) return head; quickSort(head, NULL); return head;&#125;int main()&#123; ListNode * head = new ListNode(-1); ListNode * cur = head; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; int val; scanf(\"%d\", &amp;val); ListNode * node = new ListNode(val); cur-&gt;next = node; cur = cur-&gt;next; &#125; ListNode * result = quickSortList(head-&gt;next); while (result) &#123; printf(\"%d\", result-&gt;val); result = result-&gt;next; &#125; return 0;&#125; 1560. 反转链表 题目 https://www.acwing.com/problem/content/1562/ 1234567891011121314151617181920212223242526272829303132333435363738394041给定一个常数 K 和一个单链表 L，请你在单链表上每 K 个元素做一次反转，并输出反转完成后的链表。如果链表最后一部分不足 K 个元素，则最后一部分不翻转。例如，假设 L 为 1→2→3→4→5→6，如果 K=3，则你应该输出 3→2→1→6→5→4；如果 K=4，则你应该输出 4→3→2→1→5→6。### 补充1、本题中可能包含不在链表中的节点，这些节点无需考虑。### 输入格式第一行包含头节点地址，总节点数量 N 以及常数 K。节点地址用一个 5 位非负整数表示（可能有前导 0），NULL 用 −1 表示。接下来 N 行，每行描述一个节点的信息，格式如下：`Address Data Next`其中 Address 是节点地址，Data 是一个整数，Next 是下一个节点的地址。### 输出格式将重新排好序的链表，从头节点点开始，依次输出每个节点的信息，格式与输入相同。### 数据范围1≤N≤105,1≤K≤N### 输入样例：00100 6 400000 4 9999900100 1 1230968237 6 -133218 3 0000099999 5 6823712309 2 33218### 输出样例：00000 4 3321833218 3 1230912309 2 0010000100 1 9999999999 5 6823768237 6 -1 参考链接 https://www.acwing.com/problem/","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"MQ-Kafka4-日志存储","slug":"MQ/MQ-Kafka4-日志存储","date":"2022-01-06T15:35:06.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka4-日志存储/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka4-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/","excerpt":"","text":"文件目录 为了防止Log过大，Kafka映日了日志分段(LogSegment)的概念，将Log 切分为多个LogSegment。 Log在物理上只已文件夹的形式存储，而每个LogSegment对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件(比如以&quot;.txnindex&quot;为后缀的事务索引文件) Log对应的是一个命名形式为 &lt;topic&gt;-&lt;partition&gt;的文件夹，假设有一个名为 &quot;topic-log&quot;的主题，此主题具有4个分区，那么实际物理存储上表现为：“topic-log-0”、“topic-log-1”、“topic-log-2”、“topic-log-3” 向Log 中追加消息时是顺序写入的，只有最后一个LogSegment 才能执行写入操作，在此之前所有的LogSegment 都不能写入数据 为了方便描述，将最后一个LogSegment 称为activeSegment ，表示当前活跃的日志分段。随着消息的不断写入，当 activeSegment 满足一定的条件时，就创建新的activeSegment，并将消息追加到新的activeSegment 为了便于消息的检索，每个LogSegment 中的日志文件（以“ .log ”为文件后缀）都有对应的两个索引文件： 偏移量索引文件，以“ .index ”为文件后缀 时间戳索引文件，以“ .timeindex ”为文件后缀 每个LogSegment 都有一个基准偏移量baseOffset，用来表示当前LogSegment中第一条消息的offset 。偏移量是一个64 位的长整型数，日志文件和两个索引文件都是根据基准偏移量（ baseOffset ）命名的，名称固定为20 位数字，没有达到的位数则用0 填充。比如第一个LogSegment 的基准偏移量为0 ，对应的日志文件为00000000000000000000.log 第2 个 LogSegment 对应的基准位移是133 ，也说明了该LogSegment 中的第一条消息的偏移量为133 ＇同时可以反映出第一个LogSegment 中共有133 条消息(偏移量从0 至132的消息） 消费者提交的位移是保存在Kafka 内部的主题consumer offsets中的，初始情况下这个主题并不存在，当第一次有消费者消费消息时会自动创建这个主题 Kafka文件目录布局 每一个根目录都会包含最基本的4个检查点文件（ xxx-checkpoint ）和meta.propties 文件。在创建主题的时候，如果当前broker中不止配置了一个根目录，那么会挑选分区数最少的那个根目录来完成本次创建任务 日志格式 消息集称为 Record Batch，其内部可包含一条或多条消息 在消息压缩的情形下， Record Batch Header 部分（参见图5-7 左部， 从first offset 到 records count 字段）是不被压缩的，而被压缩的是records 字段中的所有内容。生产者客户端中的ProducerBatch 对应这里的RecordBatch,而ProducerRecord 对应这里的Record Record Record包含： length ：消息总长度。 attributes ： 弃用，但还是在消息格式中占据1B 的大小， 以备未来的格式扩展。 timestamp delta ： 时间戳增量。通常一个time stamp 需要占用8 个字节，如果像这里一样保存与RecordBatch 的起始时间戳的差值，则可以进一步节省占用的字节数。 offset delta ： 位移增量。保存与RecordBatc h 起始位移的差值，可以节省占用的字节数 headers ：这个字段用来支持应用级别的扩展，包含key和value ，一个Record 里面可以包含0 至多个Header RecodeBatch包含： first offset ：表示当前RecordBatch 的起始位移。 length ：计算从partition leader epoeh 字段开始到末尾的长度。 partition leader epoeh ：分区leader 纪元，可以看作分区leader 的版本号或更新次数 magic ：消息格式的版本号，对v2 版本而言， magie 等于2 。 attributes ：消息属性(2B) 低 3 位表示压缩格式， 第4 位表示时间戳类型； 第5 位表示此RecordBatch 是否处于事务中，0 表示非事务， l 表示事务。 第6 位表示是否是控制消息(ControlBatch)。0 表示非控制消息，而 1 表示是控制消息，控制消息用来支持事务功能。 last offset delta: RecordBatch 中最后一个Record 的offset 与自rst offset 的差值。主要被broker 用来确保RecordBatch 中Record 组装的正确性。 first timestamp: RecordBatch 中第一条Record 的时间戳。 max timestamp: RecordBatch 中最大的时间戳， 一般情况下是指最后一个Record的时间戳，和last offset delta 的作用一样，用来确保消息组装的正确性。 produeer id: PID ，用来支持幂等和事务 日志索引 每个日志分段文件对应了两个索引文件，主要用来提高查找消息的效率。 偏移量索引文件用来建立消息偏移量（ offset ）到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置； 时间戳索引文件则根据指定的时间戳（ timestamp ）来查找对应的偏移量信息 Kafka 中的索引文件以 稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引页。每当写入一定量（由broker 端参数log.index.interval.bytes 指定，默认值为4096 ，即4KB ）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应地可以增加或缩小索引项的密度。 稀疏索引通过 MappedByteBuffer 将索引文件映射到内存中，以加快索引的查询速度。 偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。 时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。 当日志分段达到一定条件则创建新的日志分段，一定条件包括： 当前日志分段文件的大小超过了broker 端参数log.segment.bytes 配置的值，默认值1GB 当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.hours参数配置的值。如果同时配置了log.roll.ms 和log.roll.hours 参数，那么log.roll.ms 的优先级高。默认情况下，只配置了log.roll.hours 参数，其值为168，即7天。 偏移量索引文件或时间戳索引文件的大小达到broker 端参数 log.index.size.max.bytes，默认10M 追加的消息的偏移量与当前日志分段的偏移量之间的差值大于Integer.MAX_VALUE,即offset - baseOffset &gt; Integer.MAX_VALUE 对非当前活跃的日志分段而言，其对应的索引文件内容己经固定而不需要再写入索引项，所以会被设定为只读,而对当前活跃的日志分段C activeSegment ）而言，索引文件还会追加更多的索引项，所以被设定为可读写 在索引文件切分的时候， Kafka 会关闭当前正在写入的索引文件并置为只读模式，同时以可读写的模式创建新的索引文件 Kafka 在创建索引文件的时候会为其预分配log.index.size.max.bytes 大小的空间，注意这一点与日志分段文件不同，只有当索引文件进行切分的时候， Kafka 才会把该索引文件裁剪到实际的数据大小。也就是说，与当前活跃的日志分段对应的索引文件的大小固定为log.index.size.max.bytes，而其余日志分段对应的索引文件的大小为实际的占用空间。 偏移量索引 偏移量索引项的格式如下图，每个索引项占用8 个字节，分为两个部分。 relativeOffset：相对偏移量(4B)，表示消息相对于baseOffset 的偏移量，当前索引文件的文件名即为baseOffset 的值。 position：物理地址(4B)，也就是消息在日志分段文件中对应的物理位置 消息的偏移量(offset)占用8 个字节，也称绝对偏移量。索引项中没有直接使用绝对偏移量而改为只占用4 个字节的相对偏移量CrelativeOffset =offset - baseOffset，这样可以减小索引文件占用的空间。 举个例子， 一个日志分段的baseOffset 为32 ，则文件名是00000000000000000032.log，offset 为35 的消息在索引文件中的relativeOffset 的值为35 - 32=3 如果我们要查找偏移量为23 的消息，那么应该怎么做呢？ 首先通过二分法在偏移量索引文件中找到不大于23 的最大索引项，即［ 22 , 656 ］， 然后从日志分段文件中的物理位置656 开始顺序查找偏移量为23 的消息。 那么用户又是如何查找日志分段的呢？ 并不是顺序查找，而是使用跳跃表的结构。Kafka的每个日志对象中使用了 ConcurrentSkipListMap来保存各个日志分段，每个日志分段的baseOffset 作为key,这样可以根据指定偏移量快速定位到消息所在的日志分段 Kafka 强制要求索引文件大小必须是索引项大小的整数倍，对偏移量索引文件而言，必须为8 的整数倍 时间戳索引 每个索引项占用12 个字节，分为两个部分 timestamp ： 当前日志分段最大的时间戳。 relativeOffset ：时间戳所对应的消息的相对偏移量 时间戳索引文件中包含若干时间戳索引项， 每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的timestamp ，否则不予追加 与偏移量索引文件相似，时间戳索引文件大小必须是索引项大小(12B)的整数倍 会在偏移量索引文件和时间戳索引文件中分别增加一个偏移量索引项和时间戳索引项。两个文件增加索引项的操作是同时进行的，但并不意味着偏移量索引中的relativeOffset 和时间戳索引项中的relativeOffset 是同一个值 如果一个失败了怎么办？ 为什么会出现不是同一个值的情况 如果要查找指定时间戳 targetTimeStamp = 1526384718288 开始的消息，首先是找到不小于 指定时间戳的日志分段。这里就无法使用跳跃表来快速定位到相应的日志分段了， 需要分以下 几个步骤来完成。 步骤1 ： 将 targetTimeStamp 和每个日志分段中的最大时间戳 largestTimeStamp 逐一对比，直到找到不小于targetTimeStamp 的 largestTimeStamp 所对应的日志分段。日志分段中的 largestTimeStamp 的计算是先查询该日志分段所对应的时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于0，则取其值，否则取该日志分段的最近修改时间。 步骤2 ： 找到相应的日志分段之后，在时间戳索引文件中使用二分查找算法查找到不大于targetTimeStamp 的最大索引项，即［152638478283, 28］，如此便找到了一个相对偏移量28 。 步骤3 ： 在偏移量索引文件中使用二分算法查找到不大于28 的最大索引项，即［26, 838 ] 步骤4 ：从步骤1中找到日志分段文件中的838 的物理位置开始查找不小于targetTimeStamp的消息 日志清理 Kafka 提供了两种日志清理策略 日志删除(Log Retention)：按照一定的保留策略直接删除不符合条件的日志分段 日志压缩(Log Compaction)：针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本 通过broker端参数 log.cleanup.policy 来设置日志清理策略 “delete”：即采用日志删除的清理策略 “compact”: 即采用日志压缩的清理策略 “delete,compact”：同时迟滞日志删除和日志压缩两种策略 日志清理 在Kafka的日志管理器中有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过 broker 端参数 log.retention.check.interval.ms配置，默认5 分钟。当前日志分段的保留策略有3 种： 基于时间的保留策略 基于日志大小的保留策略 基于日志起始偏移量的保留策略 基于时间 日志删除任务 检查日志文件中是否有保留时间超过设定的阀值(retentionMs)来寻找可删除的日志分段文件集合(deletableSegments)，retentionMs 可以通过broker端参数log.retention.hours、log.retention.minutes 和log.retention.ms 来配置，默认7 天 查找过期的日志分段文件，并不是简单地根据日志分段的最近修改时间lastModifiedTime来计算的， 而是通过日志分段对应的时间戳索引文件，找出最后一条索引项(如果不大于0，则取最近修改时间lastModifiedTime) 如果所有的日志分段都己过期， 但该日志文件中还要有一个日志分段用于接收消息的写入，即必须要保证有一个活跃的日志分段acti veSegment ，在此种情况下，会先切分出一个新的日志分段作为activeSegment ， 然后执行删除操作 删除日志分段步骤 首先会从Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。 然后将日志分段所对应的所有文件添加上 .deleted 的后缀（当然也包括对应的索引文件） 。 最后交由一个以 delete-file 命名的延迟任务来删除这些以 .deleted为后缀的文件，这个任务的延迟执行时间可以通过 file.delete.delay.ms 参数默认1 分钟 基于日志大小 日志删除任务会检查当前日志的大小是否超过设定的阔值(retentionSize)来寻找可删除的日志分段的文件集合(deletableSegments)，retentionSize 可以通过broker 端参数log.retention.bytes 来配置，默认值为 -1，表示无穷大 注意 log.retention.bytes 配置的是 Log 中所有日志文件的总大小，而不是单个日志分段（确切地说应该为.log 日志文件）的大小。单个日志分段的大小由broker 端参数 log.segment.bytes 来限制，默认值为1GB 基于日志大小的保留策略与基于时间的保留策略类似 基于日志起始偏移量 基于日志起始偏移量的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量 baseOffset 是否小于等于logStartOffset，若是，则可以删除此日志分段。 如图，假设 logStartOffset 等于25，日志分段 1 的起始偏移量为0，日志分段2 的起始偏移量为11，日志分 段3 的起始偏移量为23 ，通过如下动作收集可删除的日志分段的文件集合deletableSegments : 从头开始遍历每个日志分段，日志分段 1 的下一个日志分段的起始偏移量为11 ，小于 logStartOffset 的大小，将日志分段 1 加入deletableSegments 。 日志分段2 的下一个日志偏移量的起始偏移量为23 ，也小于logStartOffset 的大小，将日志分段2 页加入deletableSegments 日志分段3 的下一个日志偏移量在 logStartOffset 的右侧，故从日志分段3 开始的所有日志分段都不会加deletableSegments logStartOffset是怎么来的？ 一般情况下：日志文件的起始偏移量 logStartOffset 等于第一个日志分段的baseOffset，但可以通过脚本或请求进行修改 日志压缩 如果只关心 key 对应的最新 value 值，则可以开启Kafka 的日志清理功能，Kafka 会定期将相同 key 的消息进行合井，只保留最新的value值 注意区分日志压缩与消息压缩 Log Compaction 执行前后，日志分段中的每条消息的偏移量和写入时的偏移量保持一致。Log Compaction 会生成新的日志分段文件，日志分段中每条消息的物理位置会重新按照新文件来组织 拉取状态是客户端保存的，这个时候如果进行了日志压缩，是否导致乱序？ 如何对日志文件中消息的Key进行筛选操作？ 每个日志清理线程都会使用 SkimpyOffsetMap的对象来构建key与offset的映射关系的哈希表 日志清理需要遍历两次日志文件 第一次：遍历把每个key的哈希值和最后出现的offset都保存在SkimpyOffsetMap中 第二次：检查每个消息的偏移量在Map中是否一样，否则就清理 墓碑消息是什么？ 执行日志压缩之后，日志分段的大小会比原来小，如何防止出现大量小文件？ 清理过程中并不对单个的日志分段进行单独清理，而是将日志文件中 offset 从 0 - firstUncleanableOffset的所有日志进行分组。每组中日志分段占用空间大小之和不超过 segmentSize( log.segment.bytes)，清理后生成一个新的日志分段 磁盘存储 在印象中，磁盘的速率要远低于内存，其实这要看我们怎么样使用磁盘。顺序写盘的速度不仅比随机写盘的速度快，而且也比随机写内存的速度快 Kafka在设计时候采用了文件追击的方式来写入消息，只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作 日志压缩是不是破坏了顺序写盘？ 页缓存 页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘I/O 的操作。其实是将磁盘中的数据缓存到内存中，将对磁盘的访问变为内存的访问 当进程准备读取磁盘上的文件时，操作系统会先查看待读取的数据所在的页(page)是否在页缓存(pagecache)中 如果存在(命中)则直接返回数据 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。 同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在， 则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性 Linux 操作系统中的vm.dirty background ratio 参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页， 一般设置为小于10 的值即可 Kafka 中大量使用了页缓存，这是Kafka 实现高吞吐的重要因素之一。 Kafka 中也提供了同步刷盘及间断性强制刷盘(fsync)的功能，这些功能可通过log.flush.interval.messages 、log.flush.int erval.ms 等参数来控制，但不建议使用，会严重影响性能，消息的可靠性应该由多副本机制保证 另外Linux会使用磁盘的一部分做为swap分区，非活跃进行调入swap分区，把进程空出来让活跃的进程使用，Kafka也应该尽量避免使用 vm.swappiness vm.swappiness 100 表示积极使用 0 表示任何时候都不要发生交换 磁盘I/O流程 磁盘IO四种场景如下： 用户调用IO操作接口，数据流为：应用程序buffer -&gt; C 库标准IObuffer -&gt; 文件系统页缓存 -&gt; 通过具体文件系统到磁盘 用户调用文件I/O，数据流为：应用程序buffer -&gt; 文件系统页缓存 -&gt; 通过具体文件系统到磁盘 用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘 用户使用类似dd工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘。 零拷贝 参考链接 《深入理解Kafka核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka3-主题与分区","slug":"MQ/MQ-Kafka3-主题与分区","date":"2022-01-06T12:43:02.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka3-主题与分区/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka3-%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA/","excerpt":"","text":"主题作为消息的归类，可以再细分为一个或多个分区，分区则可看作对消息的二次归类。 分区的划分不仅为Kafka 提供了可伸缩性、水平扩展的功能，还通过多副本机制提高数据可靠性 主题与分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段(LogSegment)，每个日志分段还可以细分为索引文件、日志存储文件和快照文件等 分区的管理 优先副本的选举 分区使用多副本机制来提升可靠性，但只有leader 副本对外提供读写服务，而follower 副本只负责在内部进行消息的同步 对同一个分区而言， 同一个broker 节点中不可能出现它的多个副本， 即Kafka 集群的一个broker 中最多只能有它的一个副本， 可以将leader 副本所在的broker 节点叫作分区的leader 节点，而follower副本所在的broker 节点叫作分区的follower 节点。 在创建主题的时候，该主题的分区及副本会尽可能均匀地分布到Kafka 集群的各个broker节点上，对应的leader 副本的分配也比较均匀。 将leader 副本所在的broker 节点叫作分区的leader 节点，而follower副本所在的broker 节点叫作分区的follower 节点。如果Kafka中leader副本过于集中在同一个节点上，集群会出现负载失衡的情况。 为此，Kafka 引入了优先副本(preferred replica) : 指在AR 集合列表中的第一个副本。也可以称之为preferred leader 。Kafka 要确保所有主题的优先副本在Kafka 集群中均匀分布，这样就保证了所有分区的leader 均衡分布。 所谓的优先副本的选举是指通过一定的方式促使优先副本选举为leader 副本，以此来促进集群的负载均衡， 这一行为也可以称为“分区平衡” 分区平衡并不意味着Kafka 集群的负载均衡！！！ 因为还要考虑集群中的分区分配是否均衡。每个分区的leader 副本的负载也是各不相同的 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的broker节点，计算每个broker 节点的分区不平衡率（ broker 中的不平衡率＝非优先副本的leader 个数／分区总数）是否超过leader .mbalance.per.broker.percentage 参数配置的比值，默认值为10% ，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。执行周期由参数leader.Imbalance.check .interval . seconds 控制，默认值为3 00 秒 分区重分配 当集群中的一个节点突然若机下线时， 如果节点上的分区是单副本的，这些分区就变得不可用了，在节点恢复前，相应的数据也就处于丢失状态； 如果节点上的分区是多副本的，位于这个节点上的 leader 副本的角色会转交到集群的其他follower副本中。 总而言之，这个节点上的分区副本都已经处于功能失效的状态， Kafka 并不会将这些失效的分区副本自动地迁移到集群中剩余的可用broker 节点上，如果放任不管，则不仅会影响整个集群的均衡负载，还会影响整体服务的可用性和可靠性 分区重分配的基本原理是 第一步：添加新副本(增加副本因子) 第二步：新副本将从分区的 leader 副本那里复制所有的数据。根据分区的大小不同，复制过程可能需要花一些时间，因为数据是通过网络复制到新副本上的。 第三步：在复制完成之后，控制器将旧副本从副本清单里移除(恢复为原先的副本因子数)注意在重分配的过程中要确保有足够的空间。 分区重分配的量如果太大必然会严重影响整体的性能，对副本间的复制流量加以限制来保证重分配期间整体服务不会受太大的影响，复制限流有两种实现方式： kafka-config. sh 脚本和kafka-reassign-partitions .sh 脚本 参考链接 《深入理解Kafka 核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka2-生产者与消费者","slug":"MQ/MQ-Kafka2-生产者与消费者","date":"2022-01-05T17:03:04.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka2-生产者与消费者/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka2-%E7%94%9F%E4%BA%A7%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E8%80%85/","excerpt":"","text":"生产者 如下图是生产者客户端的整体架构 整个生产者客户端由两个线程协调运行，这两个线程分别为 主线程：由 Producer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（ RecordAccumulator ，也称为消息收集器〉中 Sender 线程（发送线程）：负责从RecordAccumulator 中获取消息并将其发送到Kafka 中 RecordAccumulator 主要用来缓存消息以便批量发送，进而减少网络传输的资源消耗。RecordAccumulator 缓存的大小可通过客户端参数 buffer.memory 配置，默认32MB。 当生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer 的send()方法调用要么被阻塞，要么抛出异常，这个取决于参数 max.block.ms 的配置，此参数的默认值60 秒 重要的参数 acks(字符串类型)：用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的 acks = 1(默认)：生产者发送消息之后，只要分区的leader 副本成功写入消息，那么它就会收到来自服务端的成功响应。 如果消息写入leader 副本并返回成功响应给生产者，且在被其他fo llower 副本拉取之前leader 副本崩溃，那么此时消息还是会丢失，因为新选举的leader 副本中并没有这条对应的消息。 Acks = 0：生产者发送消息之后不需要等待任何服务端的响应 Acks = -1：生产者在消息发送之后，需要等待ISR 中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应 max.request.size：限制生产者客户端能发送的消息的最大值，默认值为1048576B ，即1MB retries 和retry. backoff.ms：retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作，retry. backoff.ms 则用来设置两次重试之间的间隔 compression.type：压缩方式 connections.max.idle.ms：指定在多久之后关闭闲置的连接，默认值是540000 ( ms ） ，即9 分钟 linger.ms：指定生产者发送ProducerBatch 之前等待更多消息（ ProducerRecord ）加入Producer Batch 的时间，默认值为0，生产者客户端会在ProducerBatch 被填满或等待时间超过 linger.ms 值时发迭出去 recvive.buffer.bytes：设置Socket接收消息的缓冲区(SO_RECBUF)的大小，默认值32KB send.buffer.bytes：设置Socket发送消息的缓冲区(SO_RECBUF)的大小，默认值128KB request.time.ms：配置Producer等待请求响应的最长时间，默认值为3000(ms) 消费者 消费者(Consumer)负责订阅Kafka 中的主题(Topic)，并从订阅的主题上拉取消息，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 主题中共有4 个分区(Partition) : PO 、Pl 、P2 、P3 。有两个消费组A和B 都订阅了这个主题，消费组A 中有4 个消费者(CO 、Cl 、C2 和C3)，消费组B 中有2个消费者(C4 和CS ） 。按照Kafka 默认的规则，最后的分配结果是消费组A 中的每一个消费者分配到l1个分区，消费组B 中的每一个消费者分配到2 个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。即每一个分区只能被一个消费组中的一个消费者所消费 假设目前某消费组内只有一个消费者C0 ，订阅了一个主题，这个主题包含7 个分区： PO 、Pl 、P2 、P3 、P4 、 PS 、P6 也就是说，这个消费者C0 订阅了7 个分区。消费组内又加入了一个新的消费者C1，按照既定的逻辑，需要将原来消费者C0 的部分分区分配给消费者C1消费， 彼此之间并无逻辑上的干扰 此时又加入了消费者C3，则按照上述规则继续分配。一昧地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况就会有消费者分配不到任何分区 通过以上方式 Kafka支持两种投递方式： 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布／订阅模式的应用 如何实现多副本的发布订阅与广播 每一个消费者只隶属于一个消费组 ！！！！！ 一个消费者可以订阅一个或多个主题 每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id 来配置，默认值为空宇符串 消息消费 Kafka 中的消费是基于拉模式的，Kafka 中的消息消费是一个不断轮询的过程 在默认的方式下，消费者每隔5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在拉取的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 在Kafka 消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用）。可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。 再平衡 再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，在再均衡发生期间，消费组内的消费者是无法读取消息的，即一段时间消费组会变得不可用。 另外，当一个分区被重新分配给另一个消费者时， 消费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作， 之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生 参考链接 《深入理解Kafka 核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka1-初识Kafka","slug":"MQ/MQ-Kafka1-初识Kafka","date":"2022-01-05T16:15:26.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka1-初识Kafka/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka1-%E5%88%9D%E8%AF%86Kafka/","excerpt":"","text":"Kafka一个采用Scala语言(0.9.0版本有java版本)开发的多分区、多副本且基于ZooKeeper协调的分布式消息系统，主要作用有： 消息系统：Kafka除了传统的消息中间件都有的系统解耦、流量削峰、异步通信的功能功能，还提供了消息顺序性消费保障和回溯消费的功能 存储系统：Kafka可以将消息持久化到磁盘。通过Kafka的消息持久化功能和多副本机制，可以将Kafka做长期的存储系统使用(数据可以&quot;永久&quot;保存、以及 主题日志压缩) 流式处理平台：Kafka不仅为流式处理框架提供可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、链接、变换和聚合等操作 基本概念 一个典型的Kafka系统架构 Producer：生产者，也就是消息发送方。负责创建消息，并将其投递给Kafka Broker: 服务代理节点。对于Kafka而言，Broker可以简单地看做一个独立的Kafka服务节点或Kafka服务实例。一个或多个Broker组成一个Kafka集群 Consumer：消费者，消息接收方。消费者连接到Kafka上并接收消息，进行相关的业务处理 Zookeeper：负责集群元数据的管理、控制器的选举等操作 主题与分区 主题(Topic)：主题是一个逻辑概念，Kafka的消息以主题为单位进行归类，生产者负责讲消息发送到特定的主题(发送到kafka集群中的每一条消息都要指定一个主题)，而消费者负责订阅主题并进行消费 分区(Partition): 每个主题可以分为多个分区，一个分区只属于单个主题，也叫主题分区(Tppic-Partition)。分区在存储层面可以看作是一个可追加的日志(Log)文件，消息被追加到分区日志文件的时候都会分配一个特定的偏移量(offset)。offset是消息在分区中的唯一标识，Kafka通过 offset 来保证消息在分区的顺序性。不过offset并不跨分区，也就是说，Kafka保证的是分区有序而不是主题有序 如下图，主题有四个分区，消息被顺序追加到每个分区日志文件的尾部 Kafka中的分区可以分布在不同的服务器(broker)上，也就是说，一个主题可以横跨多个broker。以此来提供比单个broker更强大的性能。 创建主题时候可以通过指定参数来设置主题的分区个数；也可以在创建完成之后去修改分区的数量，实现水平拓展 每一个消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区 问题1：分区规则是什么？？ TODO 分区容灾 Kafka分区引入了多副本(Replica)机制，同一个分区的不同副本中保存的是相同的消息(在同一时刻，副本之间并非完全相同)，副本之间是&quot;一主多从&quot;的关系，其中leader副本负责处理读写请求，follwer副本只负责与leader副本进行消息同步。 通过设置副本因子来指定分区的副本数量 follower 不支持读请求吗？ 生产者与消费者只与leader副本进行交互，follower副本只负责消息的同步 容灾概念： AR(Assigned Replicas): 分区中的所有副本统称 ISR(In-Sync Replicas): 所有与Leader副本保持一定程度同步的副本(包括leader副本在内)组成 OSR(Out-of-Sync Replicas): 与leader副本同步滞后过多的副本(不包括leader副本)组成 一定程度可以通过参数控制 leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除，如果OSR 集合中有follower 副本&quot;追上&quot;了leader 副本，那么leader 副本会把它从OSR 集合转移至ISR 集合 默认情况下，当leader副本出现故障，只有在ISR集合中的副本才有资格被选举为新的leader 以上选举规则也是可以修改的 问题1：如果所有副本都处于OSR怎么办 TODO 消息拉取 HW(High Watermark):高水位，标识一个特定的消息偏移量(offset)，消费者智能拉取到这个offset之前的消息 LEO(Log End Offset)的缩写，标识当前日志文件中下一条待写入消息的offset 上图代表一个日志文件，这个日志文件中有9条消息，第一条消息的offset(LogStartOffset)为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0-5之间的消息，而offset为6的消息对消费者而言是不可见的。 分区ISR集合中的每个副本都会维护自身的LEO，而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息 注意上图所示 HW 为5 ，LEO 为 9 消费者容灾 Consumer使用拉(Pull)模式从服务端拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时，可以根据之前保存的消费者位置重新拉取需要的消息进行消费，保证消息不会丢失 参考链接 《深入理解Kafka核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Redis","slug":"MQ/MQ-Redis","date":"2022-01-04T16:08:38.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/05/MQ/MQ-Redis/","link":"","permalink":"http://xboom.github.io/2022/01/05/MQ/MQ-Redis/","excerpt":"","text":"在进行消息队列预研的时候，发现Redis也能做为消息队列，看看Redis做为消息队列是如何实现的 Redis做为消息队列有三种方案 List Streams Pub/Sub List Redis的List是简单的字符串列表，底层由 quicklist 实现。 List消息队列原理 命令 用法 描述 LPUSH LPUSH key value [value …] 将一个或多个值value插入到列表key的表头，如果有多个value值，那么各个value值按从左到右的顺序依次插入到表头 RPUSH RPUSH key value [value …] 将一个或多个值value插入到列表key的表尾(最右边) LPOP LPOP key [count] 移除并返回列表key的头元素(count 指定出队列数目) BLPOP BLPOP key [key …] timeout 移除并获取列表的第一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 RPOP PROP key 移除并返回列表key的尾元素 BRPOP BRPOP key [key …] timeout 移除并获取列表的最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 BRPRPLPUSH BRPOPLPUSH source destination timeout 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它；如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 RPOPLPUSH RPOPLPUSH source destination 命令RPOPLPUSH 在一个原子时间内，执行以下两个动作：将列表source中的最后一个元素(尾元素)弹出，并返回给客户端。将source弹出的元素插入到猎豹destination,做为destination列表的头元素 LLEN LLEN key 返回列表key的长度。如果key不存在，则key被解释为一个空列表，返回0。如果key不是列表类型，返回一个错误 LRANGE LRANGE key start stop 返回列表key中指定区间内的元素，区间以偏移量 start 和 stop 指定 使用命令组合即可实现消息的出队入队 LPUSH、RPOP 左进右出 RPUSH、LPOP 右进左出 通过LPUSH、RPOP这样的方式，会存在一个性能风险点： 消费者要即使的处理数据，类似要在消费端添加类似 while(true) 的逻辑，不停的调用RPOP或LPOP命令，这样就会给消费者程序带来不必要的性能损失，于是 --&gt; Redis 提供了BLPOP、BRPOP这样阻塞式读取的命令(带B-Bloking的都是阻塞式)，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据(节省不必要要的CPU开销) LPUSH、BRPOP左进右阻塞出 RPUSH、BLPOP右进左阻塞出 因为 Redis 单线程的特点，所以在消费数据时，同一个消息不会同时被多个 consumer 消费掉，但是需要我们考虑消费不成功的情况 可靠队列模式 List 队列中的消息一经发送出去，便从队列里删除。如果由于网络原因消费者没有收到消息，或者消费者在处理这条消息的过程中崩溃了，就再也无法还原出这条消息。究其原因，就是缺少消息确认机制。 为了保证消息的可靠性，消息队列都会有完善的消息确认机制(Acknowledge)，即消费者向队列报告消息已收到或已处理的机制 RPOPLPUSH、BRPOPLPUSH (阻塞)从一个 list 中获取消息的同时把这条消息复制到另一个 list 里(可以当做备份)，而且这个过程是原子的 数据标识从一个 List 取出后放入另一个 List，业务操作安全执行完成后，再去删除 List 中的数据，如果有问题的话，很好回滚 延时消息 通过 zset 来实现延时消息队列，原理就是将消息加到 zset 结构后，将要被消费的时间戳设置为对应的 score 即可，只要业务数据不会是重复数据就可以 Pub/Sub 消息模型包含 点对点：Point-to-Point(P2P) 发布订阅：Publish/Subscribe(Pub/Sub) List 实现方式就是点对点模式，Redis的发布订阅模式(消息多播)就是真正的Redis MQ &quot;发布/订阅&quot;模式包含两种角色，分别是发布者和订阅者。订阅者可以订阅一个或者多个频道(channel)，而发布者可以向指定的频道(channel)发送消息，所有订阅此频道的订阅者都会收到此消息 Redis 通过 PUBLISH 、 SUBSCRIBE 等命令实现了订阅与发布模式， 这个功能提供两种信息机制 订阅/发布到频道 订阅/发布到模式 频道可以先理解为是个 Redis 的 key 值，而模式则是一个类似正则匹配的 Key，只是个可以匹配给定模式的频道。这样就不需要显式的去订阅多个名称了，可以通过模式订阅这种方式，一次性关注多个频道 Pub/Sub常用命令 命令 用法 描述 PSUBSCRIBE PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道 PUBSUB PUBSUB subcommand [argument [argument …]] 查看订阅与发布系统状态 PUBLISH PUBLISH channel message 将信息发送到指定的频道 PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]] 退订所有给定模式的频道 SUBSCRIBE SUBSCRIBE channel [channel …] 订阅给定的一个或多个频道的信息 UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]] 指退订给定的频道 频道 如上创建一个生产者和两个消费者，消费者1 subscribe channel1 channel2，消费者2subscribe channel1。当生产者使用命令PUBBLISH channel message 向 隧道channel1发送消息时，两个消费者都能。向隧道channel2发送消息时，只有消费者1能够收到消息 其中消费者每次都可以收到3个参数的消息 消息的种类 频道的名称 实际的消息 模式 订阅符合给定模式的频道，命令是 PSUBSCRIBE 如上创建一个生产者和两个消费者，一个使用 SUBSCRIBE channel1，另外一个使用 PSUBSCRIBE chann*,当生产者使用PUBLISH channel1 msg3,两个消费者都能收到消息 PSUBSCRIBE 更像是支持匹配模式的消费者 Redis 发布订阅 (pub/sub) 有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。而且也没有 Ack 机制来保证数据的可靠性，假设一个消费者都没有，那消息就直接被丢弃了。 Streams Redis 5.0 版本新增了一个更强大的数据结构——Stream。它提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 像是个仅追加内容的消息链表，把所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容。而且消息是持久化的 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 Streams 是 Redis 专门为消息队列设计的数据类型，所以提供了丰富的消息队列操作命令 Stream常用命令 描述 命令 添加消息到末尾，保证有序，可以自动生成唯一ID XADD key ID field value [field value …] 对流进行修剪，限制长度 XTRIM key MAXLEN [~] count 删除消息 XDEL key ID [ID …] 获取流包含的元素数量，即消息长度 XLEN key 获取消息列表，会自动过滤已经删除的消息 XRANGE key start end [COUNT count] 以阻塞或非阻塞方式获取消息列表 XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key …] id [id …] 创建消费者组 XGROUP [CREATE key groupname id-or-] [DESTROY key groupname] [DELCONSUMER key groupname consumername] 读取消费者组中的消息 XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key …] ID [ID …] 将消息标记为&quot;已处理&quot; XACK key group ID [ID …] 为消费者组设置新的最后递送消息ID XGROUP SETID [CREATE key groupname id-or-] [DESTROY key groupname] 删除消费者 XGROUP DELCONSUMER [CREATE key groupname id-or-] [DESTROY key groupname] 删除消费者组 XGROUP DESTROY [CREATE key groupname id-or-] [DESTROY key groupname] [DEL 显示待处理消息的相关信息 XPENDING key group [start end count] [consumer] 查看流和消费者组的相关信息 XINFO [CONSUMERS key groupname] [GROUPS key] [STREAM key] [HELP] 打印流信息 XINFO STREAM [CONSUMERS key groupname] [GROUPS key] [STREAM key] [HELP] * 号表示服务器自动生成 ID，后面顺序跟着一堆 key/value 消息ID 必须要比上个 ID 大 -表示最小值 , + 表示最大值,也可以指定最大消息ID，或最小消息ID，配合 -、+ 使用 独立消费 xread 以阻塞或非阻塞方式获取消息列表，指定BLOCK选项即表示阻塞，超过时间0ms(意味用不超时) 阻塞的从尾部读取流，开启新的客户端xadd后发现这里就读到了,block 0 表示永久阻塞 没有给流 mystream 传入一个常规的 ID，而是传入了一个特殊的 ID $ $ 意思是: XREAD 应该使用流 streamtest 已经存储的最大 ID 作为最后一个 ID 当然，也可以指定任意有效的 ID。 而且， XREAD 的阻塞形式还可以同时监听多个 Strem，只需要指定多个键名即可 1127.0.0.1:6379&gt; xread block 0 streams mystream yourstream $ $ 多个客户端监听相同的stream，那么它们都会收到消息！！！，如果想多个客户端监听同一个流怎么办呢？便是创建消费者组 创建消费者组 上述 xread 虽然分发到 N 个客户端，如果想要做的不是向许多客户端提供相同的消息流，而是从同一流向许多客户端提供不同的消息子集。比如下图这样，三个消费者按轮训的方式去消费一个 Stream Redis Stream 借鉴了很多 Kafka 的设计。 Consumer Group：有了消费组的概念，每个消费组状态独立，互不影响，一个消费组可以有多个消费者 last_delivered_id ：每个消费组会有个游标 last_delivered_id 在数组之上往前移动，表示当前消费组已经消费到哪条消息了 pending_ids ：消费者的状态变量，作用是维护消费者的未确认的 id。pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack。如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少。这个 pending_ids 变量在 Redis 官方被称之为 PEL，也就是 Pending Entries List，这是一个很核心的数据结构，它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理。 Stream 不像 Kafak 那样有分区的概念，如果想实现类似分区的功能，就要在客户端使用一定的策略将消息写到不同的 Stream。 xgroup create：创建消费者组 xgreadgroup：读取消费组中的消息 xack：ack 掉指定消息 按消费组消费 Stream 提供了 xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息 ID。它同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除。 参考链接 https://stor.51cto.com/art/202101/640335.htm http://xiaorui.cc/archives/5285","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Docker入门1-Dockerfile","slug":"Docker/Docker入门1-Dockerfile","date":"2021-11-16T15:36:01.000Z","updated":"2021-11-16T15:36:31.000Z","comments":true,"path":"2021/11/16/Docker/Docker入门1-Dockerfile/","link":"","permalink":"http://xboom.github.io/2021/11/16/Docker/Docker%E5%85%A5%E9%97%A81-Dockerfile/","excerpt":"","text":"Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明 指令详解 FROM: 定制的镜像都是基于 FROM 的镜像，即基础镜像 RUN：用于执行后面跟着的命令行命令 12345678#shell 格式RUN &lt;命令行命令&gt;# &lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。#exec 格式RUN [\"可执行文件\", \"参数1\", \"参数2\"]# 例如：# RUN [\"./test.php\", \"dev\", \"offline\"] 等价于 RUN ./test.php dev offline COPY: 复制指令，从上下文目录中复制文件或者目录到容器里指定路径 123456789#格式COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;... &lt;目标路径&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"]#[--chown=&lt;user&gt;:&lt;group&gt;]：可选参数，用户改变复制到容器内文件的拥有者和属组。#&lt;源路径&gt;：源文件或者源目录，可以是通配符表达式，例如：#&lt;目标路径&gt;：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。COPY hom* /mydir/COPY hom?.txt /mydir/ ADD：ADD 指令和 COPY 的使用格类似（同样需求下，官方推荐使用 COPY） ADD 的优点：在执行 &lt;源文件&gt; 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 &lt;目标路径&gt;。 ADD 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。 CMD: 为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。类似于 RUN 指令，但二者运行的时间点不同: CMD 在docker run 时运行 RUN 是在 docker build 注意： 如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。 CMD 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖。 123CMD &lt;shell 命令&gt; CMD [\"&lt;可执行文件或命令&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] CMD [\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数 推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh ENTRYPOINT: 类似于 CMD 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序 优点：在执行 docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。 注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。 1ENTRYPOINT [\"&lt;executeable&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] 可以搭配 CMD 命令使用：一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参 1234FROM nginxENTRYPOINT [\"nginx\", \"-c\"] # 定参CMD [\"/etc/nginx/nginx.conf\"] # 变参 不传参 123$ docker run nginx:test#容器内会默认运行以下命令，启动主进程。#nginx -c /etc/nginx/nginx.conf 传参 123docker run nginx:test -c /etc/nginx/new.conf#容器内会默认运行以下命令，启动主进程(/etc/nginx/new.conf:假设容器内已有此文件)nginx -c /etc/nginx/new.conf ENV: 环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量 1234567ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...#例如ENV NODE_VERSION 7.2.0RUN curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\" \\ &amp;&amp; curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" ARG: 构建参数，与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量 1ARG &lt;参数名&gt;[=&lt;默认值&gt;] VOLUME: 定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。 避免重要的数据，因容器重启而丢失，这是非常致命的。 避免容器不断变大 12VOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; 在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点 EXPOSE: 仅仅只是声明端口 帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。 在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] WORKDIR: 用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在 1WORKDIR &lt;工作目录路径&gt; USER: 指定执行后续命令的用户和用户组，只是切换后续命令执行的用户(用户和用户组必须提前已经存在） 1USER &lt;用户名&gt;[:&lt;用户组&gt;] HEALTHCHECK: 指定某个程序或者指令来监控 docker 容器服务的运行状态 1234HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令HEALTHCHECK [选项] CMD &lt;命令&gt; : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。 ONBUILD: 用于延迟构建命令的执行。 简单的说，就是 Dockerfile 里用 ONBUILD 指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的 Dockerfile 使用了之前构建的镜像 FROM test-build ，这时执行新镜像的 Dockerfile 构建时候，会执行 test-build 的 Dockerfile 里的 ONBUILD 指定的命令 1ONBUILD &lt;其它指令&gt; LABEL: LABEL 指令用来给镜像添加一些元数据（metadata），以键值对的形式，语法格式如下： 123LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...#比如可以添加镜像的作者LABEL org.opencontainers.image.authors=\"runoob\" 构建镜像 12FROM nginxRUN echo '这是一个本地构建的nginx镜像' &gt; /usr/share/nginx/html/index.html docker build -t nginx:v3 . 已当前文件为上下文，构建nginx:v3镜像 注意 Dockerfile 的指令每执行一次都会在 docker 上新建一层 1234FROM centosRUN yum -y install wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN tar -xvf redis.tar.gz 以上执行会创建 3 层镜像，可简化为以下格式, 以 &amp;&amp; 符号连接命令，只会创建 1 层镜像。 1234FROM centosRUN yum -y install wget \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ &amp;&amp; tar -xvf redis.tar.gz 参考链接 https://www.runoob.com/docker/docker-dockerfile.html","categories":[{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/tags/Docker/"}]},{"title":"协议2-HTTP2","slug":"Protocol/协议2-HTTP2","date":"2021-11-01T23:25:03.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/11/02/Protocol/协议2-HTTP2/","link":"","permalink":"http://xboom.github.io/2021/11/02/Protocol/%E5%8D%8F%E8%AE%AE2-HTTP2/","excerpt":"","text":"学习gRPC的过程中，发现gRPC是基于HTTP/2实现的，什么是HTTP2? HTTP/3又是因为什么而被推出 HTTP/1.1 随着网络的发展，单个页面为了显示和渲染需要的资源越来越多，这其中存在着一些问题 TCP连接数限制 对于同一个域名，浏览器最多只能同时创建 6~8 个 TCP 连接 (不同浏览器不一样)。为了解决数量限制，出现了 域名分片 技术，其实就是资源分域，将资源放在不同域名下 (比如二级子域名下)，这样就可以针对不同域名创建连接并请求，以一种讨巧的方式突破限制，但是滥用此技术也会造成很多问题，比如每个 TCP 连接本身需要经过 DNS 查询、三步握手、慢启动等，还占用额外的 CPU 和内存，对于服务器来说过多连接也容易造成网络拥挤、交通阻塞等，对于移动端来说问题更明显 在图中可以看到新建了六个 TCP 连接，每次新建连接 DNS 解析需要时间(几 ms 到几百 ms 不等)、TCP 慢启动也需要时间、TLS 握手又要时间，而且后续请求都要等待队列调度 队头阻塞(Head-Of-Line Blocking) 每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。 针对队头阻塞,有以下办法来解决: HTTP 管线化(HTTP pipelining)是将多个请求（request）整批提交的技术，而在发送过程中不需先等待服务器的回应。缺点是：第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化 将同一页面的资源分散到不同域名下，提升连接上限。对于同一个域名，Chrome默认允许同时建立 6 个 TCP持久连接，使用持久连接时。虽然能公用一个TCP管道，但一个管道同一时刻只能处理一个请求，在当前请求没有结束之前，其他的请求只能处于阻塞状态。另外如果在同一个域名下同时有10个请求发生，那么其中4个请求会进入排队等待状态，直至进行中的请求完成。 雪碧图：合并多张小图为一张大图,再用JavaScript或者CSS将小图重新“切割”出来的技术。 内联(Inlining)是另外一种防止发送很多小图请求的技巧，将图片的原始数据嵌入在CSS文件里面的URL里，减少网络请求次数 使用 quic 协议，由于使用的UDP协议，所以可以避免因为TCP自身机制而产生的对头阻塞问题 使用 SCTP 流控制传输协议 无状态特性–带来的巨大HTTP头部 报文Header一般会携带&quot;User Agent&quot;“Cookie”&quot;Accept&quot;等许多固定的头字段，存在大量重复的字段值，增加了传输的成本 明文传输–带来的不安全性 HTTP/1.1在传输数据时，所有传输的内容都是明文 不支持服务器推送消息 HTTP/2 HTTP/2由两个规范（Specification）组成： Hypertext Transfer Protocol version 2 - RFC7540 HPACK - Header Compression for HTTP/2 - RFC7541 报文解析 执行命令：curl --http2 -v nghttp2.org/robots.txt nghttp2.org/humans.txt 分析详情见：gRPC入门2-gRPC交互 HTTP优点 二进制传输 HTTP/2 采用二进制格式传输数据，而非HTTP/1.x 里纯文本形式的报文，二进制协议解析起来更高效。HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。 它把TCP协议的部分特性挪到了应用层，把原来的&quot;Header+Body&quot;的消息&quot;打散&quot;为数个小片的二进制&quot;帧&quot;(Frame),用&quot;HEADERS&quot;帧存放头数据、“DATA&quot;帧存放实体数据。HTTP/2数据分帧后&quot;Header+Body&quot;的报文结构就完全消失了，协议看到的只是一个个的&quot;碎片” HTTP/2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装 Header压缩 HTTP/2并没有使用传统的压缩算法，而是开发了专门的&quot;HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还采用哈夫曼编码来压缩整数和字符串，可以达到50%~90%的高压缩率。 具体来说: 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键-值对，对于相同的数据，不再通过每次请求和响应发送； 首部表在HTTP/2的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键-值对要么被追加到当前表的末尾，要么替换表中之前的值 例如下图中的两个请求， 请求一发送了所有的头部字段，第二个请求则只需要发送差异数据，这样可以减少冗余数据，降低开销 多路复用 在 HTTP/2 中引入了多路复用的技术。多路复用很好的解决了浏览器限制同一个域名下的请求数量的问题，同时也接更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度 在 HTTP/2 中，有了二进制分帧之后，HTTP /2 不再依赖 TCP 链接去实现多流并行了，在 HTTP/2中, 同域名下所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流。 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。 这一特性，使性能有了极大提升： 同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应,这样整个页面资源的下载过程只需要一次慢启动，同时也避免了多个TCP连接竞争带宽所带来的问题。 并行交错地发送多个请求/响应，请求/响应之间互不影响。 在HTTP/2中，每个请求都可以带一个31bit的优先值，0表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。 Server Push HTTP2还在一定程度上改变了传统的“请求-应答”工作模式，服务器不再是完全被动地响应请求，也可以新建“流”主动向客户端发送消息。比如，在浏览器刚请求HTML的时候就提前把可能会用到的JS、CSS文件发给客户端，减少等待的延迟，这被称为&quot;服务器推送&quot;（ Server Push，也叫 Cache push） 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，换句话说，服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行(怎么确认的？) 安全性 HTTP/2可以使用明文传输数据，不强制使用加密通信，不过格式还是二进制，只是不需要解密。 但由于HTTPS已是大势所趋，主流浏览器Chrome、Firefox等都公开宣布只支持加密的HTTP/2，所以“事实上”的HTTP/2是加密的。也就是说，互联网上通常所能见到的HTTP/2都是使用&quot;https”协议名，跑在TLS上面。HTTP/2协议定义了两个字符串标识符：“h2&quot;表示加密的HTTP/2，“h2c”表示明文的HTTP/2 应用层的重置连接 对于 HTTP/1 来说，是通过设置 tcp segment 里的 reset flag 来通知对端关闭连接的。这种方式会直接断开连接，下次再发请求就必须重新建立连接。HTTP/2 引入 RST_STREAM 类型的 frame，可以在不断开连接的前提下取消某个 request 的 stream，表现更好 请求优先级设置 HTTP/2 里的每个 stream 都可以设置依赖 (Dependency) 和权重，可以按依赖树分配优先级，解决了关键请求被阻塞的问题 流量控制 每个 http2 流都拥有自己的公示的流量窗口，它可以限制另一端发送数据。对于每个流来说，两端都必须告诉对方自己还有足够的空间来处理新的数据，而在该窗口被扩大前，另一端只被允许发送这么多数据 帧 - Frame 所有帧都是一个固定的 9 字节头部 (payload 之前) 跟一个指定长度的负载 (payload) Length 代表整个 frame 的长度，用一个 24 位无符号整数表示。除非接收者在 SETTINGS_MAX_FRAME_SIZE 设置了更大的值 (大小可以是 2^14(16384) 字节到 2^24-1(16777215) 字节之间的任意值)，否则数据长度不应超过 2^14(16384) 字节。头部的 9 字节不算在这个长度里 Type 定义 frame 的类型，用 8 bits 表示。帧类型决定了帧主体的格式和语义，如果 type 为 unknown 应该忽略或抛弃。 Flags 是为帧类型相关而预留的布尔标识。标识对于不同的帧类型赋予了不同的语义。如果该标识对于某种帧类型没有定义语义，则它必须被忽略且发送的时候应该赋值为 (0x0) R 是一个保留的比特位。这个比特的语义没有定义，发送时它必须被设置为 (0x0), 接收时需要忽略。 Stream Identifier 用作流控制，用 31 位无符号整数表示。客户端建立的 sid 必须为奇数，服务端建立的 sid 必须为偶数，值 (0x0) 保留给与整个连接相关联的帧 (连接控制消息)，而不是单个流 Frame Payload 是主体内容，由帧类型决定 共分为十种类型的帧: HEADERS: 报头帧 (type=0x1)，用来打开一个流或者携带一个首部块片段 DATA: 数据帧 (type=0x0)，装填主体信息，可以用一个或多个 DATA 帧来返回一个请求的响应主体 PRIORITY: 优先级帧 (type=0x2)，指定发送者建议的流优先级，可以在任何流状态下发送 PRIORITY 帧，包括空闲 (idle) 和关闭 (closed) 的流 RST_STREAM: 流终止帧 (type=0x3)，用来请求取消一个流，或者表示发生了一个错误，payload 带有一个 32 位无符号整数的错误码 (Error Codes)，不能在处于空闲 (idle) 状态的流上发送 RST_STREAM 帧 SETTINGS: 设置帧 (type=0x4)，设置此 连接 的参数，作用于整个连接 PUSH_PROMISE: 推送帧 (type=0x5)，服务端推送，客户端可以返回一个 RST_STREAM 帧来选择拒绝推送的流 PING: PING 帧 (type=0x6)，判断一个空闲的连接是否仍然可用，也可以测量最小往返时间 (RTT) GOAWAY: GOWAY 帧 (type=0x7)，用于发起关闭连接的请求，或者警示严重错误。GOAWAY 会停止接收新流，并且关闭连接前会处理完先前建立的流 WINDOW_UPDATE: 窗口更新帧 (type=0x8)，用于执行流量控制功能，可以作用在单独某个流上 (指定具体 Stream Identifier) 也可以作用整个连接 (Stream Identifier 为 0x0)，只有 DATA 帧受流量控制影响。初始化流量窗口后，发送多少负载，流量窗口就减少多少，如果流量窗口不足就无法发送，WINDOW_UPDATE 帧可以增加流量窗口大小 CONTINUATION: 延续帧 (type=0x9)，用于继续传送首部块片段序列，见 首部的压缩与解压缩 HTTP/2缺点 HTTP/2的缺点是底层支撑的 TCP 协议造成的。HTTP/2的缺点主要有以下几点： TCP 以及 TCP+TLS建立连接的延时 HTTP/2都是使用TCP协议来传输的，而如果使用HTTPS的话，还需要使用TLS协议进行安全传输，而使用TLS也需要一个握手过程，这样就需要有两个握手延迟过程： ① 在建立TCP连接的时候，需要和服务器进行三次握手来确认连接成功，也就是说需要在消耗完1.5个RTT之后才能进行数据传输。 ② 进行TLS连接，TLS有两个版本——TLS1.2和TLS1.3，每个版本建立连接所花的时间不同，大致是需要1~2个RTT。 总之，在传输数据之前，我们需要花掉 3～4 个 RTT。 TCP的队头阻塞并没有彻底解决 在HTTP/2中，多个请求是跑在一个TCP管道中的。但当出现了丢包时，HTTP/2 的表现反倒不如 HTTP/1 了。因为TCP为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，HTTP/2出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该TCP连接中的所有请求。而对于 HTTP/1.1 来说，可以开启多个 TCP 连接，出现这种情况反到只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据。 HTTP3 因为HTTP/2的问题，HTTP/3诞生，一个基于 UDP 协议的“QUIC”协议，让HTTP跑在QUIC上而不是TCP上。而这个“HTTP over QUIC”就是HTTP协议的下一个大版本。 实现了类似TCP的流量控制、传输可靠性的功能 实现了快速握手功能 集成了TLS加密功能 多路复用，彻底解决TCP中队头阻塞的问题 参考链接 https://blog.csdn.net/howgod/article/details/102597450?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link https://blog.wangriyu.wang/2018/05-HTTP2.html","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://xboom.github.io/tags/HTTP/"}]},{"title":"Grpc-02-Grpc交互","slug":"Grpc/Grpc-02-Grpc交互","date":"2021-10-31T11:25:42.000Z","updated":"2022-10-16T16:49:00.000Z","comments":true,"path":"2021/10/31/Grpc/Grpc-02-Grpc交互/","link":"","permalink":"http://xboom.github.io/2021/10/31/Grpc/Grpc-02-Grpc%E4%BA%A4%E4%BA%92/","excerpt":"","text":"gRCP有四种调用方式，通过抓包分析这四种的交互过程 一元RPC 客户端流RPC 服务端流RPC 双向流RPC 示例代码以及报文路径 服务端监听端口9001 一元RPC 参考1.pcap 1 - 3: client -&gt; server 客户端向服务器发起连接请求，完成三次握手 4 - 5: server -&gt; client 服务器向客户端发送 SETTINGS，在建立连接开始时双方都要发送 SETTINGS 帧以表明自己期许对方应做的配置，对方接收后同意配置参数便返回带有 ACK 标识的空 SETTINGS 帧表示确认，而且连接后任意时刻任意一方也都可能再发送 SETTINGS 帧调整，SETTINGS 帧中的参数会被最新接收到的参数覆盖 一个Settings帧的payload由N个参数组成，每个参数的格式如下 Identifier: 代表参数类型，比如Max frame size 是 5 Value：相应参数的值 SETTINGS 帧作用于整个连接，而不是某个流，而且 SETTINGS 帧的 stream identifier 必须是 0x0，否则接收方会认为错误 (PROTOCOL_ERROR) SETTINGS 帧包含以下参数: SETTINGS_HEADER_TABLE_SIZE (0x1): 用于解析 Header block 的 Header 压缩表的大小，初始值是 4096 字节 SETTINGS_ENABLE_PUSH (0x2): 可以关闭 Server Push，该值初始为 1，表示允许服务端推送功能 SETTINGS_MAX_CONCURRENT_STREAMS (0x3): 代表发送端允许接收端创建的最大流数目 SETTINGS_INITIAL_WINDOW_SIZE (0x4): 指明发送端所有流的流量控制窗口的初始大小，会影响所有流，该初始值是 2^16 - 1(65535) 字节，最大值是 2^31 - 1，如果超出最大值则会返回 FLOW_CONTROL_ERROR SETTINGS_MAX_FRAME_SIZE (0x5): 指明发送端允许接收的最大帧负载的字节数，初始值是 2^14(16384) 字节，如果该值不在初始值 (2^14) 和最大值 (2^24 - 1) 之间，返回 PROTOCOL_ERROR SETTINGS_MAX_HEADER_LIST_SIZE (0x6): 通知对端，发送端准备接收的首部列表大小的最大字节数。该值是基于未压缩的首部域大小，包括名称和值的字节长度，外加每个首部域的 32 字节的开销 SETTINGS 帧有以下标识 (flags): ACK: bit 0 设为 1 代表已接收到对方的 SETTINGS 请求并同意设置，设置此标志的 SETTINGS 帧 payload 必须为空 6 - 7: client -&gt; server, 客户端的前言Magic，包含一个内容为 PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n 的序列 发送完前言后双方都得向对方发送带有 ACK 标识的 SETTINGS 帧表示确认，对应上图中编号 29 和 31 的帧。请求站点的全部帧序列，帧后面的数字代表所属流的 id，最后以 GOAWAY 帧关闭连接 8 - 9：client -&gt; server, 客户端 SETTING[0]，客户端发送前言Magic 再带上一个空的 前言 10 - 11: server -&gt; client, 服务端针对客户端前言的ACK 12 - 13: client -&gt; server , 客户端针对服务端前言的ACK 14 - 15: client -&gt; server, 客户端向服务器发起 RPC的 Headers 帧 headers 帧格式： Pad Length: 指定 Padding 长度，存在则代表 PADDING flag 被设置 E: 一个比特位声明流的依赖性是否是排他的，存在则代表 PRIORITY flag 被设置 Stream Dependency: 指定一个 stream identifier，代表当前流所依赖的流的 id，存在则代表 PRIORITY flag 被设置 Weight: 一个无符号 8 为整数，代表当前流的优先级权重值 (1~256)，存在则代表 PRIORITY flag 被设置 Header Block Fragment: header 块片段 Padding: 填充字节，没有具体语义，作用与 DATA 的 Padding 一样，存在则代表 PADDING flag 被设置 HEADERS 帧有以下标识 (flags): END_STREAM: bit 0 设为 1 代表当前 header 块是发送的最后一块，但是带有 END_STREAM 标识的 HEADERS 帧后面还可以跟 CONTINUATION 帧 (这里可以把 CONTINUATION 看作 HEADERS 的一部分) END_HEADERS: bit 2 设为 1 代表 header 块结束 PADDED: bit 3 设为 1 代表 Pad 被设置，存在 Pad Length 和 Padding PRIORITY: bit 5 设为 1 表示存在 Exclusive Flag (E), Stream Dependency, 和 Weight Unused 16 - 17: client -&gt; server, 客户端向服务器发送rpc data帧 Pad Length: ? 表示此字段的出现时有条件的，需要设置相应标识 (set flag)，指定 Padding 长度，存在则代表 PADDING flag 被设置 Data: 传递的数据，其长度上限等于帧的 payload 长度减去其他出现的字段长度 Padding: 填充字节，没有具体语义，发送时必须设为 0，作用是混淆报文长度，与 TLS 中 CBC 块加密类似， DATA 帧有如下标识 (flags): END_STREAM: bit 0 设为 1 代表当前流的最后一帧 PADDED: bit 3 设为 1 代表存在 Padding 18 - 19: server -&gt; client, 发送 控制帧 WINDOW_UPDATE， 以及PING 用来判断连接是否可用 20 - 21: server -&gt; client , 服务器发送rpc 响应 22 - 23: client -&gt; server, 客户端向服务器发送 PING 帧，判断连接是否可用 24 - 25: client -&gt; server, 客户端向服务器发送 WINDOWS_UPDATE, PING帧 26 - 27: server -&gt; client, 客户端向服务器发送 PING帧 28: 客户端发送完成 退出进程 客户端流RPC 1 - 3: 完成三次握手 4 - 5: client -&gt; server, 客户端向服务器发送http/2前言 6 - 7: client -&gt; server, 客户端向服务器发送前言的时候带上空 SETTINGS 8 - 9: server -&gt; client, 服务器向客户端发送前言，设置 MAX_FREAME_SIZE 16384 10 - 11: server -&gt; client, 服务器向客户端发送 前言ACK 12 - 13: client -&gt; server, SETTINGS(ACK) + HEADERS 可以看到在流中，多个消息通过一个报文发送 14 - 15: server -&gt; client, WINDOWS_UPDATE + PING 16 - 17: server -&gt; client, HEADERS + DATA 18 - 19: client -&gt; server, PING(ACK) + WINDOWS_UPDATE + PING 20 -21: server -&gt; client , PING(ACK) 服务端流RPC 1 - 3： client -&gt; server，客户端向服务器发起连接请求，完成三次握手 4 - 5： client -&gt; server，Magic 客户端向服务器发送HTTP/2 前言 6 - 7： client -&gt; server，SETTING 客户端前言带上的空 SETTING 8 - 9: server -&gt; client，SETTING 服务端向客户端发送 HTTP/2 前言 10-11: server -&gt; client，SETTING(ACK) 12-13: client -&gt; server, SETTING(ACK) + HEADERS 14-15: server -&gt; client, WINDOWS_UPDATE + PING + HEADERS + DATA 16-17: client -&gt; server, PING(ACK) 18-19: client -&gt; server, WINDOWS_UPDATE, PING 20-21: server -&gt; client, PING(ACK) 双向流RPC 1 - 3: client -&gt; server, 客户端向服务器发起连接请求，完成三次握手 4 - 5: server -&gt; client, SETTING 服务器向客户端发送HTTP/2 前言 6 - 7: client -&gt; server，Magic 客户端向服务器发送HTTP/2 前言 8 - 9: client -&gt; server，SETTING 客户端前言带上的空 SETTING 10, 12: client -&gt; server, SETTING(ACK) + HEADERS 11, 13: server -&gt; client, SETTING(ACK) 14-15: client -&gt; server, DATA 16-17: server -&gt; client, WINDOWS + PING 18-19: client -&gt; server, PING(ACK) 20-21: server -&gt; client, HEADERS + DATA + DATA 22-23: client -&gt; server, WINDOWS + PING 24-25: server -&gt; client, PING(ACK) 26-27: client -&gt; server, DATA + DATA 28,30: server -&gt; client, WINDOWS_UPDATE, PING 29-31: server -&gt; client, DATA + DATA 32-33: client -&gt; server, PING(ACK), WINDOWS_UPDATE, PING 34-35: server -&gt; client, PING(ACK) 参考链接 https://httpwg.org/specs/rfc7540.html https://blog.wangriyu.wang/2018/05-HTTP2.html","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"G-01-相关介绍","slug":"Grpc/Grpc-01-相关介绍","date":"2021-10-31T07:25:42.000Z","updated":"2022-11-08T16:28:01.000Z","comments":true,"path":"2021/10/31/Grpc/Grpc-01-相关介绍/","link":"","permalink":"http://xboom.github.io/2021/10/31/Grpc/Grpc-01-%E7%9B%B8%E5%85%B3%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"RPC 代指远程过程调用(Remote Procedure Call)，基于 HTTP 协议的。它的调用包含了传输协议和编码（对象序列号）协议等等。允许跨语言或跨设备通信，无需额外为这个交互编程。如果A服务器上的应用c 调用B服务器上的应用d，就跟本地调用一个函数一样 示例代码路径 RPC框架 一个完整的 RPC 框架，应包含负载均衡、服务注册和发现、服务治理等功能，并具有可拓展性便于流量监控系统等接入 有些较单一的 RPC 框架，通过组合多组件也能达到这个标准 常用的RPC框架 \\ 跨语言 多 IDL 服务治理 注册中心 服务管理 gRPC √ × × × × Thrift √ × × × × Rpcx × √ √ √ √ Dubbo × √ √ √ √ Protobuf Protocol Buffers 是一种与语言、平台无关，可扩展的序列化结构化数据的方法，常用于通信协议，数据存储等等。相较于 JSON、XML，它更小、更快、更简单。 123456789101112131415161718192021222324252627282930313233343536syntax = \"proto3\"; // proto版本option go_package = \"../proto\"; // 生成go文件目录package search; //生成的包名service SearchService &#123; rpc Search(SearchRequest) returns (SearchResponse) &#123;&#125; //一元RPC rpc List(StreamRequest) returns (stream StreamResponse) &#123;&#125;; //服务端流RPC rpc Record(stream StreamRequest) returns (StreamResponse) &#123;&#125;; //客户端流PRC rpc Route(stream StreamRequest) returns (stream StreamResponse) &#123;&#125;; //双向流RPC&#125;message SearchRequest &#123; string request = 1;&#125;message SearchResponse &#123; string response = 1;&#125;message StreamPoint &#123; string name = 1; int32 value = 2;&#125;message StreamRequest &#123; StreamPoint pt = 1;&#125;message StreamResponse &#123; StreamPoint pt = 1;&#125; 使用命令: protoc --go_out=. *.proto,生成的*.pb.go 详细语法：https://developers.google.com/protocol-buffers/docs/proto3 v2 和 v3 主要区别 删除原始值字段的字段存在逻辑 删除 required 字段 删除 optional 字段，默认就是 删除 default 字段 删除扩展特性，新增 Any 类型来替代它 删除 unknown 字段的支持 新增 JSON Mapping 新增 Map 类型的支持 修复 enum 的 unknown 类型 repeated 默认使用 packed 编码 引入了新的语言实现（C＃，JavaScript，Ruby，Objective-C） 相较Protobuf，为什么不用XML 更简单 数据描述只需原来的1/10 和 1/3 解析速度是原来的20倍至100倍 减少了二义性 生成了更易使用的数据访问类 gRPC gRPC 是一个 基于 HTTP/2 协议设计的 RPC 框架，采用了 Protobuf 作为 IDL(Interactive Data Language) 多语言支持：C++、C#、Dart、Go、Java、Node.js、Objective-C、PHP、Python、Ruby、C++ 特点：HTTP/2、Protobuf、客户端与服务器基于同一份IDL 架构 客户端（gRPC Sub）调用 A 方法，发起 RPC 调用 对请求信息使用 Protobuf 进行对象序列化压缩（IDL） 服务端（gRPC Server）接收到请求后，解码请求体，进行业务逻辑处理并返回 对响应结果使用 Protobuf 进行对象序列化压缩（IDL） 客户端接受到服务端响应，解码请求体。回调被调用的 A 方法，唤醒正在等待响应（阻塞）的客户端调用并返回响应结果 gRPC分为四种调用方式 一元RPC 客户端流RPC 服务端流RPC 双向RPC 构建服务端 123456789lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))if err != nil &#123; log.Fatalf(\"failed to listen: %v\", err)&#125;grpcServer := grpc.NewServer()...pb.RegisterSearchServer(grpcServer, &amp;SearchServer&#123;&#125;)grpcServer.Serve(lis) 监听指定 TCP 端口，用于接受客户端请求 创建 gRPC Server 的实例对象 gRPC Server 内部服务和路由的注册 Serve() 调用服务器以执行阻塞等待，直到进程被终止或被 Stop() 调用 创建客户端 123456789var opts []grpc.DialOption...conn, err := grpc.Dial(*serverAddr, opts...)if err != nil &#123; log.Fatalf(\"fail to dial: %v\", err)&#125;defer conn.Close()client := pb.NewSearchClient(conn) 创建 gRPC Channel 与 gRPC Server 进行通信（需服务器地址和端口作为参数） 设置 DialOptions 凭证（例如，TLS，GCE 凭据，JWT 凭证） 创建 Search Client Stub 调用对应的服务方法 问题 什么场景下不适合使用 Protobuf，而适合使用 JSON、XML？ Protobuf 一节中提到的 packed 编码，是什么？ 参考链接 1.https://github.com/EDDYCJY/go-grpc-example","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Redis入门5-AOF","slug":"Redis/Redis入门5-AOF","date":"2021-10-10T12:42:44.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/10/10/Redis/Redis入门5-AOF/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A85-AOF/","excerpt":"","text":"除了RDB持久功能之外，Redis还提供了AOF(Append Only File)持久化功能。 RDB 持久化通过保存数据库中的键值对来记录数据库状态 AOF持久化是通过保存Redis 服务器锁执行的写命令来记录数据库状态 被写入AOF文件的命令是以Redis的命令请求协议格式保存的，Redis的命令请求协议是纯文本格式，直接打开AOF文件，观察内容 AOF 持久化的实现 AOF持久化功能的实现可以分为命令的追加(append)、文件写入、文件同步(sync)三个步骤 命令追加 当AOF持久化功能处于打开状态时，服务器在执行一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾 文件的写入与同步 这里的同步是操作系统在write 写入文件时,通常将数据保存在一个内存缓冲区，然后将缓冲区中的数据写入到磁盘的过程 Redis的服务器进程就是一个事件循环(loop)，这个循环中的文件事件负责接收客户端的命令请求以及回复，时间事件则负责执行像serverCron函数这样需要定时运行的函数 服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以每次结束一个事件循环之前，都会调用 flushAppendOnlyFile 函数，考虑是否将aof_buf缓冲区中的内容写入和保存到AOF文件里面 1234567891011def eventLoop(): while True: # 处理文件时间，接收命令请求以及发送命令回复 # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中 processFileEvents() # 处理时间事件 processTimeEvents() # 考虑是否将 aof_buf 中的内容写入和保存到 AOF 文件里面 flushAppendOnlyFile() 通过服务器配置的 appendfsync选型的值来决定AOF持久化行为 Appendfsync flushAppendOnlyFile 函数的行为 效率 安全 always 将aof_buf缓冲区中的所有内容写入并同步到AOF文件 最慢 最高 everysec 将aof_buf缓冲区的所有内容写入到AOF文件，如果上次同步AOF文件的时间距离现在超过1秒钟，那么子进程对AOF文件进行同步，并且这个同步操作是由换一个线程专门负责执行的 适中 适中 no 将aof_buf缓冲区的中的所有内容写入到AOF文件，但并不对AOF文件进行同步，何时同步由操作系统自己决定 最快 最低 为了提高文件的写入效率，在现代操作系统中，用户调用write函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面，等到缓冲区的空间被填满，或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面 导致当计算机发生停机的时候，内存缓冲区的写入数据可能会丢失。为此，系统提供了 fsync 和 fdatashync 两个同步函数，强制操作系统立即将缓冲区中的数据写入到硬盘 AOF文件的载入与数据还原 由于AOF 文件里包含了重建数据库状态所需的所有写命令，所以服务器只要读入并重新执行一遍AOF文件里面保存的写命令 Redis 读取AFO文件并还原数据库状态的详细步骤如下： 创建一个不带网络链接的伪客户端(fake client)：因为 Redis 的命令智能在客户端上下文中执行，而载入AOF文件时锁使用的命令直接来源于AOF 文件而不是网络连接，所以服务器使用一个伪客户端来执行AOF文件保存的写命令 从AOF文件中分析并读取出一条写命令 使用伪客户端执行被读出的谢命令 一直执行步骤2和步骤3，直到AOF文件中的所有写命令都被处理完毕为止 AOF 重写 因为AOF持久化是通过保存被执行的写命令来记录数据库状态，随着服务器运行时间的流逝，AOF文件中的内容会越来越多。为了解决AOF文件体积膨胀的问题，Redis提供的AOF文件重写(rewrite)功能 AOF文件重写的实现 Redis 将生成新AOF文件替换旧AOF文件的功能命令为 “AOF文件重写”。但实际上，AOF文件重写并不需要对现有的AOF文件进行任何读取、分析或写入操作，是通过读取服务器当前的数据库状态来实现的 例如： 保存当前list键(含有6个value)的状态，须在AOF文件中写六条命令，如果服务器想尽量少的命令来记录list键的状态，最简单高效的办法不是去读取和分析现有AOF文件，而是直接从数据库中读取键list的值，用 一条 RPUSH key value... 来代替保存在AOF文件中的六条命令 这就是 AOF 重写功能的实现原理，因为新AOF文件只包含还原当前数据库状态所必须的命令，所以AOF文件不会浪费任何硬盘空间 在实际执行过程中，为了避免执行命令时造成客户端输入缓冲区溢出，会检查键锁包含的元素数理那个，如果数据超过 redis.h/REDIS_AOF_REWRITE_ITEMS_PRE_CMD 敞亮的值，那么重写程序将使用多条命令来记录键的值，而不单单使用一条命令 AOF后台重写 AOF重写程序 aof_rewrite 函数可以很好地完成创建一个新AOF文件的任务，但这个函数进行了大量的写入操作，所以调用这个函数的线程将会被长时间阻塞。将其放到子进程里执行可以达到两个目的： 子进程进行AOF重写期间，服务器进程(父进程)可以继续处理命令请求 子进程带有服务器进程的数据副本，使用子进程而不是线程，避免使用锁的情况下，保证数据安全性 子进程在处理AOF重写期间，服务器还需要继续处理命令请求，可能导致新的数据库状态与AOF文件所保存的数据库状态不一致 如图：新的AOF文件只保存了k1一个键的数据，而服务器数据库现在却有k1、k2、k3、k4 四个键 为了解决这种数据不一致的问题，Redis服务器设置了一个AOF重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用 当Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区 步骤如下： 执行客户端发来的命令 将执行后的写命令追加到AOF缓冲区 将执行后的写命令追加到AOF重写缓冲区 这样可以保证： AOF缓冲区的内容讲定期被写入和同步到AOF文件，对现有AOF文件的处理工作正常进行 从创建子进程开始，服务器执行的所有写命令都会被记录到AOF重写缓冲区里面 当子进程完成AOF重写工作之后，它会向父进程发送一个信号，父进程在接到该信号之后，会调用一个信号处理函数，并执行以下工作： 将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保存的数据库状态将和服务器当前的数据库状态一致 对新的AOF文件进行改名，原子地(atomic)覆盖现有的AOF文件，完成新旧两个AOF文件的替换 这个信号处理函数执行完毕之后，父进程就可以像往常一样接受命令请求 整个AOF后台重写过程，只有信号处理函数执行时会对服务器进程(父进程)造成阻塞，在其他时候，AOF后台重写都不会阻塞父进程 总结 AOF文件通过保存所有修改数据库的写命令请求来记录服务器的数据库状态 AOF文件中国暖的所有命令都以Redis命令请求协议的格式保存 命令请求会先保存到AOF缓冲区，之后再定期写入并同步到AOF文件 appendfsync选项的不同值对AOF持久化功能的安全性以及Redis服务器的性能有很大的影响 服务器只要载入并重新执行保存在AOF文件中的命令，就可以还原数据库状态 AOF重写可以产生一个新的AOF文件，新的AOF文件和原有的AOF文件所保存的数据库状态一样。但体积更小 AOF重写是有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无需对现有AOF文件进行任何读入、分析或写入操作 执行BGREWRITEAOF命令时，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，是的新旧两个AOF文件所保存的数据库状态一致。最后通过新的AOF替换旧的AOF文件，以此来完成AOF文件重写操作 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门4-RDB","slug":"Redis/Redis入门4-RDB","date":"2021-10-10T11:11:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/10/10/Redis/Redis入门4-RDB/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A84-RDB/","excerpt":"","text":"Redis 是一个键值对数据库服务器，服务器中通常包括含着任意个非空数据库，而每个非空数据库中又可以包含任意个键值对。将服务器中的非空数据库以及它们的键值对统称为数据库状态。下图展示包含三个非空数据库的Redis服务器。这三个数据库以及数据库中的键值对就是该服务器的数据库状态 Redis提供了RDB持久化功能，将某个时间点的数据库状态保存到一个RDB文件中。RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态 RDB文件的创建与载入 有两个Redis命令用于生成RDB文件 SAVE 命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不处理任何命令请求 BGSAVE 命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程(父进程)继续处理命令请求 创建RDB文件实际工作有 rdb.c/rdbSave 函数完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667int rdbSave(char *filename, rdbSaveInfo *rsi) &#123; char tmpfile[256]; char cwd[MAXPATHLEN]; /* Current working dir path for error messages. */ FILE *fp = NULL; rio rdb; int error = 0; snprintf(tmpfile,256,\"temp-%d.rdb\", (int) getpid()); fp = fopen(tmpfile,\"w\"); if (!fp) &#123; char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \"Failed opening the RDB file %s (in server root dir %s) \" \"for saving: %s\", filename, cwdp ? cwdp : \"unknown\", strerror(errno)); return C_ERR; &#125; rioInitWithFile(&amp;rdb,fp); startSaving(RDBFLAGS_NONE); if (server.rdb_save_incremental_fsync) rioSetAutoSync(&amp;rdb,REDIS_AUTOSYNC_BYTES); if (rdbSaveRio(&amp;rdb,&amp;error,RDBFLAGS_NONE,rsi) == C_ERR) &#123; errno = error; goto werr; &#125; /* Make sure data will not remain on the OS's output buffers */ if (fflush(fp)) goto werr; if (fsync(fileno(fp))) goto werr; if (fclose(fp)) &#123; fp = NULL; goto werr; &#125; fp = NULL; /* Use RENAME to make sure the DB file is changed atomically only * if the generate DB file is ok. */ if (rename(tmpfile,filename) == -1) &#123; char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \"Error moving temp DB file %s on the final \" \"destination %s (in server root dir %s): %s\", tmpfile, filename, cwdp ? cwdp : \"unknown\", strerror(errno)); unlink(tmpfile); stopSaving(0); return C_ERR; &#125; serverLog(LL_NOTICE,\"DB saved on disk\"); server.dirty = 0; server.lastsave = time(NULL); server.lastbgsave_status = C_OK; stopSaving(1); return C_OK;werr: serverLog(LL_WARNING,\"Write error saving DB on disk: %s\", strerror(errno)); if (fp) fclose(fp); unlink(tmpfile); stopSaving(0); return C_ERR;&#125; RDB文件的载入工作是在服务器启动时自动执行的，所有Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时检测到RDB文件存在，就会自动载入RDB文件 因为AOF文件的更新频率通常比RDB文件的更新频率高，所以： 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态 只有在AOF持久性功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态 载入RDB文件 实际工作由 rdb.c/rdbLoad函数完成 由于使用BGSAVE命令是由子进程执行的，Redis服务器仍然可以继续处理客户端的命令请求。但在BGSAVE命令执行期间，服务器处理SAVE、BGSAVE、GBREWRITEAOF 三个命令的方式会和平时有所不同 在 BGSAVE 期间，客户端发送的 SAVE 和 BGSAVE 命令会被服务器拒绝，避免父进程(服务器进程)和子进程同时执行两个rdbSave调用，防止发生竞争条件 BGREWRITEAOF 和 BGSAVE 两个命令不能同时执行： 如果BGSAVE 命令正在执行，那么客户端发送的 BGREWRITEAOF 命令 会被延迟到 BGSAVE 命令执行完毕之后执行 如果 BGREWIRTEAOF 命令正在执行，那么客户端发送的 BGSAVE 命令会被服务器拒绝 虽然 BGREWRITEAOF 和 BGSAVE 两个命令的实际工作都是由子进程执行，不让同时执行考虑到都同时执行大量的磁盘写入操作 RDB文件载入时的服务器会一致处于阻塞状态，直到载入工作完成为止 自动间隔性保存 saveparams 属性是一个数组，数组中的每个元素都是一个saveparam结构。通过配置save选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令 除了 saveparams 数组之外，服务器状态还维持着一个dirty计数器，以及一个lastsave属性 dirty 计数器记录距离上一次成功执行 SAVE 命令或者 BGSAVE 命令之后，服务器对数据库状态(服务器中的所有数据库) 进行了多少次修改(包括写入、删除、更新操作) lastsave 属性 是一个 UNIX 时间戳，记录了服务器上一次成功执行 SAVE 命令或 BGSAVE 命令的时间 12345678910//向服务器提供以下配置，只要满足以下三个任意条件， BGSAVE 命令就会被执行// 以下也是save 的默认条件save 900 1 save 300 10save 60 10000/*1. 服务器在900秒之内，对数据库进行了至少1次修改2. 服务器在300秒之内，对数据库进行了至少10次修改3. 服务器在60秒之内，对数据库进行了至少10000次修改*/ Redis的服务器周期性操作函数 serverCron 默认每隔 100ms 就会执行一次，其中一项工作就是检查 save 选洗那个锁设置的保存条件是否满足，满足则执行 BGSAVE 命令 当时间来到1378271101(1378270800 + 300 = 1378271100)，服务器将自动执行一次 BGSAVE 命令，假设BGSAVE 在执行5s之后完成，那么服务器状态将更新，其中 dirty 计数器已经被重置为0，而 lastsave 属性也被更新为 1378271106 以上就是Redis 服务器根据 save 选项 锁设置的保存条件，自动执行 BGSAVE 命令，进行剑歌行数据保存的实现原理 RDB文件结构 一个完整的RDB文件所包含的各个部分 RDB文件最开头是REDIS 部分，这个部分的长度是5字节，保存着 “REDIS” 五个字符，通过这五个字符，在载入文件时快速判断是否是RDB文件 db_version 长度为4字节，它的值是一个字符串表示的整数，这个整数记录了RDB文件的版本号，比如&quot;0006&quot;表示RDB文件的版本为第六版本 database 部分包含着0个或任意多个数据库，以及各个数据库中的键值对数据 如果服务器的数据库状态为空(所有数据库都是空的)，那么这个部分也为空，长度为0字节 EOF 常量长度为1字节，标志着RDB文件的正文内容的结束，当读入程序遇到这个值的时候，它直到所有数据库的所有键值载入完毕 check_sum 是一个8字节长的无符号整数，保存着一个校验和，这个校验和是通过对 REDIS、db_version、databases、EOF四个部分计算出来的 一个RDB文件的databases部分可以保存任意多个非空数据库 每个非空数据在RDB文件中都保存为 SELECTDB、db_number、key_value_pairs三个部分 SELECTDB 常量长度为1字节，当读入程序遇到这个值，直到接下来要读入的是一个数据库号码 db_number 保存着一个数据库号码，根据号码大量的不同，这个部分的长度可以是1字节、2字节或者5字节。当程序读入db_number部分之后，服务器会调用SELECT命令，根据读入的数据库号码进行数据库切换 db_number 长度不固定，怎么判断db_number已经读完了？ key_value_pairs 部分保存了数据库中的所有键值对数(如果键值对带有过期时间，那么过期时间也会和键值对保存在一起) 不带过期时间的键值对在RDB文件中由 TYPE、key、value 三部分组成，TYPE 记录了value的类型，长度为1字节 Key 总是一个字符串常量，长度不固定 根据 TYPE 类型不同，保存的内容长度不同，value 的结构和长度也会有所不同 带有过期时间的RDB文件结构 EXPIRETIME_MS: 常量的长度为1字节，告知接下来要读入的将是一个以毫秒为单位的过期时间 ms 是一个8字节长的带符号整数，记录一个以毫秒为单位的 UNIX 时间戳，即键值对的过期时间 总结 RDB文件用于保存和还原Redis服务器所有数据库中的所有键值对数据 SAVE 命令由服务器进程执行会阻塞服务器，BGSAVE 由子进程执行保存操作，不会阻塞服务器 RDB文件是一个经过压缩的二进制文件，由多个部分组成 对于不同类型的键值对，RDB文件会使用不同的方式保存它们 BGSAVE 保存过程中，产生的新数据是如何保存的？ 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门3-数据库","slug":"Redis/Redis入门3-数据库","date":"2021-10-10T08:45:01.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/10/10/Redis/Redis入门3-数据库/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A83-%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"服务器中的数据库 Redis服务器将所有数据库都保存在redis.h/redisServer 结构的 db数组 中，每个redisDb结构代表一个数据库 1234567891011121314151617181920212223//Redis服务结构struct redisServer &#123; //... //一个数组，保存着服务器中所有的数据库 redisDb *db; //初始化时候 根据dbnum 决定创建多少个数据库 int dbnum; //...&#125;//单个数据库结构typedef struct redisDb &#123; dict *dict; /* The keyspace for this DB */ dict *expires; /* Timeout of keys with a timeout set */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP)*/ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */ unsigned long expires_cursor; /* Cursor of the active expire cycle. */ list *defrag_later; /* List of key names to attempt to defrag one by one, gradually. */&#125; redisDb; 默认情况下，会创建 dbnum=16 个数据库 每个Redis客户端都有自己的目标数据库，默认情况下，Redis客户端的目标数据库为0号数据库 不同的客户端是怎么选定目标数据库的？不同数据库之间又不会进行数据同步，都用同一个数据库其他不就没有用了 可以通过 SELECT N 来切换目标数据库 数据库健空间 redisDb结构的dict 字典保存了数据数据库中的所有键值对，称这个字典为键空间(key space) 键空间的键也就是数据库的键，每个键都是一个字符串对象 键空间的值也就是数据库的值，每个值可以是字符串对象、列表对象、哈希表对象、集合对象和有序集合对洗那个中的任意一种Redis对象 因为键空间是一个字段，所以所有针对数据库的实际都是对键空间字段进行操作来实现的 Redis 键空间是怎么解决Hash冲突的 除了读写还有一些维护操作 读取一个值之后，服务器会更新键的LRU(最近一次使用)时间，可以用来计算键的闲置时间。使用 Object idletime key 来查看键的空闲时间 如果服务器读取键已经过期，则会先删除这个键再进行其他操作 如果客户端使用WATCH 命令监视某个键，当键被修改，服务器会将键标记为脏(ditry)，从而让事务直到这个键被修改 服务器每次修改一个键之后，都会对脏(dirty)键计数器的值增1，这个计数器会触发服务器的持久化以及复制操作 如果服务器开启了数据库通知功能，那么对键修改后，会根据配置发送相应的数据库通知 键的生存时间与过期时间 redisDb结构的expires字典保存了数据库中所有键的过期时间，称为字典的过期字典 过期字典的键是一个指针，指针指向键空间中国暖的某个键对象(即是某个数据库键) 过期字典的值是一个long long 类型的整数，这个整数保存了键所有指向数据库键的过期时间–一个毫秒精度的unix 时间戳 过期键的判断 通过过期字典，程序可以通过以下步骤检查一个给定键是否过期 检查给顶键是否存在与过期字典；如果存在，那么取得键的过期时间 检查当前UNIX时间戳是否大于键的过期时间 过期键删除策略 数据键的过期时间都保存在过期字典中，如果一个键过期了，那么它是什么时候被删除的，可能存在三种不同的删除策略 定时删除：在设置键的过期时间的同时，创建一个定时器(timer)，让定时器在键的过期时间来临时，立即执行对键的删除操作 惰性删除：放任键过期不管，但每次从键空间中获取键时，都检查取得的键是否过期。过期删除，否则返回该键 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定 定时删除 定时删除策略对内存是最友好的：通过使用定时器，定时删除策略可以保证过期键会尽可能快的删除，并释放过期键锁占用的内存 缺点： 对CPU时间是最不友好的：在过期键比较多的情况下，删除过期键可能会占用相当一部分CPU时间，在内存不紧张但是CPU时间非常紧张的情况下，将CPU时间用在删除和当前任务无关的过期键上。五一对服务器的响应时间和吞吐量造成影响 创建一个定时器需要用到Redis服务器中的时间事件，而当前时间事件的实现方式–无序链表，查找一个事件的时间复杂度为O(N)–并不能高效地处理大量时间事件 惰性删除 惰性删除对CPU时间是最友好的：程序只会在取出键时才对键进行过期检查 缺点：对内存是最不友好的，如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期键不被删除，它锁占用的内存就不会释放 定期删除 上面的定时删除和惰性删除在单一使用时都有明显的缺陷： 定时删除占用太多CPU时间，影响服务器的响应时间和吞吐量 惰性删除浪费太多内存，有内存泄露的危险 定期删除策略是前两种策略的一种整合和折中：每隔一段时间执行一次删除过期键的操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响 问题是：如何设置定期删除的时间间隔，以及删除操作的执行时长 如果删除太频繁或者执行时间太长，定期删除策略就会退化成定时删除策略，以至于将CPU过多的浪费在删除过期键上 如果删除操作执行的太少或时间太短，定期删除策略又会和惰性删除策略一样，出现浪费内存的情况 Redis 的过期删除策略 Redis 实际使用的是 惰性删除和定期删除两种策略 惰性删除策略：所有读写的Redis命令在执行前都调用 expireIfNeeded 函数对键进行检查，如果键过期，函数将键从数据库删除 1234567891011121314151617181920212223int expireIfNeeded(redisDb *db, robj *key) &#123; if (!keyIsExpired(db,key)) return 0; /* If we are running in the context of a slave, instead of * evicting the expired key from the database, we return ASAP: * the slave key expiration is controlled by the master that will * send us synthesized DEL operations for expired keys. * * Still we try to return the right information to the caller, * that is, 0 if we think the key should be still valid, 1 if * we think the key is expired at this time. */ if (server.masterhost != NULL) return 1; /* If clients are paused, we keep the current dataset constant, * but return to the client what we believe is the right state. Typically, * at the end of the pause we will properly expire the key OR we will * have failed over and the new primary will send us the expire. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1;&#125; 注意上面的 server.masterhost != NULL 如果不为空，说明当前服务器为备机。那么即使当前键是过期的，也仅仅返回后状态1，而不会进行下面的删除操作 定时删除策略：每当Redis的服务器周期性操作 serverCron 函数执行时，activeExpireCycle 函数就会被调用，在规定时间内，分多次遍历服务器中各个数据库，从数据库的 expires 字典中随机检查一部分键的过期时间，并删除其中的过期键 函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查，并删除其中的过期键 全局变量 current_db记录当前 activeExpireCycle函数检查的进度，并在下一次 activeExpireCycle 函数调用时，接着上一次的进度进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206void activeExpireCycle(int type) &#123; /* Adjust the running parameters according to the configured expire * effort. The default effort is 1, and the maximum configurable effort * is 10. */ unsigned long effort = server.active_expire_effort-1, /* Rescale from 0 to 9. */ config_keys_per_loop = ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP + ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP/4*effort, config_cycle_fast_duration = ACTIVE_EXPIRE_CYCLE_FAST_DURATION + ACTIVE_EXPIRE_CYCLE_FAST_DURATION/4*effort, config_cycle_slow_time_perc = ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC + 2*effort, config_cycle_acceptable_stale = ACTIVE_EXPIRE_CYCLE_ACCEPTABLE_STALE- effort; /* This function has some global state in order to continue the work * incrementally across calls. */ static unsigned int current_db = 0; /* Next DB to test. */ static int timelimit_exit = 0; /* Time limit hit in previous call? */ static long long last_fast_cycle = 0; /* When last fast cycle ran. */ int j, iteration = 0; int dbs_per_call = CRON_DBS_PER_CALL; long long start = ustime(), timelimit, elapsed; /* When clients are paused the dataset should be static not just from the * POV of clients not being able to write, but also from the POV of * expires and evictions of keys not being performed. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return; if (type == ACTIVE_EXPIRE_CYCLE_FAST) &#123; /* Don't start a fast cycle if the previous cycle did not exit * for time limit, unless the percentage of estimated stale keys is * too high. Also never repeat a fast cycle for the same period * as the fast cycle total duration itself. */ if (!timelimit_exit &amp;&amp; server.stat_expired_stale_perc &lt; config_cycle_acceptable_stale) return; if (start &lt; last_fast_cycle + (long long)config_cycle_fast_duration*2) return; last_fast_cycle = start; &#125; /* We usually should test CRON_DBS_PER_CALL per iteration, with * two exceptions: * * 1) Don't test more DBs than we have. * 2) If last time we hit the time limit, we want to scan all DBs * in this iteration, as there is work to do in some DB and we don't want * expired keys to use memory for too much time. */ if (dbs_per_call &gt; server.dbnum || timelimit_exit) dbs_per_call = server.dbnum; /* We can use at max 'config_cycle_slow_time_perc' percentage of CPU * time per iteration. Since this function gets called with a frequency of * server.hz times per second, the following is the max amount of * microseconds we can spend in this function. */ timelimit = config_cycle_slow_time_perc*1000000/server.hz/100; timelimit_exit = 0; if (timelimit &lt;= 0) timelimit = 1; if (type == ACTIVE_EXPIRE_CYCLE_FAST) timelimit = config_cycle_fast_duration; /* in microseconds. */ /* Accumulate some global stats as we expire keys, to have some idea * about the number of keys that are already logically expired, but still * existing inside the database. */ long total_sampled = 0; long total_expired = 0; for (j = 0; j &lt; dbs_per_call &amp;&amp; timelimit_exit == 0; j++) &#123; /* Expired and checked in a single loop. */ unsigned long expired, sampled; redisDb *db = server.db+(current_db % server.dbnum); /* Increment the DB now so we are sure if we run out of time * in the current DB we'll restart from the next. This allows to * distribute the time evenly across DBs. */ current_db++; /* Continue to expire if at the end of the cycle there are still * a big percentage of keys to expire, compared to the number of keys * we scanned. The percentage, stored in config_cycle_acceptable_stale * is not fixed, but depends on the Redis configured \"expire effort\". */ do &#123; unsigned long num, slots; long long now, ttl_sum; int ttl_samples; iteration++; /* If there is nothing to expire try next DB ASAP. */ if ((num = dictSize(db-&gt;expires)) == 0) &#123; db-&gt;avg_ttl = 0; break; &#125; slots = dictSlots(db-&gt;expires); now = mstime(); /* When there are less than 1% filled slots, sampling the key * space is expensive, so stop here waiting for better times... * The dictionary will be resized asap. */ if (slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (num*100/slots &lt; 1)) break; /* The main collection cycle. Sample random keys among keys * with an expire set, checking for expired ones. */ expired = 0; sampled = 0; ttl_sum = 0; ttl_samples = 0; if (num &gt; config_keys_per_loop) num = config_keys_per_loop; /* Here we access the low level representation of the hash table * for speed concerns: this makes this code coupled with dict.c, * but it hardly changed in ten years. * * Note that certain places of the hash table may be empty, * so we want also a stop condition about the number of * buckets that we scanned. However scanning for free buckets * is very fast: we are in the cache line scanning a sequential * array of NULL pointers, so we can scan a lot more buckets * than keys in the same time. */ long max_buckets = num*20; long checked_buckets = 0; while (sampled &lt; num &amp;&amp; checked_buckets &lt; max_buckets) &#123; for (int table = 0; table &lt; 2; table++) &#123; if (table == 1 &amp;&amp; !dictIsRehashing(db-&gt;expires)) break; unsigned long idx = db-&gt;expires_cursor; idx &amp;= db-&gt;expires-&gt;ht[table].sizemask; dictEntry *de = db-&gt;expires-&gt;ht[table].table[idx]; long long ttl; /* Scan the current bucket of the current table. */ checked_buckets++; while(de) &#123; /* Get the next entry now since this entry may get * deleted. */ dictEntry *e = de; de = de-&gt;next; ttl = dictGetSignedIntegerVal(e)-now; if (activeExpireCycleTryExpire(db,e,now)) expired++; if (ttl &gt; 0) &#123; /* We want the average TTL of keys yet * not expired. */ ttl_sum += ttl; ttl_samples++; &#125; sampled++; &#125; &#125; db-&gt;expires_cursor++; &#125; total_expired += expired; total_sampled += sampled; /* Update the average TTL stats for this database. */ if (ttl_samples) &#123; long long avg_ttl = ttl_sum/ttl_samples; /* Do a simple running average with a few samples. * We just use the current estimate with a weight of 2% * and the previous estimate with a weight of 98%. */ if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl; db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50); &#125; /* We can't block forever here even if there are many keys to * expire. So after a given amount of milliseconds return to the * caller waiting for the other active expire cycle. */ if ((iteration &amp; 0xf) == 0) &#123; /* check once every 16 iterations. */ elapsed = ustime()-start; if (elapsed &gt; timelimit) &#123; timelimit_exit = 1; server.stat_expired_time_cap_reached_count++; break; &#125; &#125; /* We don't repeat the cycle for the current database if there are * an acceptable amount of stale keys (logically expired but yet * not reclaimed). */ &#125; while (sampled == 0 || (expired*100/sampled) &gt; config_cycle_acceptable_stale); &#125; elapsed = ustime()-start; server.stat_expire_cycle_time_used += elapsed; latencyAddSampleIfNeeded(\"expire-cycle\",elapsed/1000); /* Update our estimate of keys existing but yet to be expired. * Running average with this sample accounting for 5%. */ double current_perc; if (total_sampled) &#123; current_perc = (double)total_expired/total_sampled; &#125; else current_perc = 0; server.stat_expired_stale_perc = (current_perc*0.05)+ (server.stat_expired_stale_perc*0.95);&#125; AOF、RDB和复制功能对过期键的处理 生成RDB文件：执行 SAVE 或 BGSAVE 创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中 载入RDB文件：在启动Redis服务器是，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入 如果服务器是以主服务器模式运行，那么载入RDB文件时，程序会对文件中保存键进行检查，过期键将被忽略。 如果服务器是已从服务器模式运行，那么载入RDB文件时，所有键不论是否过期，都将被载入到数据库中。因为主从服务器在进行数据同步时候，从服务器的数据库就会被清空，过期键对载入RDB文件的从服务器不会造成影响 AOF文件写入：当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但还没有被惰性或定期删除，AOF文件不会产生任何变化。如果被删除策略删除后，程序会向AOF文件追加AOF文件一个DEL命令，来显示的记录该键被删除 AOF重写：在执行AOF重写过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中 复制：当服务器运行在复制模式下，从服务器的过期键删除动作由主服务器控制 主服务器删除一个过期键之后，会限制地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键 从服务器在执行客户端发送的读命令式，即使碰到过期键页不会将过期键删除，而是继续将处理未过期的键一样来处理过期键 从服务器只有在街道主服务器发来的DEL命令之后，才会删除过期键 12345678910111213141516171819202122232425262728293031robj *lookupKeyReadWithFlags(redisDb *db, robj *key, int flags) &#123; robj *val; if (expireIfNeeded(db,key) == 1) &#123; /* If we are in the context of a master, expireIfNeeded() returns 1 * when the key is no longer valid, so we can return NULL ASAP. */ if (server.masterhost == NULL) goto keymiss; //如果备机是只读模式，那么返回的就是空值 if (server.current_client &amp;&amp; server.current_client != server.master &amp;&amp; server.current_client-&gt;cmd &amp;&amp; server.current_client-&gt;cmd-&gt;flags &amp; CMD_READONLY) &#123; goto keymiss; &#125; &#125; val = lookupKey(db,key,flags); if (val == NULL) goto keymiss; server.stat_keyspace_hits++; return val;keymiss: if (!(flags &amp; LOOKUP_NONOTIFY)) &#123; notifyKeyspaceEvent(NOTIFY_KEY_MISS, \"keymiss\", key, db-&gt;id); &#125; server.stat_keyspace_misses++; return NULL;&#125; 如果从服务器是只读模式，那么返回的就是空值 否则即使键值已经过期，但是从服务器仍然能够返回该值 总结 Redis服务器所有数据库 都保存在 redisServer.db数组中，而数据库的数量则由 redisServer.dbnum属性保存 客户端通过修改目标数据库指针，让它指向redisServer.db数组中的不同元素来切换不同的数据库 数据库主要有dict 和 expires 两个字段构成，其中 dict字典负责保存键值对，而 expires 字典则负责保存键的过期时间 数据库的键总是一个字符串对象，而值则可以是任意一种Redis对象类型 expires 字典的键指向数据库中的某个键，而值则记录了数据库键的过期时间，过期时间是一个以毫秒为单位的UNIX时间戳 Redis 使用惰性删除和定期删除两种策略来删除过期的键 当一个过期键被删除之后，服务器会追加一条DEL命令到现在AOF文件的末尾，显示地删除过期键。从服务器即使发现过期键也不会主动删除它，而是等待主节点发来DEL命令 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门2-结构对象","slug":"Redis/Redis入门2-结构对象","date":"2021-09-12T10:34:16.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/09/12/Redis/Redis入门2-结构对象/","link":"","permalink":"http://xboom.github.io/2021/09/12/Redis/Redis%E5%85%A5%E9%97%A82-%E7%BB%93%E6%9E%84%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Redis 底层数据结构，比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合，等等。 Redis 并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象，每种对象都用到了至少一种前面所介绍的数据结构。 其中最新的版本中数据结构又进行了优化： Redis 的对象系统还实现了基于引用计数技术的内存回收机制：当程序不再使用某个对象的时候，对象所占用的内存就会被自动释放； Redis 还通过引用计数技术实现了对象共享机制，这一机制可以在适当的条件下，通过让多个数据库键共享同一个对象来节约内存。 最后，Redis 的对象带有访问时间记录信息，该信息可以用于计算数据库键的空转时长，在服务器启用了 maxmemory 功能的情况下，空转时长较大的那些键可能会优先被服务器删除。 对象的类型与编码 Redis 使用对象来表示数据库中的键和值，每当在 Redis 的数据库中新创建一个键值对时，至少会创建两个对象，一个对象用作键值对的键（键对象），另一个对象用作键值对的值（值对象）。 举个例子，以下 SET 命令在数据库中创建了一个新的键值对，其中键值对的键是一个包含了字符串值 &quot;msg&quot; 的对象，而键值对的值则是一个包含了字符串值 &quot;hello world&quot; 的对象： 12redis&gt; SET msg \"hello world\"OK Redis 中的每个对象都由一个 redisObject 结构表示，该结构中和保存数据有关的三个属性分别是 type 属性、 encoding 属性和 ptr 属性： 1234567891011121314typedef struct redisObject &#123; // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 指向底层实现数据结构的指针 void *ptr; // ... &#125; robj; 类型 对象的 type 属性记录了对象的类型，这个属性的值可以是表中列出的常量的其中一个。 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 对于 Redis 数据库保存的键值对来说，键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中一种，因此： 当我们称呼一个数据库键为“字符串键”时，我们指的是“这个数据库键所对应的值为字符串对象”； 当我们称呼一个键为“列表键”时，我们指的是“这个数据库键所对应的值为列表对象”， 诸如此类。 TYPE 命令的实现也与此类似，对一个数据库键执行 TYPE 命令时，返回的结果为数据库键对应的值对象的类型，而不是键对象的类型： 12345678910111213141516171819202122232425262728293031323334# 键为字符串对象，值为字符串对象redis&gt; SET msg \"hello world\"OK redis&gt; TYPE msgstring # 键为字符串对象，值为列表对象redis&gt; RPUSH numbers 1 3 5(integer) 6 redis&gt; TYPE numberslist # 键为字符串对象，值为哈希对象redis&gt; HMSET profile name Tome age 25 career ProgrammerOK redis&gt; TYPE profilehash # 键为字符串对象，值为集合对象redis&gt; SADD fruits apple banana cherry(integer) 3 redis&gt; TYPE fruitsset # 键为字符串对象，值为有序集合对象redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 redis&gt; TYPE pricezset TYPE 命令在面对不同类型的值对象时所产生的输出: 对象 对象 type 属性的值 TYPE 命令的输出 字符串对象 REDIS_STRING &quot;string&quot; 列表对象 REDIS_LIST &quot;list&quot; 哈希对象 REDIS_HASH &quot;hash&quot; 集合对象 REDIS_SET &quot;set&quot; 有序集合对象 REDIS_ZSET &quot;zset&quot; 编码和底层实现 对象的 ptr 指针指向对象的底层实现数据结构，而这些数据结构由对象的 encoding 属性决定。 encoding 属性记录了对象所使用的编码，即这个对象使用了什么数据结构作为对象的底层实现，这个属性的值以下常量的其中一个。 编码常量 编码所对应的底层数据结构 REDIS_ENCODING_INT long 类型的整数 REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 每种类型的对象都至少使用了两种不同的编码，每种类型的对象可以使用的编码: REDIS_STRING REDIS_ENCODING_INT 使用整数值实现的字符串对象。 REDIS_STRING REDIS_ENCODING_EMBSTR 使用 embstr 编码的简单动态字符串实现的字符串对象。 REDIS_STRING REDIS_ENCODING_RAW 使用简单动态字符串实现的字符串对象。 REDIS_LIST REDIS_ENCODING_ZIPLIST 使用压缩列表实现的列表对象。 REDIS_LIST REDIS_ENCODING_LINKEDLIST 使用双端链表实现的列表对象。 REDIS_HASH REDIS_ENCODING_ZIPLIST 使用压缩列表实现的哈希对象。 REDIS_HASH REDIS_ENCODING_HT 使用字典实现的哈希对象。 REDIS_SET REDIS_ENCODING_INTSET 使用整数集合实现的集合对象。 REDIS_SET REDIS_ENCODING_HT 使用字典实现的集合对象。 REDIS_ZSET REDIS_ENCODING_ZIPLIST 使用压缩列表实现的有序集合对象。 REDIS_ZSET REDIS_ENCODING_SKIPLIST 使用跳跃表和字典实现的有序集合对象。 使用 OBJECT ENCODING 命令可以查看一个数据库键的值对象的编码： 1234567891011121314151617181920212223redis&gt; SET msg \"hello wrold\"OK redis&gt; OBJECT ENCODING msg\"embstr\" redis&gt; SET story \"long long long long long long ago ...\"OK redis&gt; OBJECT ENCODING story\"raw\" redis&gt; SADD numbers 1 3 5(integer) 3 redis&gt; OBJECT ENCODING numbers\"intset\" redis&gt; SADD numbers \"seven\"(integer) 1 redis&gt; OBJECT ENCODING numbers\"hashtable\" OBJECT ENCODING 对不同编码的输出 对象所使用的底层数据结构 编码常量 OBJECT ENCODING 命令输出 整数 REDIS_ENCODING_INT &quot;int&quot; embstr 编码的简单动态字符串（SDS） REDIS_ENCODING_EMBSTR &quot;embstr&quot; 简单动态字符串 REDIS_ENCODING_RAW &quot;raw&quot; 字典 REDIS_ENCODING_HT &quot;hashtable&quot; 双端链表 REDIS_ENCODING_LINKEDLIST &quot;linkedlist&quot; 压缩列表 REDIS_ENCODING_ZIPLIST &quot;ziplist&quot; 整数集合 REDIS_ENCODING_INTSET &quot;intset&quot; 跳跃表和字典 REDIS_ENCODING_SKIPLIST &quot;skiplist&quot; 字符串对象 字符串对象的编码可以是 int 、 raw 或者 embstr 。 如果一个字符串对象保存的是整数值，并且这个整数值可以用 long 类型来表示，那么字符串对象会将整数值保存在字符串对象结构的 ptr 属性里面（将 void* 转换成 long ），并将字符串对象的编码设置为 int 。 举个例子，如果执行以下 SET 命令，那么服务器将创建一个如图 8-1 所示的 int 编码的字符串对象作为 number 键的值： 12345redis&gt; SET number 10086OK redis&gt; OBJECT ENCODING number\"int\" 如果字符串对象保存的是一个字符串值，并且这个字符串值的长度大于 39 字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为 raw 。 举个例子，如果我们执行以下命令，那么服务器将创建一个如图 8-2 所示的 raw 编码的字符串对象作为 story 键的值： 12345678redis&gt; SET story \"Long, long, long ago there lived a king ...\"OK redis&gt; STRLEN story(integer) 43 redis&gt; OBJECT ENCODING story\"raw\" 如果字符串对象保存的是一个字符串值，且这个字符串值的长度小于等于 39 字节，字符串对象将使用 embstr 编码的方式来保存这个字符串值。 embstr 编码是专门用于保存短字符串的一种优化编码方式，这种编码和 raw 编码一样，都使用 redisObject 结构和 sdshdr 结构来表示字符串对象，但 raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构，而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含 redisObject 和 sdshdr 两个结构，如图 8-3 所示 embstr 编码的字符串对象在执行命令时，产生的效果和 raw 编码的字符串对象执行命令时产生的效果是相同的，但使用 embstr 编码的字符串对象来保存短字符串值有以下好处： embstr 编码将创建字符串对象所需的内存分配次数从 raw 编码的两次降低为一次。 释放 embstr 编码的字符串对象只需要调用一次内存释放函数，而释放 raw 编码的字符串对象需要调用两次内存释放函数。 因为 embstr 编码的字符串对象的所有数据都保存在一块连续的内存里面，所以这种编码的字符串对象比起 raw 编码的字符串对象能够更好地利用缓存带来的优势。作为例子，以下命令创建了一个 embstr 编码的字符串对象作为 msg 键的值，值对象的样子如图 8-4 所示： 12345redis&gt; SET msg \"hello\"OK redis&gt; OBJECT ENCODING msg\"embstr\" 最后要说的是，可以用 long double 类型表示的浮点数在 Redis 中也是作为字符串值来保存的：如果我们要保存一个浮点数到字符串对象里面，那么程序会先将这个浮点数转换成字符串值，然后再保存起转换所得的字符串值。 举个例子，执行以下代码将创建一个包含 3.14 的字符串表示 &quot;3.14&quot; 的字符串对象： 12345redis&gt; SET pi 3.14OK redis&gt; OBJECT ENCODING pi\"embstr\" 在有需要的时候，程序会将保存在字符串对象里面的字符串值转换回浮点数值，执行某些操作，然后再将执行操作所得的浮点数值转换回字符串值，并继续保存在字符串对象里面。 举个例子，如果我们执行以下代码的话： 12345redis&gt; INCRBYFLOAT pi 2.0\"5.14\" redis&gt; OBJECT ENCODING pi\"embstr\" 那么程序首先会取出字符串对象里面保存的字符串值 &quot;3.14&quot; ，将它转换回浮点数值 3.14 ，然后把 3.14 和 2.0 相加得出的值 5.14 转换成字符串 &quot;5.14&quot; ，并将这个 &quot;5.14&quot; 保存到字符串对象里面。 总结并列出了字符串对象保存各种不同类型的值所使用的编码方式 值 编码 可以用 long 类型保存的整数。 int 可以用 long double 类型保存的浮点数。 embstr 或者 raw 字符串值，或者因为长度太大而没办法用 long 类型表示的整数，又或者因为长度太大而没办法用 long double 类型表示的浮点数。 embstr 或者 raw 编码的转换 int 编码的字符串对象和 embstr 编码的字符串对象在条件满足的情况下，会被转换为 raw 编码的字符串对象。 对于 int 编码的字符串对象来说，如果我们向对象执行了一些命令，使得这个对象保存的不再是整数值，而是一个字符串值，那么字符串对象的编码将从 int 变为 raw 。 在下面的示例中，我们通过 APPEND 命令，向一个保存整数值的字符串对象追加了一个字符串值，因为追加操作只能对字符串值执行，所以程序会先将之前保存的整数值 10086 转换为字符串值 &quot;10086&quot; ，然后再执行追加操作，操作的执行结果就是一个 raw 编码的、保存了字符串值的字符串对象： 1234567891011121314redis&gt; SET number 10086OK redis&gt; OBJECT ENCODING number\"int\" redis&gt; APPEND number \" is a good number!\"(integer) 23 redis&gt; GET number\"10086 is a good number!\" redis&gt; OBJECT ENCODING number\"raw\" 另外，因为 Redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序（只有 int 编码的字符串对象和 raw 编码的字符串对象有这些程序），所以 embstr 编码的字符串对象实际上是只读的：当我们对 embstr 编码的字符串对象执行任何修改命令时，程序会先将对象的编码从 embstr 转换成 raw ，然后再执行修改命令；因为这个原因，embstr 编码的字符串对象在执行修改命令之后，总会变成一个 raw 编码的字符串对象。 以下代码展示了一个 embstr 编码的字符串对象在执行 APPEND 命令之后，对象的编码从 embstr 变为 raw 的例子： 1234567891011redis&gt; SET msg \"hello world\"OK redis&gt; OBJECT ENCODING msg\"embstr\" redis&gt; APPEND msg \" again!\"(integer) 18 redis&gt; OBJECT ENCODING msg\"raw\" 字符串命令的实现 因为字符串键的值为字符串对象，所以用于字符串键的所有命令都是针对字符串对象来构建的， 列举部分字符串命令，以及这些命令在不同编码的字符串对象下的实现方法。 命令 int 编码的实现方法 embstr 编码的实现方法 raw 编码的实现方法 SET 使用 int 编码保存值。 使用 embstr 编码保存值。 使用 raw 编码保存值。 GET 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，然后向客户端返回这个字符串值。 直接向客户端返回字符串值。 直接向客户端返回字符串值。 APPEND 将对象转换成 raw 编码，然后按 raw编码的方式执行此操作。 将对象转换成 raw 编码，然后按 raw编码的方式执行此操作。 调用 sdscatlen 函数，将给定字符串追加到现有字符串的末尾。 INCRBYFLOAT 取出整数值并将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。 取出字符串值并尝试将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。如果字符串值不能被转换成浮点数，那么向客户端返回一个错误。 取出字符串值并尝试将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。如果字符串值不能被转换成浮点数，那么向客户端返回一个错误。 INCRBY 对整数值进行加法计算，得出的计算结果会作为整数被保存起来。 embstr 编码不能执行此命令，向客户端返回一个错误。 raw 编码不能执行此命令，向客户端返回一个错误。 DECRBY 对整数值进行减法计算，得出的计算结果会作为整数被保存起来。 embstr 编码不能执行此命令，向客户端返回一个错误。 raw 编码不能执行此命令，向客户端返回一个错误。 STRLEN 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，计算并返回这个字符串值的长度。 调用 sdslen 函数，返回字符串的长度。 调用 sdslen 函数，返回字符串的长度。 SETRANGE 将对象转换成 raw 编码，然后按 raw编码的方式执行此命令。 将对象转换成 raw 编码，然后按 raw编码的方式执行此命令。 将字符串特定索引上的值设置为给定的字符。 GETRANGE 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，然后取出并返回字符串指定索引上的字符。 直接取出并返回字符串指定索引上的字符。 直接取出并返回字符串指定索引上的字符。 列表对象 列表对象的编码可以是 ziplist 或者 linkedlist 。 ziplist 编码的列表对象使用压缩列表作为底层实现，每个压缩列表节点（entry）保存了一个列表元素。 举个例子，如果执行以下 RPUSH 命令，那么服务器将创建一个列表对象作为 numbers 键的值： 12redis&gt; RPUSH numbers 1 \"three\" 5(integer) 3 如果 numbers 键的值对象使用的是 ziplist 编码，这个这个值对象将会是图 8-5 所展示的样子。 另一方面，linkedlist 编码的列表对象使用双端链表作为底层实现，每个双端链表节点（node）都保存了一个字符串对象，而每个字符串对象都保存了一个列表元素。 举个例子，如果前面所说的 numbers 键创建的列表对象使用的不是 ziplist 编码，而是 linkedlist 编码，那么 numbers 键的值对象将是图 8-6 所示的样子 注意 linkedlist 编码的列表对象在底层的双端链表结构中包含了多个字符串对象，这种嵌套字符串对象的行为在稍后介绍的哈希对象、集合对象和有序集合对象中都会出现，字符串对象是 Redis 五种类型的对象中唯一一种会被其他四种类型对象嵌套的对象。 为了简化字符串对象的表示，在图 8-6 使用了一个带有 StringObject 字样的格子来表示一个字符串对象，而 StringObject 字样下面的是字符串对象所保存的值。 比如说，图 8-7 代表的就是一个包含了字符串值 &quot;three&quot; 的字符串对象，它是 8-8 的简化表示。 编码的转换 当列表对象可以同时满足以下两个条件时，列表对象使用 ziplist 编码： 列表对象保存的所有字符串元素的长度都小于 64 字节； 列表对象保存的元素数量小于 512 个；不能满足这两个条件的列表对象需要使用 linkedlist 编码。 以上两个条件的上限值是可以修改的，具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明 对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在压缩列表里的所有列表元素都会被转移并保存到双端链表里面，对象的编码也会从 ziplist 变为 linkedlist 。 会转换回来吗？ 以下代码展示了列表对象因为保存了长度太大的元素而进行编码转换的情况： 1234567891011121314# 所有元素的长度都小于 64 字节redis&gt; RPUSH blah \"hello\" \"world\" \"again\"(integer) 3 redis&gt; OBJECT ENCODING blah\"ziplist\" # 将一个 65 字节长的元素推入列表对象中redis&gt; RPUSH blah \"wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\"(integer) 4 # 编码已改变redis&gt; OBJECT ENCODING blah\"linkedlist\" 除此之外，以下代码展示了列表对象因为保存的元素数量过多而进行编码转换的情况： 1234567891011121314151617# 列表对象包含 512 个元素redis&gt; EVAL \"for i=1,512 do redis.call('RPUSH', KEYS[1], i) end\" 1 \"integers\"(nil) redis&gt; LLEN integers(integer) 512 redis&gt; OBJECT ENCODING integers\"ziplist\" # 再向列表对象推入一个新元素，使得对象保存的元素数量达到 513 个redis&gt; RPUSH integers 513(integer) 513 # 编码已改变redis&gt; OBJECT ENCODING integers\"linkedlist\" 列表命令的实现 因为列表键的值为列表对象，所以用于列表键的所有命令都是针对列表对象来构建的，表 8-8 列出了其中一部分列表键命令，以及这些命令在不同编码的列表对象下的实现方法 命令 ziplist 编码的实现方法 linkedlist 编码的实现方法 LPUSH 调用 ziplistPush 函数，将新元素推入到压缩列表的表头。 调用 listAddNodeHead 函数，将新元素推入到双端链表的表头。 RPUSH 调用 ziplistPush 函数，将新元素推入到压缩列表的表尾。 调用 listAddNodeTail 函数，将新元素推入到双端链表的表尾。 LPOP 调用 ziplistIndex 函数定位压缩列表的表头节点，在向用户返回节点所保存的元素之后，调用 ziplistDelete 函数删除表头节点。 调用 listFirst 函数定位双端链表的表头节点，在向用户返回节点所保存的元素之后，调用 listDelNode 函数删除表头节点。 RPOP 调用 ziplistIndex 函数定位压缩列表的表尾节点，在向用户返回节点所保存的元素之后，调用 ziplistDelete 函数删除表尾节点。 调用 listLast 函数定位双端链表的表尾节点，在向用户返回节点所保存的元素之后，调用 listDelNode 函数删除表尾节点。 LINDEX 调用 ziplistIndex 函数定位压缩列表中的指定节点，然后返回节点所保存的元素。 调用 listIndex 函数定位双端链表中的指定节点，然后返回节点所保存的元素。 LLEN 调用 ziplistLen 函数返回压缩列表的长度。 调用 listLength 函数返回双端链表的长度。 LINSERT 插入新节点到压缩列表的表头或者表尾时，使用 ziplistPush 函数；插入新节点到压缩列表的其他位置时，使用 ziplistInsert 函数。 调用 listInsertNode 函数，将新节点插入到双端链表的指定位置。 LREM 遍历压缩列表节点，并调用 ziplistDelete 函数删除包含了给定元素的节点。 遍历双端链表节点，并调用 listDelNode 函数删除包含了给定元素的节点。 LTRIM 调用 ziplistDeleteRange 函数，删除压缩列表中所有不在指定索引范围内的节点。 遍历双端链表节点，并调用 listDelNode 函数删除链表中所有不在指定索引范围内的节点。 LSET 调用 ziplistDelete 函数，先删除压缩列表指定索引上的现有节点，然后调用 ziplistInsert 函数，将一个包含给定元素的新节点插入到相同索引上面。 调用 listIndex 函数，定位到双端链表指定索引上的节点，然后通过赋值操作更新节点的值。 哈希对象 哈希对象的编码可以是 ziplist 或者 hashtable 。 ziplist 编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此： 保存了同一键值对的两个节点总是紧挨在一起，保存键的节点在前，保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向，而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 举个例子，如果我们执行以下 HSET 命令，那么服务器将创建一个列表对象作为 profile 键的值： 12345678redis&gt; HSET profile name \"Tom\"(integer) 1 redis&gt; HSET profile age 25(integer) 1 redis&gt; HSET profile career \"Programmer\"(integer) 1 如果 profile 键的值对象使用 ziplist 编码，那么这个值对象将会是图 8-9 所示的样子，其中对象所使用的压缩列表如图 8-10 所示 另一方面，hashtable 编码的哈希对象使用字典作为底层实现，哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象，对象中保存了键值对的键； 字典的每个值都是一个字符串对象，对象中保存了键值对的值。 举个例子，如果前面 profile 键创建的不是 ziplist 编码的哈希对象，而是 hashtable 编码的哈希对象，那么这个哈希对象应该会是图 8-11 所示的样子。 编码转换 当哈希对象可以同时满足以下两个条件时，哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； 不能满足这两个条件的哈希对象需要使用 hashtable 编码 注意： 两个条件的上限值可以修改，具体请看配置文件中 hash-max-ziplist-value 选项和 hash-max-ziplist-entries 选项的说明 对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在压缩列表里的所有键值对都会被转移并保存到字典里面，对象的编码也会从 ziplist 变为 hashtable 。 以下代码展示了哈希对象因为键值对的键长度太大而引起编码转换的情况： 1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET book name \"Mastering C++ in 21 days\"(integer) 1 redis&gt; OBJECT ENCODING book\"ziplist\" # 向哈希对象添加一个新的键值对，键的长度为 66 字节redis&gt; HSET book long_long_long_long_long_long_long_long_long_long_long_description \"content\"(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING book\"hashtable\" 除了键的长度太大会引起编码转换之外，值的长度太大也会引起编码转换，以下代码展示了这种情况的一个示例： 1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET blah greeting \"hello world\"(integer) 1 redis&gt; OBJECT ENCODING blah\"ziplist\" # 向哈希对象添加一个新的键值对，值的长度为 68 字节redis&gt; HSET blah story \"many string ... many string ... many string ... many string ... many\"(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING blah\"hashtable\" 最后，以下代码展示了哈希对象因为包含的键值对数量过多而引起编码转换的情况： 1234567891011121314151617181920# 创建一个包含 512 个键值对的哈希对象redis&gt; EVAL \"for i=1, 512 do redis.call('HSET', KEYS[1], i, i) end\" 1 \"numbers\"(nil) redis&gt; HLEN numbers(integer) 512 redis&gt; OBJECT ENCODING numbers\"ziplist\" # 再向哈希对象添加一个新的键值对，使得键值对的数量变成 513 个redis&gt; HMSET numbers \"key\" \"value\"OK redis&gt; HLEN numbers(integer) 513 # 编码改变redis&gt; OBJECT ENCODING numbers\"hashtable\" 哈希命令的实现 因为哈希键的值为哈希对象，所以用于哈希键的所有命令都是针对哈希对象来构建的，下表列出了其中一部分哈希键命令，以及这些命令在不同编码的哈希对象下的实现方法。 命令 ziplist 编码实现方法 hashtable 编码的实现方法 HSET 首先调用 ziplistPush 函数，将键推入到压缩列表的表尾，然后再次调用 ziplistPush 函数，将值推入到压缩列表的表尾。 调用 dictAdd 函数，将新节点添加到字典里面。 HGET 首先调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后调用 ziplistNext 函数，将指针移动到键节点旁边的值节点，最后返回值节点。 调用 dictFind 函数，在字典中查找给定键，然后调用 dictGetVal 函数，返回该键所对应的值。 HEXISTS 调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，如果找到的话说明键值对存在，没找到的话就说明键值对不存在。 调用 dictFind 函数，在字典中查找给定键，如果找到的话说明键值对存在，没找到的话就说明键值对不存在。 HDEL 调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后将相应的键节点、以及键节点旁边的值节点都删除掉。 调用 dictDelete 函数，将指定键所对应的键值对从字典中删除掉。 HLEN 调用 ziplistLen 函数，取得压缩列表包含节点的总数量，将这个数量除以 2 ，得出的结果就是压缩列表保存的键值对的数量。 调用 dictSize 函数，返回字典包含的键值对数量，这个数量就是哈希对象包含的键值对数量。 HGETALL 遍历整个压缩列表，用 ziplistGet函数返回所有键和值（都是节点）。 遍历整个字典，用 dictGetKey 函数返回字典的键，用 dictGetVal 函数返回字典的值。 集合对象 集合对象的编码可以是 intset 或者 hashtable 。 intset 编码的集合对象使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合里面。 举个例子，以下代码将创建一个如图 8-12 所示的 intset 编码集合对象： 12redis&gt; SADD numbers 1 3 5(integer) 3 另一方面，hashtable 编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为 NULL 。 举个例子，以下代码将创建一个如图 8-13 所示的 hashtable 编码集合对象： 12redis&gt; SADD fruits \"apple\" \"banana\" \"cherry\"(integer) 3 编码转换 当集合对象可以同时满足以下两个条件时，对象使用 intset 编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过 512 个； 不能满足这两个条件的集合对象需要使用 hashtable 编码。 注意 第二个条件的上限值是可以修改的，具体请看配置文件中关于 set-max-intset-entries 选项的说明。 对于使用 intset 编码的集合对象来说，当使用 intset 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在整数集合中的所有元素都会被转移并保存到字典里面，并且对象的编码也会从 intset 变为 hashtable 。 举个例子，以下代码创建了一个只包含整数元素的集合对象，该对象的编码为 intset ： 12345redis&gt; SADD numbers 1 3 5(integer) 3 redis&gt; OBJECT ENCODING numbers\"intset\" 不过，只要我们向这个只包含整数元素的集合对象添加一个字符串元素，集合对象的编码转移操作就会被执行： 1234redis&gt; SADD numbers \"seven\"(integer) 1 redis&gt; OBJECT ENCODING numbers\"hashtable\" 除此之外，如果我们创建一个包含 512 个整数元素的集合对象，那么对象的编码应该会是 intset ： 123456redis&gt; EVAL \"for i=1, 512 do redis.call('SADD', KEYS[1], i) end\" 1 integers(nil) redis&gt; SCARD integers(integer) 512 redis&gt; OBJECT ENCODING integers\"intset\" 但是，只要我们再向集合添加一个新的整数元素，使得这个集合的元素数量变成 513 ，那么对象的编码转换操作就会被执行 12345678redis&gt; SADD integers 10086(integer) 1 redis&gt; SCARD integers(integer) 513 redis&gt; OBJECT ENCODING integers\"hashtable\" 集合命令的实现 因为集合键的值为集合对象，所以用于集合键的所有命令都是针对集合对象来构建的，表 8-10 列出了其中一部分集合键命令，以及这些命令在不同编码的集合对象下的实现方法。 命令 intset 编码的实现方法 hashtable 编码的实现方法 SADD 调用 intsetAdd 函数，将所有新元素添加到整数集合里面。 调用 dictAdd ，以新元素为键， NULL 为值，将键值对添加到字典里面。 SCARD 调用 intsetLen 函数，返回整数集合所包含的元素数量，这个数量就是集合对象所包含的元素数量。 调用 dictSize 函数，返回字典所包含的键值对数量，这个数量就是集合对象所包含的元素数量。 SISMEMBER 调用 intsetFind 函数，在整数集合中查找给定的元素，如果找到了说明元素存在于集合，没找到则说明元素不存在于集合。 调用 dictFind 函数，在字典的键中查找给定的元素，如果找到了说明元素存在于集合，没找到则说明元素不存在于集合。 SMEMBERS 遍历整个整数集合，使用 intsetGet 函数返回集合元素。 遍历整个字典，使用 dictGetKey 函数返回字典的键作为集合元素。 SRANDMEMBER 调用 intsetRandom 函数，从整数集合中随机返回一个元素。 调用 dictGetRandomKey 函数，从字典中随机返回一个字典键。 SPOP 调用 intsetRandom 函数，从整数集合中随机取出一个元素，在将这个随机元素返回给客户端之后，调用 intsetRemove 函数，将随机元素从整数集合中删除掉。 调用 dictGetRandomKey 函数，从字典中随机取出一个字典键，在将这个随机字典键的值返回给客户端之后，调用 dictDelete 函数，从字典中删除随机字典键所对应的键值对。 SREM 调用 intsetRemove 函数，从整数集合中删除所有给定的元素。 调用 dictDelete 函数，从字典中删除所有键为给定元素的键值对 有序集合对象 有序集合的编码可以是 ziplist 或者 skiplist 。 ziplist 编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。 压缩列表内的集合元素按分值从小到大进行排序，分值较小的元素被放置在靠近表头的方向，而分值较大的元素则被放置在靠近表尾的方向。 举个例子，如果我们执行以下 ZADD 命令，那么服务器将创建一个有序集合对象作为 price 键的值： 12redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 如果 price 键的值对象使用的是 ziplist 编码，那么这个值对象将会是图 8-14 所示的样子，而对象所使用的压缩列表则会是 8-15 所示的样子。 skiplist 编码的有序集合对象使用 zset 结构作为底层实现，一个 zset 结构同时包含一个字典和一个跳跃表： 1234567typedef struct zset &#123; zskiplist *zsl; dict *dict; &#125; zset; zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素：跳跃表节点的 object 属性保存了元素的成员，而跳跃表节点的 score 属性则保存了元素的分值。通过这个跳跃表，程序可以对有序集合进行范围型操作，比如 ZRANK 、 ZRANGE 等命令就是基于跳跃表 API 来实现的。 除此之外，zset 结构中的 dict 字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，而字典的值则保存了元素的分值。通过这个字典，程序可以用 O(1) 复杂度查找给定成员的分值，ZSCORE 命令就是根据这一特性实现的，而很多其他有序集合命令都在实现的内部用到了这一特性。 有序集合每个元素的成员都是一个字符串对象，而每个元素的分值都是一个 double 类型的浮点数。值得一提的是，虽然 zset 结构同时使用跳跃表和字典来保存有序集合元素，但这两种数据结构都会通过指针来共享相同元素的成员和分值，所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值，也不会因此而浪费额外的内存。 为什么有序集合需要同时使用跳跃表和字典来实现？ 在理论上来说，有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现，但无论单独使用字典还是跳跃表，在性能上对比起同时使用字典和跳跃表都会有所降低。 举个例子，如果我们只使用字典来实现有序集合，那么虽然以 O(1) 复杂度查找成员的分值这一特性会被保留，但是，因为字典以无序的方式来保存集合元素，所以每次在执行范围型操作 ——比如 ZRANK 、 ZRANGE 等命令时，程序都需要对字典保存的所有元素进行排序，完成这种排序需要至少 O(N \\log N) 时间复杂度，以及额外的 O(N) 内存空间（因为要创建一个数组来保存排序后的元素）。 另一方面，如果我们只使用跳跃表来实现有序集合，那么跳跃表执行范围型操作的所有优点都会被保留，但因为没有了字典，所以根据成员查找分值这一操作的复杂度将从 O(1) 上升为 O(\\log N) 。 因为以上原因，为了让有序集合的查找和范围型操作都尽可能快地执行，Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。 举个例子，如果前面 price 键创建的不是 ziplist 编码的有序集合对象，而是 skiplist 编码的有序集合对象，那么这个有序集合对象将会是图 8-16 所示的样子，而对象所使用的 zset 结构将会是图 8-17 所示的样子。 注意 为了展示方便，图 8-17 在字典和跳跃表中重复展示了各个元素的成员和分值，但在实际中，字典和跳跃表会共享元素的成员和分值，所以并不会造成任何数据重复，也不会因此而浪费任何内存 编码的转换 当有序集合对象可以同时满足以下两个条件时，对象使用 ziplist 编码： 有序集合保存的元素数量小于 128 个； 有序集合保存的所有元素成员的长度都小于 64 字节； 不能满足以上两个条件的有序集合对象将使用 skiplist 编码。 注意 以上两条件的上限值可修改，具体请看配置文件中 zset-max-ziplist-entries 选项和 zset-max-ziplist-value 选项的说明。 对于使用 ziplist 编码的有序集合对象来说，当使用 ziplist 编码所需的两个条件中的任意一个不能被满足时，程序就会执行编码转换操作，将原本储存在压缩列表里面的所有集合元素转移到 zset 结构里面，并将对象的编码从 ziplist 改为 skiplist 。 以下代码展示了有序集合对象因为包含了过多元素而引发编码转换的情况： 123456789101112131415161718192021# 对象包含了 128 个元素redis&gt; EVAL \"for i=1, 128 do redis.call('ZADD', KEYS[1], i, i) end\" 1 numbers(nil) redis&gt; ZCARD numbers(integer) 128 redis&gt; OBJECT ENCODING numbers\"ziplist\" # 再添加一个新元素redis&gt; ZADD numbers 3.14 pi(integer) 1 # 对象包含的元素数量变为 129 个redis&gt; ZCARD numbers(integer) 129 # 编码已改变redis&gt; OBJECT ENCODING numbers\"skiplist\" 以下代码则展示了有序集合对象因为元素的成员过长而引发编码转换的情况： 1234567891011121314# 向有序集合添加一个成员只有三字节长的元素redis&gt; ZADD blah 1.0 www(integer) 1 redis&gt; OBJECT ENCODING blah\"ziplist\" # 向有序集合添加一个成员为 66 字节长的元素redis&gt; ZADD blah 2.0 oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING blah\"skiplist\" 有序集合命令的实现 因为有序集合键的值为有序集合对象，所以用于有序集合键的所有命令都是针对有序集合对象来构建的，表 8-11 列出了其中一部分有序集合键命令，以及这些命令在不同编码的有序集合对象下的实现方法。 命令 ziplist 编码的实现方法 zset 编码的实现方法 ZADD 调用 ziplistInsert 函数，将成员和分值作为两个节点分别插入到压缩列表。 先调用 zslInsert 函数，将新元素添加到跳跃表，然后调用 dictAdd 函数，将新元素关联到字典。 ZCARD 调用 ziplistLen 函数，获得压缩列表包含节点的数量，将这个数量除以 2 得出集合元素的数量。 访问跳跃表数据结构的 length 属性，直接返回集合元素的数量。 ZCOUNT 遍历压缩列表，统计分值在给定范围内的节点的数量。 遍历跳跃表，统计分值在给定范围内的节点的数量。 ZRANGE 从表头向表尾遍历压缩列表，返回给定索引范围内的所有元素。 从表头向表尾遍历跳跃表，返回给定索引范围内的所有元素。 ZREVRANGE 从表尾向表头遍历压缩列表，返回给定索引范围内的所有元素。 从表尾向表头遍历跳跃表，返回给定索引范围内的所有元素。 ZRANK 从表头向表尾遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 从表头向表尾遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 ZREVRANK 从表尾向表头遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 从表尾向表头遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 ZREM 遍历压缩列表，删除所有包含给定成员的节点，以及被删除成员节点旁边的分值节点。 遍历跳跃表，删除所有包含了给定成员的跳跃表节点。并在字典中解除被删除元素的成员和分值的关联。 ZSCORE 遍历压缩列表，查找包含了给定成员的节点，然后取出成员节点旁边的分值节点保存的元素分值。 直接从字典中取出给定成员的分值。 类型检查和命令多态 Redis 中用于操作键的命令基本上可以分为两种类型。 其中一种命令可以对任何类型的键执行，比如说 DEL 命令、 EXPIRE 命令、 RENAME 命令、 TYPE 命令、 OBJECT 命令，等等。 举个例子，以下代码就展示了使用 DEL 命令来删除三种不同类型的键： 1234567891011121314151617181920# 字符串键redis&gt; SET msg \"hello\"OK # 列表键redis&gt; RPUSH numbers 1 2 3(integer) 3 # 集合键redis&gt; SADD fruits apple banana cherry(integer) 3 redis&gt; DEL msg(integer) 1 redis&gt; DEL numbers(integer) 1 redis&gt; DEL fruits(integer) 1 而另一种命令只能对特定类型的键执行，比如说： SET 、 GET 、 APPEND 、 STRLEN 等命令只能对字符串键执行； HDEL 、 HSET 、 HGET 、 HLEN 等命令只能对哈希键执行； RPUSH 、 LPOP 、 LINSERT 、 LLEN 等命令只能对列表键执行； SADD 、 SPOP 、 SINTER 、 SCARD 等命令只能对集合键执行； ZADD 、 ZCARD 、 ZRANK 、 ZSCORE 等命令只能对有序集合键执行； 举个例子，可以用 SET 命令创建一个字符串键，然后用 GET 命令和 APPEND 命令操作这个键，但如果我们试图对这个字符串键执行只有列表键才能执行的 LLEN 命令，那么 Redis 将向我们返回一个类型错误： 1234567891011121314redis&gt; SET msg \"hello world\"OK redis&gt; GET msg\"hello world\" redis&gt; APPEND msg \" again!\"(integer) 18 redis&gt; GET msg\"hello world again!\" redis&gt; LLEN msg(error) WRONGTYPE Operation against a key holding the wrong kind of value 类型检查的实现 从上面发生类型错误的代码示例可以看出，为了确保只有指定类型的键可以执行某些特定的命令，在执行一个类型特定的命令之前，Redis 会先检查输入键的类型是否正确，然后再决定是否执行给定的命令。 类型特定命令所进行的类型检查是通过 redisObject 结构的 type 属性来实现的： 在执行一个类型特定命令之前，服务器会先检查输入数据库键的值对象是否为执行命令所需的类型，如果是的话，服务器就对键执行指定的命令； 否则，服务器将拒绝执行命令，并向客户端返回一个类型错误。 举个例子，对于 LLEN 命令来说： 在执行 LLEN 命令之前，服务器会先检查输入数据库键的值对象是否为列表类型，也即是，检查值对象 redisObject 结构 type 属性的值是否为 REDIS_LIST ，如果是的话，服务器就对键执行 LLEN 命令； 否则的话，服务器就拒绝执行命令并向客户端返回一个类型错误； 多态命令的实现 Redis 除了会根据值对象的类型来判断键是否能够执行指定命令之外，还会根据值对象编码方式，选择正确的命令实现代码来执行命令。 举个例子，在前面介绍列表对象的编码时我们说过，列表对象有 ziplist 和 linkedlist 两种编码可用，其中前者使用压缩列表 API 来实现列表命令，而后者则使用双端链表 API 来实现列表命令。 现在，考虑这样一个情况，如果我们对一个键执行 LLEN 命令，那么服务器除了要确保执行命令的是列表键之外，还需要根据键的值对象所使用的编码来选择正确的 LLEN 命令实现： 如果列表对象的编码为 ziplist ，那么说明列表对象的实现为压缩列表，程序将使用 ziplistLen 函数来返回列表的长度； 如果列表对象的编码为 linkedlist ，那么说明列表对象的实现为双端链表，程序将使用 listLength 函数来返回双端链表的长度； 借用面向对象方面的术语来说，我们可以认为 LLEN 命令是多态（polymorphism）的：只要执行 LLEN 命令的是列表键，那么无论值对象使用的是 ziplist 编码还是 linkedlist 编码，命令都可以正常执行。 图 8-19 展示了 LLEN 命令从类型检查到根据编码选择实现函数的整个执行过程，其他类型特定命令的执行过程也是类似的。 实际上，我们可以将 DEL 、 EXPIRE 、 TYPE 等命令也称为多态命令，因为无论输入的键是什么类型，这些命令都可以正确地执行。 DEL 、 EXPIRE 等命令和 LLEN 等命令的区别在于，前者是基于类型的多态 —— 一个命令可以同时用于处理多种不同类型的键，而后者是基于编码的多态 —— 一个命令可以同时用于处理多种不同编码 内存回收 因为 C 语言并不具备自动的内存回收功能，所以 Redis 在自己的对象系统中构建了一个引用计数（reference counting）技术实现的内存回收机制，通过这一机制，程序可以通过跟踪对象的引用计数信息，在适当的时候自动释放对象并进行内存回收。 每个对象的引用计数信息由 redisObject 结构的 refcount 属性记录： 12345678910typedef struct redisObject &#123; // ... // 引用计数 int refcount; // ... &#125; robj; 对象的引用计数信息会随着对象的使用状态而不断变化： 在创建一个新对象时，引用计数的值会被初始化为 1 ； 当对象被一个新程序使用时，它的引用计数值会被增一； 当对象不再被一个程序使用时，它的引用计数值会被减一； 当对象的引用计数值变为 0 时，对象所占用的内存会被释放。 函数 作用 incrRefCount 将对象的引用计数值增一。 decrRefCount 将对象的引用计数值减一，当对象的引用计数值等于 0 时，释放对象。 resetRefCount 将对象的引用计数值设置为 0 ，但并不释放对象，这个函数通常在需要重新设置对象的引用计数值时使用。 对象的整个生命周期可以划分为创建对象、操作对象、释放对象三个阶段。 作为例子，以下代码展示了一个字符串对象从创建到释放的整个过程： 12345678// 创建一个字符串对象 s ，对象的引用计数为 1robj *s = createStringObject(...) // 对象 s 执行各种操作 ... // 将对象 s 的引用计数减一，使得对象的引用计数变为 0// 导致对象 s 被释放decrRefCount(s) 谁来负责查看数量以及触发回收呢？ 对象共享 除了用于实现引用计数内存回收机制之外，对象的引用计数属性还带有对象共享的作用。 举个例子，假设键 A 创建了一个包含整数值 100 的字符串对象作为值对象，如图 8-20 所示。 如果这时键 B 也要创建一个同样保存了整数值 100 的字符串对象作为值对象，那么服务器有以下两种做法： 为键 B 新创建一个包含整数值 100 的字符串对象； 让键 A 和键 B 共享同一个字符串对象；以上两种方法很明显是第二种方法更节约内存。 在 Redis 中，让多个键共享同一个值对象需要执行以下两个步骤： 将数据库键的值指针指向一个现有的值对象； 将被共享的值对象的引用计数增一。举个例子，图 8-21 就展示了包含整数值 100 的字符串对象同时被键 A 和键 B 共享之后的样子，可以看到，除了对象的引用计数从之前的 1 变成了 2 之外，其他属性都没有变化。 它是怎么知道有相同的值的？ 共享对象机制对于节约内存非常有帮助，数据库中保存的相同值对象越多，对象共享机制就能节约越多的内存。 比如说，假设数据库中保存了整数值 100 的键不只有键 A 和键 B 两个，而是有一百个，那么服务器只需要用一个字符串对象的内存就可以保存原本需要使用一百个字符串对象的内存才能保存的数据。 目前来说，Redis 会在初始化服务器时，创建一万个字符串对象，这些对象包含了从 0 到 9999 的所有整数值，当服务器需要用到值为 0 到 9999 的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象。 注意：创建共享字符串对象的数量可以通过修改 redis.h/REDIS_SHARED_INTEGERS 常量来修改。 举个例子，如果我们创建一个值为 100 的键 A ，并使用 OBJECT REFCOUNT 命令查看键 A 的值对象的引用计数，我们会发现值对象的引用计数为 2 ： 12345redis&gt; SET A 100OK redis&gt; OBJECT REFCOUNT A(integer) 2 引用这个值对象的两个程序分别是持有这个值对象的服务器程序，以及共享这个值对象的键 A ，如图 8-22 所示。 如果这时再创建一个值为 100 的键 B ，那么键 B 也会指向包含整数值 100 的共享对象，使得共享对象的引用计数值变为 3 ： 12345678redis&gt; SET B 100OK redis&gt; OBJECT REFCOUNT A(integer) 3 redis&gt; OBJECT REFCOUNT B(integer) 3 另外，这些共享对象不单单只有字符串键可以使用，那些在数据结构中嵌套了字符串对象的对象（linkedlist 编码的列表对象、 hashtable 编码的哈希对象、 hashtable 编码的集合对象、以及 zset 编码的有序集合对象）都可以使用这些共享对象 为什么 Redis 不共享包含字符串的对象？ 当服务器考虑将一个共享对象设置为键的值对象时，程序需要先检查给定的共享对象和键想创建的目标对象是否完全相同，只有在共享对象和目标对象完全相同的情况下，程序才会将共享对象用作键的值对象，而一个共享对象保存的值越复杂，验证共享对象和目标对象是否相同所需的复杂度就会越高，消耗的 CPU 时间也会越多： 如果共享对象是保存整数值的字符串对象，那么验证操作的复杂度为 O(1) ； 如果共享对象是保存字符串值的字符串对象，那么验证操作的复杂度为 O(N) ； 如果共享对象是包含了多个值（或者对象的）对象，比如列表对象或者哈希对象，那么验证操作的复杂度将会是 O(N^2) 。 因此，尽管共享更复杂的对象可以节约更多的内存，但受到 CPU 时间的限制，Redis 只对包含整数值的字符串对象进行共享。 对象的空转时长 除了前面介绍过的 type 、 encoding 、 ptr 和 refcount 四个属性之外，redisObject 结构包含的最后一个属性为 lru 属性，该属性记录了对象最后一次被命令程序访问的时间： 123456789typedef struct redisObject &#123; // ... unsigned lru:22; // ... &#125; robj; OBJECT IDLETIME 命令可以打印出给定键的空转时长，这一空转时长就是通过将当前时间减去键的值对象的 lru 时间计算得出的： 123456789101112131415161718redis&gt; SET msg \"hello world\"OK # 等待一小段时间redis&gt; OBJECT IDLETIME msg(integer) 20 # 等待一阵子redis&gt; OBJECT IDLETIME msg(integer) 180 # 访问 msg 键的值redis&gt; GET msg\"hello world\" # 键处于活跃状态，空转时长为 0redis&gt; OBJECT IDLETIME msg(integer) 0 注意: OBJECT IDLETIME 命令的实现是特殊的，这个命令在访问键的值对象时，不会修改值对象的 lru 属性。 除了可以被 OBJECT IDLETIME 命令打印出来之外，键的空转时长还有另外一项作用：如果服务器打开了 maxmemory 选项，并且服务器用于回收内存的算法为 volatile-lru 或者 allkeys-lru ，那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存。 配置文件的 maxmemory 选项和 maxmemory-policy 选项的说明介绍了关于这方面的更多信息 总结 Redis 数据库中的每个键值对的键和值都是一个对象。 Redis 共有字符串、列表、哈希、集合、有序集合五种类型的对象，每种类型的对象至少都有两种或以上的编码方式，不同的编码可以在不同的使用场景上优化对象的使用效率。 服务器在执行某些命令之前，会先检查给定键的类型能否执行指定的命令，而检查一个键的类型就是检查键的值对象的类型。 Redis 的对象系统带有引用计数实现的内存回收机制，当一个对象不再被使用时，该对象所占用的内存就会被自动释放。 Redis 会共享值为 0 到 9999 的字符串对象。 对象会记录自己的最后一次被访问的时间，这个时间可以用于计算对象的空转时间 参考链接 https://www.bookstack.cn/read/redisbook/433f95a1e435a4da.md https://www.cnblogs.com/remcarpediem/p/11755860.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门1-底层结构","slug":"Redis/Redis入门1-底层结构","date":"2021-09-12T09:24:54.000Z","updated":"2023-03-10T16:57:04.060Z","comments":true,"path":"2021/09/12/Redis/Redis入门1-底层结构/","link":"","permalink":"http://xboom.github.io/2021/09/12/Redis/Redis%E5%85%A5%E9%97%A81-%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/","excerpt":"","text":"前言 常见的数据结构： 底层数据结构实现： 简单动态字符串 Redis 自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将 SDS 用作 Redis 的默认字符串表示。SDS是一种可以被修改的字符串 Redis 也会用到 C 语言传统字符，但只作为字符串字面量（string literal），用在一些无须对字符串值进行修改的地方，比如打印日志： 1redisLog(REDIS_WARNING,\"Redis is now ready to exit, bye bye...\"); 当执行命令： 12redis&gt; SET msg \"hello world\"OK Redis 将在数据库中创建了一个新的键值对，其中： 键值对的键是一个字符串对象，对象的底层实现是一个保存着字符串 &quot;msg&quot; 的 SDS 。 键值对的值也是一个字符串对象，对象的底层实现是一个保存着字符串 &quot;hello world&quot; 的 SDS 。 除了用来保存数据库中的字符串值之外，AOF 模块中的 AOF 缓冲区，以及客户端状态中的输入缓冲区，都是由 SDS 实现的 SDS定义 每个 sds.h/sdshdr 结构表示一个 SDS 值： 123456789struct sdshdr &#123; // 记录 buf 数组中已使用字节的数量 // 等于 SDS 所保存字符串的长度 int len; // 记录 buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[]; &#125;; free 属性的值为 0 ，表示这个 SDS 没有分配任何未使用空间。 len 属性的值为 5 ，表示这个 SDS 保存了一个五字节长的字符串。 buf 属性是一个 char 类型的数组，数组的前五个字节分别保存了 'R' 、 'e' 、 'd' 、 'i' 、 's' 五个字符，而最后一个字节则保存了空字符 '\\0' 。 SDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，这个空字符对于 SDS 的使用者来说是完全透明的。 遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。 举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 s ，那么我们可以直接使用 stdio.h/printf 函数，通过执行以下语句： 这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 &quot;Redis&quot; 。 这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 buf 数组分配了五字节未使用空间，所以它的 free 属性的值为 5（图中使用五个空格来表示五字节的未使用空间） 其中 保存空字符的 1 字节空间不计算在 SDS 的 len 和 free 属性里面 SDS与C字符串的区别 C 语言使用长度为 N+1 的字符数组来表示长度为 N 的字符串，并且字符数组的最后一个元素总是空字符 '\\0' 。 比如说，图 2-3 就展示了一个值为 &quot;Redis&quot; 的 C 字符串 C 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，接下来将说明 SDS 比 C 字符串更适用于 Redis 的原因 常数复杂度获取字符串长度 因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。 举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程 和 C 字符串不同，因为 SDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。 举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 len 属性，就可以立即知道 SDS 的长度为 5 字节： 又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 len 属性，就可以立即知道 SDS 的长度为 11 字节。 设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。 通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，即使对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 杜绝缓冲区溢出 除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出 (buffer overflow) 举个例子，&lt;string.h&gt;/strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾： 1char *strcat(char *dest, const char *src); 因为 C 字符串不记录自身的长度，所以 strcat 假定用户在执行这个函数时，已经为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。 举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 s1 和 s2 ，其中 s1 保存了字符串 &quot;Redis&quot; ，而 s2 则保存了字符串 &quot;MongoDB&quot; ，如图 2-7 所示 如果一个程序员决定通过执行： 1strcat(s1, \" Cluster\"); 将 s1 的内容修改为 &quot;Redis Cluster&quot; ，但粗心的他却忘了在执行 strcat 之前为 s1 分配足够的空间，那么在 strcat 函数执行之后，s1 的数据将溢出到 s2 所在的空间中，导致 s2 保存的内容被意外地修改，如图 2-8 所示 与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。 举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 sdscat 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，sdscat 会先检查给定 SDS 的空间是否足够，如果不够的话，sdscat 就会先扩展 SDS 的空间，然后才执行拼接操作 比如说，如果我们执行： 1sdscat(s, \" Cluster\"); 其中 SDS 值 s 如图 2-9 所示，那么 sdscat 将在执行拼接操作之前检查 s 的长度是否足够，在发现 s 目前的空间不足以拼接 &quot; Cluster&quot; 之后，sdscat 就会先扩展 s 的空间，然后才执行拼接 &quot; Cluster&quot; 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。 注意图 2-10 所示的 SDS ：sdscat 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 13 字节的未使用空间，并且拼接之后的字符串也正好是 13 字节长，分配的空间 SDS 的空间分配策略有关 减少修改带来的内存重分配次数 正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 N 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 N+1 个字符长的数组（额外的一个字符空间用于保存空字符）。 因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作： 如果是增长字符串的操作，比如拼接操作(append)，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。 如果是缩短字符串的操作，比如截断操作(trim)，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。 问题1：如果是分配，那么每次分配多少呢? 问题2：如果是释放不再使用的空间，那么什么时候释放呢，或者使用什么策略？ 举个例子，如果我们持有一个值为 &quot;Redis&quot; 的 C 字符串 s ，那么为了将 s 的值改为 &quot;Redis Cluster&quot; ，在执行： 1strcat(s, \" Cluster\"); 之前，我们需要先使用内存重分配操作，扩展 s 的空间。 之后，如果我们又打算将 s 的值从 &quot;Redis Cluster&quot; 改为 &quot;Redis Cluster Tutorial&quot; ，那么在执行： 1strcat(s, \" Tutorial\"); 之前，我们需要再次使用内存重分配扩展 s 的空间，诸如此类。 因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作： 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。 但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。 为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，buf 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 free 属性记录。 通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略 空间预分配 空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。 额外分配的未使用空间数量由以下公式决定： 如果对 SDS 进行修改之后，SDS 的长度（也即是 len 属性的值）将小于 1 MB ，那么程序分配和 len 属性同样大小的未使用空间，这时 SDS len 属性的值将和 free 属性的值相同。举个例子，如果进行修改之后，SDS 的 len 将变成 13 字节，那么程序也会分配 13 字节的未使用空间，SDS 的 buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 SDS 进行修改之后，SDS 的长度将大于等于 1 MB ，那么程序会分配 1 MB 的未使用空间。举个例子，如果进行修改之后，SDS 的 len 将变成 30 MB ，那么程序会分配 1 MB 的未使用空间，SDS 的 buf 数组的实际长度将为 30 MB + 1 MB + 1 byte 。 通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。 举个例子，对于图 2-11 所示的 SDS 值 s 来说，如果我们执行： 1sdscat(s, \" Cluster\"); 那么 sdscat 将执行一次内存重分配操作，将 SDS 的长度修改为 13 字节，并将 SDS 的未使用空间同样修改为 13 字节，如图 2-12 所示。 如果这时，我们再次对 s 执行： 1sdscat(s, \" Tutorial\"); 那么这次 sdscat 将不需要执行内存重分配：因为未使用空间里面的 13 字节足以保存 9 字节的 &quot; Tutorial&quot; ，执行 sdscat 之后的 SDS 如图 2-13 所示 在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。 通过这种预分配策略，SDS 将连续增长 N 次字符串所需的内存重分配次数从必定 N 次降低为最多 N 次。 惰性空间释放 惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来使用。 举个例子，sdstrim 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。 比如对于图 2-14 所示的 SDS 值 s 来说，执行： 1sdstrim(s, &quot;XY&quot;); &#x2F;&#x2F; 移除 SDS 字符串中的所有 &#39;X&#39; 和 &#39;Y&#39; 会将 SDS 修改成图 2-15 所示的样子 注意执行 sdstrim 之后的 SDS 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。 举个例子，如果现在对 s 执行： 1sdscat(s, &quot; Redis&quot;); 那么完成这次 sdscat 操作将不需要执行内存重分配：因为 SDS 里面预留的 8 字节空间已经足以拼接 6 个字节长的 &quot; Redis&quot; ，如图 2-16 所示。 通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。 什么时候或者有什么接口进行内存释放 二进制安全 C 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 有个问题：到底是怎么存进去并计算长度的 如果是先计算长度，然后再存进去，那么可能出现长度误判，导致整体长度不够的情况 如果是先存进去再计算长度(不可能，需要先分配空间) 原来它用的是二进制数据？ 举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 &quot;Redis&quot; ，而忽略之后的 &quot;Cluster&quot; 。 虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的(binary-safe): 所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。 将 SDS 的 buf 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。 比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示 通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据 兼容部分C函数 虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 buf 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 &lt;string.h&gt; 库定义的函数。 举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 sds ，那么我们就可以重用 &lt;string.h&gt;/strcasecmp 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串： 1strcasecmp(sds-&gt;buf, &quot;hello world&quot;); 这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。 与此类似，我们还可以将一个保存文本数据的 SDS 作为 strcat 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面： 1strcat(c_string, sds-&gt;buf); 这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。 通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 &lt;string.h&gt; 函数库，从而避免了不必要的代码重复。 总结 C 字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 &lt;string.h&gt; 库中的函数。 可以使用一部分 &lt;string.h&gt; 库中的函数。 接口实现：https://www.cnblogs.com/yinbiao/p/10740212.html 链表 链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。 除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer） 链表和链表节点的实现 每个链表节点使用一个 adlist.h/listNode 结构来表示： 12345678typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode; 多个 listNode 可以通过 prev 和 next 指针组成双端链表，如图 3-1 所示 虽然仅仅使用多个 listNode 结构就可以组成链表，但使用 adlist.h/list 来持有链表的话，操作起来会更方便 12345678910111213141516171819typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key);&#125; list; list 结构为链表提供了表头指针 head 、表尾指针 tail ，以及链表长度计数器 len ，而 dup 、 free 和 match 成员则是用于实现多态链表所需的类型特定函数： dup 函数用于复制链表节点所保存的值； free 函数用于释放链表节点所保存的值； match 函数则用于对比链表节点所保存的值和另一个输入值是否相等 图 3-2 是由一个 list 结构和三个 listNode 结构组成的链表： Redis 的链表实现的特性可以总结如下： 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ，对链表的访问以 NULL 为终点。 带表头指针和表尾指针：通过 list 结构的 head 指针和 tail 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器：程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。 多态：链表节点使用 void* 指针来保存节点值，并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。 总结 链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。 每个链表节点由一个 listNode 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ，所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。 字典 在字典中，一个键（key）可以和一个值（value）进行关联（或者说将键映射为值），这些关联的键和值就被称为键值对。 举个例子，当执行命令： 12redis&gt; SET msg \"hello world\"OK 在数据库中创建一个键为 &quot;msg&quot; ，值为 &quot;hello world&quot; 的键值对时，这个键值对就是保存在代表数据库的字典里面的。 除了用来表示数据库之外，字典还是哈希键的底层实现之一：当一个哈希键包含的键值对比较多，又或者键值对中的元素都是比较长的字符串时，Redis 就会使用字典作为哈希键的底层实现。 举个例子，website 是一个包含 10086 个键值对的哈希键，这个哈希键的键都是一些数据库的名字，而键的值就是数据库的主页网址： 123456789redis&gt; HLEN website(integer) 10086 redis&gt; HGETALL website1) \"Redis\"2) \"Redis.io\"3) \"MariaDB\"4) \"MariaDB.org\"5) \"MongoDB\"6) \"MongoDB.org\"# ... website 键的底层实现就是一个字典，字典中包含了 10086 个键值对： 其中一个键值对的键为 &quot;Redis&quot; ，值为 &quot;Redis.io&quot; 。 另一个键值对的键为 &quot;MariaDB&quot; ，值为 &quot;MariaDB.org&quot; ； 还有一个键值对的键为 &quot;MongoDB&quot; ，值为 &quot;MongoDB.org&quot; ； 诸如此类。除了用来实现数据库和哈希键之外，Redis 的不少功能也用到了字典，在后续的章节中会不断地看到字典在 Redis 中的各种不同应用。 字典的实现 Redis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。 接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现 Hash表 Redis 字典所使用的哈希表由 dict.h/dictht 结构定义 12345678910111213141516typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; &#125; dictht; table 属性是一个数组，数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针，每个 dictEntry 结构保存着一个键值对。 size 属性记录了哈希表的大小，也即是 table 数组的大小，而 used 属性则记录了哈希表目前已有节点（键值对）的数量。 sizemask 属性的值总是等于 size - 1 ，这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。 图 4-1 展示了一个大小为 4 的空哈希表（没有包含任何键值对） 哈希表节点 哈希表节点使用 dictEntry 结构表示，每个 dictEntry 结构都保存着一个键值对： 12345678910111213141516typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next; &#125; dictEntry; key 属性保存着键值对中的键，而 v 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 uint64_t 整数，又或者是一个 int64_t 整数。 next 属性是指向另一个哈希表节点的指针，可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题 举个例子，图 4-2 就展示了如何通过 next 指针，将两个索引值相同的键 k1 和 k0 连接在一起。 字典 12345678910111213141516typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ &#125; dict; type 属性和 privdata 属性是针对不同类型的键值对，为创建多态字典而设置的： type 属性是一个指向 dictType 结构的指针，每个 dictType 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。 而 privdata 属性则保存了需要传给那些类型特定函数的可选参数 123456789101112131415161718192021typedef struct dictType &#123; // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj); &#125; dictType; ht 属性是一个包含两个项的数组，数组中的每个项都是一个 dictht 哈希表，一般情况下，字典只使用 ht[0] 哈希表，ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用。 除了 ht[1] 之外，另一个和 rehash 有关的属性就是 rehashidx ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 -1 。 图 4-3 展示了一个普通状态下（没有进行 rehash）的字典 Hash算法 当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 Redis 计算哈希值和索引值的方法如下： 123456# 使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key); # 使用哈希表的 sizemask 属性和哈希值，计算出索引值# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 k0 和 v0 添加到字典里面，那么程序会先使用语句： 1hash &#x3D; dict-&gt;type-&gt;hashFunction(k0); 计算键 k0 的哈希值。 假设计算得出的哈希值为 8 ，那么程序会继续使用语句： 1index &#x3D; hash &amp; dict-&gt;ht[0].sizemask &#x3D; 8 &amp; 3 &#x3D; 0; 计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。 MurmurHash 算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。 MurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：http://code.google.com/p/smhasher/ 。 解决键冲突 当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时，称这些键发生了冲突（collision）。 Redis 的哈希表使用链地址法（separate chaining）来解决键冲突：每个哈希表节点都有一个 next 指针，多个哈希表节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来，这就解决了键冲突的问题。 举个例子，假设程序要将键值对 k2 和 v2 添加到图 4-6 所示的哈希表里面，并且计算得出 k2 的索引值为 2 ，那么键 k1 和 k2 将产生冲突，而解决冲突的办法就是使用 next 指针将键 k2 和 k1 所在的节点连接起来，如图 4-7 所示。 因为 dictEntry 节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置（复杂度为 O(1)），排在其他已有节点的前面 链表使用的是双向链表所以可以直接添加到链表结尾，但是字典的链表中并只有一个指向next的指针，所以如果发生hash冲突，则直接添加到链表头部 rehash 随着操作的不断执行，哈希表保存的键值对会逐渐地增多或者减少，为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩。 负载因子是什么时候定义的，怎么去判断什么时候去rehash呢？ rehash过程中会停掉插入和获取吗，新来的值怎么办？ 查询的时候会同时查询 hash[0] 和 hash[1] 吗 扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成，Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的 ht[1] 哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及 ht[0] 当前包含的键值对数量（也即是 ht[0].used属性的值）： 如果执行的是扩展操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面：rehash 指的是重新计算键的哈希值和索引值，然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后（ht[0] 变为空表），释放 ht[0] ，将 ht[1] 设置为 ht[0] ，并在 ht[1] 新创建一个空白哈希表，为下一次 rehash 做准备。 举个例子，假设程序要对图 4-8 所示字典的 ht[0] 进行扩展操作，那么程序将执行以下步骤： ht[0].used 当前的值为 4 ，4 * 2 = 8 ，而 8 （2^3）恰好是第一个大于等于 4 的 2 的 n 次方，所以程序会将 ht[1] 哈希表的大小设置为 8 。图 4-9 展示了 ht[1] 在分配空间之后，字典的样子。 将 ht[0] 包含的四个键值对都 rehash 到 ht[1] ，如图 4-10 所示。 释放 ht[0] ，并将 ht[1] 设置为 ht[0] ，然后为 ht[1] 分配一个空白哈希表，如图 4-11 所示。 至此，对哈希表的扩展操作执行完毕，程序成功将哈希表的大小从原来的 4 改为了现在的 8 。 hash表的拓展和收缩 当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 5 ； 其中哈希表的负载因子可以通过公式： 1# 负载因子 = 哈希表已保存节点数量 / 哈希表大小load_factor = ht[0].used / ht[0].size 计算得出。比如说，这个哈希表的负载因子为： 12345# 对于一个大小为 `4` ，包含 `4` 个键值对的哈希表来说load_factor = 4 / 4 = 1# 对于一个大小为 `512` ，包含 `256` 个键值对的哈希表来说load_factor = 256 / 512 = 0.5 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行，服务器执行扩展操作所需的负载因子并不相同 因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中，Redis 需要创建当前服务器进程的子进程，而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率，所以在子进程存在期间，服务器会提高执行扩展操作所需的负载因子，从而尽可能地避免在子进程存在期间进行哈希表扩展操作，这可以避免不必要的内存写入操作，最大限度地节约内存 渐进式rehash 扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面，但rehash 动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的。原因在于，如果哈希表里保存大量键值对，要一次性将这些键值对全部 rehash 到 ht[1] 的话，庞大的计算量可能会导致服务器在一段时间内停止服务。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间，让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ，并将它的值设置为 0 ，表示 rehash 工作正式开始。 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ，当 rehash 工作完成之后，程序将 rehashidx 属性的值增一。 随着字典操作的不断执行，最终在某个时间点上，ht[0] 的所有键值对都会被 rehash 至 ht[1] ，这时程序将 rehashidx 属性的值设为 -1 ，表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式，将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 而带来的庞大计算量 同时在两个ht都存在数据，那么 增删改查 在哪个进行操作的，怎么判断哪个ht才是目的地 图 4-12 至图 4-17 展示了一次完整的渐进式 rehash 过程，注意观察在整个 rehash 过程中，字典的 rehashidx 属性是如何变化的 渐进式 rehash 执行期间的哈希表操作 因为在进行渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个哈希表，所以在渐进式 rehash 进行期间，字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行：比如说，要在字典里面查找一个键的话，程序会先在 ht[0] 里面进行查找，如果没找到的话，就会继续到 ht[1] 里面进行查找，诸如此类。 另外，在渐进式 rehash 执行期间，新添加到字典的键值对一律会被保存到 ht[1] 里面，而 ht[0] 则不再进行任何添加操作：这一措施保证了 ht[0] 包含的键值对数量会只减不增，并随着 rehash 操作的执行而最终变成空表 总结 字典被广泛用于实现 Redis 的各种功能，其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现，每个字典带有两个哈希表，一个用于平时使用，另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突，被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时，程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面，并且这个 rehash 过程并不是一次性地完成的，而是渐进式地完成的 跳跃表 跳跃表（skiplist）是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。 跳跃表支持平均 O(log N) 最坏 O(N) 复杂度的节点查找，还可以通过顺序性操作来批量处理节点 在大部分情况下，跳跃表的效率可以和平衡树相媲美，并且因为跳跃表的实现比平衡树要来得更为简单，所以有不少程序都使用跳跃表来代替平衡树 TODO 跳跃表与平衡树的比较 有序的条件是什么，怎么实现的有序 Redis 使用跳跃表作为有序集合键的底层实现之一：如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员（member）是比较长的字符串时，Redis 就会使用跳跃表来作为有序集合键的底层实现。 举个例子，fruit-price 是一个有序集合键，这个有序集合以水果名为成员，水果价钱为分值，保存了 130 款水果的价钱： 12345678910redis&gt; ZRANGE fruit-price 0 2 WITHSCORES1) \"banana\"2) \"5\"3) \"cherry\"4) \"6.5\"5) \"apple\"6) \"8\" redis&gt; ZCARD fruit-price(integer) 130 fruit-price 有序集合的所有数据都保存在一个跳跃表里面，其中每个跳跃表节点（node）都保存了一款水果的价钱信息，所有水果按价钱的高低从低到高在跳跃表里面排序： 跳跃表的第一个元素的成员为 &quot;banana&quot; ，它的分值为 5 ； 跳跃表的第二个元素的成员为 &quot;cherry&quot; ，它的分值为 6.5 ； 跳跃表的第三个元素的成员为 &quot;apple&quot; ，它的分值为 8 ； 和链表、字典等数据结构被广泛地应用在 Redis 内部不同，Redis 只在两个地方用到了跳跃表，一个是实现有序集合键，另一个是在集群节点中用作内部数据结构，除此之外，跳跃表在 Redis 里面没有其他用途。 跳表的实现 有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位(注意跳表是有序的) Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针，等等 图 5-1 展示了一个跳跃表示例，位于图片最左边的是 zskiplist 结构，该结构包含以下属性： header ：指向跳跃表的表头节点 tail ：指向跳跃表的表尾节点 level ：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内） length ：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内） 位于 zskiplist 结构右方的是四个 zskiplistNode 结构，该结构包含以下属性： 层（level）：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 后退（backward）指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值（score）：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。 成员对象（obj）：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象。 注意表头节点和其他节点的构造是一样的：表头节点也有后退指针、分值和成员对象，不过表头节点的这些属性都不会被用到，所以图中省略了这些部分，只显示了表头节点的各个层。 本节接下来的内容将对 zskiplistNode 和 zskiplist 两个结构进行更详细的介绍。 跳跃表节点 跳跃表节点的实现由 redis.h/zskiplistNode 结构定义： 1234567891011121314151617181920212223typedef struct zskiplistNode &#123; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[]; &#125; zskiplistNode; 层 跳跃表节点的 level 数组可以包含多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。 每次创建一个新跳跃表节点的时候，程序都根据幂次定律（power law，越大的数出现的概率越小）随机生成一个介于 1 和 32 之间的值作为 level 数组的大小，这个大小就是层的“高度”。 图 5-2 分别展示了三个高度为 1 层、 3 层和 5 层的节点，因为 C 语言的数组索引总是从 0 开始的，所以节点的第一层是 level[0] ，而第二层是 level[1] ，以此类推。 前进指针 每个层都有一个指向表尾方向的前进指针（level[i].forward 属性），用于从表头向表尾方向访问节点。 图 5-3 用虚线表示出了程序从表头向表尾方向，遍历跳跃表中所有节点的路径： 迭代程序首先访问跳跃表的第一个节点（表头），然后从第四层的前进指针移动到表中的第二个节点。 在第二个节点时，程序沿着第二层的前进指针移动到表中的第三个节点。 在第三个节点时，程序同样沿着第二层的前进指针移动到表中的第四个节点。 当程序再次沿着第四个节点的前进指针移动时，它碰到一个 NULL ，程序知道这时已经到达了跳跃表的表尾，于是结束这次遍历 跨度 层的跨度（level[i].span 属性）用于记录两个节点之间的距离： 两个节点之间的跨度越大，它们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0 ，因为它们没有连向任何节点。 初看上去，很容易以为跨度和遍历操作有关，但实际上并不是这样 ——遍历操作只使用前进指针就可以完成了，跨度实际上是用来计算排位（rank）的：在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位。 举个例子，图 5-4 用虚线标记了在跳跃表中查找分值为 3.0 、成员对象为 o3 的节点时，沿途经历的层：查找的过程只经过了一个层，并且层的跨度为 3 ，所以目标节点在跳跃表中的排位为 3 。 再举个例子，图 5-5 用虚线标记了在跳跃表中查找分值为 2.0 、成员对象为 o2 的节点时，沿途经历的层：在查找节点的过程中，程序经过了两个跨度为 1 的节点，因此可以计算出，目标节点在跳跃表中的排位为 2 后退指针 节点的后退指针（backward 属性）用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。 图 5-6 用虚线展示了如果从表尾向表头遍历跳跃表中的所有节点：程序首先通过跳跃表的 tail 指针访问表尾节点，然后通过后退指针访问倒数第二个节点，之后再沿着后退指针访问倒数第三个节点，再之后遇到指向 NULL 的后退指针，于是访问结束 分值与成员 节点的分值（score 属性）是一个 double 类型的浮点数，跳跃表中的所有节点都按分值从小到大来排序。 节点的成员对象（obj 属性）是一个指针，它指向一个字符串对象，而字符串对象则保存着一个 SDS 值。 在同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却可以是相同的：分值相同的节点将按照成员对象在字典序中的大小来进行排序，成员对象较小的节点会排在前面（靠近表头的方向），而成员对象较大的节点则会排在后面（靠近表尾的方向）。 举个例子，在图 5-7 所示的跳跃表中，三个跳跃表节点都保存了相同的分值 10086.0 ，但保存成员对象 o1 的节点却排在保存成员对象 o2 和 o3 的节点之前，而保存成员对象 o2 的节点又排在保存成员对象 o3 的节点之前，由此可见，o1 、 o2 、 o3 三个成员对象在字典中的排序为 o1 &lt;= o2 &lt;= o3 。 跳跃表 虽然仅靠多个跳跃表节点就可以组成一个跳跃表，如图 5-8 所示。 但通过使用一个 zskiplist 结构来持有这些节点，程序可以更方便地对整个跳跃表进行处理，比如快速访问跳跃表的表头节点和表尾节点，又或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息，如图 5-9 所示。 zskiplist 结构的定义如下 123456789101112typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level; &#125; zskiplist; header 和 tail 指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的复杂度为 O(1) 。 通过使用 length 属性来记录节点的数量，程序可以在 O(1) 复杂度内返回跳跃表的长度。 level 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量，注意表头节点的层高并不计算在内。 跳跃条是怎么增删改查的 总结 跳跃表是有序集合的底层实现之一，除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成，其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度），而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中，多个节点可以包含相同的分值，但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序，当分值相同时，节点按照成员对象的大小进行排序。 整数集合 整数集合（intset）是集合键的底层实现之一：当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集合键的底层实现。 举个例子，如果创建一个只包含五个元素的集合键，并且集合中的所有元素都是整数值，那么这个集合键的底层实现就会是整数集合： 12345redis&gt; SADD numbers 1 3 5 7 9(integer) 5 redis&gt; OBJECT ENCODING numbers\"intset\" 整数集合的实现 整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构，它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值，并且保证集合中不会出现重复元素。 每个 intset.h/intset 结构表示一个整数集合： 123456789101112typedef struct intset &#123; // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; &#125; intset; contents 数组是整数集合的底层实现：整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复项。 通过什么决定排序？ length 属性记录了整数集合包含的元素数量，也即是 contents 数组的长度。 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值 ——contents 数组的真正类型取决于 encoding 属性的值： 如果 encoding 属性的值为 INTSET_ENC_INT16 ，那么 contents 就是一个 int16_t 类型的数组，数组里的每个项都是一个 int16_t 类型的整数值（最小值为 -32,768 ，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ，那么 contents 就是一个 int32_t 类型的数组，数组里的每个项都是一个 int32_t 类型的整数值（最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ，那么 contents 就是一个 int64_t 类型的数组，数组里的每个项都是一个 int64_t 类型的整数值（最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ） 这是怎么实现的？ 图 6-1 展示了一个整数集合示例： encoding 属性的值为 INTSET_ENC_INT16 ，表示整数集合的底层实现为 int16_t 类型的数组，而集合保存的都是 int16_t 类型的整数值。 length 属性的值为 5 ，表示整数集合包含五个元素。 contents 数组按从小到大的顺序保存着集合中的五个元素。 因为每个集合元素都是 int16_t 类型的整数值，所以 contents 数组的大小等于 sizeof(int16_t) *5 = 16* 5 = 80 位。 图 6-2 展示了另一个整数集合示例： encoding 属性的值为 INTSET_ENC_INT64 ，表示整数集合的底层实现为 int64_t 类型的数组，而数组中保存的都是 int64_t 类型的整数值。 length 属性的值为 4 ，表示整数集合包含四个元素。 contents 数组按从小到大的顺序保存着集合中的四个元素。 因为每个集合元素都是 int64_t 类型的整数值，所以 contents 数组的大小为 sizeof(int64_t) *4 = 64* 4 = 256 位。 虽然 contents 数组保存的四个整数值中，只有 -2675256175807981027 是真正需要用 int64_t 类型来保存的，而其他的 1 、 3 、 5 三个值都可以用 int16_t 类型来保存，不过根据整数集合的升级规则，当向一个底层为 int16_t 数组的整数集合添加一个 int64_t 类型的整数值时，整数集合已有的所有元素都会被转换成 int64_t 类型，所以 contents 数组保存的四个整数值都是 int64_t 类型的，不仅仅是 -2675256175807981027 升级 每当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrade），然后才能将新元素添加到整数集合里面。 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。举个例子，假设现在有一个 INTSET_ENC_INT16 编码的整数集合，集合中包含三个 int16_t 类型的元素，如图 6-3 所示。 因为每个元素都占用 16 位空间，所以整数集合底层数组的大小为 3 * 16 = 48 位，图 6-4 展示了整数集合的三个元素在这 48 位里的位置 现在，假设我们要将类型为 int32_t 的整数值 65535 添加到整数集合里面，因为 65535 的类型 int32_t 比整数集合当前所有元素的类型都要长，所以在将 65535 添加到整数集合之前，程序需要先对整数集合进行升级。 升级首先要做的是，根据新类型的长度，以及集合元素的数量（包括要添加的新元素在内），对底层数组进行空间重分配。 整数集合目前有三个元素，再加上新元素 65535 ，整数集合需要分配四个元素的空间，因为每个 int32_t 整数值需要占用 32 位空间，所以在空间重分配之后，底层数组的大小将是 32 * 4 = 128 位，如图 6-5 所示。 虽然程序对底层数组进行了空间重分配，但数组原有的三个元素 1 、 2 、 3 仍然是 int16_t 类型，这些元素还保存在数组的前 48 位里面，所以程序接下来要做的就是将这三个元素转换成 int32_t 类型，并将转换后的元素放置到正确的位上面，而且在放置元素的过程中，需要维持底层数组的有序性质不变。 首先，因为元素 3 在 1 、 2 、 3 、 65535 四个元素中排名第三，所以它将被移动到 contents 数组的索引 2 位置上，也即是数组 64 位至 95 位的空间内，如图 6-6 所示。 接着，因为元素 2 在 1 、 2 、 3 、 65535 四个元素中排名第二，所以它将被移动到 contents 数组的索引 1 位置上，也即是数组的 32 位至 63 位的空间内，如图 6-7 所示。 之后，因为元素 1 在 1 、 2 、 3 、 65535 四个元素中排名第一，所以它将被移动到 contents 数组的索引 0 位置上，也即是数组的 0 位至 31 位的空间内，如图 6-8 所示 然后，因为元素 65535 在 1 、 2 、 3 、 65535 四个元素中排名第四，所以它将被添加到 contents 数组的索引 3 位置上，也即是数组的 96 位至 127 位的空间内，如图 6-9 所示 最后，程序将整数集合 encoding 属性的值从 INTSET_ENC_INT16 改为 INTSET_ENC_INT32 ，并将 length 属性的值从 3 改为 4 ，设置完成之后的整数集合如图 6-10 所示 因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为 O(N) 。 其他类型的升级操作，比如从 INTSET_ENC_INT16 编码升级为 INTSET_ENC_INT64 编码，或者从 INTSET_ENC_INT32 编码升级为 INTSET_ENC_INT64 编码，升级的过程都和上面展示的升级过程类似。 升级之后新元素的摆放位置 因为引发升级的新元素的长度总是比整数集合现有所有元素的长度都大，所以这个新元素的值要么就大于所有现有元素，要么就小于所有现有元素： 在新元素小于所有现有元素的情况下，新元素会被放置在底层数组的最开头（索引 0 ）； 在新元素大于所有现有元素的情况下，新元素会被放置在底层数组的最末尾（索引 length-1 ） 升级的好处 提升灵活度 一般只使用 int16_t 类型的数组来保存 int16_t 类型的值，只使用 int32_t 类型的数组来保存 int32_t 类型的值，诸如此类。 整数集合通过自动升级底层数组来适应新元素，可以随意地将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中，而不必担心出现类型错误，这种做法非常灵活。 节约内存 让一个数组可以同时保存 int16_t 、 int32_t 、 int64_t 三种类型的值，最简单的做法就是直接使用 int64_t 类型的数组作为整数集合的底层实现。即使添加到整数集合里面的都是 int16_t 类型或者 int32_t 类型的值，数组都需要使用 int64_t 类型的空间去保存它们，从而出现浪费内存的情况。 整数集合的做法既可以让集合能同时保存三种不同类型的值，又可以确保升级操作只会在有需要的时候进行，这可以尽量节省内存。 如果一直只向整数集合添加 int16_t 类型的值，那么整数集合的底层实现就会一直是 int16_t 类型的数组，只有在将 int32_t 类型或者 int64_t 类型的值添加到集合时，程序才会对数组进行升级 降级 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。 举个例子，对于图 6-11 所示的整数集合来说，即使我们将集合里唯一一个真正需要使用 int64_t 类型来保存的元素 4294967295 删除了，整数集合的编码仍然会维持 INTSET_ENC_INT64 ，底层数组也仍然会是 int64_t 类型的，如图 6-12 所示 总结 整数集合是集合键的底层实现之一。 整数集合的底层实现为数组，这个数组以有序、无重复的方式保存集合元素，在有需要时，程序会根据新添加元素的类型，改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性，并且尽可能地节约了内存。 整数集合只支持升级操作，不支持降级操作。 压缩列表 压缩列表（ziplist）是列表键和哈希键的底层实现之一。 当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做列表键的底层实现。 比如说，执行以下命令将创建一个压缩列表实现的列表键： 1234redis&gt; RPUSH lst 1 3 5 10086 \"hello\" \"world\"(integer) 6 redis&gt; OBJECT ENCODING lst\"ziplist\" 因为列表键里面包含的都是 1 、 3 、 5 、 10086 这样的小整数值，以及 &quot;hello&quot; 、 &quot;world&quot; 这样的短字符串。 另外，当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做哈希键的底层实现。 举个例子，执行以下命令将创建一个压缩列表实现的哈希键： 1234redis&gt; HMSET profile \"name\" \"Jack\" \"age\" 28 \"job\" \"Programmer\"OK redis&gt; OBJECT ENCODING profile\"ziplist\" 因为哈希键里面包含的所有键和值都是小整数值或者短字符串。 本章将对压缩列表的定义以及相关操作进行详细的介绍 压缩列表 压缩列表是 Redis 为了节约内存而开发的，由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。 一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值。 图 7-1 展示了压缩列表的各个组成部分，表 7-1 则记录了各个组成部分的类型、长度、以及用途。 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数：在对压缩列表进行内存重分配，或者计算 zlend 的位置时使用。 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节：通过这个偏移量，程序无须遍历整个压缩列表就可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量：当这个属性的值小于 UINT16_MAX （65535）时，这个属性的值就是压缩列表包含节点的数量；当这个值等于 UINT16_MAX 时，节点的真实数量需要遍历整个压缩列表才能计算得出。 entryX 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。 图 7-2 展示了一个压缩列表示例： 列表 zlbytes 属性的值为 0x50 （十进制 80），表示压缩列表的总长为 80 字节。 列表 zltail 属性的值为 0x3c （十进制 60），这表示如果我们有一个指向压缩列表起始地址的指针 p ，那么只要用指针 p 加上偏移量 60 ，就可以计算出表尾节点 entry3 的地址。 列表 zllen 属性的值为 0x3 （十进制 3），表示压缩列表包含三个节点 图 7-3 展示了另一个压缩列表示例： 列表 zlbytes 属性的值为 0xd2 （十进制 210），表示压缩列表的总长为 210 字节。 列表 zltail 属性的值为 0xb3 （十进制 179），这表示如果我们有一个指向压缩列表起始地址的指针 p ，那么只要用指针 p 加上偏移量 179 ，就可以计算出表尾节点 entry5 的地址。 列表 zllen 属性的值为 0x5 （十进制 5），表示压缩列表包含五个节点。 压缩列表节点构成 每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种： 长度小于等于 63 （2^{6}-1）字节的字节数组； 长度小于等于 16383 （2^{14}-1） 字节的字节数组； 长度小于等于 4294967295 （2^{32}-1）字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4 位长，介于 0 至 12 之间的无符号整数； 1 字节长的有符号整数； 3 字节长的有符号整数； int16_t 类型整数； int32_t 类型整数； int64_t 类型整数。 每个压缩列表节点都由 previous_entry_length 、 encoding 、 content 三个部分组成，如图 7-4 所示 previous_entry_length 节点的 previous_entry_length 属性以字节为单位，记录了压缩列表中前一个节点的长度。 previous_entry_length 属性的长度可以是 1 字节或者 5 字节： 如果前一节点的长度小于 254 字节，那么 previous_entry_length 属性的长度为 1 字节：前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节，那么 previous_entry_length 属性的长度为 5 字节：其中属性的第一字节会被设置为 0xFE （十进制值 254），而之后的四个字节则用于保存前一节点的长度。 图 7-5 展示了一个包含一字节长 previous_entry_length 属性的压缩列表节点，属性的值为 0x05 ，表示前一节点的长度为 5 字节 图 7-6 展示了一个包含五字节长 previous_entry_length 属性的压缩节点，属性的值为 0xFE00002766 ， 值的最高位字节 0xFE 表示这是一个五字节长的 previous_entry_length 属性， 之后的四字节 0x00002766 （十进制值 10086 ）才是前一节点的实际长度 因为节点的 previous_entry_length 属性记录了前一个节点的长度，所以程序可以通过指针运算，根据当前节点的起始地址来计算出前一个节点的起始地址。 举个例子，如果我们有一个指向当前节点起始地址的指针 c ，那么我们只要用指针 c 减去当前节点 previous_entry_length 属性的值，就可以得出一个指向前一个节点起始地址的指针 p ，如图 7-7 所示。 压缩列表的从表尾向表头遍历操作就是使用这一原理实现的：只要我们拥有了一个指向某个节点起始地址的指针，那么通过这个指针以及这个节点的 previous_entry_length 属性，程序就可以一直向前一个节点回溯，最终到达压缩列表的表头节点。 图 7-8 展示了一个从表尾节点向表头节点进行遍历的完整过程： 首先，我们拥有指向压缩列表表尾节点 entry4 起始地址的指针 p1（指向表尾节点的指针可以通过指向压缩列表起始地址的指针加上 zltail 属性的值得出）； 通过用 p1 减去 entry4 节点 previous_entry_length 属性的值，我们得到一个指向 entry4 前一节点 entry3 起始地址的指针 p2 ； 通过用 p2 减去 entry3 节点 previous_entry_length 属性的值，我们得到一个指向 entry3 前一节点 entry2 起始地址的指针 p3 ； 通过用 p3 减去 entry2 节点 previous_entry_length 属性的值，我们得到一个指向 entry2 前一节点 entry1 起始地址的指针 p4 ，entry1 为压缩列表的表头节点； 最终，我们从表尾节点向表头节点遍历了整个列表。 encoding 节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度： 一字节、两字节或者五字节长，值的最高位为 00 、 01 或者 10 的是字节数组编码：这种编码表示节点的 content 属性保存着字节数组，数组的长度由编码除去最高两位之后的其他位记录； 一字节长，值的最高位以 11 开头的是整数编码：这种编码表示节点的 content 属性保存着整数值，整数值的类型和长度由编码除去最高两位之后的其他位记录； 表 7-2 记录了所有可用的字节数组编码，而表 7-3 则记录了所有可用的整数编码。表格中的下划线 _ 表示留空，而 b 、 x 等变量则代表实际的二进制数据，为了方便阅读，多个字节之间用空格隔开。 表7-2: 编码 编码长度 content 属性保存的值 00bbbbbb 1 字节 长度小于等于 63 字节的字节数组。 01bbbbbb xxxxxxxx 2 字节 长度小于等于 16383 字节的字节数组。 10**__** aaaaaaaa bbbbbbbb cccccccc dddddddd 5 字节 长度小于等于 4294967295 的字节数组。 表7-3: 编码 编码长度 content 属性保存的值 11000000 1 字节 int16_t 类型的整数。 11010000 1 字节 int32_t 类型的整数。 11100000 1 字节 int64_t 类型的整数。 11110000 1 字节 24 位有符号整数。 11111110 1 字节 8 位有符号整数。 1111xxxx 1 字节 使用这一编码的节点没有相应的 content 属性，因为编码本身的 xxxx 四个位已经保存了一个介于 0 和 12 之间的值，所以它无须 content 属性。 content 节点的 content 属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的 encoding 属性决定。 图 7-9 展示了一个保存字节数组的节点示例： 编码的最高两位 00 表示节点保存的是一个字节数组； 编码的后六位 001011 记录了字节数组的长度 11 ； content 属性保存着节点的值 &quot;hello world&quot; 。 图 7-10 展示了一个保存整数值的节点示例： 编码 11000000 表示节点保存的是一个 int16_t 类型的整数值； content 属性保存着节点的值 10086 。 连锁更新 前面说过，每个节点的 previous_entry_length 属性都记录了前一个节点的长度： 如果前一节点的长度小于 254 字节，那么 previous_entry_length 属性需要用 1 字节长的空间来保存这个长度值。 如果前一节点的长度大于等于 254 字节，那么 previous_entry_length 属性需要用 5 字节长的空间来保存这个长度值。 考虑这样一种情况：在一个压缩列表中，有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ，如图 7-11 所示。 因为 e1 至 eN 的所有节点的长度都小于 254 字节，所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性，换句话说，e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。 如果将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点，那么 new 将成为 e1 的前置节点，如图 7-12 所示。 因为 e1 的 previous_entry_length 属性仅长 1 字节，它没办法保存新节点 new 的长度，所以程序将对压缩列表执行空间重分配操作，并将 e1 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 现在，麻烦的事情来了 ——e1 原本的长度介于 250 字节至 253 字节之间，在为 previous_entry_length 属性新增四个字节的空间之后，e1 的长度就变成了介于 254 字节至 257 字节之间，而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。 因此，为了让 e2 的 previous_entry_length 属性可以记录下 e1 的长度，程序需要再次对压缩列表执行空间重分配操作，并将 e2 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 正如扩展 e1 引发了对 e2 的扩展一样，扩展 e2 也会引发对 e3 的扩展，而扩展 e3 又会引发对 e4 的扩展……为了让每个节点的 previous_entry_length 属性都符合压缩列表对节点的要求，程序需要不断地对压缩列表执行空间重分配操作，直到 eN 为止。 Redis 将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update），图 7-13 展示了这一过程。 除了添加新节点可能会引发连锁更新之外，删除节点也可能会引发连锁更新。 考虑图 7-14 所示的压缩列表，如果 e1 至 eN 都是大小介于 250 字节至 253 字节的节点，big 节点的长度大于等于 254 字节（需要 5 字节的 previous_entry_length 来保存），而 small 节点的长度小于 254 字节（只需要 1 字节的 previous_entry_length 来保存），那么当我们将 small 节点从压缩列表中删除之后，为了让 e1 的 previous_entry_length 属性可以记录 big 节点的长度，程序将扩展 e1 的空间，并由此引发之后的连锁更新 因为连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配操作，而每次空间重分配的最坏复杂度为 O(N) ，所以连锁更新的最坏复杂度为 O(N^2) 。 要注意的是，尽管连锁更新的复杂度较高，但它真正造成性能问题的几率是很低的： 首先，压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点，连锁更新才有可能被引发，在实际中，这种情况并不多见； 其次，即使出现连锁更新，但只要被更新的节点数量不多，就不会对性能造成任何影响：比如说，对三五个节点进行连锁更新是绝对不会影响性能的； 因为以上原因，ziplistPush 等命令的平均复杂度仅为 O(N) ，在实际中，我们可以放心地使用这些函数，而不必担心连锁更新会影响压缩列表的性能。 总结 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表，或者从压缩列表中删除节点，可能会引发连锁更新操作，但这种操作出现的几率并不高。 参考链接 https://www.bookstack.cn/read/redisbook/ https://www.cnblogs.com/yinbiao/p/10740212.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Go入门11-基础类型","slug":"Go/Go入门11-基础类型","date":"2021-09-12T05:15:43.000Z","updated":"2021-09-12T05:16:12.000Z","comments":true,"path":"2021/09/12/Go/Go入门11-基础类型/","link":"","permalink":"http://xboom.github.io/2021/09/12/Go/Go%E5%85%A5%E9%97%A811-%E5%9F%BA%E7%A1%80%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"go源码下载地址 golang的数据类型和数据结构的底层实现 Go的类型系统 Golang的类型分为以下几类： 命名类型[named (defined) type]具有名称的类型：int,int64,float32,string,bool。这些GO预先声明好了 通过类型声明(type declaration)创建的所有类型都是命令类型 123var i int //named typetype myInt int //named typevar b bool //named type 一个命名类型一定和其他类型不同 未命名类型(unnamed type):数组，结构体，指针，函数，接口，切片,map,通道都是未命令类型 123[]string // unnamed typemap[string]string // unnamed type[10]int // unnamed type 虽然没有名字，但却有一个类型字面量(type literal)来描述它们由什么构成 基础类型 任何类型T都有基础类型 如果T 是预先声明类型：boolean, numeric, or string（布尔，数值，字符串）中的一个，或者是一个类型字面量(type literal)，他们对应的基础类型就是T自身。 T的基础类型就是T所引用的那个类型的类型声明(type declaration) 基本类型 在/src/runtime/type.go 对 type类型进行了定义 123456789101112131415161718type _type struct &#123; size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; string类型 在 /src/runtime/string.go 对string类型进行了声明 1234type stringStruct struct &#123; str unsafe.Pointer len int&#125; 参考链接 深入研究Go(golang) Type类型系统","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门3-Groutine","slug":"Go/Go入门3-Groutine","date":"2021-09-08T16:08:34.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/09/09/Go/Go入门3-Groutine/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go%E5%85%A5%E9%97%A83-Groutine/","excerpt":"","text":"创建一个协程 1go func()//通过go关键字启动一个协程来运行函数 那么它到底干了什么 123456789101112131415func newproc(siz int32, fn *funcval) &#123; argp := add(unsafe.Pointer(&amp;fn), sys.PtrSize) gp := getg() pc := getcallerpc() systemstack(func() &#123; newg := newproc1(fn, argp, siz, gp, pc) _p_ := getg().m.p.ptr() runqput(_p_, newg, true) if mainStarted &#123; wakep() &#125; &#125;)&#125; 关键术语 并发：一个cpu上能同时执行多项任务，在很短时间内，cpu来回切换任务执行(在某段很短时间内执行程序a，然后又迅速得切换到程序b去执行)，有时间上的重叠（宏观上是同时的，微观仍是顺序执行）,这样看起来多个任务像是同时执行，这就是并发 并行：当系统有多个CPU时,每个CPU同一时刻都运行任务，互不抢占自己所在的CPU资源，同时进行，称为并行 进程：cpu在切换程序的时候，如果不保存上一个程序的状态（也就是context–上下文），直接切换下一个程序，就会丢失上一个程序的一系列状态，于是引入了进程这个概念，用以划分好程序运行时所需要的资源。因此进程就是一个程序运行时候的所需要的基本资源单位（也可以说是程序运行的一个实体） 线程：cpu切换多个进程的时候，会花费不少的时间，因为切换进程需要切换到内核态，而每次调度需要内核态都需要读取用户态的数据，进程一旦多起来，cpu调度会消耗一大堆资源，因此引入了线程的概念，线程本身几乎不占有资源，他们共享进程里的资源，内核调度起来不会那么像进程切换那么耗费资源 协程：协程拥有自己的寄存器上下文和栈。 协程与线程 goroutine与thread的不同 内存占用 一个 goroutine 的栈内存消耗为 2 KB(如果栈空间不够用，会自动进行扩容)。 一个 thread 则需要消耗 1 MB 栈内存，还需要一个被称为 “a guard page” 的区域(用于与其他thread隔离) 创建和销毀 goroutine的切换会消耗200ns(用户态，3个寄存器)，相当于2400-3600条指令 除了使用时需要陷入内核，线程切换会消耗1000-1500ns 1ns平均可执行12-18条指令 当 threads 切换时，需要保存各种寄存器，以便将来恢复： 16 general purpose registers: 通用寄存器 PC (Program Counter): 程序计数器 SP (Stack Pointer): 栈指针 segment registers: 段寄存器 16 XMM registers: FP coprocessor state 16 AVX registers all MSRs etc 而 goroutines 切换只需保存三个寄存器 Program Counter Stack Pointer BP：基址指针寄存器，常用于在访问内存时存放内存单元的偏移地址 Thread内存堆栈 创建一个 thread 为了尽量避免极端情况下操作系统线程栈的溢出，默认会为其分配一个较大的栈内存( 1 - 8 MB 栈内存，线程标准 POSIX Thread)，而且还需要一个被称为 guard page 的区域用于和其他 thread 的栈空间进行隔离。而栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险 调度模型 Go 程序的执行由两层组成：Go Program，Runtime(即用户程序和运行时) Go创建M个线程(CPU执行调度的单元，内核的task_struck)，之后创建N个goroutine会依附在M个线程上执行即M:N模型。当M指定了线程栈，则M.stack-&gt;G.stack，M的PC寄存器指向G提供的函数，然后执行 GMP Go的调度器内部有四个重要的结构：M，P，G，Sched M:M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息 G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。 P:代表一个Processor(Core 虚拟处理器)，它的主要用途就是用来执行goroutine的，维护了一个需要执行goroutine的队列(LRQ Local Run Queue) Sched:代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等 LRQ：本地运行队列，它属于每个处理器，以便管理分配给要执行的goroutines GRQ：全局运行队列，存在于未分配的goroutine中 下面四种情形下，Go scheduler有机会进行调度 使用关键字go: go创建一个新的goroutine, Go scheduler 会考虑调度 GC: 由于进行 GC 的 goroutine 也需要在 M 上运行，因此肯定会发生调度。当然，Go scheduler 还会做很多其他的调度，例如调度不涉及堆访问的 goroutine 来运行。GC 不管栈上的内存，只会回收堆上的内存 系统调用：当 goroutine 进行系统调用时，会阻塞 M，所以它会被调度走，同时一个新的 goroutine 会被调度上来 内存同步访问：atomic，mutex，channel 操作等会使 goroutine 阻塞，因此会被调度走。等条件满足后（例如其他 goroutine 解锁了）还会被调度上来继续运行 M:N 模型 Go runtime 会负责 goroutine 的生老病死，从创建到销毁。Runtime 会在程序启动的时候，创建 M 个线程(CPU 执行调度的单位)，之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行。这就是 M:N 模型： 在同一时刻，一个线程上只能跑一个 goroutine。当 goroutine 发生阻塞(例如向一个 channel 发送数据，被阻塞)时，runtime 会把当前 goroutine 调度走，让其他 goroutine 来执行。目的就是不让一个线程闲着，榨干 CPU 的每一滴油水 工作窃取 Go scheduler 的职责就是将所有处于 runnable 的 goroutines 均匀分布到在 P 上运行的 M。当一个 P 发现自己的 LRQ 已经没有 G 时，会从其他 P “偷” 一些 G 来运行。这被称为 Work-stealing Go scheduler 使用 M:N 模型，在任一时刻，M 个 goroutines（G） 要分配到 N 个内核线程（M），这些 M 跑在个数最多为 GOMAXPROCS 的逻辑处理器（P）上。每个 M 必须依附于一个 P，每个 P 在同一时刻只能运行一个 M。如果 P 上的 M 阻塞了，那它就需要其他的 M 来运行 P 的 LRQ 里的 goroutines Go scheduler 每一轮调度要做的工作就是找到处于 runnable 的 goroutines，并执行它。找的顺序如下： 12345678runtime.schedule() &#123; // only 1/61 of the time, check the global runnable queue for a G. // if not found, check the local queue. // if not found, // try to steal from other Ps. // if not, check the global runnable queue. // if not found, poll network.&#125; 找到一个可执行的 goroutine 后，就会一直执行下去，直到被阻塞 当 P2 上的一个 G 执行结束，就会去 LRQ 获取下一个 G 来执行。如果 LRQ 已经空了，就是说本地可运行队列已经没有 G 需要执行，并且这时 GRQ 也没有 G 了。这时，P2 会随机选择一个 P（称为 P1），P2 会从 P1 的 LRQ “偷”过来一半的 G 状态切换 GPM 共同成就 Go scheduler。G 需要在 M 上才能运行，M 依赖 P 提供的资源，P 则持有待运行的 G。 M 会从与它绑定的 P 的本地队列获取可运行的 G，也会从 network poller 里获取可运行的 G，还会从其他 P 偷 G G 的状态流转： 省略了一些垃圾回收的状态 P 的状态流转： 通常情况下（在程序运行时不调整 P 的个数），P 只会在上图中的四种状态下进行切换。 当程序刚开始运行进行初始化时，所有的 P 都处于 _Pgcstop 状态， 随着 P 的初始化（runtime.procresize），会被置于 _Pidle。 当 M 需要运行时，会 runtime.acquirep 来使 P 变成 Prunning 状态，并通过 runtime.releasep 来释放。 当 G 执行时需要进入系统调用，P 会被设置为 _Psyscall， 如果这个时候被系统监控抢夺（runtime.retake），则 P 会被重新修改为 _Pidle。 如果在程序运行中发生 GC，则 P 会被设置为 _Pgcstop， 并在 runtime.startTheWorld 时重新调整为 _Prunning M 的状态变化： M 只有自旋和非自旋两种状态。自旋的时候，会努力找工作；找不到的时候会进入非自旋状态，之后会休眠，直到有工作需要处理时，被其他工作线程唤醒，又进入自旋状态 调度器 Go调度程序不是抢占式调度程序，而是协作式调度程序。 成为协作调度程序意味着调度程序需要在代码的安全点明确定义用户空间事件，以制定调度决策。 以下是调度的关键点： 启动协程的关键字go 垃圾回收GC 系统调用 对于异步系统调用例如网络请求，将可能阻止的goroutine移到网络轮询器，让处理程序可以执行下一个 处理像文件IO同步请求，当前的G和M对将与G，P，M模型分开。 同时，将创建一台新机器，以保持原始的G，P，M模型正常工作，并且在系统调用完成时将收回块goroutine 地鼠(gopher)用小车运着一堆待加工的砖。M就可以看作图中的地鼠，P就是小车，G就是小车里装的砖。一图胜千言啊，弄清楚了它们三者的关系，下面我们就开始重点聊地鼠是如何在搬运砖块的 启动过程 runtime·schedinit 调度器初始化：主要是根据用户设置的GOMAXPROCS来创建一批小车§,不管设置多大，最多也只能创建256个小车§。这些小车§初始创建好后都是闲置状态，也就是还没开始使用，所以它们都放置在调度器结构(Sched)的pidle字段维护的链表中存储起来了，以备后续之需。 runtime.newproc 创建一个协程 runtime.main runtime·mstart 源码阅读 G 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990type g struct &#123; // goroutine 使用的栈 stack stack // offset known to runtime/cgo // 用于栈的扩张和收缩检查，抢占标志 stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink _defer *_defer // innermost defer //当前与g 绑定的 m m *m // current m; offset known to arm liblink // goroutine 的运行现场 sched gobuf syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc stktopsp uintptr // expected sp at top of stack, to check in traceback // wakeup 时传入的参数 param unsafe.Pointer atomicstatus uint32 stackLock uint32 // sigprof/scang lock; TODO: fold in to atomicstatus goid int64 schedlink guintptr // g 被阻塞之后的近似时间 waitsince int64 // approx time when the g become blocked // g 被阻塞的原因 waitreason waitReason // if status==Gwaiting // 抢占调度标志。这个为 true 时，stackguard0 等于 stackpreempt preempt bool // preemption signal, duplicates stackguard0 = stackpreempt preemptStop bool // transition to _Gpreempted on preemption; otherwise, just deschedule preemptShrink bool // shrink stack at synchronous safe point // asyncSafePoint is set if g is stopped at an asynchronous // safe point. This means there are frames on the stack // without precise pointer information. asyncSafePoint bool paniconfault bool // panic (instead of crash) on unexpected fault address gcscandone bool // g has scanned stack; protected by _Gscan bit in status throwsplit bool // must not split stack // activeStackChans indicates that there are unlocked channels // pointing into this goroutine's stack. If true, stack // copying needs to acquire channel locks to protect these // areas of the stack. activeStackChans bool // parkingOnChan indicates that the goroutine is about to // park on a chansend or chanrecv. Used to signal an unsafe point // for stack shrinking. It's a boolean value, but is updated atomically. parkingOnChan uint8 raceignore int8 // ignore race detection events sysblocktraced bool // StartTrace has emitted EvGoInSyscall about this goroutine tracking bool // whether we're tracking this G for sched latency statistics trackingSeq uint8 // used to decide whether to track this G runnableStamp int64 // timestamp of when the G last became runnable, only used when tracking runnableTime int64 // the amount of time spent runnable, cleared when running, only used when tracking // syscall 返回之后的 cputicks，用来做 tracing sysexitticks int64 // cputicks when syscall has returned (for tracing) traceseq uint64 // trace event sequencer tracelastp puintptr // last P emitted an event for this goroutine // 如果调用了 LockOsThread，那么这个 g 会绑定到某个 m 上 lockedm muintptr sig uint32 writebuf []byte sigcode0 uintptr sigcode1 uintptr sigpc uintptr // 创建该 goroutine 的语句的指令地址 gopc uintptr // pc of go statement that created this goroutine ancestors *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors) // goroutine 函数的指令地址 startpc uintptr // pc of goroutine function racectx uintptr waiting *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order cgoCtxt []uintptr // cgo traceback context labels unsafe.Pointer // profiler labels // time.Sleep 缓存的定时器 timer *timer // cached timer for time.Sleep selectDone uint32 // are we participating in a select and did someone win the race? // Per-G GC state // gcAssistBytes is this G's GC assist credit in terms of // bytes allocated. If this is positive, then the G has credit // to allocate gcAssistBytes bytes without assisting. If this // is negative, then the G must correct this by performing // scan work. We track this in bytes to make it fast to update // and check for debt in the malloc hot path. The assist ratio // determines how this corresponds to scan work debt. gcAssistBytes int64&#125; 其中g结构体关联了两个比较简单的结构体，stack 表示 goroutine 运行时的栈 123456789101112131415161718192021// 描述栈的数据结构，栈的范围：[lo, hi)type stack struct &#123; // 栈顶，低地址 lo uintptr // 栈低，高地址 hi uintptr&#125;type gobuf struct &#123; // 存储 rsp 寄存器的值 sp uintptr // 存储 rip 寄存器的值 pc uintptr // 指向 goroutine g guintptr ctxt unsafe.Pointer // this has to be a pointer so that gc scans it // 保存系统调用的返回值 ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer&#125; M 再来看 M，取 machine 的首字母，它代表一个工作线程，或者说系统线程。G 需要调度到 M 上才能运行，M 是真正工作的人。结构体 m 就是我们常说的 M，它保存了 M 自身使用的栈信息、当前正在 M 上执行的 G 信息、与之绑定的 P 信息…… 当 M 没有工作可做的时候，在它休眠前，会“自旋”地来找工作：检查全局队列，查看 network poller，试图执行 gc 任务，或者“偷”工作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// m 代表工作线程，保存了自身使用的栈信息type m struct &#123; // 记录工作线程（也就是内核线程）使用的栈信息。在执行调度代码时需要使用 // 执行用户 goroutine 代码时，使用用户 goroutine 自己的栈，因此调度时会发生栈的切换 g0 *g // goroutine with scheduling stack morebuf gobuf // gobuf arg to morestack divmod uint32 // div/mod denominator for arm - known to liblink // Fields not known to debuggers. procid uint64 // for debuggers, but offset not hard-coded gsignal *g // signal-handling g goSigStack gsignalStack // Go-allocated signal handling stack sigmask sigset // storage for saved signal mask tls [tlsSlots]uintptr // thread-local storage (for x86 extern register) mstartfn func() curg *g // current running goroutine caughtsig guintptr // goroutine running during fatal signal p puintptr // attached p for executing go code (nil if not executing go code) nextp puintptr oldp puintptr // the p that was attached before executing a syscall id int64 mallocing int32 throwing int32 preemptoff string // if != \"\", keep curg running on this m locks int32 dying int32 profilehz int32 spinning bool // m is out of work and is actively looking for work blocked bool // m is blocked on a note newSigstack bool // minit on C thread called sigaltstack printlock int8 incgo bool // m is executing a cgo call freeWait uint32 // if == 0, safe to free g0 and delete m (atomic) fastrand [2]uint32 needextram bool traceback uint8 ncgocall uint64 // number of cgo calls in total ncgo int32 // number of cgo calls currently in progress cgoCallersUse uint32 // if non-zero, cgoCallers in use temporarily cgoCallers *cgoCallers // cgo traceback if crashing in cgo call doesPark bool // non-P running threads: sysmon and newmHandoff never use .park park note alllink *m // on allm schedlink muintptr lockedg guintptr createstack [32]uintptr // stack that created this thread. lockedExt uint32 // tracking for external LockOSThread lockedInt uint32 // tracking for internal lockOSThread nextwaitm muintptr // next m waiting for lock waitunlockf func(*g, unsafe.Pointer) bool waitlock unsafe.Pointer waittraceev byte waittraceskip int startingtrace bool syscalltick uint32 freelink *m // on sched.freem // mFixup is used to synchronize OS related m state // (credentials etc) use mutex to access. To avoid deadlocks // an atomic.Load() of used being zero in mDoFixupFn() // guarantees fn is nil. mFixup struct &#123; lock mutex used uint32 fn func(bool) bool &#125; // these are here because they are too large to be on the stack // of low-level NOSPLIT functions. libcall libcall libcallpc uintptr // for cpu profiler libcallsp uintptr libcallg guintptr syscall libcall // stores syscall parameters on windows vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call) vdsoPC uintptr // PC for traceback while in VDSO call // preemptGen counts the number of completed preemption // signals. This is used to detect when a preemption is // requested, but fails. Accessed atomically. preemptGen uint32 // Whether this is a pending preemption signal on this M. // Accessed atomically. signalPending uint32 dlogPerM mOS // Up to 10 locks held by this m, maintained by the lock ranking code. locksHeldLen int locksHeld [10]heldLockInfo&#125; P 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140type p struct &#123; id int32 status uint32 // one of pidle/prunning/... link puintptr schedtick uint32 // incremented on every scheduler call syscalltick uint32 // incremented on every system call sysmontick sysmontick // last tick observed by sysmon m muintptr // back-link to associated m (nil if idle) mcache *mcache pcache pageCache raceprocctx uintptr deferpool [5][]*_defer // pool of available defer structs of different sizes (see panic.go) deferpoolbuf [5][32]*_defer // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen. goidcache uint64 goidcacheend uint64 // Queue of runnable goroutines. Accessed without lock. runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready'd by // the current G and should be run next instead of what's in // runq if there's time remaining in the running G's time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready'd // goroutines to the end of the run queue. // // Note that while other P's may atomically CAS this to zero, // only the owner P can CAS it to a valid G. runnext guintptr // Available G's (status == Gdead) gFree struct &#123; gList n int32 &#125; sudogcache []*sudog sudogbuf [128]*sudog // Cache of mspan objects from the heap. mspancache struct &#123; // We need an explicit length here because this field is used // in allocation codepaths where write barriers are not allowed, // and eliminating the write barrier/keeping it eliminated from // slice updates is tricky, moreso than just managing the length // ourselves. len int buf [128]*mspan &#125; tracebuf traceBufPtr // traceSweep indicates the sweep events should be traced. // This is used to defer the sweep start event until a span // has actually been swept. traceSweep bool // traceSwept and traceReclaimed track the number of bytes // swept and reclaimed by sweeping in the current sweep loop. traceSwept, traceReclaimed uintptr palloc persistentAlloc // per-P to avoid mutex _ uint32 // Alignment for atomic fields below // The when field of the first entry on the timer heap. // This is updated using atomic functions. // This is 0 if the timer heap is empty. timer0When uint64 // The earliest known nextwhen field of a timer with // timerModifiedEarlier status. Because the timer may have been // modified again, there need not be any timer with this value. // This is updated using atomic functions. // This is 0 if there are no timerModifiedEarlier timers. timerModifiedEarliest uint64 // Per-P GC state gcAssistTime int64 // Nanoseconds in assistAlloc gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic) // gcMarkWorkerMode is the mode for the next mark worker to run in. // That is, this is used to communicate with the worker goroutine // selected for immediate execution by // gcController.findRunnableGCWorker. When scheduling other goroutines, // this field must be set to gcMarkWorkerNotWorker. gcMarkWorkerMode gcMarkWorkerMode // gcMarkWorkerStartTime is the nanotime() at which the most recent // mark worker started. gcMarkWorkerStartTime int64 // gcw is this P's GC work buffer cache. The work buffer is // filled by write barriers, drained by mutator assists, and // disposed on certain GC state transitions. gcw gcWork // wbBuf is this P's GC write barrier buffer. // // TODO: Consider caching this in the running G. wbBuf wbBuf runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point // statsSeq is a counter indicating whether this P is currently // writing any stats. Its value is even when not, odd when it is. statsSeq uint32 // Lock for timers. We normally access the timers while running // on this P, but the scheduler can also do it from a different P. timersLock mutex // Actions to take at some time. This is used to implement the // standard library's time package. // Must hold timersLock to access. timers []*timer // Number of timers in P's heap. // Modified using atomic instructions. numTimers uint32 // Number of timerDeleted timers in P's heap. // Modified using atomic instructions. deletedTimers uint32 // Race context used while executing timer functions. timerRaceCtx uintptr // preempt is set to indicate that this P should be enter the // scheduler ASAP (regardless of what G is running on it). preempt bool // Padding is no longer needed. False sharing is now not a worry because p is large enough // that its size class is an integer multiple of the cache line size (for any of our architectures).&#125; Sched Go scheduler 在源码中的结构体为 schedt，保存调度器的状态信息、全局的可运行 G 队列等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112// 保存调度器的信息type schedt struct &#123; // accessed atomically. keep at top to ensure alignment on 32-bit systems. // 需以原子访问访问。 // 保持在 struct 顶部，以使其在 32 位系统上可以对齐 goidgen uint64 lastpoll uint64 // time of last network poll, 0 if currently polling pollUntil uint64 // time to which current poll is sleeping lock mutex // 由空闲的工作线程组成的链表 midle muintptr // idle m's waiting for work // 空闲的工作线程数量 nmidle int32 // number of idle m's waiting for work // 空闲的且被 lock 的 m 计数 nmidlelocked int32 // number of locked m's waiting for work // 已经创建的工作线程数量 mnext int64 // number of m's that have been created and next M ID // 表示最多所能创建的工作线程数量 maxmcount int32 // maximum number of m's allowed (or die) // goroutine 的数量，自动更新 nmsys int32 // number of system m's not counted for deadlock //累计释放的m数量 nmfreed int64 // cumulative number of freed m's // 由空闲的 p 结构体对象组成的链表 ngsys uint32 // number of system goroutines; updated atomically // 空闲的 p 结构体对象的数量 pidle puintptr // idle p's npidle uint32 nmspinning uint32 // See \"Worker thread parking/unparking\" comment in proc.go. // Global runnable queue. // 全局可运行的 G队列 runq gQueue runqsize int32 // disable controls selective disabling of the scheduler. // // Use schedEnableUser to control this. // // disable is protected by sched.lock. disable struct &#123; // user disables scheduling of user goroutines. user bool runnable gQueue // pending runnable Gs n int32 // length of runnable &#125; // Global cache of dead G's. // dead G 的全局缓存 // 已退出的 goroutine 对象，缓存下来 // 避免每次创建 goroutine 时都重新分配内存 gFree struct &#123; lock mutex stack gList // Gs with stacks noStack gList // Gs without stacks n int32 &#125; // Central cache of sudog structs. // sudog 结构的集中缓存 sudoglock mutex sudogcache *sudog // Central pool of available defer structs of different sizes. // 不同大小的可用的 defer struct 的集中缓存池 deferlock mutex deferpool [5]*_defer // freem is the list of m's waiting to be freed when their // m.exited is set. Linked through m.freelink. freem *m gcwaiting uint32 // gc is waiting to run stopwait int32 stopnote note sysmonwait uint32 sysmonnote note // While true, sysmon not ready for mFixup calls. // Accessed atomically. sysmonStarting uint32 // safepointFn should be called on each P at the next GC // safepoint if p.runSafePointFn is set. safePointFn func(*p) safePointWait int32 safePointNote note profilehz int32 // cpu profiling rate procresizetime int64 // nanotime() of last change to gomaxprocs totaltime int64 // ∫gomaxprocs dt up to procresizetime // sysmonlock protects sysmon's actions on the runtime. // // Acquire and hold this mutex to block sysmon from interacting // with the rest of the runtime. sysmonlock mutex _ uint32 // ensure timeToRun has 8-byte alignment // timeToRun is a distribution of scheduling latencies, defined // as the sum of time a G spends in the _Grunnable state before // it transitions to _Grunning. // // timeToRun is protected by sched.lock. timeToRun timeHistogram&#125; 还有一些重要的常量 1234567891011121314151617181920212223242526// 所有 g 的长度allglen uintptr// 保存所有的 gallgs []*g// 保存所有的 mallm *m// 保存所有的 p，_MaxGomaxprocs = 1024allp [_MaxGomaxprocs + 1]*p// p 的最大值，默认等于 ncpugomaxprocs int32// 程序启动时，会调用 osinit 函数获得此值ncpu int32// 调度器结构体对象，记录了调度器的工作状态sched schedt// 代表进程的主线程m0 m// m0 的 g0，即 m0.g0 = &amp;g0g0 g 参考文献： https://colobu.com/2017/05/04/golang-runtime-scheduler/ http://morsmachine.dk/go-scheduler https://www.zhihu.com/question/20862617 https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html https://www.bookstack.cn/read/go-internals/zh-05.1.md https://golang.design/go-questions/sched/goroutine-vs-thread/ https://learnku.com/articles/41728 https://www.codercto.com/a/38162.html https://www.cnblogs.com/flhs/p/12677335.html","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门6-内存管理","slug":"Go/Go入门6-内存管理","date":"2021-09-08T16:08:22.000Z","updated":"2023-03-09T11:53:41.985Z","comments":true,"path":"2021/09/09/Go/Go入门6-内存管理/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go%E5%85%A5%E9%97%A86-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"带着问题看世界 内存是如何管理的 如何根据指定的大小分配内存的 基础概念 Go在程序启动的时候，会先向操作系统申请一块内存(注意这时还只是一段虚拟的地址空间，并不会真正地分配内存)，切成小块后自己进行管理。申请到的内存块被分配了三个区域，在X64上分别是512MB，16GB，512GB大小。 arena区域就是所谓的堆区，Go动态分配的内存都是在这个区域，它把内存分割成8KB大小的页，一些页组合起来称为mspan。 bitmap区标识arena区域哪些地址保存了对象，并且用4bit标志位表示对象是否包含指针、GC标记信息。bitmap中一个byte大小的内存对应arena区域中4个指针大小(指针大小为 8B)的内存，所以bitmap区域的大小是512GB/(4*8B)=16GB 从上图其实还可以看到bitmap的高地址部分指向arena区域的低地址部分，也就是说bitmap的地址是由高地址向低地址增长的。 spans区域存放mspan(也就是一些arena分割的页组合起来的内存管理基本单元)的指针，每个指针对应一页，所以spans区域的大小就是512GB/8KB*8B=512MB。除以8KB是计算arena区域的页数，而最后乘以8是计算spans区域所有指针的大小。创建mspan的时候，按页填充对应的spans区域，在回收object时，根据地址很容易就能找到它所属的mspan 内存管理单元 mspan：Go中内存管理的基本单元，是由一片连续的8KB的页组成的大块内存。它是一个包含起始地址、mspan规格、页的数量等内容的双端链表。 每个mspan按照它自身的属性Size Class的大小分割成若干个object，每个object可存储一个对象。并且会使用一个位图来标记其尚未使用的object，属性Size Class决定object大小，而mspan只会分配给和object尺寸大小接近的对象，当然，对象的大小要小于object大小。还有一个概念：Span Class，它和Size Class的含义差不多，Size_Class = Span_Class / 2。 为什么乘以2，因为每个 Size Class有两个mspan，也就是有两个Span Class。其中一个分配给含有指针的对象，另一个分配给不含有指针的对象。如何寻找为对象寻span，寻找span的流程如下： 计算对象所需内存大小size。 根据size到size class映射，计算出所需的size class。 根据size class和对象是否包含指针计算出span class。 获取该span class指向的span。 如下图，mspan由一组连续的页组成，按照一定大小划分成object。 mspan的Size Class共有67种，每种mspan分割的object大小是8*2n的倍数，这个是写死在代码里的： 1234// path: /usr/local/go/src/runtime/sizeclasses.goconst _NumSizeClasses = 67var class_to_size = [_NumSizeClasses]uint16&#123;0, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536,1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768&#125; 数组里最大的数是32768，也就是32KB，超过此大小就是大对象；类型Size Class为0表示大对象，它实际上直接由堆内存分配，而小对象都要通过mspan来分配。 对于mspan来说，它的Size Class会决定它所能分到的页数，这也是写死在代码里的： 1234// path: /usr/local/go/src/runtime/sizeclasses.goconst _NumSizeClasses = 67var class_to_allocnpages = [_NumSizeClasses]uint8&#123;0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 2, 3, 4, 5, 6, 1, 7, 6, 5, 4, 3, 5, 7, 2, 9, 7, 5, 8, 3, 10, 7, 4&#125; 比如要申请一个object大小为32B的mspan的时候，在class_to_size里对应的索引是3，而索引3在class_to_allocnpages数组里对应的页数就是1。 mspan结构体定义 12345678910111213141516171819202122232425262728type mspan struct &#123; //链表前向指针，用于将span链接起来 next *mspan //链表前向指针，用于将span链接起来 prev *mspan // 起始地址，也即所管理页的地址 startAddr uintptr // 管理的页数 npages uintptr // 块个数，表示有多少个块可供分配 nelems uintptr //分配位图，每一位代表一个块是否已分配 allocBits *gcBits // 已分配块的个数 allocCount uint16 // class表中的class ID，和Size Classs相关 spanclass spanClass // class表中的对象大小，也即块大小 elemsize uintptr &#125; 将mspan放到更大的视角来看： 上图可以看到有两个S指向了同一个mspan，因为这两个S指向的P是同属一个mspan的。所以，通过arena上的地址可以快速找到指向它的S，通过S就能找到mspan，回忆一下前面我们说的mspan区域的每个指针对应一页。 假设最左边第一个mspan的Size Class等于10，根据前面的class_to_size数组，得出这个msapn分割的object大小是144B，算出可分配的对象个数是8KB/144B=56.89个，取整56个，所以会有一些内存浪费掉了，Go的源码里有所有Size Class的mspan浪费的内存的大小；再根据class_to_allocnpages数组，得到这个mspan只由1个page组成；假设这个mspan是分配给无指针对象的，那么spanClass等于20。 startAddr直接指向arena区域的某个位置，表示这个mspan的起始地址，allocBits指向一个位图，每位代表一个块是否被分配了对象；allocCount则表示总共已分配的对象个数。 这样，左起第一个mspan的各个字段参数就如下图所示： 内存管理元件 内存分配由内存分配器完成。分配器由3种组件构成：mcache, mcentral, mheap Span 是 Go 内存管理的基本单位，代码中为 mspan，由一组连续的 page 组成 mcache 与TCMalloc中的ThreadCache类似，mcache保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以无锁访问。 Go中是每个P拥有1个mcache，最多需要 GOMAXPROCS 个mcache就可以保证各线程对mcache的无锁访问 mcentral 与TCMalloc中的CentralCache不同的是CentralCache 是每个级别的Span有1个链表，mcache是每个级别的Span有2个链表。为什么有两个？ mheap 与TCMalloc中的PageHeap不同点的是 mheap把Span组织成了树结构，而不是链表，并且还是2棵树，然后把Span分配到heapArena进行管理，它包含地址映射和span是否包含指针等位图，这样做的主要原因是为了更高效的利用内存：分配、回收和再利用 Go的内存管理基本单位是span，每个span通过spanclass标识属于哪种规格的span，golang的span规格一共有67种，详细可查看 src/runtime/sizeclasses.go mcache mcache：每个工作线程都会绑定一个mcache，本地缓存可用的mspan资源，这样就可以直接给Goroutine分配，因为不存在多个Goroutine竞争的情况，所以不会消耗锁资源。 mcache的结构体定义： 12345type mcache struct &#123; alloc [numSpanClasses]*mspan&#125;numSpanClasses = _NumSizeClasses &lt;&lt; 1 mcache用Span Classes作为索引管理多个用于分配的mspan，它包含所有规格的mspan。它是_NumSizeClasses的2倍，也就是67*2=134 为什么有一个两倍的关系：为了加速之后内存回收的速度，数组里一半的mspan中分配的对象不包含指针，另一半则包含指针。而无指针的mspan在进行垃圾回收的时候是不需要扫描它是否引用了其他活跃对象的。 mcache在初始化的时候是没有任何mspan资源的，在使用过程中会动态地从mcentral申请，之后会缓存下来。当对象小于等于32KB大小时，使用mcache的相应规格的mspan进行分配 mcentral mcentral：为所有mcache提供切分好的mspan资源。每个central保存一种特定大小的全局mspan列表，包括已分配出去的和未分配出去的。 每个mcentral对应一种mspan，而mspan的种类导致它分割的object大小不同。当工作线程的mcache中没有合适（也就是特定大小的）的mspan时就会从mcentral获取。 mcentral被所有的工作线程共同享有，存在多个Goroutine竞争的情况，因此会消耗锁资源。结构体定义： 1234567891011121314151617//path: /usr/local/go/src/runtime/mcentral.gotype mcentral struct &#123; // 互斥锁 lock mutex // 规格 sizeclass int32 // 尚有空闲object的mspan链表 nonempty mSpanList // 没有空闲object的mspan链表，或者是已被mcache取走的msapn链表 empty mSpanList // 已累计分配的对象个数 nmalloc uint64 &#125; empty表示这条链表里的mspan都被分配了object，或者是已经被cache取走了的mspan，这个mspan就被那个工作线程独占了。而nonempty则表示有空闲对象的mspan列表。每个central结构体都在mheap中维护。 mcache从mcentral获取和归还mspan的流程： 获取 加锁； 从nonempty链表找到一个可用的mspan； 并将其从nonempty链表删除； 将取出的mspan加入到empty链表； 将mspan返回给工作线程； 解锁。 归还 加锁； 将mspan从empty链表删除； 将mspan加入到nonempty链表； 解锁。 mheap mheap：代表Go程序持有的所有堆空间，Go程序使用一个mheap的全局对象_mheap来管理堆内存。 当mcentral没有空闲的mspan时，会向mheap申请。 而mheap没有资源时，会向操作系统申请新内存。 mheap主要用于大对象的内存分配，以及管理未切割的mspan，用于给mcentral切割成小对象。 mheap中含有所有规格的mcentral，所以，当一个mcache从mcentral申请mspan时，只需要在独立的mcentral中使用锁，并不会影响申请其他规格的mspan。 mheap结构体定义： 12345678910111213141516171819202122232425//path: /usr/local/go/src/runtime/mheap.gotype mheap struct &#123; lock mutex // spans: 指向mspans区域，用于映射mspan和page的关系 spans []*mspan // 指向bitmap首地址，bitmap是从高地址向低地址增长的 bitmap uintptr // 指示arena区首地址 arena_start uintptr // 指示arena区已使用地址位置 arena_used uintptr // 指示arena区末地址 arena_end uintptr central [67*2]struct &#123; mcentral mcentral pad [sys.CacheLineSize - unsafe.Sizeof(mcentral&#123;&#125;)%sys.CacheLineSize]byte &#125;&#125; 内存分配流程 Go的内存分配器在分配对象时，根据对象的大小，分成三类：小对象（小于等于16B）、一般对象（大于16B，小于等于32KB）、大对象（大于32KB）。 大体上的分配流程： 大于 32KB 的对象，直接从mheap上分配； 小于等于 16B 的对象使用mcache的tiny分配器分配； (16B,32KB] 的对象，首先计算对象的规格大小，然后使用mcache中相应规格大小的mspan分配； 如果mcache没有相应规格大小的mspan，则向mcentral申请 如果mcentral没有相应规格大小的mspan，则向mheap申请 如果mheap中也没有合适大小的mspan，则向操作系统申请 地址空间 Go 语言的运行时构建了操作系统的内存管理抽象层，将运行时管理的地址空间分成以下四种状态 None: 内存没有被保留或者映射，是地址空间的默认状态 Reserved: 运行时持有该地址空间，但是访问该内存会导致错误 Prepared: 内存被保留，一般没有对应的物理内存访问该片内存的行为是未定义的可以快速转换到 Ready 状态 Ready: 可以被安全访问 不同状态之间的转换过程： 运行时中包含多个操作系统实现的状态转换方法，所有的实现都包含在以 mem_ 开头的文件中，本节将介绍 Linux 操作系统对上图中方法的实现： runtime.sysAlloc: 会从操作系统中获取一大块可用的内存空间，可能为几百 KB 或者几 MB； runtime.sysFree: 会在程序发生内存不足（Out-of Memory，OOM）时调用并无条件地返回内存； runtime.sysReserve 会保留操作系统中的一片内存区域，访问这片内存会触发异常； runtime.sysMap 保证内存区域可以快速转换至就绪状态； runtime.sysUsed 通知操作系统应用程序需要使用该内存区域，保证内存区域可以安全访问； runtime.sysUnused 通知操作系统虚拟内存对应的物理内存已经不再需要，可以重用物理内存； runtime.sysFault 将内存区域转换成保留状态，主要用于运行时的调试； 运行时使用 Linux 提供的 mmap、munmap 和 madvise 等系统调用实现了操作系统的内存管理抽象层 参考链接 https://qcrao.com/2019/03/13/graphic-go-memory-allocation/ https://www.infoq.cn/article/IEhRLwmmIM7-11RYaLHR https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-memory-allocator/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门4-Select","slug":"Go/Go入门4-Select","date":"2021-09-08T16:08:10.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/09/09/Go/Go入门4-Select/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go%E5%85%A5%E9%97%A84-Select/","excerpt":"","text":"select 实例 实例1与实例2有什么不同？ 123456789101112131415161718192021222324252627282930313233func main()&#123; var count int for &#123; select &#123; case &lt;-time.Tick(time.Millisecond * 500): fmt.Println(\"hello\") count++ fmt.Println(\"count---&gt;\" , count) case &lt;-time.Tick(time.Millisecond * 499) : fmt.Println(\"world\") count++ fmt.Println(\"count---&gt;\" , count) &#125; &#125;&#125;func main()&#123; t1 := time.Tick(time.Second) t2 := time.Tick(time.Second) var count int for &#123; select &#123; case &lt;-t1: fmt.Println(\"hello\") count++ fmt.Println(\"count---&gt;\" , count) case &lt;-t2 : fmt.Println(\"world\") count++ fmt.Println(\"count---&gt;\" , count) &#125; &#125;&#125; 执行结果为： 实例1：只会输出含有world日志的case 实例2：交替执行t1与t2 select原理 select通过select.go实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314/* cas0 为*scase指针 指向[ncases]*scase的数组 order0 *uint16 ncases 记录case个数 返回选中第几个case以及是否收到值*/func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) &#123; //debug // NOTE: In order to maintain a lean stack size, the number of scases // is capped at 65536. cas1 := (*[1 &lt;&lt; 16]scase)(unsafe.Pointer(cas0)) order1 := (*[1 &lt;&lt; 17]uint16)(unsafe.Pointer(order0)) scases := cas1[:ncases:ncases] pollorder := order1[:ncases:ncases] lockorder := order1[ncases:][:ncases:ncases] //遍历case如果chan为空且不是default则置位空case对象 scase&#123;&#125; for i := range scases &#123; cas := &amp;scases[i] if cas.c == nil &amp;&amp; cas.kind != caseDefault &#123; *cas = scase&#123;&#125; &#125; &#125; // generate permuted order for i := 1; i &lt; ncases; i++ &#123; j := fastrandn(uint32(i + 1)) pollorder[i] = pollorder[j] pollorder[j] = uint16(i) &#125; // sort the cases by Hchan address to get the locking order. // simple heap sort, to guarantee n log n time and constant stack footprint. //根据chan的地址进行重新排序 for i := 0; i &lt; ncases; i++ &#123; j := i // Start with the pollorder to permute cases on the same channel. c := scases[pollorder[i]].c for j &gt; 0 &amp;&amp; scases[lockorder[(j-1)/2]].c.sortkey() &lt; c.sortkey() &#123; k := (j - 1) / 2 lockorder[j] = lockorder[k] j = k &#125; lockorder[j] = pollorder[i] &#125; for i := ncases - 1; i &gt;= 0; i-- &#123; o := lockorder[i] c := scases[o].c lockorder[i] = lockorder[0] j := 0 for &#123; k := j*2 + 1 if k &gt;= i &#123; break &#125; if k+1 &lt; i &amp;&amp; scases[lockorder[k]].c.sortkey() &lt; scases[lockorder[k+1]].c.sortkey() &#123; k++ &#125; if c.sortkey() &lt; scases[lockorder[k]].c.sortkey() &#123; lockorder[j] = lockorder[k] j = k continue &#125; break &#125; lockorder[j] = o &#125; // lock all the channels involved in the select sellock(scases, lockorder) var ( gp *g sg *sudog c *hchan k *scase sglist *sudog sgnext *sudog qp unsafe.Pointer nextp **sudog )loop: // pass 1 - look for something already waiting var dfli int var dfl *scase var casi int var cas *scase var recvOK bool for i := 0; i &lt; ncases; i++ &#123; casi = int(pollorder[i]) cas = &amp;scases[casi] c = cas.c switch cas.kind &#123; case caseNil: continue case caseRecv: sg = c.sendq.dequeue() if sg != nil &#123; goto recv &#125; if c.qcount &gt; 0 &#123; goto bufrecv &#125; if c.closed != 0 &#123; goto rclose &#125; case caseSend: if c.closed != 0 &#123; goto sclose &#125; sg = c.recvq.dequeue() if sg != nil &#123; goto send &#125; if c.qcount &lt; c.dataqsiz &#123; goto bufsend &#125; case caseDefault: dfli = casi dfl = cas &#125; &#125; if dfl != nil &#123; selunlock(scases, lockorder) casi = dfli cas = dfl goto retc &#125; // pass 2 - enqueue on all chans gp = getg() if gp.waiting != nil &#123; throw(\"gp.waiting != nil\") &#125; nextp = &amp;gp.waiting for _, casei := range lockorder &#123; casi = int(casei) cas = &amp;scases[casi] if cas.kind == caseNil &#123; continue &#125; c = cas.c sg := acquireSudog() sg.g = gp sg.isSelect = true // No stack splits between assigning elem and enqueuing // sg on gp.waiting where copystack can find it. sg.elem = cas.elem sg.releasetime = 0 if t0 != 0 &#123; sg.releasetime = -1 &#125; sg.c = c // Construct waiting list in lock order. *nextp = sg nextp = &amp;sg.waitlink switch cas.kind &#123; case caseRecv: c.recvq.enqueue(sg) case caseSend: c.sendq.enqueue(sg) &#125; &#125; // wait for someone to wake us up gp.param = nil // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(&amp;gp.parkingOnChan, 1) gopark(selparkcommit, nil, waitReasonSelect, traceEvGoBlockSelect, 1) gp.activeStackChans = false sellock(scases, lockorder) gp.selectDone = 0 sg = (*sudog)(gp.param) gp.param = nil // pass 3 - dequeue from unsuccessful chans // otherwise they stack up on quiet channels // record the successful case, if any. // We singly-linked up the SudoGs in lock order. casi = -1 cas = nil sglist = gp.waiting // Clear all elem before unlinking from gp.waiting. for sg1 := gp.waiting; sg1 != nil; sg1 = sg1.waitlink &#123; sg1.isSelect = false sg1.elem = nil sg1.c = nil &#125; gp.waiting = nil for _, casei := range lockorder &#123; k = &amp;scases[casei] if k.kind == caseNil &#123; continue &#125; if sglist.releasetime &gt; 0 &#123; k.releasetime = sglist.releasetime &#125; if sg == sglist &#123; // sg has already been dequeued by the G that woke us up. casi = int(casei) cas = k &#125; else &#123; c = k.c if k.kind == caseSend &#123; c.sendq.dequeueSudoG(sglist) &#125; else &#123; c.recvq.dequeueSudoG(sglist) &#125; &#125; sgnext = sglist.waitlink sglist.waitlink = nil releaseSudog(sglist) sglist = sgnext &#125; if cas == nil &#123; // We can wake up with gp.param == nil (so cas == nil) // when a channel involved in the select has been closed. // It is easiest to loop and re-run the operation; // we'll see that it's now closed. // Maybe some day we can signal the close explicitly, // but we'd have to distinguish close-on-reader from close-on-writer. // It's easiest not to duplicate the code and just recheck above. // We know that something closed, and things never un-close, // so we won't block again. goto loop &#125; c = cas.c if cas.kind == caseRecv &#123; recvOK = true &#125; selunlock(scases, lockorder) goto retcbufrecv: recvOK = true qp = chanbuf(c, c.recvx) if cas.elem != nil &#123; typedmemmove(c.elemtype, cas.elem, qp) &#125; typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.qcount-- selunlock(scases, lockorder) goto retcbufsend: // can send to buffer typedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ selunlock(scases, lockorder) goto retcrecv: // can receive from sleeping sender (sg) recv(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) recvOK = true goto retcrclose: // read at end of closed channel selunlock(scases, lockorder) recvOK = false if cas.elem != nil &#123; typedmemclr(c.elemtype, cas.elem) &#125; goto retcsend: // can send to a sleeping receiver (sg) send(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) if debugSelect &#123; print(\"syncsend: cas0=\", cas0, \" c=\", c, \"\\n\") &#125; goto retcretc: if cas.releasetime &gt; 0 &#123; blockevent(cas.releasetime-t0, 1) &#125; return casi, recvOKsclose: // send on closed channel selunlock(scases, lockorder) panic(plainError(\"send on closed channel\"))&#125; 流程图如下： 参考链接 https://blog.csdn.net/u011957758/article/details/82230316","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门12-Interface","slug":"Go/Go入门12-Interface","date":"2021-09-05T10:36:41.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/09/05/Go/Go入门12-Interface/","link":"","permalink":"http://xboom.github.io/2021/09/05/Go/Go%E5%85%A5%E9%97%A812-Interface/","excerpt":"","text":"interface赋值问题 123456789101112131415161718192021package mainimport ( \"fmt\")type People interface &#123; Speak(string) string&#125;type Stduent struct&#123;&#125;func (stu *Stduent) Speak(think string) (talk string) &#123; if think == \"love\" &#123; talk = \"You are a good boy\" &#125; else &#123; talk = \"hi\" &#125; return&#125;func main() &#123; var peo People = Stduent&#123;&#125; think := \"love\" fmt.Println(peo.Speak(think))&#125; 多态的几个要素： 1、有interface接口，并且有接口定义的方法。 2、有子类去重写interface的接口。 3、有父类指针指向子类的具体对象 所以上述代码报错的地方在var peo People = Stduent{}这条语句， Student{}已经重写了父类People{}中的Speak(string) string方法，那么只需要用父类指针指向子类对象即可。 所以应该改成var peo People = &amp;Student{} 即可编译通过。（People为interface类型，就是指针类型） interface的内部构造(非空接口iface情况) 123456789101112131415161718192021package mainimport ( \"fmt\")type People interface &#123; Show()&#125;type Student struct&#123;&#125;func (stu *Student) Show() &#123;&#125;func live() People &#123; var stu *Student return stu&#125;func main() &#123; if live() == nil &#123; fmt.Println(\"AAAAAAA\") &#125; else &#123; fmt.Println(\"BBBBBBB\") &#125;&#125; 结果是: BBBBBB interface在使用的过程中，共有两种表现形式 一种为空接口(empty interface)，定义如下： 1var MyInterface interface&#123;&#125; 另一种为非空接口(non-empty interface), 定义如下： 1type MyInterface interface &#123;function()&#125; 这两种interface类型分别用两种struct表示，空接口为eface, 非空接口为iface. 空接口eface 空接口eface结构，由两个属性构成，一个是类型信息_type，一个是数据信息。其数据结构声明如下： 1234type eface struct &#123; //空接口 _type *_type //类型信息 data unsafe.Pointer //指向数据的指针(go语言中特殊的指针类型unsafe.Pointer类似于c语言中的void*)&#125; _type属性：是GO语言中所有类型的公共描述，Go语言几乎所有的数据结构都可以抽象成 _type，是所有类型的公共描述，**type负责决定data应该如何解释和操作，**type的结构代码如下: 12345678910111213type _type struct &#123; size uintptr //类型大小 ptrdata uintptr //前缀持有所有指针的内存大小 hash uint32 //数据hash值 tflag tflag align uint8 //对齐 fieldalign uint8 //嵌入结构体时的对齐 kind uint8 //kind 有些枚举值kind等于0是无效的 alg *typeAlg //函数指针数组，类型实现的所有方法 gcdata *byte str nameOff ptrToThis typeOff&#125; data属性: 表示指向具体的实例数据的指针，他是一个unsafe.Pointer类型，相当于一个C的万能指针void* 非空接口iface iface 表示 non-empty interface 的数据结构，非空接口初始化的过程就是初始化一个iface类型的结构，其中data的作用同eface的相同，这里不再多加描述。 1234type iface struct &#123; tab *itab data unsafe.Pointer&#125; iface结构中最重要的是itab结构（结构如下），每一个 itab 都占 32 字节的空间。itab可以理解为pair&lt;interface type, concrete type&gt; 。itab里面包含了interface的一些关键信息，比如method的具体实现。 12345678type itab struct &#123; inter *interfacetype // 接口自身的元信息 _type *_type // 具体类型的元信息 link *itab bad int32 hash int32 // _type里也有一个同样的hash，此处多放一个是为了方便运行接口断言 fun [1]uintptr // 函数指针，指向具体类型所实现的方法&#125; 其中值得注意的字段： interface type包含了一些关于interface本身的信息，比如package path，包含的method。这里的interfacetype是定义interface的一种抽象表示。 type表示具体化的类型，与eface的 type类型相同。 hash字段其实是对_type.hash的拷贝，它会在interface的实例化时，用于快速判断目标类型和接口中的类型是否一致。另，Go的interface的Duck-typing机制也是依赖这个字段来实现。 fun字段其实是一个动态大小的数组，虽然声明时是固定大小为1，但在使用时会直接通过fun指针获取其中的数据，并且不会检查数组的边界，所以该数组中保存的元素数量是不确定的 1234func live() People &#123; var stu *Student return stu &#125; stu是一个指向nil的空指针，但是最后return stu 会触发匿名变量 People = stu值拷贝动作，所以最后live()放回给上层的是一个People insterface{}类型，也就是一个iface struct{}类型。 stu为nil，只是iface中的data 为nil而已。 但是iface struct{}本身并不为nil. Interface 内部构造(空接口eface情况) 1234567891011func Foo(x interface&#123;&#125;) &#123; if x == nil &#123; fmt.Println(\"empty interface\") return &#125; fmt.Println(\"non-empty interface\")&#125;func main() &#123; var p *int = nil Foo(p)&#125; 结果为 “non-empty interface” 因为 Fool()的形参 x interface{} 是一个空接口类型eface struct{},在执行Foo(p)的时候，触发x interface{} = p语句的时候，此X的结构为 所以 x 结构体本身不为nil，而是data指针指向的p为nil Interface{}与*Interface{} 1234567891011121314type S struct &#123;&#125;func f(x interface&#123;&#125;) &#123;&#125;func g(x *interface&#123;&#125;) &#123;&#125;func main() &#123; s := S&#123;&#125; p := &amp;s f(s) //A g(s) //B f(p) //C g(p) //D&#125; 结果： 12345B、D两行错误B错误为： cannot use s (type S) as type *interface &#123;&#125; in argument to g: *interface &#123;&#125; is pointer to interface, not interfaceD错误为：cannot use p (type *S) as type *interface &#123;&#125; in argument to g: *interface &#123;&#125; is pointer to interface, not interface 看到这道题需要第一时间想到的是Golang是强类型语言，interface是所有golang类型的父类 函数中func f(x interface{})的interface{}可以支持传入golang的任何类型，包括指针，但是函数func g(x *interface{})只能接受*interface{} 参考链接 https://www.bookstack.cn/read/aceld-golang/4、interface.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门7-垃圾回收","slug":"Go/Go入门7-垃圾回收","date":"2021-09-03T15:37:38.000Z","updated":"2023-03-12T02:18:37.959Z","comments":true,"path":"2021/09/03/Go/Go入门7-垃圾回收/","link":"","permalink":"http://xboom.github.io/2021/09/03/Go/Go%E5%85%A5%E9%97%A87-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"","text":"垃圾回收 GC，全称 Garbage Collection，即垃圾回收，是一种自动内存管理的机制 通常，垃圾回收器的执行过程被划分为两个半独立的组件： 赋值器（Mutator）：这一名称本质上是在指代用户态的代码。因为对垃圾回收器而言，用户态的代码仅仅只是在修改对象之间的引用关系，也就是在对象图（对象之间引用关系的一个有向图）上进行操作。 回收器（Collector）：负责执行垃圾回收的代码 根对象在垃圾回收的术语中又叫做根集合，它是垃圾回收器在标记过程时最先检查的对象，包括： 全局变量：程序在编译期就能确定的那些存在于程序整个生命周期的变量。 执行栈：每个 goroutine 都包含自己的执行栈，这些执行栈上包含栈上的变量及指向分配的堆内存区块的指针。 寄存器：寄存器的值可能表示一个指针，参与计算的这些指针可能指向某些赋值器分配的堆内存区块。 所有的 GC 算法其存在形式可归结为追踪(Tracing)和引用计数(Reference Counting)这两种形式的混合运用 追踪式 GC:从根对象出发，根据对象之间的引用信息，一步步推进直到扫描完毕整个堆并确定需要保留的对象，从而回收所有可回收的对象。Go、 Java、V8 对 JavaScript 的实现等均为追踪式 GC。 引用计数式 GC:每个对象自身包含一个被引用的计数器，当计数器归零时自动得到回收。因为此方法缺陷较多，在追求高性能时通常不被应用。Python、Objective-C 等均为引用计数式 GC。 目前比较常见的 GC 实现方式包括： 追踪式，分为多种不同类型，例如： 标记清扫：从根对象出发，将确定存活的对象进行标记，并清扫可以回收的对象。 标记整理：为了解决内存碎片问题而提出，在标记过程中，将对象尽可能整理到一块连续的内存上。 增量式：将标记与清扫的过程分批执行，每次执行很小的部分，从而增量的推进垃圾回收，达到近似实时、几乎无停顿的目的。 增量整理：在增量式的基础上，增加对对象的整理过程。 分代式：将对象根据存活时间的长短进行分类，存活时间小于某个值的为年轻代，存活时间大于某个值的为老年代，永远不会参与回收的对象为永久代。并根据分代假设（如果一个对象存活时间不长则倾向于被回收，如果一个对象已经存活很长时间则倾向于存活更长时间）对对象进行回收。 引用计数：根据对象自身的引用计数来回收，当引用计数归零时立即回收。 Go 的 GC 目前使用的是无分代(对象没有代际之分)、不整理(回收过程中不对对象进行移动与整理)、并发(与用户代码并发执行)的三色标记清扫算法。原因在于： 对象整理的优势是解决内存碎片问题以及“允许”使用顺序内存分配器。但 Go 运行时的分配算法基于 tcmalloc，基本上没有碎片问题。 并且顺序内存分配器在多线程的场景下并不适用。Go 使用的是基于 tcmalloc 的现代内存分配算法，对对象进行整理不会带来实质性的性能提升。 分代 GC 依赖分代假设，即 GC 将主要的回收目标放在新创建的对象上（存活时间短，更倾向于被回收），而非频繁检查所有对象。但 Go 的编译器会通过逃逸分析将大部分新生对象存储在栈上（栈直接被回收），只有那些需要长期存在的对象才会被分配到需要进行垃圾回收的堆中。也就是说，分代 GC 回收的那些存活时间短的对象在 Go 中是直接被分配到栈上，当 goroutine 死亡后栈也会被直接回收，不需要 GC 的参与，进而分代假设并没有带来直接优势。并且 Go 的垃圾回收器与用户代码并发执行，使得 STW 的时间与对象的代际、对象的 size 没有关系。Go 团队更关注于如何更好地让 GC 与用户代码并发执行（使用适当的 CPU 来执行垃圾回收），而非减少停顿时间这一单一目标上。 三色标记法 理解三色标记法的关键是理解对象的三色抽象以及波面（wavefront）推进这两个概念。三色抽象只是一种描述追踪式回收器的方法，在实践中并没有实际含义，它的重要作用在于从逻辑上严密推导标记清理这种垃圾回收方法的正确性。也就是说，当我们谈及三色标记法时，通常指标记清扫的垃圾回收。 从垃圾回收器的视角来看，三色抽象规定了三种不同类型的对象，并用不同的颜色相称： 白色对象（可能死亡）：未被回收器访问到的对象。在回收开始阶段，所有对象均为白色，当回收结束后，白色对象均不可达。 灰色对象（波面）：已被回收器访问到的对象，但回收器需要对其中的一个或多个指针进行扫描，因为他们可能还指向白色对象。 黑色对象（确定存活）：已被回收器访问到的对象，其中所有字段都已被扫描，黑色对象中任何一个指针都不可能直接指向白色对象。 这样三种不变性所定义的回收过程其实是一个波面不断前进的过程，这个波面同时也是黑色对象和白色对象的边界，灰色对象就是这个波面。 当垃圾回收开始时，只有白色对象。随着标记过程开始进行时，灰色对象开始出现（着色），这时候波面便开始扩大。当一个对象的所有子节点均完成扫描时，会被着色为黑色。当整个堆遍历完成时，只剩下黑色和白色对象，这时的黑色对象为可达对象，即存活；而白色对象为不可达对象，即死亡。这个过程可以视为以灰色对象为波面，将黑色对象和白色对象分离，使波面不断向前推进，直到所有可达的灰色对象都变为黑色对象为止的过程 三色标记法 实际上就是通过三个阶段的标记来确定清楚的对象都有哪些. 第一步: 就是只要是新创建的对象,默认的颜色都是标记为“白色”. 这里面需要注意的是, 所谓“程序”, 则是一些对象的根节点集合。 第二步: 每次GC回收开始, 然后从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合，类似层序遍历 第三步: 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，灰色被遍历之后就放入黑色集合 第四步: 重复第三步, 直到灰色中无任何对象 第五步: 回收所有的白色标记表的对象. 也就是回收垃圾 以上就是三色并发标记法，Go是如何解决标记-清除(mark and sweep)算法中的卡顿(stw，stop the world)问题？ 没有STW的三色标记法 一定要依赖STW的。因为如果不暂停程序，程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性。例如 已经标记为灰色的对象2有指针指向白色的对象3 在还没有扫描到对象2，已经标记为黑色的对象4创建指针q，指向对象3 于此同时对象2将指针p移除，对象3就被挂载了已经扫描完成的黑色的对象4下面 正常执行逻辑，对象2和对象7 被标记为黑色，而对象3因为对象4不在被扫描，而等待被回收 对象3被无辜的清除掉了 当下列两个条件同时满足时, 就会出现对象丢失现象! 条件1: 一个白色对象被黑色对象引用**(白色被挂在黑色下)**，它的下游对象也会一并被清理掉 条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏**(灰色同时丢了该白色)** 为了防止这种现象的发生，最简单的方式就是STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是STW的过程有明显的资源浪费，对所有的用户程序都有很大影响，如何能在保证对象不丢失的情况下合理的尽可能的提高GC效率，减少STW时间呢？ 答案就是, 那么我们只要使用一个机制,来破坏上面的两个条件就可以了 屏蔽机制 让GC回收器,满足下面两种情况之一时,可保对象不丢失. 所以引出两种方式. 强-弱三色不变式 强三色不变式: 不存在黑色对象引用到白色对象的指针 弱三色不变式: 所有被黑色对象引用的白色对象都处于灰色保护状态 为了遵循上述的两个方式,Golang团队初步得到了如下具体的两种屏障方式“插入屏障”, “删除屏障”。 插入屏障 具体操作: 在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色) 满足: 强三色不变式. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色) 黑色对象的内存槽有两种位置, 栈和堆. 栈空间的特点是容量小,但是要求相应速度快,因为函数调用弹出频繁使用, 所以“插入屏障”机制,在栈空间的对象操作中不使用. 而仅仅使用在堆空间对象的操作中。 第一步：程序起初创建，全部标记为白色，将所有对象放入白色集合中 第二步：遍历Root Set(非递归形式，只遍历一次，得到灰色节点) 第三步：遍历Grey 灰色标记表。将可达的对象从白色标记为灰色遍历之后的灰色，标记为黑色 第四步：如果此刻外界向对象4添加对象8，对象1添加对象9。对象4在堆区触发插入屏蔽机制，对象1不触发 第五步：由于插入写屏障(黑色对象添加白色，将白色改为灰色)，对象8变为灰色，对象9依然时i白色 第六步：继续循环上述流程进行三色标记，直到没有灰色节点 但是如果栈不添加,当全部三色标记扫描之后,栈上有可能依然存在白色对象被引用的情况(如上图的对象9). 所以要对栈重新进行三色标记扫描, 但这次为了对象不丢失, 要对本次标记扫描启动STW暂停. 直到栈空间的三色标记结束 第七步：在准备回收白色前，重新遍历扫描一次栈空间。此时加STW暂停保护栈，防止外界干扰 第八步：在STW中，将栈中的对象一次三色标记，直到没有灰色标记 第八步：停止STW 第十步: 最后将栈和堆空间 扫描剩余的全部 白色节点清除. 这次STW大约的时间在10~100ms间 删除屏障 具体操作: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。 满足: 弱三色不变式. (保护灰色对象到白色对象的路径不会断) 第一步：程序起初创建，全部标记为白色，将所有对象放入白色集合 第二步：遍历Root Set(非递归形式，只遍历一次)，得到灰色节点 第三步：灰色对象1删除对象5，如果不触发删除写屏障，5-2-3路径与主链路断开，最后均会被清除 第四步：触发删除写屏障，被删除对象5，被标记为黑色 第五步：遍历Grey 灰色标记表，将可达的对象，从白色标记为灰色。遍历之后的灰色，标记为黑色 第六步：继续循环上述流程进行三色标记，直到没有灰色节点 第七步：这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉 第八步：清除白色 混合写屏障(hybrid write barrier)机制 插入写屏障和删除写屏障的短板： 插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活； 删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象。 Go V1.8版本引入了混合写屏障机制（hybrid write barrier），避免了对栈re-scan的过程，极大的减少了STW的时间。结合了两者的优点 具体操作: 1、GC开始将栈上的对象全部扫描并标记为黑色(之后不再进行第二次重复扫描，无需STW)， 2、GC期间，任何在栈上创建的新对象，均为黑色。 3、被删除的对象标记为灰色。 4、被添加的对象标记为灰色。 第一步：GC刚刚开始，都标记为白色 第二步：优先扫描栈对象，将可达对象全部标记为黑色 场景一 对象被一个堆对象删除引用，成为栈对象的下游 第一步：将对象7添加到对象1的下游，因为栈不启动写屏障，所以直接挂载下面 第二步：对象4删除对象7的引用关系，因为对象4是堆区，所以触发写屏障，标记被删除的对象7为灰色(删除即赋新值为nil) 参考文献 https://golang.design/go-questions/memgc/principal/ https://www.bookstack.cn/read/aceld-golang/5、Golang三色标记+混合写屏障GC模式全分析.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Linux深入3-malloc","slug":"Linux/Linux深入3-malloc","date":"2021-07-31T07:28:42.000Z","updated":"2022-10-03T09:10:30.000Z","comments":true,"path":"2021/07/31/Linux/Linux深入3-malloc/","link":"","permalink":"http://xboom.github.io/2021/07/31/Linux/Linux%E6%B7%B1%E5%85%A53-malloc/","excerpt":"","text":"基础概念 每个进程都有独立的虚拟地址空间，进程访问的虚拟地址并不是真正的物理地址 虚拟地址是通过每个进程的页表(在每个进程的内核虚拟地址空间)与物理地址进行映射，获得真正物理地址； 如果虚拟地址对应物理地址不在物理内存中，则产生缺页中断，真正分配物理地址，同时更新进程的页表；如果此时物理内存已耗尽，则根据内存替换算法淘汰部分页面至物理磁盘中 空间操作 CPU通过地址总线、数据总线和控制总线实现对内存的访问 数据总线：在CPU和内存之间传递数据的通道 控制总线：在CPU和内存之间传递各种控制/状态信号的通道 地址总线：传递地址信号，以确定所要访问的内存地址 当 CPU 执行内存中一条指令的时候，首先把 VMA（虚拟内存区域）中的逻辑地址转换为线性地址，转化过程通过 MMU（内存管理单元）实现 系统在内存分配的时候，其实并没有申请相应的物理页帧，只有在真正赋值的时候才会申请物理页帧。这也是 VSZ（进程虚拟内存大小）和 RSS（常驻物理内存大小）的最大区别。 空间分配 Linux 使用虚拟地址空间， 对32位操作系统而言，它的寻址空间(虚拟存储空间)为4G(2的32次方)，将最高的1G字节(从虚拟地址0xC0000000到0xFFFFFFFF)，供内核使用，称为内核空间，而将较低的3G字节(从虚拟地址0x00000000到0xBFFFFFFF)，供各个进程使用，称为用户空间。 由低地址到高地址分别为： 只读段：该部分空间只能读不可写(包括：代码段、rodata 段(C常量字符串和#define定义的常量) ) 数据段：保存全局变量、静态变量的空间 堆 ：就是平时所说的动态内存， malloc/new 大部分都来源于此。其中堆顶的位置可通过函数 brk 和 sbrk 进行动态调整。 文件映射区域：如动态库、共享内存等映射物理空间的内存，一般是 mmap 函数所分配的虚拟地址空间 栈：用于维护函数调用的上下文空间，一般为 8M ，可通过 ulimit –s 查看，每个线程都有自己专属的栈 内核虚拟空间：用户代码不可见的内存区域，由内核管理(页表就存放在内核虚拟空间) 分配原理 进程分配内存有两种方式，分别由两个系统调用完成：brk 和 mmap (不考虑共享内存) brk 是将数据段（.data）的最高地址指针 _edata 往高地址推 mmap 是在进程的虚拟地址空间中（堆和栈中间，称为“文件映射区域”的地方）找一块空闲的虚拟内存。 两种方式分配的都是虚拟内存，没有分配物理内存。第一次访问已分配的虚拟地址空间的时候，发生缺页中断进程会陷入内核态，执行以下操作： 检查要访问的虚拟地址是否合法 查找/分配一个物理页 填充物理页内容（读取磁盘，或者直接置0，或者什么都不做） 如果需要读取磁盘，那么这次缺页就是 majfit(major fault：大错误) 否则就是 minflt(minor fault：小错误) 建立映射关系（虚拟地址到物理地址的映射关系） 重复执行发生缺页中断的那条指令 使用命令查看缺页中断的次数 分配过程 第一种情况：malloc小于128K的内存，使用brk 将_edata往高地址推(只分配虚拟空间，不对应物理内存(因此没有初始化)，第一次读/写数据时，引起内核缺页中断，内核才分配对应的物理内存，然后虚拟地址空间建立映射关系)，如下图： 进程启动的时候，其（虚拟）内存空间的初始布局(1)所示 进程调用A=malloc(30K)以后，内存空间 (2), malloc函数会调用brk系统调用，将_edata指针往高地址推30K，就完成虚拟内存分配 _edata+30K只是完成虚拟地址的分配，A这块内存现在还是没有物理页与之对应的，等到进程第一次读写A这块内存的时候，发生缺页中断，这个时候，内核才分配A这块内存对应的物理页。也就是说，如果用malloc分配了A这块内容，然后从来不访问它，那么，A对应的物理页是不会被分配的。 进程调用B=malloc(40K)以后，内存空间如(3) 第二种情况：malloc 大于 128K 的内存，使用 mmap 分配（munmap 释放） 进程调用C=malloc(200K)以后，内存空间如(4) 进程调用D=malloc(100K)以后，内存空间如(5) 进程调用free©以后，C对应的虚拟内存和物理内存一起释放 默认情况下，malloc函数分配内存，如果请求内存大于128K（可由M_MMAP_THRESHOLD选项调节），那就不是去推_edata(指向数据段)指针了，而是利用mmap系统调用，从堆和栈的中间分配一块虚拟内存 这样子做是因为 brk分配的内存需要等到高地址内存释放以后才能释放（例如，在B释放之前，A是不可能释放的，因为只有一个_edata 指针，这就是内存碎片产生的原因，先进后出），而mmap分配的内存可以单独释放 进程调用free(B)以后，如(7)所示:B对应的虚拟内存和物理内存都没有释放，因为只有一个_edata指针，如果往回推，那么D这块内存怎么办呢？当然，B这块内存，是可以重用的，如果这个时候再来一个40K的请求，那么malloc很可能就把B这块内存返回回去了 进程调用free(D)以后，如图(8)所示：B和D连接起来，变成一块140K的空闲内存，当最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作(trim)。在上一个步骤free的时候，发现最高地址空闲内存超过128K，于是内存紧缩，变成(9)所示 总结: 当开辟的空间小于 128K 时，调用 brk()函数，malloc 的底层实现是系统调用函数 brk()，其主要移动指针 _enddata(此时的 _enddata 指的是 Linux 地址空间中堆段的末尾地址，不是数据段的末尾地址) 当开辟的空间大于 128K 时，mmap()系统调用函数来在虚拟地址空间中（堆和栈中间，称为“文件映射区域”的地方）找一块空间来开辟 参考链接 malloc 底层实现原理","categories":[{"name":"Linux Depth","slug":"Linux-Depth","permalink":"http://xboom.github.io/categories/Linux-Depth/"}],"tags":[{"name":"Memory","slug":"Memory","permalink":"http://xboom.github.io/tags/Memory/"}]},{"title":"微服务-布隆过滤器","slug":"Microservices/微服务-布隆过滤器","date":"2021-07-27T16:36:57.000Z","updated":"2022-07-19T14:34:16.000Z","comments":true,"path":"2021/07/28/Microservices/微服务-布隆过滤器/","link":"","permalink":"http://xboom.github.io/2021/07/28/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","excerpt":"","text":"如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢。三种结构的检索时间复杂度分别为O(n)，O(logn)，O(1) 哈希函数 哈希函数的概念是：将任意大小的输入数据转换成特定大小的输出数据的函数，转换后的数据称为哈希值或哈希编码，也叫散列值。下面是一幅示意图 所有散列函数都有如下基本特性： 如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。 散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“散列碰撞（collision）”。 布隆过滤器 BloomFilter 是由一个固定大小的二进制向量或者位图（bitmap）和一系列映射函数组成的。 在初始状态时，对于长度为 m 的位数组，它的所有位都被置为0，如下图所示： 当有变量被加入集合时，通过 K 个映射函数将这个变量映射成位图中的 K 个点，把它们置为 1（假定有两个变量都通过 3 个映射函数） 查询某个变量的时候我们只要看看这些点是不是都是 1 就可以大概率知道集合中有没有它了 如果这些点有任何一个 0，则被查询变量一定不在； 如果都是 1，则被查询变量很可能存在 Coding 添加元素 将要添加的元素给 k 个哈希函数 得到对应于位数组上的 k 个位置 将这k个位置设为 1 查询元素 将要查询的元素给k个哈希函数 得到对应于位数组上的k个位置 如果k个位置有一个为 0，则肯定不在集合中 如果k个位置全部为 1，则可能在集合中 特点 一个元素如果判断结果为存在的时候元素不一定存在，但是判断结果为不存在的时候则一定不存在。 布隆过滤器可以添加元素，但是不能删除元素。同一个位置可能被多个值引用，删掉元素会导致误判率增加 解决办法： 定时异步重建布隆过滤器 计数Bloom Filter 过滤器的值存储到Redis中 代码实现 总结 优点： 相比于其它的数据结构，布隆过滤器存储空间和插入/查询时间都是常数 O(K)，另外，散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 布隆过滤器可以表示全集，其它任何数据结构都不能； 缺点： 随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加 1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 怎么降低误判几率： 增加二进制位数 增加Hash次数 布隆过滤器数据其实也是存在Redis中的 应用场景： 数据库防止穿库 Google Bigtable，Apache HBase和Apache Cassandra以及Postgresql 使用BloomFilter来减少不存在的行或列的磁盘查找。避免代价高昂的磁盘查找会大大提高数据库查询操作的性能。 如同一开始的业务场景。如果数据量较大，不方便放在缓存中。需要对请求做拦截防止穿库。 缓存宕机 缓存宕机的场景，使用布隆过滤器会造成一定程度的误判。原因是除了Bloom Filter 本身有误判率，宕机之前的缓存不一定能覆盖到所有DB中的数据，当宕机后用户请求了一个以前从未请求的数据，这个时候就会产生误判。当然，缓存宕机时使用布隆过滤器作为应急的方式，这种情况应该也是可以忍受的。 WEB拦截器 相同请求拦截防止被攻击。用户第一次请求，将请求参数放入BloomFilter中，当第二次请求时，先判断请求参数是否被BloomFilter命中。可以提高缓存命中率 恶意地址检测 chrome 浏览器检查是否是恶意地址。 首先针对本地BloomFilter检查任何URL，并且仅当BloomFilter返回肯定结果时才对所执行的URL进行全面检查（并且用户警告，如果它也返回肯定结果）。 比特币加速 bitcoin 使用BloomFilter来加速钱包同步 参考链接 布隆过滤器 布隆过滤器","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"Linux基础4-进程","slug":"Linux/Linux基础4-进程","date":"2021-07-25T22:54:33.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/26/Linux/Linux基础4-进程/","link":"","permalink":"http://xboom.github.io/2021/07/26/Linux/Linux%E5%9F%BA%E7%A1%804-%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"基础知识 进程就是一个程序的执行流程，内部保存程序运行所需的资源 在UNIX系统中，直邮fork系统调用才可以创建进程 12345678910111213#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main() &#123; pid_t id = fork(); if (id &lt; 0) &#123; perror(\"fork\\n\"); &#125; else if (id == 0) &#123; // 子进程 printf(\"子进程\\n\"); &#125; else &#123; // 父进程 printf(\"父进程\\n\"); &#125; return 0;&#125; 进程创建之后，父子进程都有各自不同的地址空间，其中一个进程在其地址空间的修改对另一个进程不可见。子进程的初始化空间是父进程的一个副本，这里涉及两个不同地址空间，不可写的内存区是共享的，某些UNIX的实现使程序正文在两者间共享，因为它是不可修改的。 还有一种写时复制共享技术，子进程共享父进程的所有内存，一旦两者之一想要修改部分内存，则这块内存被复制确保修改发生在当前进程的私有内存区域 PCB 进程控制块(PCB),操作系统为每个进程都维护一个PCB，用来保存与该进程有关的各种状态信息。进程可以抽象理解为就是一个PCB，PCB是进程存在的唯一标志，用PCB来描述进程的基本情况以及运行变化的过程 PCB包含进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号和调度信息，以及其它在进程由运行态转换到就绪态或阻塞态时必须保存的信息，从而保证该进程随后能再次启动，就像从未中断过一样。 中断发生后操作系统最底层做了什么？ 硬件压入堆栈程序计数器 硬件从中断向量装入新的程序计数器 汇编语言过程保存寄存器的值 汇编语言过程设置新的堆栈 C中断服务例程运行（典型的读和缓冲输入） 调度程序决定下一个将运行的进程 C过程返回到汇编代码 汇编语言过程开始运行新的当前进程 进程控制块中存储的信息 进程标识信息：如本进程的标识，本进程的父进程标识，用户标识等。 处理机状态信息保护区：用于保存进程的运行现场信息： 用户可见寄存器：用户程序可以使用的数据，地址等寄存器 控制和状态寄存器：程序计数器，程序状态字 栈指针：过程调用、系统调用、中断处理和返回时需要用到它 进程控制信息： 调度和状态信息：用于操作系统调度进程使用 进程间通信信息：为支持进程间与通信相关的各种标识、信号、信件等，这些信息存在接收方的进程控制块中 存储管理信息：包含有指向本进程映像存储空间的数据结构 进程所用资源：说明由进程打开使用的系统资源，如打开的文件等 有关数据结构连接信息：进程可以连接到一个进程队列中，或连接到相关的其他进程的PCB 进程状态 进程因为等待输入而被阻塞，进程从运行态转换到阻塞态 调度程序选择了另一个进程执行时，当前程序就会从运行态转换到就绪态 被调度程序选择的程序会从就绪态转换到运行态 当阻塞态的进程等待的一个外部事件发生时，就会从阻塞态转换到就绪态，此时如果没有其他进程运行时，则立刻从就绪态转换到运行态！ 12345678910pid=fork(); // 创建一个与父进程一样的子进程pid=waitpid(); // 等待子进程终止s=execve(); // 替换进程的核心映像exit(); // 终止进程运行并返回状态值s=sigaction(); // 定义信号处理的动作s=sigprocmask(); // 检查或更换信号掩码s=sigpending(); // 获得阻塞信号集合s=sigsuspend(); // 替换信号掩码或挂起进程alarm(); // 设置定时器pause(); // 挂起调用程序直到下一个信号出现 进程调度 一个CPU同一时刻只会有一个进程处于运行状态，操作系统通过调度算法选择下一个要运行的进程 什么时候进行调度 系统调用创建一个新进程后，需要决定是运行父进程还是运行子进程 一个进程退出时需要做出调度决策，需要决定下一个运行的是哪个进程 当一个进程阻塞在I/O和信号量或者由于其它原因阻塞时，必须选择另一个进程运行 当一个I/O中断发生时，如果中断来自IO设备，而该设备现在完成了工作，某些被阻塞的等待该IO的进程就成为可运行的就绪进程了，是否让新就绪的进程运行，或者让中断发生时运行的进程继续运行，或者让某个其它进程运行，这就取决于调度程序的抉择了 调度算法可以分类 非抢占式调度算法：让进程运行直至被阻塞，或者直到该进程自动释放CPU。在时钟中断发生时不会进行调度，在处理完时钟中断后，如果没有更高优先级的进程等待，则被中断的进程会继续执行。简单来说，调度程序必须等待事件结束。 非抢占方式引起进程调度的条件： 进程执行结束，或发生某个事件而不能继续执行 正在运行的进程因有I/O请求而暂停执行 进程通信或同步过程中执行了某些原语操作（wait、block等） 抢占式调度算法：进程运行固定时段。如果时段结束时进程仍在运行，就被挂起，而调度程序挑选另一个进程运行，进行抢占式调度处理，需要在时间间隔的末端发生时钟中断，以便CPU控制返回给调度程序，如果没有可用的时钟，那么非抢占式调度就是唯一的选择。简单来说，防止单一进程长时间独占CPU资源。 进程间通信 匿名管道 匿名管道就是pipe，pipe只能在父子进程间通信，且数据是单向流动（半双工通信） 使用方式： 父进程创建管道，会得到两个文件描述符，分别指向管道的两端； 父进程创建子进程，从而子进程也有两个文件描述符指向同一管道； 父进程可写数据到管道，子进程就可从管道中读出数据，从而实现进程间通信，下面的示例代码中通过pipe实现了每秒钟父进程向子进程都发送消息的功能 123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;int main() &#123; int _pipe[2]; int ret = pipe(_pipe); if (ret &lt; 0) &#123; perror(\"pipe\\n\"); &#125; pid_t id = fork(); if (id &lt; 0) &#123; perror(\"fork\\n\"); &#125; else if (id == 0) &#123; // 子进程 close(_pipe[1]); int j = 0; char _mesg[100]; while (j &lt; 100) &#123; memset(_mesg, '\\0', sizeof(_mesg)); read(_pipe[0], _mesg, sizeof(_mesg)); printf(\"%s\\n\", _mesg); j++; &#125; &#125; else &#123; // 父进程 close(_pipe[0]); int i = 0; char *mesg = NULL; while (i &lt; 100) &#123; mesg = \"父进程来写消息了\"; write(_pipe[1], mesg, strlen(mesg) + 1); sleep(1); ++i; &#125; &#125; return 0;&#125; 平时也会使用 | 关于管道的命令行，如 ls | less 创建管道 为ls创建一个进程，设置stdout为管道写端 为less创建一个进程，设置stdin为管道读端 高级管道 通过popen将另一个程序当作一个新的进程在当前进程中启动，它算作当前进程的子进程，高级管道只能用在有亲缘关系的进程间通信，这种亲缘关系通常指父子进程，下面的GetCmdResult函数可以获取某个Linux命令执行的结果，实现方式就是通过popen 123456789101112131415161718192021222324252627282930std::string GetCmdResult(const std::string &amp;cmd, int max_size = 10240) &#123; char *data = (char *)malloc(max_size); if (data == NULL) &#123; return std::string(\"malloc fail\"); &#125; memset(data, 0, max_size); const int max_buffer = 256; char buffer[max_buffer]; // 将标准错误重定向到标准输出 FILE *fdp = popen((cmd + \" 2&gt;&amp;1\").c_str(), \"r\"); int data_len = 0; if (fdp) &#123; while (!feof(fdp)) &#123; if (fgets(buffer, max_buffer, fdp)) &#123; int len = strlen(buffer); if (data_len + len &gt; max_size) &#123; cout &lt;&lt; \"data size larger than \" &lt;&lt; max_size; break; &#125; memcpy(data + data_len, buffer, len); data_len += len; &#125; &#125; pclose(fdp); &#125; std::string ret(data, data_len); free(data); return ret;&#125; 命名管道 匿名管道有个缺点就是通信的进程一定要有亲缘关系，而命名管道就不需要这种限制。 命名管道其实就是一种特殊类型的文件，所谓的命名其实就是文件名，文件对各个进程都可见，通过命名管道创建好特殊文件后，就可以实现进程间通信。 通过mkfifo创建一个特殊的类型的文件，参数读者看名字应该就了解，一个是文件名，一个是文件的读写权限： 1int mkfifo(const char* filename, mode_t mode); 当返回值为0时，表示该命名管道创建成功，至于如何通信，其实就是个读写文件的问题 消息队列 队列想必大家都知道，像FIFO一样，这里可以有多个进程写入数据，也可以有多个进程从队列里读出数据，但消息队列有一点比FIFO还更高级，它读消息不一定要使用先进先出的顺序，每个消息可以赋予类型，可以按消息的类型读取，不是指定类型的数据还存在队列中。本质上MessageQueue是存放在内核中的消息链表，每个消息队列链表会由消息队列标识符表示，这个消息队列存于内核中，只有主动的删除该消息队列或者内核重启时，消息队列才会被删除 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// 创建和访问一个消息队列int msgget(key_t, key, int msgflg);// 用来把消息添加到消息队列中int msgsend(int msgid, const void *msg_ptr, size_t msg_sz, int msgflg);// msg_ptr是结构体数据的指针，结构第一个字段要有个类型：struct Msg &#123; long int message_type; // 想要传输的数据&#125;;// 从消息队列中获取消息int msgrcv(int msgid, void *msg_ptr, size_t msg_st, long int msgtype, int msgflg);// 用来控制消息队列，不同的command参数有不同的控制方式int msgctl(int msgid, int command, struct msgid_ds *buf);#include &lt;errno.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/msg.h&gt;#include &lt;chrono&gt;#include &lt;iostream&gt;#include &lt;thread&gt;using namespace std;#define BUFFER_SIZ 20typedef struct &#123; long int msg_type; char text[BUFFER_SIZ];&#125; MsgWrapper;void Receive() &#123; MsgWrapper data; long int msgtype = 2; int msgid = msgget((key_t)1024, 0666 | IPC_CREAT); if (msgid == -1) &#123; cout &lt;&lt; \"msgget error \\n\"; return; &#125; while (true) &#123; if (msgrcv(msgid, (void *)&amp;data, BUFFER_SIZ, msgtype, 0) == -1) &#123; cout &lt;&lt; \"error \" &lt;&lt; errno &lt;&lt; endl; &#125; cout &lt;&lt; \"read data \" &lt;&lt; data.text &lt;&lt; endl; if (strlen(data.text) &gt; 6) &#123; // 发送超过6个字符的数据，结束 break; &#125; &#125; if (msgctl(msgid, IPC_RMID, 0) == -1) &#123; cout &lt;&lt; \"msgctl error \\n\"; &#125; cout &lt;&lt; \"Receive ok \\n\";&#125;void Send() &#123; MsgWrapper data; long int msgtype = 2; int msgid = msgget((key_t)1024, 0666 | IPC_CREAT); if (msgid == -1) &#123; cout &lt;&lt; \"msgget error \\n\"; return; &#125; data.msg_type = msgtype; for (int i = 0; i &lt; 10; ++i) &#123; memset(data.text, 0, BUFFER_SIZ); char a = 'a' + i; memset(data.text, a, 1); if (msgsnd(msgid, (void *)&amp;data, BUFFER_SIZ, 0) == -1) &#123; cout &lt;&lt; \"msgsnd error \\n\"; return; &#125; std::this_thread::sleep_for(std::chrono::seconds(1)); &#125; memcpy(data.text, \"1234567\", 7); if (msgsnd(msgid, (void *)&amp;data, BUFFER_SIZ, 0) == -1) &#123; cout &lt;&lt; \"msgsnd error \\n\"; return; &#125;&#125;int main() &#123; std::thread r(Receive); r.detach(); std::thread s(Send); s.detach(); std::this_thread::sleep_for(std::chrono::seconds(20)); return 0;&#125; 消息队列收发消息自动保证了同步，不需要由进程自己来提供同步方法，而命名管道需要自行处理同步问题 消息队列接收数据可以根据消息类型有选择的接收特定类型的数据，不需要像命名管道一样默认接收数据。 消息队列有一个缺点就是发送和接收的每个数据都有最大长度的限制 共享内存 可开辟中一块内存，用于各个进程间共享，使得各个进程可以直接读写同一块内存空间，就像线程共享同一块地址空间一样，该方式基本上是最快的进程间通信方式，因为没有系统调用干预，也没有数据的拷贝操作，但由于共享同一块地址空间，数据竞争的问题就会出现，需要自己引入同步机制解决数据竞争问题。 共享内存只是一种方式，它的实现方式有很多种，主要的有mmap系统调用、Posix共享内存以及System V共享内存等。通过这三种“工具”共享地址空间后，通信的目的自然就会达到 信号量 信号量semaphore，是操作系统中一种常用的同步与互斥的机制； 信号量允许多个进程（计数值&gt;1）同时进入临界区； 如果信号量的计数值为1，一次只允许一个进程进入临界区，这种信号量叫二值信号量； 信号量可能会引起进程睡眠，开销较大，适用于保护较长的临界区； 与读写自旋锁类似，linux内核也提供了读写信号量的机制； 流程分析 可以将信号量比喻成一个盒子，初始化时在盒子里放入N把钥匙，钥匙先到先得，当N把钥匙都被拿走完后，再来拿钥匙的人就需要等待了，只有等到有人将钥匙归还了，等待的人才能拿到钥匙 1234567891011struct semaphore &#123; raw_spinlock_t lock; //自旋锁，用于count值的互斥访问 unsigned int count; //计数值，能同时允许访问的数量，也就是上文中的N把锁 struct list_head wait_list; //不能立即获取到信号量的访问者，都会加入到等待列表中&#125;;struct semaphore_waiter &#123; struct list_head list; //用于添加到信号量的等待列表中 struct task_struct *task; //用于指向等待的进程，在实际实现中，指向current bool up; //用于标识是否已经释放&#125;; down接口用于获取信号量 如果sem-&gt;count &gt; 0时，也就是盒子里边还有多余的锁，直接自减并返回了 当sem-&gt;count == 0时，表明盒子里边的锁被用完了，当前任务会加入信号量的等待列表中，设置进程的状态，并调用schedule_timeout来睡眠指定时间，实际上这个时间设置的无限等待，也就是只能等着被唤醒，当前任务才能继续运行； up用于释放信号量 如果等待列表为空，表明没有多余的任务在等待信号量，直接将sem-&gt;count自加即可。 如果等待列表非空，表明有任务正在等待信号量，那就需要对等待列表中的第一个任务（等待时间最长）进行唤醒操作，并从等待列表中将需要被唤醒的任务进行删除操作 信号量缺点 Semaphore与Mutex在实现上有一个重大的区别：ownership。Mutex被持有后有一个明确的owner，而Semaphore并没有owner，当一个进程阻塞在某个信号量上时，它没法知道自己阻塞在哪个进程（线程）之上； 没有ownership会带来以下几个问题： 在保护临界区的时候，无法进行优先级反转的处理； 系统无法对其进行跟踪断言处理，比如死锁检测等； 信号量的调试变得更加麻烦； 因此，在Mutex能满足要求的情况下，优先使用Mutex 其他接口 信号量提供了多种不同的信号量获取的接口，介绍如下： 12345678/* 未获取信号量时，进程轻度睡眠：TASK_INTERRUPTIBLE */int down_interruptible(struct semaphore *sem)/* 未获取到信号量时，进程中度睡眠：TASK_KILLABLE */int down_killable(struct semaphore *sem)/* 非等待的方式去获取信号量 */int down_trylock(struct semaphore *sem)/* 获取信号量，并指定等待时间 */int down_timeout(struct semaphore *sem, long timeout) 读写信号量 读写自旋锁，读写信号量的功能类似，它能有效提高并发性，包含以下特点： 允许多个读者同时进入临界区 读者与写者不能同时进入临界区（读者与写者互斥） 写者与写者不能同时进入临界区（写者与写者互斥） 读写信号量的数据结构与信号量的结构比较相似： 12345678910111213141516struct rw_semaphore &#123; atomic_long_t count; //用于表示读写信号量的计数 struct list_head wait_list; //等待列表，用于管理在该信号量上睡眠的任务 raw_spinlock_t wait_lock; //锁，用于保护count值的操作#ifdef CONFIG_RWSEM_SPIN_ON_OWNER struct optimistic_spin_queue osq; /* spinner MCS lock */ //MCS锁，参考上一篇文章Mutex中的介绍 /* * Write owner. Used as a speculative check to see * if the owner is running on the cpu. */ struct task_struct *owner; //当写者成功获取锁时，owner会指向锁的持有者#endif#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;; 读写自旋锁，读写自旋锁中的lock字段，bit[31]用于写锁的标记，bit[30:0]用于读锁的统计，而读写信号量的count字段也大体类似； 孤儿进程与僵尸进程 在linux中，正常情况下子进程是通过父进程创建的，父进程无法预测子进程结束时间，需要调用 wait/waitpid 系统调用取的子进程的终止状态 孤儿进程 如果父进程先退出，而它的子进程还在运行，那么子进程被称为孤儿进程。孤儿进程被根进程(进程号为1)所收养，由根进程管理 僵尸进程 任何一个子进程(根进程除外)在退出之后，并非马上消失，内核释放该进程资源，包括打开的文件，占用的内存等。但是仍然为其保留一定的信息 进程号 the process ID 退出状态 the Termination status of the process 运行时间 the amount of CPU time taken by the process 留下一个称为 僵尸进程(Zombie)的数据结构，直到父进程通过 wait/waitpid 来取时才会释放。如果父进程不调用 wait/waitpid 的话，那么保留的信息就不会释放，其进程号就会一直被占用，但系统所使用的进程号时有限的，如果存在大量僵尸进程，可能因为没有可用进程号导致系统不能产生新的进程 通过命令 ulimit -a 查看 max user processes 通过 ps 命令查看，状态为 Z 定位僵尸进程 ps -A -ostat,ppid,pid,cmd |grep -e '^[Zz]’ -A 参数列出所有进程 -o 自定义输出字段 stat（状态）、ppid（进程父id）、pid（进程id）、cmd（命令） 因为状态为z或者Z的进程为僵尸进程，所以我们使用grep抓取stat状态为zZ进程 僵尸进程ID:3457，父进程ID:3425 僵尸进程ID:3533，父进程ID:3511 处理僵尸进程 使用 kill -HUP &lt;僵尸进程ID&gt; 往往无法杀死僵尸进程，-hup会让进程挂起/休眠。此时需要使用 kill -HUP &lt;僵尸进程父ID&gt; 来杀死进程(注意这里有可能杀死根进程，杀的时候注意) 杀死后可通过上述查询命令查看僵尸进程是否存在 僵尸进程代码解决 方法一：wait/waitpid 函数: pid_t wait(int *status) 父进程在调用wait函数之后将自己阻塞，由wait自动分析是否当前进程的某个子进程已经退出 如果找到了变成僵尸的子进程，wait就会收集这个子进程信息，并将它测底销毁后返回 如果没有找到，wait就会一直阻塞在这里，直到有一个出现为止。其中参数 status 用来保存被收集进程退出时的一些状态，是一个指向int类型的指针(不关心如何死掉，直接使用 wait(NULL) )，一个wait也只能处理一个僵尸进程 方法2：将父进程中对SIGCHILD信号的处理函数设为 使用信号，引入 sinal.h，子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程(SIG_IGN 或直接忽略) 1234567891011121314151617181920212223242526272829303132333435363738394041 1 #include &lt;stdio.h&gt; 2 #include &lt;unistd.h&gt; 3 #include &lt;errno.h&gt; 4 #include &lt;stdlib.h&gt; 5 #include &lt;signal.h&gt; 6 7 static void sig_child(int signo); 8 9 int main()10 &#123;11 pid_t pid;12 //创建捕捉子进程退出信号13 signal(SIGCHLD,sig_child);14 pid = fork();15 if (pid &lt; 0)16 &#123;17 perror(\"fork error:\");18 exit(1);19 &#125;20 else if (pid == 0)21 &#123;22 printf(\"I am child process,pid id %d.I am exiting.\\n\",getpid());23 exit(0);24 &#125;25 printf(\"I am father process.I will sleep two seconds\\n\");26 //等待子进程先退出27 sleep(2);28 //输出进程信息29 system(\"ps -o pid,ppid,state,tty,command\");30 printf(\"father process is exiting.\\n\");31 return 0;32 &#125;33 34 static void sig_child(int signo)35 &#123;36 pid_t pid;37 int stat;38 //处理僵尸进程39 while ((pid = waitpid(-1, &amp;stat, WNOHANG)) &gt;0)40 printf(\"child %d terminated.\\n\", pid);41 &#125; **方法3：**将子进程成为孤儿进程，从而其父进程变为根进程，通过根进程处理子进程 进程P 创建子进程 Pc1 之后，子进程再创建子进程 Pcc1， 父进程等待第一个子进程 Pc1退出，让Pc1退出，Pcc1 处理问题称为孤儿进程，由根进程处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt; int main()&#123; pid_t pid; //创建第一个子进程 pid = fork(); if (pid &lt; 0) &#123; perror(\"fork error:\"); exit(1); &#125; //第一个子进程 else if (pid == 0) &#123; //子进程再创建子进程 printf(\"I am the first child process.pid:%d\\tppid:%d\\n\",getpid(),getppid()); pid = fork(); if (pid &lt; 0) &#123; perror(\"fork error:\"); exit(1); &#125; //第一个子进程退出 else if (pid &gt;0) &#123; printf(\"first procee is exited.\\n\"); exit(0); &#125; //第二个子进程 //睡眠3s保证第一个子进程退出，这样第二个子进程的父亲就是根进程里 sleep(3); printf(\"I am the second child process.pid: %d\\tppid:%d\\n\",getpid(),getppid()); exit(0); &#125; //父进程处理第一个子进程退出 if (waitpid(pid, NULL, 0) != pid) //等待进程ID为 pid 的子进程，只要子进程没有结束，就一直等下去 &#123; perror(\"waitepid error:\"); exit(1); &#125; exit(0); return 0;&#125; 参考文献 https://www.cnblogs.com/Anker/p/3271773.html https://mp.weixin.qq.com/s/wTicQwTu8Ta8gLv2fZ3rCA https://mp.weixin.qq.com/s/Lu1nqXfrGPsFcFHH_R_rYA","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"微服务-一致性Hash","slug":"Microservices/微服务-一致性Hash","date":"2021-07-25T14:29:14.000Z","updated":"2022-07-24T05:07:46.000Z","comments":true,"path":"2021/07/25/Microservices/微服务-一致性Hash/","link":"","permalink":"http://xboom.github.io/2021/07/25/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-%E4%B8%80%E8%87%B4%E6%80%A7Hash/","excerpt":"","text":"问题背景 以缓存为例，在整个微服务系统中，缓存可以是多个节点 一是为了提高稳定，单节点宕机情况下，整个存储就面临服务不可用 二是数据容错，同样单节点数据物理损毁，而多节点情况下，节点有备份，除非互为备份的节点同时损毁 为了更好的访问各个节点，使用了Hash算法。把任意长度的输入通过散列算法变换成固定长度的输出，该输出就是散列值，对应不同的节点。如果直接使用取模的方式，如： value=key%Nvalue = key \\% N value=key%N 其中N为缓存节点(访问单元)的数目，当节点因为异常而退出的时候 如果未感知N的变化，那么单个节点所有key都无效导致无法访问，容易引起缓存雪崩 如果感知到N的变化 N = N - 1，那么整个集群所有的 Key 都移位，更加容易引起缓存雪崩 所以就引入了一致性Hash(Consistent Hash) 实现原理 基础类型 最基础的一致性 hash 算法就是把节点直接分布到环上，从而划分出值域， key 经过 hash(key) 之后，落到不同的值域，则由对应的节点处理(图中逆时针) 新增节点 当在N2节点与N3节点之间新增N5节点的时候，只有d的位置受到了影响，其他不变 删除节点 当删除N2节点的时候，也只有c的位置收到的影响 这样仍然存在问题： 随机分布节点的方式使得很难均匀的分布哈希值域(物理节点较少) 在动态增加节点后，即使原来是均匀分布后面也不再均匀分布； 增删节点带来的一个较为严重的缺点是： 当一个节点异常时，该节点的压力全部转移到相邻的一个节点； 当一个新节点加入时只能为一个相邻节点分摊压力； 虚拟节点 针对基础一致性 hash 的缺点一种改进算法是引入虚节点(virtual node)的概念。这个本质的改动：值域不再由物理节点划分，而是由固定的虚拟节点划分，这样值域的不均衡就不存在了。 注意： 虚拟节点的个数要远大于物理节点的个数 虚拟节点的分布是交错的，如果只是虚拟节点增加而不交叉，会无法在删除/新增节点时分摊压力 每个物理节点对应的虚拟节点也是相等的，可以使节点平均分配 操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点（旁白：所以这部分元数据是需要存下来的); 那么这个时候再来删除和新增节点的时候，就能很好的分摊压力了 增节点的时候要能为多个节点分摊压力 删节点的时候要能让多个节点承担压力 存在的问题 如何通过Key的Hash值找到对应范围所属的节点 认为Hash计算的值是随机的，符合概率分布。计算完所有节点的Hash之后需要进行一个排序 排序完怎么做到虚拟节点的交叉 通过计算每个虚拟节点的Hash值，然后整体进行排序 技术内幕 这里看看go-zero是如何实现一致性Hash的 对象定义 根据原理可知，实现一个一致性Hash需要知道 物理节点对应的虚拟节点的数目 Hash函数 物理节点与虚拟节点的对应关系 环中虚拟节点的排序 123456789101112131415161718192021222324252627282930313233const ( TopWeight = 100 //权重 minReplicas = 100 //虚拟节点:物理节点 prime = 16777619 //名称前缀)type ( // Func defines the hash method. Func func(data []byte) uint64 // A ConsistentHash is a ring hash implementation. ConsistentHash struct &#123; hashFunc Func //Hash函数 replicas int //虚拟节点的数量 keys []uint64 //保存虚拟节点的Hash ring map[uint64][]interface&#123;&#125; //存储虚拟节点与物理节点的关系 nodes map[string]lang.PlaceholderType //存储节点的信息 lock sync.RWMutex &#125;)// 可以理解为确定node字符串值的序列化方法// 在遇到哈希冲突时需要重新对key进行哈希计算// 为了减少冲突的概率前面追加了一个质数prime来减小冲突的概率func innerRepr(v interface&#123;&#125;) string &#123; return fmt.Sprintf(\"%d:%v\", prime, v)&#125;// 可以理解为确定node字符串值的序列化方法func repr(node interface&#123;&#125;) string &#123; return mapping.Repr(node)&#125; 新增节点 增加节点副本数 123456789101112131415161718192021222324252627func (h *ConsistentHash) AddWithReplicas(node interface&#123;&#125;, replicas int) &#123; h.Remove(node) //如果同名则先删除 if replicas &gt; h.replicas &#123; //虚拟节点数量不能超过一致性Hash数量 replicas = h.replicas &#125; nodeRepr := repr(node) //找到节点表达式 h.lock.Lock() defer h.lock.Unlock() h.addNode(nodeRepr) //添加添加 //添加虚拟节点 for i := 0; i &lt; replicas; i++ &#123; hash := h.hashFunc([]byte(nodeRepr + strconv.Itoa(i))) //计算虚拟节点名称获取Hash值 h.keys = append(h.keys, hash) //保存虚拟节点 h.ring[hash] = append(h.ring[hash], node) //为虚拟节点添加物理节点 &#125; sort.Slice(h.keys, func(i, j int) bool &#123; //进行虚拟节点的排序 return h.keys[i] &lt; h.keys[j] &#125;)&#125;func (h *ConsistentHash) addNode(nodeRepr string) &#123; h.nodes[nodeRepr] = lang.Placeholder&#125; 注意： h.ring[hash]这里使用的是切片，为了防止虚拟节点存在冲突，也就是一个虚拟节点可能对应多个物理节点。 设置权重节点 1234567// 按权重添加节点// 通过权重来计算方法因子，最终控制虚拟节点的数量// 权重越高，虚拟节点数量越多func (h *ConsistentHash) AddWithWeight(node interface&#123;&#125;, weight int) &#123; replicas := h.replicas * weight / TopWeight h.AddWithReplicas(node, replicas)&#125; 删除节点 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (h *ConsistentHash) Remove(node interface&#123;&#125;) &#123; nodeRepr := repr(node) //找到节点表达式 h.lock.Lock() defer h.lock.Unlock() //如果不包含该节点直接退出 if !h.containsNode(nodeRepr) &#123; return &#125; for i := 0; i &lt; h.replicas; i++ &#123; hash := h.hashFunc([]byte(nodeRepr + strconv.Itoa(i))) //虚拟节点Hash值 index := sort.Search(len(h.keys), func(i int) bool &#123; //查找虚拟节点 return h.keys[i] &gt;= hash //二分法进行节点查找 &#125;) if index &lt; len(h.keys) &amp;&amp; h.keys[index] == hash &#123; //如果是要找的虚拟节点则删除这个虚拟节点 h.keys = append(h.keys[:index], h.keys[index+1:]...) &#125; h.removeRingNode(hash, nodeRepr) //删除虚拟节点对应物理节点 &#125; //删除物理节点 h.removeNode(nodeRepr)&#125;func (h *ConsistentHash) removeRingNode(hash uint64, nodeRepr string) &#123; if nodes, ok := h.ring[hash]; ok &#123; newNodes := nodes[:0] for _, x := range nodes &#123; //遍历物理节点 if repr(x) != nodeRepr &#123; //如果物理节点与指定的不一样 newNodes = append(newNodes, x) //就把节点存入新的节点中 &#125; &#125; //虚拟节点存在多个物理节点则仅保留新的，否则直接删除虚拟节点 if len(newNodes) &gt; 0 &#123; h.ring[hash] = newNodes &#125; else &#123; delete(h.ring, hash) &#125; &#125;&#125;func (h *ConsistentHash) removeNode(nodeRepr string) &#123; delete(h.nodes, nodeRepr)&#125; 注意： 使用二分法进行节点查找 由于一个虚拟节点对应多个物理节点，不能直接删除虚拟节点 获取节点 根据v顺时针找到最近的虚拟节点，再通过虚拟节点映射找到真实节点 1234567891011121314151617181920212223242526272829func (h *ConsistentHash) Get(v interface&#123;&#125;) (interface&#123;&#125;, bool) &#123; h.lock.RLock() defer h.lock.RUnlock() if len(h.ring) == 0 &#123; return nil, false &#125; // 二分查找 // 找到比Hash的第一个节点，可能返回的 index = len(h.keys) // 所以这里取余回到第一个节点 hash := h.hashFunc([]byte(repr(v))) //Key的Hash值 index := sort.Search(len(h.keys), func(i int) bool &#123; return h.keys[i] &gt;= hash &#125;) % len(h.keys) nodes := h.ring[h.keys[index]] //获取虚拟节点对应物理节点 switch len(nodes) &#123; case 0: return nil, false case 1: return nodes[0], true default: //存在Hash冲突 // 取模的方式存获取真实物理节点 innerIndex := h.hashFunc([]byte(innerRepr(v))) pos := int(innerIndex % uint64(len(nodes))) return nodes[pos], true &#125;&#125; 注意: 如果虚拟节点对应多个物理节点那么通过hash获取在节点中的位置 总结 通过增加虚拟节点的数目来使的hash的分布更加均匀 虚拟节点的应该是交叉的(实现中并不是严格交叉，而是通过Hash再排序让虚拟节点交叉存在) 参考链接 分布式系统基石一 搞懂一致性Hash https://talkgo.org/t/topic/3098","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"Mysql入门8-备份与恢复","slug":"MySql/MySql入门8-备份与恢复","date":"2021-07-25T03:43:51.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/25/MySql/MySql入门8-备份与恢复/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql%E5%85%A5%E9%97%A88-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/","excerpt":"","text":"概述 根据备份方法的不同，备份可以分为： 热备 Hot Backup：指数据库运行中直接备份，对正在运行的数据库没有任何影响，也称在线备份(Online Backup) 冷备 Code Backup: 指备份操作是在数据库停止的情况下，也称离线备份(Offline Backup) 温备份 Warm Backup: 指数据库运行中进行的备份操作，但是对当前数据库操作有影响。如加一个全局读锁保证数据一致性 按照备份后的文件的内容可以分为： 逻辑备份： 指备份出的文件内容是可读的，内容一般是由一条条SQL语句或表内容实际数据组成 一般用于数据库升级与迁移 缺点是 恢复所需要的时间往往比较长 裸文件备份: 复制数据库的物理文件，即可以是在数据库运行中的复制，也可以是数据库停止运行时直接的数据文件复制。 恢复时间往往比逻辑备份短的多 按照备份数据库的内容可分为： 完全备份：对数据库进行一个完全的备份 增量备份：在上次完全备份的基础上，对于更改的数据进行备份 日志备份：主要指对MySQL数据库二进制日志的备份 MySQL数据库复制(replication)的原理就是异步实时的将二进制日志重做传送并应用到从(slave/standby)数据库 冷备 对于InnoDB存储引擎的冷备，只需要备份MySQL数据库的frm文件，共享表空间文件，独立表空间文件(*.idb)，重做日志。 另外建议定期备份MySQL数据库的配置文件my.cnf 最好将本地产生的备份存放到一台远程服务器中，却被不会应为本地数据库的宕机而影响备份文件的使用 优点： 备份简单，只需要复制相关文件 恢复简单，只需要把文件恢复到指定位置即可 备份文件可在不同操作系统，不同MySQL版本上恢复 恢复速度快，不需要执行任何SQL语句，也不需要重建索引 缺点: InnoDB存储引擎冷备的文件通常比逻辑文件大很多，因为表空间存放着很多其他数据，如undo段，插入缓冲等信息 冷备跨平台可能存在操作系统，MySQL的版本，文件大小写敏感和浮点数格式都可能称为问题 热备 ibbackup是InnoDB存储引擎官方提供的热备工具，可以同时备份MyISAM存储引擎。原理是： 记录备份开始时，InnoDB存储引擎重做日志文件检查点的LSN 复制共享表空间文件以及独立表空间文件 记录复制完表空间文件后，InnoDB存储引擎重做日志文件检查点LSN 复制在备份时产生的重做日志 优点： 在线备份，不阻塞任何的SQL语句 备份性能好，备份的实质是复制数据库文件和重做日志文件 支持压缩备份，通过选项，支持不同级别的压缩 跨平台 逻辑备份 123456789101112131415161718#1. 备份所有数据库mysqldump --all-databases &gt; dump.sql#2. 备份指定数据库mysqldump --databases db1 db2 db3 &gt;dump.sql#3. 导出指定数据库指定条件的数据(test数据库，表a)mysqldump --single-transaction --where&#x3D;&#39;b&gt;2&#39; test a &gt; a.sql#4. 备份恢复(直接执行mysql语句)mysql -uroot -p &lt; test_backup.sql#5. 执行使用source导出逻辑备份文件mysql&gt; source &#x2F;home&#x2F;mysql&#x2F;test_bakcup.sql# 二进制日志备份与恢复#1. 推荐二进制日志的服务器配置log-bin &#x3D; mysql-binsync_binlog &#x3D; 1innodb_suuport_xa &#x3D; 1#2. 使用mysqlbinlog恢复日志shell&gt; mysqlbinlog binlog.00001 &gt; &#x2F;tmp&#x2F;statements.sql 注意： mysqldump 无法导出视图，需要独立导出视图的定义或者备份视图定义 frm文件 备份二进制日志文件前，可以通过FLUSH LOGS 命令来生成一个新的二进制日志文件，然后备份 快照备份 指通过文件系统的快照功能进行数据库备份，MySQL本身不支持快照功能 复制(replication) replication的工作原理可以分为3个步骤： 主服务器(master)把数据更改记录到二进制日志(binlog)中 从服务器(slave)把主服务器的二进制日志复制到自己的中继日志(relay log)中 从服务器重做中继日志中的日志，把更改应用到自己的数据库 复制不是完全实时地进行同步，而是异步实时 主服务器上有一个线程负责发送二进制日志 从服务器有2个线程 I/O线程，负责读取主服务器的二进制日志，并将其保存为中继日志 SQL线程负责复制执行中继日志 在主从架构下，当主服务器误操作发生，从库也会跟着执行，这该怎样恢复？ 可对从服务器上的数据库所在分区做快照，避免误操作对复制造成的影响，然后根据二进制日志进行point-in-time的恢复，因此快照+复制的备份架构如下： 参考文献 《MySql技术内幕》","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门5-事务","slug":"MySql/MySql入门5-事务","date":"2021-07-25T03:20:52.000Z","updated":"2023-03-12T01:49:10.295Z","comments":true,"path":"2021/07/25/MySql/MySql入门5-事务/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql%E5%85%A5%E9%97%A85-%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"概述 InnoDB存储引擎中的事务完全符合ACID特性： 原子性(atomicity)：整个事务是不可分割的工作单位，任何一个SQL语句执行失败，已经执行成功SQL语句也必须撤销，数据库状态退回到执行事务前的状态。默认情况下一条SQL就是一个单独事务，事务是自动提交的。只有显式的使用start transaction开启一个事务，才能将一个代码块放在事务中执行 一致性(consistency)：一致性事务指数据库从一个状态转变为下一种一致的状态。事务开始前和事务结束后，数据库完整性并没有被破坏。如事务执行后数据库唯一约束被破坏，则自动撤销事务，返回初始化状态。 隔离性(isolation)：事务的隔离性要求事务提交前对其他事务不可见 持久性(durability)：事务一旦提交，其结果就是永久性的。即使数据库崩崩溃而恢复，提交的数据也不会丢失 事务的分类： 隐式事务：事务没有明显的开始和结束的标记。MySql的每一条DML(增删改)语句都是一个单独的事务，每条语句DML执行完毕自动提交事务。MySql默认自动提交事务 SHOW VARIABLES LIKE 'autocommit'; 显示事务：事务具有明显的开启和结束的标记，前提是必须设置自动提交功能为禁用 开启事务 - 执行SQL语句 - 成功 - 提交事务 开启事务 - 执行SQL语句 - 失败 - 回滚事务 还能设置回滚点： 123savepoint &lt;回滚点名字&gt;rollback to &lt;回滚点名字&gt; 事务的执行流程如下 sql:update test set name = ‘test’ where id=2; 事务完整流程: 1.事务开始 2.申请锁资源，对id=2这行数据上排他锁 3.将需要修改的data pages读取到innodb_buffer_cache 4.记录id=2的数据到undo log 5.记录id=2修改后的数据到redo log buffer 6.将buffer cache中id=2得name改为test 7.commit，触发二阶段提交2pc 8.事务结束 知识点科普: WAL：write ahead logging 针对数据文件的修改，必须遵循日志先行原则。也即是将数据持久化到磁盘之前必须确保redo log落盘。 二阶段提交(2pc two phase commit): 二阶段提交,首先redo log prepare,然后写入binlog,最后redo log commit。主要是保证redo log事务写入顺序和binlog 事务顺序一致(通过事务id保证一致)。 完整流程如下: prepare阶段：redo持久化到磁盘(redo group commit)，并将回滚段置为prepared状态，此时binlog不做操作 commit阶段：innodb释放锁，释放回滚段，设置undo log提交状态，binlog持久化到磁盘，然后存储引擎层提交 隔离级别 并发可能产生的问题： 脏读 dirty read: 读到其他事务未提交的数据(针对其他事务未提交的操作) 不可重复读 non-repeatable read: 前后读取的记录内容不一致(针对其他事务修改或删除的操作) 幻读 phantom read: 前后读取的记录数量不一致(针对其他事务新增的操作) 针对解决上述问题，将事务的隔离级别分为(由低到高)： 读未提交 read uncommitted: 一个事务未提交，改动能被另一个事务看到 =&gt; 都发生 读提交 read committed: 一个事务提交了，改动才能被另一个事务看到 =&gt; 没有赃读 可重复读 repeatable read(默认级别): 一个事务提交了，改动也不能被另一个事务看到 =&gt; 只有幻读 串行化 serializable: 后访问的事务必须等待前一个事务执行完成才能访问 =&gt; 都解决 所以总结下来就是： 隔离级别 脏读 不可重复读 幻读 read-uncommitted读取未提交 会出现 会出现 会出现 read-committed读已提交 解决 会出现 会出现 repeatable-read可重复读 解决 解决 会出现 serializable可串行化 解决 解决 解决 修改隔离级别语句： 123456set session transaction isolation level read uncommitted; #读未提交set session transaction isolation level read committed; #读已提交set session transaction isolation level repeatable read; #可重复读set session transaction isolation level serializable; #可串行化set autocommit &#x3D; 0; #取消自动提交select @@tx_isolation; #查询隔离级别 当你不再需要该表时， 用 drop 当你仍要保留该表，但要删除所有记录时， 用 truncate，不可回滚 当你要删除部分记录时（always with a WHERE clause), 用 delete 事务的基础 数据库的隔离性包括(ACID)，由不同的功能实现 Atomicity 原子性: 使用undo log 实现回滚 Consistency 一致性: 通过原子性、持久性、隔离性实现数据一致性 Lsolation 隔离性: 使用锁以及MVCC 实现读写分离、读读并行、读写并行 Durability 持久性: 通过redo log 恢复，和在并发环境下的隔离做到一致性 redo log 称为重做日志，是物理日志，记录页的物理修改操作，用来恢复提交事务修改的页操作 undo log 称为回滚日志，是逻辑日志，根据每行记录进行记录。用来回滚行记录到某个特定版本 redo log redo log 也做重做日志，日志文件由两部分组成： 重做日志缓冲 redo log buffer 重做日志文件 redo log file 1234567start transaction;select balance from bank where name=\"zhangsan\";#生成 重做日志 balance=600 update bank set balance = balance - 400; # 生成 重做日志 amount=400 update finance set amount = amount + 400;commit; 为了提升性能不会把每次的修改都实时同步到磁盘，而是会先存到Boffer Pool(缓冲池)里头，把这个当作缓存来用。然后使用后台线程去做缓冲池和磁盘之间的同步，如果还没有来得及同步就宕机怎么办？ 通过Force Log at Commit机制实现事务持久性，即当事务提交(COMMIT)时，必须先将该事务的所有日志写入到重做日志进行持久化，待事务的COMMIT操作完成才算完成 为了确保每次日志都写入重做日志，在每次将重做日志缓冲写入重做日志文件后，InnoDB存储引擎还需要调用一次fsync操作。由于重做日志文件打开并没有使用O_DIRECT选项，因此重做日志缓冲先写入文件系统缓存，为了确保重做日志写入磁盘，必须进行一次fsync操作 系统重启之后在读取redo log恢复最新数据 通过 innodb_flush_log_at_trx_commit 来控制充足哦日志刷新到磁盘的策略 1：事务提交时必须调用一次fsync操作(默认) 0：事务提交时不进行写入重做日志，而是通过master thread 每1秒进行一次重做日志文件的fsync操作 2： 事务提交时将重做日志写入重做日志文件，但仅仅写入文件系统缓存，不进行fsync操作 与二进制日志 binlog的区别是： redo log 是在InnoDB存储引擎层产生的，而二进制文件是服务层产生的 binlog记录的是逻辑日志，记录的是对应的SQL日志。而redo log是物理日志，记录的是对每个页的修改 存入磁盘的时间点不一样 binlog 只在事务提交完成后进行一次写入 redo log在事务进行中不断地被写入，表现为日志并不是随事务提交的顺序进行写入的 总结：redo log是用来恢复数据的 用于保障，已提交事务的持久化特性 undo log undo 也叫做回滚日志，为了回滚需要将之前的操作都记录下来 每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log 当发生系统错误或执行回滚的时候使用undo log undo log 记录事务修改之前版本的数据信息，因此假如由于系统错误或者rollback操作而回滚的话可以根据undo log的信息来进行回滚到没被修改前的状态 (1) 如果在回滚日志里有新增数据记录，则生成删除该条的语句 (2) 如果在回滚日志里有删除数据记录，则生成生成该条的语句 (3) 如果在回滚日志里有修改数据记录，则生成修改到原先数据的语句 总结：undo log是用来回滚数据的用于保障 未提交事务的原子性 MVCC MVCC是通过在每行记录的后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存了行的过期时间，存储的不是实际的时间值而是系统版本号，主要实现思想是通过数据多版本来做到读写分离。从而实现不加锁读进而做到读写并行 MVCC 在mysql中的实现依赖的是undo log 与 read view undo log: unlog 中就某行数据的多个版本数据 read view: 用来判断当前版本数据的可行性 原子性的实现 一个事务必须被视为不可分割的最小工作单位，一个事务中的所有操作要么全部成功提交，要么全部失败回滚，对于一个事务来说不可能只执行其中的部分操作，这就是事务的原子性，通过上述undo log 即可以实现 持久性的实现 事务一旦提交，其所作做的修改会永久保存到数据库中，此时即使系统崩溃修改的数据也不会丢失 InnoDB提供了缓冲池(Buffer Pool)，Buffer Pool中包含了磁盘数据页的映射，可以当做缓存来使用 读数据：会首先从缓冲池中读取，如果缓冲池中没有，则从磁盘读取在放入缓冲池 写数据：会首先写入缓冲池，缓冲池中的数据会定期同步到磁盘中 如果数据已提交，但在缓冲池里还未来得及磁盘持久化，需要一种机制保存已提交事务的数据，为恢复数据使用。redo log 即可解决这个问题，既然redo log也需要存储，也涉及磁盘IO为啥还用它？ redo log 的存储是顺序存储，而缓存同步是随机操作 缓存同步是以数据页为单位的，每次传输的数据大小大于redo log 隔离性的实现 读未提交 读未提交可以理解为没有隔离 串行话(读写锁) 串行话读的时候加共享锁，其他事务可以并发读但不能写。写的时候加排它锁，其他事务不能并发写也不能并发读 可重复读 MVCC:多版本并发控制，通过undo log版本链和read-view实现事务隔离 以可重复读为例 每条记录在更新时除了记录一条变更记录到redo log中，还会记录一条变更相反的回滚操作记录在undo log中 例如一个值从1被按顺序改成了2、3、4，在回滚日志里就会有如下记录： 当前值是4，但查询这条记录的时候，不同时刻启动的事务有不同的read-view 在视图 A、B、C 里面，这一个记录的值分别是 1、2、4 对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到 即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图(也叫快照)，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现 日志什么时候删除？ 日志怎么存储 长事务导致undolog一致存在不被删除，什么是长事务 什么是视图、MVCC 对于一个视图(快照)来说，它能够读到那些版本数据，要遵循以下规则： 当前事务内的更新，可以读到； 版本未提交，不能读到； 版本已提交，但是却在快照创建后提交的，不能读到； 版本已提交，且是在快照创建前提交的，可以读到； 可重复读和读提交的区别在于：在快照的创建上，可重复读仅在事务开始是创建一次，而读提交每次执行语句的时候都要重新创建一次 什么时候删除回滚日志undo-log 当没有比回滚日志更早的读视图（读视图在事务开启时创建）的时候，这个数据不会再有谁驱使它回滚了，这个回滚日志也就失去了用武之地，可以删除了 ibdata文件是共享表空间数据文件。 5.7版本支持单独配置undo log的路径和表空间文件。 为什么回滚到清理后，文件还是不会变小？这个“清理”的意思是 “逻辑上这些文件位置可以复用”，但是并没有删除文件，也没有把文件变小。那到底什么时候删除呢？ 不同的事务拥有不同的Read view，如果一个事务长时间没有提交，意味着会存在很多老的事务视图，同时也保存了大量回滚日志，占用磁盘空间，且在mysql5.5之前，回滚日志和字典都保存在ibdata文件中，即使长事务被提交了，回滚段被清理，文件也不会变小。同时长事务还长期占用锁资源，降低并发效率 幻读 并发写问题的解决方式就是行锁，而解决幻读用的也是锁，叫做间隙锁，MySQL 把行锁和间隙锁合并在一起，解决了并发写和幻读的问题，这个锁叫做 Next-Key锁。 假设现在表中有两条记录，并且 age 字段已经添加了索引，两条记录 age 的值分别为 10 和 30 如图所示，分成了3 个区间，(负无穷,10]、(10,30]、(30,正无穷]，在这3个区间是可以加间隙锁的。 之后，我用下面的两个事务演示一下加锁过程 在事务A提交之前，事务B的插入操作只能等待，这就是间隙锁起得作用。当事务A执行update user set name='风筝2号’ where age = 10; 的时候，由于条件 where age = 10 ，数据库不仅在 age =10 的行上添加了行锁，而且在这条记录的两边，也就是(负无穷,10]、(10,30]这两个区间加了间隙锁，从而导致事务B插入操作无法完成，只能等待事务A提交。不仅插入 age = 10 的记录需要等待事务A提交，age&lt;10、10&lt;age&lt;30 的记录页无法完成，而大于等于30的记录则不受影响，这足以解决幻读问题了。 这是有索引的情况，如果 age 不是索引列，那么数据库会为整个表加上间隙锁。所以，如果是没有索引的话，不管 age 是否大于等于30，都要等待事务A提交才可以成功插入 当前读 两个事务，对同一条数据做修改。结果应该时间靠后的结果。且更新之前要先读数据，这里所说的读和上面说到的读不一样，更新之前的读叫做“当前读”：总是当前版本的数据，多版本中最新一次提交的那版。 假设事务A执行 update 操作， update 的时候要对所修改的行加行锁，这个行锁会在提交之后才释放。而在事务A提交之前，事务B也想 update 这行数据，于是申请行锁，但是由于已经被事务A占有，事务B是申请不到的，此时，事务B就会一直处于等待状态，直到事务A提交，事务B才能继续执行，如果事务A的时间太长，那么事务B很有可能出现超时异常。 加锁的过程要分有索引和无索引两种情况，比如下面这条语句 1update user set age&#x3D;11 where id &#x3D; 1 id 是这张表的主键，是有索引的情况，那么 MySQL 直接就在索引数中找到了这行数据，然后干净利落的加上行锁就可以了 而下面这条语句 1update user set age&#x3D;11 where age&#x3D;10 表中并没有为 age 字段设置索引，所以， MySQL 无法直接定位到这行数据。那怎么办呢，当然也不是加表锁了。MySQL 会为这张表中所有行加行锁，没错，是所有行。但是呢，在加上行锁后，MySQL 会进行一遍过滤，发现不满足的行就释放锁，最终只留下符合条件的行。虽然最终只为符合条件的行加了锁。 参考链接 https://time.geekbang.org/column/article/68963 https://blog.csdn.net/youanyyou/article/details/108722263 https://blog.csdn.net/kongliand/article/details/107953656 《MySQL技术内幕》","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门4-锁","slug":"MySql/MySql入门4-锁","date":"2021-07-25T03:20:33.000Z","updated":"2022-08-13T04:05:55.000Z","comments":true,"path":"2021/07/25/MySql/MySql入门4-锁/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql%E5%85%A5%E9%97%A84-%E9%94%81/","excerpt":"","text":"lock与latch 数据库中lock与latch都称为锁，但两者有截然不同的意思 latch 称为闩锁(轻量级)，要求锁的时间必须非常短。在Innodb引擎中，有分mutex(互斥锁)和rwlock(读写锁)。用来保证并发线程操作临界资源的正确性，没有死锁检测机制 lock的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit或rollback后进行释放(补孕酮事务隔离级别释放的时间可能不一样) 全局锁 如果数据库需要进行全局逻辑备份 在支持事务的存储引擎中，在可重复读隔离级别下开启一个事务，数据库备份的同时可以更新(mysqldump工具) 在不支持事务的存储引擎中，需要通过FTWRL方法进行全局加锁备份 要全库只读，为什么不使用set global readonly=true? 有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。 且在slave上，用户有超级权限的话readonly是失效的 在异常处理机制上有差异。 如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。 如果将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 全局锁是对整个数据库实例加锁，整个数据库处于只读状态，下面语句将会被阻塞： 数据更新语句(数据的增删改) 数据定义语句(建表、修改表结构等) 更新类事务的提交语句 加锁 Flush tables with read lock(FTWRL) 解锁 unlock tables 整个数据库只读可能存在的问题： 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟 表级锁 MySql里面表级别的锁有两种： 表锁 元数据锁（meta data lock，MDL) 表锁的语法是 lock tables … read/write，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放 需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象 MDL(metadata lock)，MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。 DDL操作对表加MDL读锁，保证的是表结构不能修改，而与表数据无关，是可以CRUD的 DML操作对表加MDL写锁，保证的是表结构不能被并行修改，同时表数据也不能读了 如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的 虽然MDL是默认添加的，但还是会碰到一个问题：给一个小表加个字段，导致整个库挂了 因为sessionB加了MDL读锁，导致后面的sessionC阻塞。如果sessionB一直没有完成select，那么sessionC申请写锁被阻塞，将会导致后面的sessionD等申请读锁都被阻塞。这个时候客户端如果有频繁重试的逻辑就会导致不停的和数据库建立连接，把连接池打满导致库不可用 事务中的 MDL 锁，在语句执行开始时申请，但在语句结束后并不会马上释放，而会等到整个事务提交后释放 如何安全的给数据库加字段？ 首先需要解决长事务问题：事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务 如果要变更的表是一个热点表，虽然数据量不大但请求很频繁，而你不得不加个字段，你该怎么做呢？ 比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程 行锁 行锁是由各个引擎自己实现的，而有的引擎并不支持行锁如MyISAM InnoDB实现了如下两种标准的行级锁： 共享锁(S Lock)，允许事务读一行数据 排他锁(X Lock)，允许事务删除或更新一行数据 InnoDB支持多粒度(granular)锁，允许事务在行级上的锁和表级上的锁同时存在，称为意向锁(Intention Lock)，表级别的锁 意向共享锁(IS Lock)，事务想要获得一张表中某几行的共享锁 意向排他锁(IX Lock)，事务想要获得一张表中某几行的排他锁 由于InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫描以外的任何请求 IS IX S X IS 兼容 兼容 兼容 不兼容 IX 兼容 兼容 不兼容 不兼容 S 兼容 不兼容 兼容 不兼容 X 不兼容 不兼容 不兼容 不兼容 若将上锁的对象看成一颗树，那么对最下层的对象上锁，首先要对上层对象上锁 当事务A commit结束之后，事务B才执行 InnoDB存储引擎有3种行锁的算法，分别是： Record Lock: 单个行记录上的锁 Gap Lock: 间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock: Gap Lock + Record Lock，锁定一个范围，并且锁定记录本身 在REPEATABLE READ下，存储引擎使用Next-Key Locking机制来避免幻读(一个事务中，两次读到的数据数量不一致) 其他锁 一致性非锁写读 在默认配置即事务隔离级别为REPEATABLE READ模式下，InnoDB存储引擎的SELECT操作使用一致性非锁定读 一致性非锁定读(consistent nonlocking read)是指InnoDB存储引擎通过行多版本控制(multi versioning)的方式来读取当前执行时间数据库中的行的数据。如果读取的行正在执行DELETE或UPDATE操作，读取操作不会等待行上锁的释放而是去读取行的一个快照 快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。undo用来在事务中进行回滚的，因此快照数据本身没有额外的开销 一个行记录可能不止一个快照数据，由此的并发控制，称为多版本并发控制(Multi Version Concurrency Control, MVCC) 在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据 在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本 一致性锁定读 某些情况下需要显示对数据库读取操作加锁已保证数据逻辑一致性。对于SELECT的只读操作，InnoDB存储引擎对于SELECT语句支持两种一致性的锁定读(locking read) SELECT … FOR UPDATE : 对读取的行记录加一个X锁，其他事务不能加上任何锁 SELECT … LOCK IN SHARE MODE： 对读取的行记录加一个S锁，其他事务锁定的行加S锁，但是如果加X锁，会被阻塞 自增长与锁 在InnoDB存储引擎的内存结构中，对每个含有自增长值的表都有一个自增长在计数器(auto-increment counter)，对含有自增长的计数器的表进行插入操作时，这个计数器会被初始化，执行如下的语句来得到计数器的值： select MAX(auto_inc_col) FROM t FOR UPDATE; 插入操作会依据这个计数器值加1服务自增长列，称为AUTO-INC Locking。它并不是在一个事务完成后才释放，而是在完成对自增长值插入的SQL语句后立即释放 自增长值的列必须是缩影，同时必须是索引的第一列，不是则抛出异常 死锁和死锁检查 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略： 一种策略是，直接进入等待直到超时。超时时间可通过参数 innodb_lock_wait_timeout 来设置(默认50s)。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑 不能将第一种策略的超时时间设置的太短，可能导致误伤了不是死锁的情况 每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。死锁检测会消耗大量的CPU。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的 怎么解决由这种热点行更新导致的性能问题呢？ 可以考虑将一行改成逻辑上的多行来减少冲突。如果一个账户的金额等于这10条记录的总和，这样给一个账号添加金额的时候可以随机选择一条来进行，这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗 参考链接 https://time.geekbang.org/column/article/69862","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门2-文件","slug":"MySql/MySql入门2-文件","date":"2021-07-24T02:44:44.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/24/MySql/MySql入门2-文件/","link":"","permalink":"http://xboom.github.io/2021/07/24/MySql/MySql%E5%85%A5%E9%97%A82-%E6%96%87%E4%BB%B6/","excerpt":"","text":"构成Mysql和InnoDB存储引擎表的各类文件包括： 参数文件：初始化参数、内存结构大小等设置 日志文件：记录MySql实例的各种类型活动 socket文件：当用UNIX域套接字方式进行连接时所需要的文件 pid文件：MySql实例的进程ID文件(作用是？) 存储引擎文件：每个引擎会有自己的文件保存数据(如记录和索引) 参数文件 当MySql实例启动时，会先读参数文件(如果没有则使用默认值)，用来初始化数据库 1234# 1. 查看数据库参数文件mysql --help |grep my.cnf# 2. 查看数据库参数SHOW VARIABLES 日志文件 MySql常见的日志文件有： 错误日志文件(error log) 二进制日志文件(binlog) 慢查询日志文件(slow query log) 查询日志文件(log) 慢查询日志 开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能 可用于定位可能存在问题的SQL语句，默认情况下并不启动慢查询日志 12345678#查询慢查询阀值(等于阀值的并不会记录)SHOW VARIABLES LIKE 'long_query_time'#查询慢查询开关SHOW VARIABLES LIKE 'log_slow_queries'#慢查询没有使用索引(如果运行SQL而没有使用索引)SHOW VARIABLES LIKE 'log_queries_not_using_indexes'#表示每分钟允许记录到slow log且未使用索引的SQL语句(默认为0，防止语句频繁记录到slow log导致文件不断增大)SHOW VARIABLES LIKE 'log_throttle_queries_not_using_indexes' 12#查看慢查询日志mysqldumpslow nh122-190-slow.log MySQL5.1 开始 可以将慢查询的日志记录放入一张表中，名为 slow_log 123456789101112#指定慢查询输出的格式，默认为FILE#TABLE则将日志写入slow_log表中SHOW VARIABLES LIKE &#39;log_output&#39;#指定逻辑IO次数超过的记录写入slow log(默认100)SHOW VARIABLES LIKE &#39;long_query_io&#39;SHOW VARIABLES LIKE &#39;slow_query_type&#39; #表示slow log的启动方式#0 表示不将sql语句记录到slow log#1 表示根据运行时间将sql语句记录到slow log#2 表示根据逻辑IO次数将sql语句记录到slow log#3 表示根据运行时间及逻辑IO次数将sql语句记录到slow log 逻辑IO：包含所有的读取，不管是磁盘还是缓冲池 物理IO：指从磁盘进行IO读取的次数 查看日志文件：记录了对MySql数据库所有请求的信息，无论这些请求是否得到正确的执行。默认文件名为：主机名.log。和慢查询日志一样，可以将查询日志放入mysql架构下的general_log表中 二进制日志 记录了对MySql数据库执行更改的所有操作，但不包括select和show这类操作，因为这类对数据本身并没有修改 1SHOW BINLOG EVENTS #查看二进制日志 主要有以下几点作用： 恢复(recovery)：某些数据的恢复需要二进制日志。例如在一个数据库全备文件恢复后，用户可通过二进制日志进行point-in-time的恢复 复制(replication)：通过复制和执行二进制日志使一台远程MySql数据库(一般为slave或standby)与一台MySql数据库(master或primary)进行实时同步 审计(audit)：用户可以通过二进制日志中的信息来进行审计，判断是否有对数据库进行注入攻击 12#启动二进制文件my.cnflog-bin[=name] 如果不指定name，则默认二进制日志文件名为主机名，后缀为二进制日志的序列号(bin_log.00001)，所在路径为数据库所在目录(datadir) 其他影响二进制日志记录的参数： max_binlog_size：指定单个二进制日志文件的最大值，超过则新建二进制日志文件，后缀序列号+1 并记录到.index文件中(默认1G) binlog_cache_size： 当使用事务的表，所有未提交的二进制日志会被记录到一个缓存中，等该事务提交再将缓冲写入二进制日志文件。缓存大小由binlog_cache_size决定(默认32K) binlog_cache_size是基于会话的，但一个线程开始一个事务，就会自动分配一个binlog_cache_size 当事务记录大小大于缓冲时，回将缓冲中的日志写入一个临时文件中 缓冲区大小的设置可以通过 SHOW GLOBASL STATUS命令查看 binlog_cache_use：记录使用缓冲区二进制日志的次数 binlog_cache_disk_use：记录使用临时文件写二进制日志的次数 binlog-do-db：表示需要写入哪些库的日志 binlog-ignore-db：表示需要忽略写入哪些库的日志 log-slave-update：如果是slave角色，则不会将从master取得并执行的二进制日志写入自己的二进制日志文件中。如果需要写入，则设置log–slave-update。如果搭建master=&gt;slae=&gt;slave架构复制，必须设置该值 Binlog-format：日志存储的不同格式(STATEMENT/ROW/MIX) 删除binlog 12# 查看过期时间(单位day)show variable like 'expire_logs_days' expire_logs_days = 0: 表示所有binlog日志永久都不会失效，不会自动删除 触发条件： 当 binlog大小超过max_binlog_size的时候 手动flush logs 其他操作可参考链接 其他文件 套接字文件：UNIX系统下本地连接MySql可采用UNIX域套接字方式，需要一个套接字文件， 12#查看套接字文件位置SHOW VARIABLES LIKE 'socket' PID文件：当MySQL实例启动，会将自己的进程ID写入到pid文件 12#查看PID文件位置SHOW VARIABLES LIKE 'pid_file' InnoDB存储引擎文件 表空间文件 InnoDB将存储的数据按表空间进行存放。默认配置下会有一个初始大小为10MB的默认表空间文件，名为ibdata1的文件。 1Innodb_data_file_path = /db/ibdata1:2000M;/dr2/db/ibdata2:2000M:autoextend 将/db/ibdata1和/dr2/db/ibdata2两个文件组成表空间。 若两个文件处于不同的磁盘上，磁盘的负载可能被平均。可以提供数据库的整体性能 如果文件ibdata2用完了，则可以继续自动增长(autoextend) 设置innodb_data_file_path之后，所有基于InnoDB存储引擎的表数据都会记录到该共享表空间 设置innodb_file_per_table,将每个基于InnoDB的表都产生一个独立表空间，命令规则为：表名.ibd 注意：单独表空间文件仅存储该表的数据、索引和插入缓冲BITMAP等信息，其余信息还存放在默认表空间中 重做日志文件 默认情况下，在InnoDB存储引擎的数据目录下会有两个名为ib_logfile0和ib_logfile1的文件，称为重做日志文件(redo log file)。用于记录对于InnoDB存储引擎的事务日志。 每个InnoDB存储引擎至少有1个重做日志文件组(group)，每个文件组至少有2个重做日志文件，如默认的ib_logfile0和ib_logfile1。 为了得到更高的可靠性，可设置多个镜像日志组(mirrored log groups)，将不同的文件组放在不同的磁盘上 采用循环写入的方式运行：在三个重做日志文件的重做日志文件组中，InnoDB存储引擎先写重写日志文件ib_logfile0，当到达文件最后时，会切换直重做日志文件1，当文件1也满了切到文件2，最后回到文件0，重复循环 innodb_log_file_size：指定每个日志文件大小(&lt;512GB) innodb_log_files_in_group：日志文件组中重做日志文件的数量，默认为2 innodb_mirrored_log_groups：指定日志镜像文件组的数量，默认为1(若磁盘高可用，如磁盘阵列，可不开) Innodb_log_group_home_dir：指定日志文件组所在的目录 重做日志文件不能设置的太大，如果设置太大，恢复时需要太长时间，如果设置太小，可能导致一个事务的日志需要多次切换重做日志文件，会导致频繁的发生 async checkpoint 重做日志文件与二进制日志区别： 二进制日志记录所有与数据库相关的日志，而InnoDB存储的重做日志只记录该InnoDB本身的事务日志 二进制日志记录的都是关于一个事务的具体操作内容，即该日志为逻辑日志。而重做日志记录的是关于每个页(Page)的更改的物理情况 写入时间不同，二进制日志仅在事务提交前进行提交，即只写磁盘一次，不论事务多大。而在事务进行的过程中，会不断有重做日志条目(redo entry)被写入到重做日志文件中 重做日志由四部分组成： redo_log_type占用1字节，表示重做日志的类型 space表示空间的ID，但采用压缩的方式，占用空间可能小于4字节 page_no表示页的偏移量，铜梁采用压缩的方式 redo_log_body表示每个重做日志的数据部分，恢复时需要调用相应的函数进行解析 重做日志文件的操作不是直接写，而是先写入一个重做日志缓冲(redo log buffer)，再按照一定的条件顺序地写入日志文件 重做日志缓冲进行磁盘写入时，是按512字节也就是一个扇区的大小进行写入。因为扇区是写入的最小单位，因此可以保证写入必定是成功的 写入条件： 主线程中每秒会将重做日志缓冲写入磁盘的重做日志文件中，不论事务是否已经提交 另外一个由参数 innodb_flush_log_at_trx_commit控制，表示提交(commit)操作时，处理重做日志方式 0 代表等待主线程每秒的刷新 1 表示执行commit时将重做日志缓冲同步写到磁盘，伴有fsync的调用 2 表示重做日志异步写到磁盘，即写到文件系统的缓存中，不能保证在执行commit时肯定会写入重做日志文件 为了保证事务的持久性，必须将innodb_flush_log_at_trx_commit设置为1，即使宕机也能通过重做日志文件恢复 0和2都有可能恢复时部分事务丢失，不同在于，为2时如果数据库宕机而操作系统没有宕机，此时日志保存在文件系统缓存中，恢复时同样能保证数据不丢失 写入过程 只要redo log和binlog保证持久化到磁盘，就能确保MySql异常重启后，数据可以恢复 binlog的写入机制 事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中 系统为每个线程分配了一片binlog cache内存，但共用一份binlog文件 参数binlog_cache_size控制单个线程内binlog cache大小，一个事务的binlog是不能拆分的(不论事务多大，都要确保一次性写入)，如果超过这个大小就要暂存到磁盘 事务提交的时候，执行器把binlog cache 里完整的事务写入binlog中，并清空binlog cache 12ssize_t write(int fd, const void *buf, size_t count);int fsync(int fd); write 是把日志写入到文件系统的page cache，内存中，没有持久化到磁盘，所以速度比较快，函数返回并不代表已经写入磁盘 fsync 是将数据持久化到磁盘，占用磁盘的IOPS，通知内核将数据写到硬盘中 write 和 fsync 的时机，是由参数 sync_binlog 控制的 sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync(如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志) redo log的写入机制 redo log存在三种状态： 存在redo log buffer中，物理上是在MySql进程内存中 写到磁盘(write)，但是没有持久化(fsync)，物理上是在文件系统的page cache 持久化到磁盘，对应的是hard disk 为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数 0 表示每次事务提交时都只是把 redo log 留在 redo log buffer 中，等待主线程每秒刷新 1 表示每次事务提交时都将 redo log 直接持久化到磁盘(fsync)； 2 表示每次事务提交时都只是把 redo log 写到 page cache(文件系统缓存) 另外InnoDB有一个后台线程，每隔1s把redo log buffer中的日志，调用write写到文件系统的page cache,然后调用fsync持久化到磁盘 注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也可能已经持久化到磁盘的 实际上，除了后台线程没每秒一次的轮询操作，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中 redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache 并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘 一条update语句的执行过程： 执行器先找引擎取 ID=2 这一行 如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器 否则，需要先从磁盘读入内存，然后再返回 执行器拿到引擎给的行数据，更新行得到新的一行数据，再调用引擎接口写入这行新数据 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务 执行器生成这个操作的 binlog，并把 binlog 写入磁盘 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成 问题 为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的 一个事务的 binlog 必须连续写，整个事务完成后，再一起写到文件里。而 redo log 记录的是，中间有生成的日志可以写到 redo log buffer 中。其他事务提交的时候可以被一起写到磁盘中 为什么执行器更新数据之后不直接提交事务接口，而是和引擎有一个交互(这里是redo log)，其他引擎并没有redo log 更新的时候数据量太大，内存放不下怎么办 LRU 参考链接 《MySql技术内幕》 https://time.geekbang.org/column/article/76161","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门1-架构","slug":"MySql/MySql入门1-架构","date":"2021-07-21T23:20:02.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/22/MySql/MySql入门1-架构/","link":"","permalink":"http://xboom.github.io/2021/07/22/MySql/MySql%E5%85%A5%E9%97%A81-%E6%9E%B6%E6%9E%84/","excerpt":"","text":"MySQL架构 连接器 连接器：负责跟客户端建立连接、获取权限、维持和管理连接 连接命令：mysql -h$ip -P$port -u$user -p 连接默认端口是3306，可通过修改my.cnf配置文件指定端口 如果客户端太长时间没动静，连接器会自动断开。由参数wait_timeout控制的，默认值是8小时 show variables like ‘wait_timeout’; 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可在每次执行一个比较大的操作后，通过执 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态 可以使用TCP/IP套接字，命名管道和共享内存，UNIX域套接字进行连接 查询缓存 查询缓存：连接建立完成后，就可以执行 select 语句了，执行查询缓存。 之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中，key 是查询的语句，value 是查询的结果 逻辑存储结构 InnoDB存储引擎将所有数据都逻辑的存放在一个空间中，称为表空间，表空间又由以下结构组成 段(segment) 区(extent) 页(page) 行(row) 表空间(Tablespace)：默认情况InnoDB有一个共享表空间ibdata1, 即所有数据都存放在和这个表空间内。 如果用户开启 innodb_file_or_table，则每张表的数据(数据、索引、插入缓冲BitMap)可以单独放到一个表空间中，而回滚(undo)信息，插入缓冲索引页、系统事务信息、二次写缓冲(Double write buffer)还是存放在原来的共享表空间 InnoDB数据页由以下7个部分组成 File Header(文件头) Page Heade(页头) Infimun 和 Supermum Records User Records(用户记录) Free Space(空闲空间) Page Directory(页目录) File Trailer(文件结尾信息) 其中 File Header 38 字节, Page Header 56 字节, File Trailer 8 字节，用来标记页信息 而 User Records, Free Sapce, Page Directory 为实际的行存储空间，所以大小是动态的 Infimun 记录的是比任何主键都要小的值， Supermum Records 指可能比任何可能的值还要大的值，这两个值在页创建的时候建立，且永远不会被删除 关键特征 后台线程 后台存在多个线程处理不同的任务 master thread: 负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性 IO Thread: 大量使用AIO(Async IO)来处理写IO请求。而IO Thread的主要工作是负责这些IO请求的回调处理 Purge Thread: 事务提交后，其锁使用的 undolog 可能不再需要，一次通过PurgeThread来回收已经使用并分配的undo页 Page Cleaner Thread: 脏页的刷新操作 内存 缓冲池 InnoDB存储引擎是基于磁盘存储的。使用缓冲池技术降低CPU与磁盘之间的速率鸿沟 读取页的操作，首先将从磁盘读到的页存放在缓冲池中国暖，这个过程称为将页&quot;FIX&quot;在缓冲池中，下一次再读相同的页时，首先判断该页是否存在缓冲池中 修改页的操作，首先修改在缓冲池中的页，然后再已一定的频率刷新到磁盘上，页从缓冲池刷新回磁盘的操作并不是在每次页发生更新时触发，而是通过一种称为 CheckPoint的机制刷新回磁盘。 32位操作系统最大支持64G内存，而64位操作系统支持512G内存，通过 innodb_buffer_pool_size设置 LRU 缓冲池通过LRU(Latest Recent Used，最近最少使用)算法来进行管理的。即最频繁使用的页在LRU列表的前端，而最少使用的页在LRU列表的尾端 InnoDB对LRU进行了一些优化，还加入了midpoint位置，默认在LRU列表长度的5/8处。新读到的页并不是直接放到LRU列表的首部，而是放入到LRU列表的midpoint位置 为什么不直接放到LRU的首部？ 某些操作可能会使缓存的页被刷出，从而影响缓冲池的效率。如索引或数据的扫描操作 使用innodb_old_blocks_time 表示页读取到mid位置后需要等待多久才能加入到LRU列表的热端 Free LRU 用来管理已经读取的页，但当数据库刚启动时，LRU列表是空的，这时页都放在Free列表中 当需要从缓冲池中分页时，首先从Free列表中查找是否有可用的空闲页，若有则从Free列表中删除，放入到LRU列表中。否则淘汰LRU列表末尾的页，将该内存空间分配给新的页。 当页从LRU列表的old部分加入到new部分时，称此时发生的操作为page made young 当页因innodb_old_blocks_time而没有从old部分移动到new部分的操作称为page not made young FLUSH 在LRU列表中的页被修改后，称该页为脏页(dirty page),即缓冲池中的页和磁盘上的页数据不一致。数据库会通过CHECKPOINT机制将脏页刷新回磁盘，而Flush列表中的页即为脏页列表。 注意：脏页既存在于LRU列表中，也存在于Flush列表中。 LRU列表用来管理缓冲池中页的可用性 Flush列表用来管理将页刷新回磁盘，二者互不影响 重做日志缓冲 内存区域除了缓冲池，还有重做日志缓冲(redo log buffer)。InnoDB存储引擎首先将重做日志先放入到这个缓冲区，然后按一定频率将其刷新到重做日志文件 重做日志缓冲区不需要设置太大，因为一般没买哦就会将重做日志缓冲刷新到日志文件 实际上，8M的重做日志缓冲池足矣，因为下列三种情况会将重做日志缓冲内容刷新到外部磁盘的重做日志文件中 Master Thread 每一秒将重做日志缓冲刷新到重做日志文件 每个事务提交时会将重做日志缓冲刷新到重做日志文件 当重做日志缓冲池剩余空间小于1/2时，将重做日志缓冲刷新到重做日志文件 额外内存池 在InnoDB存储引擎中，对内存的管理是通过一种称为内存堆(heap)的方式进行的，在对一些数据结构本身的内存进行分配时，需要从额外的内存池中进行申请，当该区域的内存不够时，会从缓冲池中神行 例如：分配了缓冲池(innodb_buffer_pool)，但每个缓冲池中的帧缓冲(frame buffer)还有对应的缓冲控制对象(buffer control block)需要从额外内存池中申请 CheckPoint技术 当出现脏页，即缓冲池中页的版本比磁盘的新，需要将新版本的页从缓冲池刷新到磁盘。若每次一个页发生变化，就将新页的版本刷新到磁盘，那么这个开销将是非常大。同时如果刷新到磁盘的时候发生宕机，则数据不可恢复 所以事务数据库系统普遍采用Write Ahead Log策略，即当事务提交时，先写重做日志，再修改页。 如果重做日志可以无限地增大，同时缓冲池页足够大，能够缓冲数据库的数据，就不需要将缓冲池中的页的新版本刷新回磁盘。当发生宕机直接使用重做日志恢复整个系统到宕机发生的时刻，这就需要： 缓冲池可以缓存数据库中所有的数据 重做日志可以无限增大 因此提出CheckPoint技术解决： 缩短数据库的恢复时间：当数据库发生宕机，不需要重做所有日志，因为CheckPoint之前的页都已经刷新回磁盘，只需要恢复CheckPoint之后的重做日志 缓冲池不够用时，将脏页刷新到磁盘：根据LRU算法会溢出最近最少使用的页，若此页为脏页，也需要执行CheckPoint刷回磁盘 重做日志不可用时，刷新脏页：因为重做日志是循环使用，不可用是指部分重做日志已经不再需要了 InnoDB存储引擎通过LSN(Log Sequence Number)来标记版本(8字节)，每个页有LSN，重做日志也有LSN，CheckPoint也有LSN 在Innodb引擎内部，有两种CheckPoint: Sharp Checkpoint：发生在数据库关闭时将所有的脏页都刷新回磁盘(默认) Fuzzy Checkpoint: 若在数据库运行时使用 Sharp CheckPoint 将影响数据库使用，所以运行时使用Fuzzy Master Thread CheckPoint: Master Thread 每秒从缓冲池的脏页列表刷新一定比例回磁盘(异步) FLUSH_LRU_LIST checkPoint: 为了保证LRU列表中有大约100个空闲页，检查(阻塞)LRU中是否有足够的空闲列表，没有则从LRU列表尾端的页移除，如果这些页中有脏页，则进行CheckPoint。这个检查后来单独的Page Ceaner线程中进行 Async/Sync Flush CheckPoint: 重做日志不可用，则从脏页列表中选取。将已经写入到重做日志的LSN记为redo_lsn，将已经刷新回磁盘最新页的LSN记为checkpoint_lsn Dirty Page too much CheckPoint：当脏页太多强制执行CheckPoint 插入缓冲 虽然缓冲池中有Inert Buffer信息，但Insert Buffer和数据页一样，也是物理页的一个组成部分 如果对于非聚集索引叶子节点的插入不是顺序的(例如创建时间递增插入，则也是顺序的)，这时就需要离散地访问非聚集索引导致插入性能下降 对于非聚集索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中， 若存在，则直接插入 若不在，则先放入到一个Insert Buffer对象中，然后通过一定频率和情况进行Insert Buffer和辅助索引页子节点的merge操作 Insert Buffer 使用需要同时满足以下两个条件： 索引是辅助索引(secondary index) 索引不是唯一(unique)的 若MySql数据库发生了宕机，势必有大量Insert Buffer并没有合并到实际的非聚集索引中 辅助索引不能是唯一的，因为在插入缓冲时，数据库并不去查找索引页来判断插入记录的唯一性，如果去查找又会有离散读取的情况发生，从而导致Insert Buffer失去意义 Innsert Buffer 的数据结构是一颗B+树，全局有一颗 Insert Buffer B+ 树，负责对所有的表的辅助索引进行Insert Buffer。这课B+树存放在共享表空间中，默认也就是ibdata1中 由此也有叶节点和非叶节点组成，非叶节点存放的是查询的search key(键值) search key 一共占用 9个字节， space(4字节)表示待插入记录所在的表的表空间id,在Innodb存储引擎中，每个表有一个唯一的space id marker(1字节) 用来兼容老版本的Insert Buffer Offsert 表示页所在的偏移量，占用4字节 当一个辅助索引要插入到页(space, offset)时，如果这个页不在缓冲池中，那么InnoDB存储引擎首先根据上述规则构造一个search key,接下来查询Insert Buffer这颗B+树，然后将这条记录插入到Insert Buffer B+树的叶子节点中 两次写 如果Insert Buffer 带给InnoDB存储引擎是性能上的提升，那么doublewrite带来的就是数据页的可靠性 当数据库宕机，InnoDB可能正在写入某个页到表中，而这个页只写了一部分(比如一个页16KB的页，只写了4KB) 重做日志中记录的是对页的物理操作。如果这个页本身已经发生了损坏，在对其进行重做是没有意义的，也就是说，在应用(apply)重做日志前，用户需要一个页的副本，当写入失效发生时，先通过页的副本还原该页，再进行重做，这就是doublewrite doublewrite 由两部分组成: 一部分是内存中的doublewrite buffer 大小为2MB 另一部分是物理磁盘上共享表空间中连续的128个页，即2个区(extent)，大小同样为2MB 在对缓冲池脏页进行刷新时，并不直接写磁盘，而是通过memcpy函数先将脏页复制到内存中的doublewrite buffer，之后通过doublewrite buffer 再分两次，每次1MB顺序地写入共享表空间的物理磁盘是上，马上调用fsync函数，避免缓冲写带来的问题。因为doublewrite页是连续的，这个过程是循序写的。在完成doublewrite页的写入后，再将doublewrite buffer中的页写入各个表空间文件中，此时的写入则是离散的。 自适应哈希索引 哈希(Hash)是一种非常快的查找方法，时间复杂度为O(1)。而B+树在取决于树的高度(在生产环境中，B+树的高度一般为3~4层) InnoDB存储引擎会监控对表上各索引页的查询。通过缓冲池的B+树页构造而来，称为自适应哈希索引(Adaptive Hash Index, AHI)。自动根据访问的频率和模式来自动地为某些热点页建立哈希索引。 建立AHI的要求如下： 访问模式一样是指查询条件一样。如 where a=xxx 和 where a=xxx and b=xx 交替查询则不会构造AHI 以该模式访问了100次 页通过该模式访问了N次，其中N=页中记录 * 1/16 异步IO 当前的数据库系统都采用异步IO(Asynchronous IO, AIO)的方式来处理磁盘操作 Sync IO是每进行一次IO操作需要等待此次操作结束才能继续接下来的操作。如果用户发出的是一条索引扫描的查询，那么SQL查询语句需要扫描多个索引页，每扫描一个页并等待其完成后再进行下一次的扫描没必要。 AIO是用户发出了一个IO请求之后立即再发出另一个IO请求，当全部IO请求发送完毕后，等待所有IO操作的完成。最大的优势就是进行IO Merge操作。例如需要访问页(space, page_no)为(8,6)、(8,7)、(8,8) 每个页的大小为16KB，AIO会判断这三个页是连续的,因此AIO底层会发送一个IO请求，从(8,6)开始，读取48KB的页 刷新邻接页 当刷新一个脏页是，InnoDB存储引擎会检查该页所在区(extent)的所有页，如果是脏页，那么一起进行刷新。这样通过AIO可以将多个IO写入从左合并为一个IO操作。但存在这样的问题： 是不是可能将不怎么脏的页进行了写入，而该页之后又会很快变成脏页 固态硬盘有这较高的IOPS，是否还需要这个特性 可以通过 innodb_flush_neighbors 控制开启 存储引擎比较 特征 MyISAM BDB Memory InnoDB Archive NDB Sotrage Limits Y 64TB Y Transactions Y Y Locking granularity 表 页 表 行 行 行 MVCC/Snapshot Read Y Y Y Geospatial support Y B-Tree indexes Y Y Y Y Y Hash indexes Y Y Full text search index Y Data Caches Y Y Y Index Caches Y Y Y Compressed data Y Y Encrypted data Y Y Y Y Y Y Bulk Insert Speed High High High Low Very High High Storage cost Low Low N/A High VeryLlow Low Memory cost Low Low Medium High Low High Cluster database support Y Replication support Y Y Y Y Y Y Foreign Key support Y Backup/Point-in-time recovery Y Y Y Y Y Y Query cache support Y Y Y Y Y Y Update Statistics for Data Dictionary Y Y Y Y Y Y","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门3-索引","slug":"MySql/MySql入门3-索引","date":"2021-07-21T23:18:54.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/22/MySql/MySql入门3-索引/","link":"","permalink":"http://xboom.github.io/2021/07/22/MySql/MySql%E5%85%A5%E9%97%A83-%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引是一种为了提升查询效率的数据结构,B+树索引并不能找到一个给定键值的具体行。只能找到数据行所在的页，然后数据库通过把页读到内存中进行查找。每个数据页都通过一个双向链表来进行链接 数据页存放的是完整的每行的记录，而在非数据页的索引页中，存放的仅仅是键值及指向数据页的偏移量，而不是一个完整的行记录 索引模型 哈希表：O(1)的时间复杂度，适用于等值查询，不支持范围查询 有序数组：lgN的时间复杂度(二分查找)，支持等值查询和范围查询，但插入效率低，适用于静态存储引擎(数据不再变化) 搜索树：InnoDB使用B+树结构建立索引，父节点左子树所有节点的值小于父节点的值，右子树所有节点的值大于父节点的值。 InnoDB的为N叉树，差不多为1200 MySql默认一个节点的长度为16K，整数(bigint)字段索引长度为 8B，每个索引还跟着6B的指向其子树的指针；所以16K/14B ≈ 1170 在 InnoDB 中，每一张表其实就是多个 B+ 树，即一个主键索引树和多个非主键索引树。 执行查询的效率，使用主键索引 &gt; 使用非主键索引 &gt; 不使用索引 为什么主键比非主键快(主键索引和非主键索引)？ 主键索引的b+树的叶子节点存储的是具体的行数据，非叶子节点存储的是主键的值。叶子节点之间通过链表连接，也称为 聚簇索引 非主键索引的叶子节点存储的是主键的值，所以通过非主键索引查询数据时，先找到主键，再去主键索引上根据主键找到具体的行数据，也称 二级索引。这个过程叫做回表 如果不使用索引进行查询，则从主索引 B+ 树的叶子节点进行遍历 例如：存在下列这样一个表 12345mysql&gt; create table T( id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6) 则 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引(clustered index) 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引(secondary index) 索引维护 在B+树中，所有记录节点都是按键值的大小顺序存放在同一层的叶子节点上，由各叶子节点进行连接 B+树的插入操作 Leaf Page 满 Index Page 满 操作 No No 1. 直接将数据插入叶子节点 Yes No 1. 拆分叶子节点2. 将中间的额节点存放入 Index Page中3. 小于中间节点的记录放左边4. 大于或等于中间节点的记录方右边 Yes Yes 1. 拆分叶子节点2. 小于中间接待你的记录方左边3. 大于或等于中间节点的记录放右边4. 拆分Index Page5. 小于中间节点的记录放左边6. 大于中间节点的记录放右边7. 中间节点放入上一层 Index Page 旋转发生在Leaf Page已经满了，但是其左右兄弟节点并没有满的情况下。B+树并不会急于拆分页的操作，而是将记录移动所在页的兄弟节点上。通常情况下，左兄弟会被首先检查用来做旋转操作 B+树的删除操作 叶子节点小于填充因子 中间节点小于填充因子 操作 No No 1. 直接将记录从叶子节点删除，如果该节点还是Index Page的节点，用该节点的右节点代替 Yes No 1. 合并叶子节点和它的兄弟节点，同时更新Index Page Yes Yes 1. 合并叶子节点和它的兄弟节点2. 更新Index Page3. 合并Index Page 和它的兄弟节点 最好使用自增主键做为主索引，如上图所示：参见 B+树的插入与删除 如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。 如果新插入的 ID 值为 400，需要逻辑上挪动后面的数据，空出位置。而更糟的情况是： 如果 R5 所在数据页满了，则申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂，影响性能 原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程，影响空间利用率 所以在主键选择的时候： 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 在KV场景下只有一个索引，且必须是唯一索引，在直接使用业务字段做主键 索引要根据表中的每一行的记录值来创建，所以需要全表扫描 加字段或修改字段，也要修改每一行记录中的对应列的数据，所以也要全表扫描 查询过程 执行查询语句 select id from T where id = 5 id为普通索引，查询过程为： 先通过 B+ 树从树根开始，按层搜索到叶子节点及数据页 然后在数据页内部通过二分法来定位记录 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索 InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB 执行查询语句 select * from T where k between 3 and 5 查询过程为： 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束 执行查询语句 select ID from T where k between 3 and 5 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 在 k 索引树取下一个值 k=5，取得 ID=500； 在 k 索引树取下一个值 k=6，不满足条件，循环结束 ID 的值已经在 k 索引树上了，通过普通索引直接查询到结果不需要回表，索引 k 已经“覆盖了”查询需求，称为覆盖索引 B+树的最左前缀原则 1234567891011# 身份证信息CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`)) ENGINE=InnoDB where name like ‘张 %’。这时，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止 Where b &gt; 10，是无法使用 (a,b) 这个联合索引的，不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引 为了考虑空间，字段长(name,age)的只建立一次，短(age)的建立两次 以联合索引（name, age）为例，检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩&quot; InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次 【索引下推】Index Condition Pushdown，简称 ICP。 是Mysql 5.6版本引入的技术优化。旨在 在“仅能利用最左前缀索的场景”下（而不是能利用全部联合索引），对不在最左前缀索引中的其他联合索引字段加以利用——在遍历索引时，就用这些其他字段进行过滤(where条件里的匹配)。过滤会减少遍历索引查出的主键条数，从而减少回表次数，提示整体性能。 ------------------ 如果查询利用到了索引下推ICP技术，在Explain输出的Extra字段中会有“Using index condition”。即代表本次查询会利用到索引，且会利用到索引下推。 ------------------ 索引下推技术的实现——在遍历索引的那一步，由只传入可以利用到的字段值，改成了多传入下推字段值 更新过程 change buffer 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作 change buffer可以看成也是一个数据页，需要被持久化到 系统表空间（ibdata1），以及把这个change buffer页的改动记录在redo log里，事后刷进系统表空间（ibdata1） 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 change buffer使用场景 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了 普通索引和唯一索引的查询性能几乎一样, 但是写性能是普通索引快, 因为可以用到change buffer, 唯一索引会导致内存命中率下降 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。 设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50% 设置为0 表示关闭change buffer功能 语句更新过程 如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的 第一种情况当记录要更新的目标页在内存中 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束 第二种情况当记录要更新的目标页不在内存中 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。之前我就碰到过一件事儿，有个 DBA 的同学跟我反馈说，他负责的某个业务的库内存命中率突然从 99% 降低到了 75%，整个系统处于阻塞状态，更新语句全部堵住。而探究其原因后，我发现这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引 innodb 普通索引修改成唯一索引产生的 生产事故, 写多读少使用 changebuffer 可以加快执行速度(减少数据页磁盘 io); 但是,如果业务模型是 写后立马会做查询, 则会触发 changebuff 立即 merge 到磁盘, 这样 的场景磁盘 io 次数不会减少,反而会增加 changebuffer 的维护代价 change buffer 的使用场景 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价 change buffer 和 redo log redo log是物理日志，记录的是数据页的修改，但是如果数据页不在内存中怎么办呢？都没有去修改数据页，redo log中记什么？所以这时候change buffer登场了，如果修改的不是唯一索引，而是普通索引，不用再去磁盘随机IO了，直接将这个修改记录在change buffer中。也可以说，数据页在内存，那就修改数据页，写redo log，如果数据页不在内存，修改的也不是唯一索引，而是普通索引，那就写change buffer。并把这个change buffer写入到redo log，防止更新丢失。 现在让我们看看在表中插入一条数据 1mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2); 假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图 数据表空间：就是一个个的表数据文件，对应的磁盘文件就是“表名.ibd”； 系统表空间：用来放系统信息，如数据字典等，对应的磁盘文件是“ibdata1” 更新语句的操作如下： Page 1在内存中，直接更新内存 Page 2 不在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中 做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。同时 写入 change buffer 和 数据表空间都是后台空间，不影响更新的响应时间 然后执行查询语句，如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了 1select * from t where k in (k1, k2) 读 Page 1 的时候，直接从内存返回。 WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗 redo日志有分几十种类型的。redo做的事情，简单讲就是记录页的变化（WAL将页变化的乱序写转换成了顺序写）。页是分多种的，比如 B+树索引页（主键 / 二级索引）、undo页（数据的多版本MVCC）、以及现在的change buffer页等等，这些页被redo记录后，就可以不着急刷盘了。 change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成； redo日志记录的是change buffer页的变化。 change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？ 1.change buffer有一部分在内存有一部分在ibdata. 做purge操作,应该就会把change buffer里相应的数据持久化到ibdata 2.redo log里记录了数据页的修改以及change buffer新写入的信息 如果掉电,持久化的change buffer数据已经purge,不用恢复。主要分析没有持久化的数据 情况又分为以下几种: change buffer写入,redo log虽然做了fsync但未commit,binlog未fsync到磁盘,这部分数据丢失 change buffer写入,redo log写入但没有commit,binlog以及fsync到磁盘,先从binlog恢复redo log,再从redo log恢复change buffer change buffer写入,redo log和binlog都已经fsync.那么直接从redo log里恢复。 merge 的执行流程是这样的：从磁盘读入数据页到内存（老版本的数据页）；从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 索引选择 在 MySQL 中一张表可以支持多个索引。但写 SQL 语句的时候并没有主动指定使用哪个索引，MySql是怎么决定使用哪个索引的 1force Index(a) #强制使用指定的索引 使用下面三条SQL语句查看查询过程 123set long_query_time&#x3D;0;select * from t where a between 10000 and 20000; &#x2F;*Q1*&#x2F;select * from t force index(a) where a between 10000 and 20000;&#x2F;*Q2*&#x2F; 将慢查询的日志阀值设置为0，则接下来所有查询操作都记录到慢查询日志中 Q1 使用默认查询语句 Q2 强制使用索引a进行查询 优化器的逻辑 行数 临时表 是否排序 扫描行数是怎么判断的？ MySql在执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数 一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好 使用 show index 方法，看到一个索引的基数。 如果正确的使用索引 使用强制语句 force index(a) 引导 MySQL 使用我们期望的索引，如 order by b limit 1 改成 order by b,a limit 1 之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a 参考链接 《高性能myql》 《InnoDB技术内幕-索引与算法》 https://time.geekbang.org/column/article/70848","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"Go入门5-Map","slug":"Go/Go入门5-Map","date":"2021-07-11T04:39:49.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/11/Go/Go入门5-Map/","link":"","permalink":"http://xboom.github.io/2021/07/11/Go/Go%E5%85%A5%E9%97%A85-Map/","excerpt":"","text":"什么是Map Map是一种通过key来获取value的一个数据结构，其底层存储方式为数组。 一种特殊的数据结构：一种元素对（pair）的无序集合，pair 的一个元素是 key，对应的另一个元素是 value 一种引用类型，未初始化的 map 的值是 nil，可以通过如下方式声明： 12var map1 map[keytype]valuetypevar map1 map[string]int key 可以是任意可以用 == 或者 != 操作符比较的类型，比如 string、int、float。所以数组、切片和结构体不能作为 key，但是指针和接口类型可以。如果要用结构体作为 key 可以提供 Key() 和 Hash() 方法，这样可以通过结构体的域计算出唯一的数字或者字符串的 key。 value 可以是任意类型的；通过使用空接口类型,可以存储任意值，但是使用这种类型作为值时需要先做一次类型断言 Hash表 使用Hash需要解决哈希碰撞的问题，常见方法的就是开放寻址法和拉链法。 需要注意的是，这里提到的哈希碰撞不是多个键对应的哈希完全相等，可能是多个哈希的部分相等，例如：两个键对应哈希的前四个字节相同 开放寻址法 开放寻址法核心思想是依次探测和比较数组中的元素以判断目标键值对是否存在于哈希表中，使用开放寻址法来实现哈希表，那么实现哈希表底层的数据结构就是数组，不过因为数组的长度有限，想哈希表写入 (author, draven) 这个键值对时会从如下的索引开始遍历 1index := hash(\"author\") % array.len 向当前哈希表写入新的数据时，如果发生了冲突，就会将键值对写入到下一个索引不为空的位置 当 Key3 与已经存入哈希表中的两个键值对 Key1 和 Key2 发生冲突时，Key3 会被写入 Key2 后面的空闲位置。当我们再去读取 Key3 对应的值时就会先获取键的哈希并取模，这会先帮助我们找到 Key1，找到 Key1 后发现它与 Key 3 不相等，所以会继续查找后面的元素，直到内存为空或者找到目标元素 当需要查找某个键对应的值时，会从索引的位置开始线性探测数组，找到目标键值对或者空内存就意味着这一次查询操作的结束。 开放寻址法中对性能影响最大的是装载因子，它是数组中元素的数量与数组大小的比值。随着装载因子的增加，线性探测的平均用时就会逐渐增加，这会影响哈希表的读写性能。当装载率超过 70% 之后，哈希表的性能就会急剧下降，而一旦装载率达到 100%，整个哈希表就会完全失效，这时查找和插入任意元素的时间复杂度都是 𝑂(𝑛)O(n) 的，这时需要遍历数组中的全部元素，所以在实现哈希表时一定要关注装载因子的变化 拉链法 与开放地址法相比，平均查找的长度也比较短，各个用于存储节点的内存都是动态申请的，可以节省比较多的存储空间。 实现拉链法一般会使用数组加上链表，一些编程语言会在拉链法的哈希中引入红黑树以优化性能，拉链法会使用链表数组作为哈希底层的数据结构，可以将它看成可以扩展的二维数组 如上图所示，当我们需要将一个键值对 (Key6, Value6) 写入哈希表时，键值对中的键 Key6 都会先经过一个哈希函数，哈希函数返回的哈希会帮助我们选择一个桶，和开放地址法一样，选择桶的方式是直接对哈希返回的结果取模： 1index := hash(\"Key6\") % array.len 选择了 2 号桶后就可以遍历当前桶中的链表了，在遍历链表的过程中会遇到以下两种情况： 找到键相同的键值对 — 更新键对应的值； 没有找到键相同的键值对 — 在链表的末尾追加新的键值对； 如果要在哈希表中获取某个键对应的值，会经历如下的过程 Key11 展示了一个键在哈希表中不存在的例子，当哈希表发现它命中 4 号桶时，它会依次遍历桶中的链表，然而遍历到链表的末尾也没有找到期望的键，所以哈希表中没有该键对应的值。 在一个性能比较好的哈希表中，每一个桶中都应该有 0~1 个元素，有时会有 2~3 个，很少会超过这个数量。计算哈希、定位桶和遍历链表三个过程是哈希表读写操作的主要开销，使用拉链法实现的哈希也有装载因子这一概念： 装载因子:=元素数量÷桶数量装载因子:=元素数量÷桶数量 与开放地址法一样，拉链法的装载因子越大，哈希的读写性能就越差。在一般情况下使用拉链法的哈希表装载因子都不会超过 1，当哈希表的装载因子较大时会触发哈希的扩容，创建更多的桶来存储哈希中的元素，保证性能不会出现严重的下降。如果有 1000 个桶的哈希表存储了 10000 个键值对，它的性能是保存 1000 个键值对的 1/10，但是仍然比在链表中直接读写好 1000 倍 Map结构 1234567891011121314151617181920// A header for a Go map.type hmap struct &#123; count int //表示当前hash表元素个数 flags uint8 //记录当前hash表状态，map是非线程安全的 B uint8 //表示当前哈希表持有的 buckets 数量，桶的数量都是2的倍数，该字段存储对数，也就是len(buckets) == 2^B noverflow uint16 // overflow 的 bucket 近似数 hash0 uint32 // 哈希种子，能为哈希函数的结果引入随机性， 值在创建哈希表时确定，并在调用哈希函数时作为参数传入 buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // 哈希在扩容时用于保存之前 buckets 的字段，它的大小是当前 buckets 的一半 nevacuate uintptr // 指示扩容进度，小于此地址的 buckets 迁移完成 extra *mapextra // optional fields&#125;type mapextra struct &#123; overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap&#125; 这里B是map的bucket数组长度的对数，每个bucket里面存储了kv对。buckets是一个指针，指向实际存储的bucket数组的首地址。 bucket的结构体如下: 123456789// A bucket for a Go map.type bmap struct &#123; //top hash通常包含该bucket中每个键的hash值的高八位 //如果tophash[0]小于mintophash，则tophash[0]为桶疏散状态 //bucketCnt 的初始值是8 tophash [bucketCnt]uint8 //注意：将所有键打包在一起，然后将所有值打包在一起，使得代码比交替key/elem/key/elem/...更复杂。 //但它允许我们消除可能需要的填充，例如map[int64]int8./后面跟一个溢出指针&#125;&#125; 上面这个数据结构并不是 golang runtime 时的结构，在编译时候编译器会给它动态创建一个新的结构，如下： 1234567type bmap struct &#123; topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr&#125; bmap 就是我们常说的“bucket”结构，每个 bucket 里面最多存储 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置） 当map的key和value 每个 bmap 都能存储 8 个键值对，当哈希表中存储的数据过多，单个桶无法装满时就会使用 extra.nextOverflow 中桶存储溢出数据 当作为函数传参时候，传递的实例其实是指针 map是一个hash表，数据被存放在一组buckets(滚筒)中，每个buckets包含8个键值对，低位的buckets用来给数据选择存储的buckets，每个存储buckets包含每个哈希的一些高阶位，以区分单个存储buckets中的条目 如果一个buckets中超过8个，那么将使用其他buckets存储 当哈希表增长时，分配一组新buckets是原来的两倍大，并且会将旧数据复制到新buckets中存储 映射迭代器遍历存储桶数组，并按行走顺序返回键（存储桶编号，然后是溢出链顺序，然后是存储桶索引）。 为了维持迭代语义，我们绝不会在键的存储桶中移动键（如果这样做，键可能会返回0或2次）。 在增加表时，迭代器将继续在旧表中进行迭代，并且必须检查新表是否将要迭代的存储桶（“撤离”）到新表中 常用常量 1234567891011121314151617181920212223242526272829303132333435363738394041424344const ( // 一个桶能装的最大键值对 1&lt;&lt;3 bucketCntBits = 3 bucketCnt = 1 &lt;&lt; bucketCntBits //负载因子计算 uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen) loadFactorNum = 13 loadFactorDen = 2 /* if t.key.size &gt; maxKeySize &amp;&amp; (!t.indirectkey() || t.keysize != uint8(sys.PtrSize)) || t.key.size &lt;= maxKeySize &amp;&amp; (t.indirectkey() || t.keysize != uint8(t.key.size)) &#123; throw(\"key size wrong\") &#125; if t.elem.size &gt; maxElemSize &amp;&amp; (!t.indirectelem() || t.elemsize != uint8(sys.PtrSize)) || t.elem.size &lt;= maxElemSize &amp;&amp; (t.indirectelem() || t.elemsize != uint8(t.elem.size)) &#123; throw(\"elem size wrong\") &#125; */ maxKeySize = 128 maxElemSize = 128 //bmap 偏移 int64 topHash dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64 &#125;&#123;&#125;.v) emptyRest = 0 // 这bmap中这一格为空，在更高的索引中没有更多的非空细胞 emptyOne = 1 // 这bmap中这一格为空 evacuatedX = 2 // 元素可得，但已经迁移到新桶的前半部分 evacuatedY = 3 // 元素可得，但已经迁移到新桶的后半部分 evacuatedEmpty = 4 // 格子为空，bucket已经迁移. minTopHash = 5 // ophash 的最小正常值 // flags iterator = 1 // 可能有迭代器在使用buckets oldIterator = 2 // 可能有迭代器在使用oldbuckets hashWriting = 4 // 有协程正在写 sameSizeGrow = 8 // 等量扩容 // sentinel bucket ID for iterator checks noCheck = 1&lt;&lt;(8*sys.PtrSize) - 1) 常用函数 1234//查找下一个overflow 的bmapfunc (b *bmap) overflow(t *maptype) *bmap &#123; return *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))&#125; 创建Map 通过调用make来创建map，底层为 makemap 函数 1234567891011121314151617181920212223242526272829303132333435func makemap(t *maptype, hint int, h *hmap) *hmap &#123; mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem &gt; maxAlloc &#123; hint = 0 &#125; // 初始化hmap if h == nil &#123; h = new(hmap) &#125; h.hash0 = fastrand() // 找到一个 B使 hint &gt; B &amp;&amp; hint &gt; (2^B) * 6.5 为false 否则B就增大 // 当hint为1 则 B == 0 表示存1个数，只需要用一个桶 // 当hint为6 则 B == 0 表示存6个数，也只需要一个桶 // 当hint为7 则 B == 1 表示存7个树，就需要用两个桶 B := uint8(0) for overLoadFactor(hint, B) &#123; B++ &#125; h.B = B // allocate initial hash table // if B == 0, the buckets field is allocated lazily later (in mapassign) // If hint is large zeroing this memory could take a while. if h.B != 0 &#123; var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil &#123; h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow &#125; &#125; return h&#125; 注意： 返回的是*hmap而 func makeslice(et *_type, len, cap int) slice返回的是结构体 虽然Go 语言中的函数传参都是值传递，在函数内部，map参数会影响外部 而slice部分会，因为slice内部的数组是指针类型，对数组的修改会影响外部，但是长度和容量不会 判断装载因子，13 * 1&lt;&lt;B / 2 即 桶数目的6.5倍 1234//判断装载因子 func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125; Key定位 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机忽略），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。如果 B = 5，则桶数量即 buckets 数组的长度是 2^5 = 32 Key的Hash值计算为： 110010111 | 000011110110110010001111001010100010010110010101010 │ 00110 低5 位 00110，6号桶 高 8 位10010111，十进制为151 在6好bucket中找到tophash值(HOBhash)为151的key，对应2号槽位， 这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入 如果冲突了怎么办 是怎么存入的，什么顺序 查找怎么办 如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket 迁移过程中怎么查找 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; //如果map为空，或者map中数量为0 则返回0值 if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]) &#125; //如果写冲突，则直接报错 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; // 计算哈希值，加入hash0引入随机值 hash := t.hasher(key, uintptr(h.hash0)) // 比如 B=5，那 m 就是31，二进制是全 1 // m = 1&lt;&lt;B - 1 m := bucketMask(h.B) // b 就是 bucket 的地址，hash&amp;m*uintptr(t.bucketsize) 第几个bucket b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) //如果oldbuckets不为空，说明发生了扩容 if c := h.oldbuckets; c != nil &#123; // 如果不是同 size 扩容（看后面扩容的内容） // 对应条件 1 的解决方案 if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. // 新 bucket 数量是老的 2 倍 m &gt;&gt;= 1 &#125; // 求出 key 在老的 map 中的 bucket 位置 oldb := (*bmap)(add(c, (hash&amp;m)*uintptr(t.bucketsize))) // 如果 oldb 没有搬迁到新的 bucket // 那就在老的 bucket 中寻找 if !evacuated(oldb) &#123; b = oldb &#125; &#125; // 计算出高 8 位的 hash // 相当于右移 56 位，只取高8位 top := tophash(hash)bucketloop: for ; b != nil; b = b.overflow(t) &#123; //如果b中没有，则查看b的overflow for i := uintptr(0); i &lt; bucketCnt; i++ &#123; //遍历topHash if b.tophash[i] != top &#123; //如果topHash不等于top if b.tophash[i] == emptyRest &#123; //如果位置为空，后面都没有非空元素 break bucketloop //说明不是顺序填充，相等的topHash值会存到下一个bmap &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) //key 是指针 if t.indirectkey() &#123; //解引用 k = *((*unsafe.Pointer)(k)) &#125; //如果key相等(可能存在topHash相等，key不想等的情况) if t.key.equal(key, k) &#123; //定位value的位置 //b bmap 地址 // dataOffset(数据对齐) // bucketCnt*t.keysize 8个key的大小(固定8个) //i*t.elemsize 第i个元素 e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) // val解引用 if t.indirectelem() &#123; e = *((*unsafe.Pointer)(e)) &#125; return e &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0])&#125; 这里的dataOffset 是 key 相对于 bmap 起始地址的偏移，包含了topHash数组 int64 1234dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64&#125;&#123;&#125;.v) 因此 bucket 里 key 的起始地址就是 unsafe.Pointer(b)+dataOffset minTopHash minTopHash当一个 cell 的 tophash 值小于 minTopHash 时，标志这个 cell 的状态。因为这个状态值是放在 tophash 数组里，为了和正常的哈希值区分开，会给 key 计算出来的哈希值一个增量：minTopHash。这样就能区分正常的 tophash 值和表示状态的哈希值 源码里判断是否搬迁完毕使用函数evacuated，当第一个元素状态为迁移标识符中的三个 1234func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; emptyOne &amp;&amp; h &lt; minTopHash&#125; Map获取 Go 语言中读取 map 有两种语法：带 comma 和 不带 comma，两种分别对应不同的函数 123// src/runtime/hashmap.gofunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointerfunc mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) &#123; //这里不会panic if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]), false &#125; //并发读写错误 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + (hash&amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil &#123; if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. m &gt;&gt;= 1 &#125; oldb := (*bmap)(unsafe.Pointer(uintptr(c) + (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) &#123; b = oldb &#125; &#125; top := tophash(hash)bucketloop: for ; b != nil; b = b.overflow(t) &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; if t.key.equal(key, k) &#123; e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() &#123; e = *((*unsafe.Pointer)(e)) &#125; return e, true &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0]), false&#125; Map扩容 使用哈希表的目的就是要快速查找到目标 key，然而，随着向 map 中添加的 key 越来越多，key 发生碰撞的概率也越来越大。bucket 中的 8 个 cell 会被逐渐塞满，查找、插入、删除 key 的效率也会越来越低。 Go有一个衡量标准 装载因子 来描述存储情况 1loadFactor := count / (2^B) 触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容： 装载因子超过阈值，源码里定义的阈值是 6.5。 overflow 的 bucket 数量过多： 当 B &lt; 15，即 bucket 总数 2^B &lt; 2^15 时，如果 overflow 的 bucket 数量超过 2^B； 当 B &gt;= 15，即 bucket 总数 2^B &gt;= 2^15，如果 overflow 的 bucket 数量超过 2^15 123456789101112131415161718if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again&#125;// 装载因子超过 6.5func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125;// overflow buckets 太多func tooManyOverflowBuckets(noverflow uint16, B uint8) bool &#123; if B &gt; 15 &#123; B = 15 &#125; // The compiler doesn't see here that B &lt; 16; mask B to generate shorter shift code. return noverflow &gt;= uint16(1)&lt;&lt;(B&amp;15)&#125; 第 1 点：每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时，表明很多 bucket 都快要装满了。在这个时候进行扩容是有必要的。 扩容策略：将 B 加 1，bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。而且，新 bucket 只是最大数量变为原来最大数量（2^B）的 2 倍（2^B * 2） B 还没有变，新bucket怎么记录大小呢？ 第 2 点：是对第 1 点的补充。当 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket） 扩容策略：开辟一个新 bucket 空间，将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。原来在 overflow bucket 中的 key 可以移动到 bucket 中来。 一个极端的情况：如果插入 map 的 key 哈希都一样，就会落到同一个 bucket 里，超过 8 个就会产生 overflow bucket，结果也会造成 overflow bucket 数过多。移动元素其实解决不了问题，因为这时整个哈希表已经退化成了一个链表，操作效率变成了 O(n)。 Go map 的扩容采取了一种称为“渐进式”地方式，并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。 准备扩容 hashGrow() 函数表示开始扩容前的处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344func hashGrow(t *maptype, h *hmap) &#123; bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) &#123; //判断是否超过负载因子 bigger = 0 h.flags |= sameSizeGrow //没有说明是第二种情况 &#125; oldbuckets := h.buckets //先将桶指向旧的指针 //h.B+bigger 说明是桶是原来的两倍 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) //&amp;^按位置为0 flags := h.flags &amp;^ (iterator | oldIterator) if h.flags&amp;iterator != 0 &#123; flags |= oldIterator &#125; // 提交 grow 的动作 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets //搬迁进度为0 h.nevacuate = 0 //overflow buckets数量为0 h.noverflow = 0 //搬迁extra到老的extra if h.extra != nil &amp;&amp; h.extra.overflow != nil &#123; // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil &#123; throw(\"oldoverflow is not nil\") &#125; h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil &#125; if nextOverflow != nil &#123; if h.extra == nil &#123; h.extra = new(mapextra) &#125; h.extra.nextOverflow = nextOverflow &#125; // the actual copying of the hash table data is done incrementally // by growWork() and evacuate().&#125; 扩容工作 123456789func growWork(t *maptype, h *hmap, bucket uintptr) &#123; // 确认搬迁老的 bucket 对应正在使用的 bucket evacuate(t, h, bucket&amp;h.oldbucketmask()) // 再搬迁一个 bucket，以加快搬迁进程 if h.growing() &#123; evacuate(t, h, h.nevacuate) &#125;&#125; h.growing 用来判断是否在搬迁 1234func (h *hmap) growing() bool &#123; return h.oldbuckets != nil&#125;//当oldbuckets非空，表示正在扩容 bucket&amp;h.oldbucketmask()是为了确认搬迁的 bucket 是我们正在使用的 bucket。oldbucketmask() 函数返回扩容前的 map 的 bucketmask。 bucketmask，作用就是将 key 计算出来的哈希值与 bucketmask 相与，得到的结果就是 key 应该落入的桶。比如 B = 5，则返回11111 搬迁核心evacuate 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156type evacDst struct &#123; b *bmap // 当前搬迁的桶 i int // b中的k/v索引 k unsafe.Pointer // 指针指向当前k存储位置 e unsafe.Pointer // 指针指向当前v存储位置&#125;func evacuate(t *maptype, h *hmap, oldbucket uintptr) &#123; //定位老的buckets地址(桶的开始位置+桶的数目*桶的大小) b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) //返回旧的桶数，如果B为5，那么结果为32 newbit := h.noldbuckets() //如果没有搬迁过 if !evacuated(b) &#123; //xy 包含高低两个搬迁目的地 var xy [2]evacDst //低搬迁目的地 x := &amp;xy[0] //记录新的buckets地址(桶开始的位置+桶数目*大小) ==&gt; 地址是连续的? TODO x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) //k的位置为bmap开始的位置+topHash x.k = add(unsafe.Pointer(x.b), dataOffset) //v的位置为k结束的位置 x.e = add(x.k, bucketCnt*uintptr(t.keysize)) //如果不是等量扩容 if !h.sameSizeGrow() &#123; //使用y来进行搬迁 y := &amp;xy[1] //y.b表示获取新桶的地址 y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) //y.b表示key开始的地址 y.k = add(unsafe.Pointer(y.b), dataOffset) //y.e表示v开始的地址 y.e = add(y.k, bucketCnt*uintptr(t.keysize)) &#125; // 遍历所有的 bucket，包括 overflow buckets // b 是老的 bucket 地址 for ; b != nil; b = b.overflow(t) &#123; k := add(unsafe.Pointer(b), dataOffset) e := add(k, bucketCnt*uintptr(t.keysize)) // 遍历 bucket 中的所有 cell for i := 0; i &lt; bucketCnt; i, k, e = i+1, add(k, uintptr(t.keysize)), add(e, uintptr(t.elemsize)) &#123; // 当前 cell 的 top hash 值 top := b.tophash[i] // 如果 cell 为空，即没有 key if isEmpty(top) &#123; //x &lt;= emptyOne b.tophash[i] = evacuatedEmpty // 那就标志它被\"搬迁\"过 continue // 继续下个 cell &#125; // 正常不会出现这种情况 // 未被搬迁的 cell 只可能是 emptyRest 或者 emptyOne // 正常的 top hash（大于 minTopHash） if top &lt; minTopHash &#123; throw(\"bad map state\") &#125; k2 := k // 如果 key 是指针，则解引用 if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; var useY uint8 //如果不是等量扩容 if !h.sameSizeGrow() &#123; //计算hash值 hash := t.hasher(k2, uintptr(h.hash0)) //如果正在遍历map 且 出现过相同key 的两者相同(float变得NAN) if h.flags&amp;iterator != 0 &amp;&amp; !t.reflexivekey() &amp;&amp; !t.key.equal(k2, k2) &#123; useY = top &amp; 1 top = tophash(hash) &#125; else &#123; if hash&amp;newbit != 0 &#123; useY = 1 &#125; &#125; &#125; if evacuatedX+1 != evacuatedY || evacuatedX^1 != evacuatedY &#123; throw(\"bad evacuatedN\") &#125; b.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY dst := &amp;xy[useY] // evacuation destination if dst.i == bucketCnt &#123; dst.b = h.newoverflow(t, dst.b) dst.i = 0 dst.k = add(unsafe.Pointer(dst.b), dataOffset) dst.e = add(dst.k, bucketCnt*uintptr(t.keysize)) &#125; dst.b.tophash[dst.i&amp;(bucketCnt-1)] = top // mask dst.i as an optimization, to avoid a bounds check if t.indirectkey() &#123; *(*unsafe.Pointer)(dst.k) = k2 // copy pointer &#125; else &#123; typedmemmove(t.key, dst.k, k) // copy elem &#125; if t.indirectelem() &#123; *(*unsafe.Pointer)(dst.e) = *(*unsafe.Pointer)(e) &#125; else &#123; typedmemmove(t.elem, dst.e, e) &#125; dst.i++ // These updates might push these pointers past the end of the // key or elem arrays. That's ok, as we have the overflow pointer // at the end of the bucket to protect against pointing past the // end of the bucket. dst.k = add(dst.k, uintptr(t.keysize)) dst.e = add(dst.e, uintptr(t.elemsize)) &#125; &#125; // 如果没有协程在使用老的 buckets，就把老 buckets 清除掉，帮助gc if h.flags&amp;oldIterator == 0 &amp;&amp; t.bucket.ptrdata != 0 &#123; b := add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)) // Preserve b.tophash because the evacuation // state is maintained there. ptr := add(b, dataOffset) n := uintptr(t.bucketsize) - dataOffset memclrHasPointers(ptr, n) &#125; &#125; // 更新搬迁进度 // 如果此次搬迁的 bucket 等于当前进度 if oldbucket == h.nevacuate &#123; advanceEvacuationMark(h, t, newbit) &#125;&#125;func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) &#123; //进度+1 h.nevacuate++ // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. //尝试往后看 1024 个 bucket stop := h.nevacuate + 1024 if stop &gt; newbit &#123; stop = newbit &#125; // 寻找没有搬迁的 bucket for h.nevacuate != stop &amp;&amp; bucketEvacuated(t, h, h.nevacuate) &#123; h.nevacuate++ &#125; // 现在 h.nevacuate 之前的 bucket 都被搬迁完毕 if h.nevacuate == newbit &#123; // newbit == # of oldbuckets h.oldbuckets = nil // 清除老的 overflow bucket // 回忆一下：[0] 表示当前 overflow bucket // [1] 表示 old overflow bucket if h.extra != nil &#123; h.extra.oldoverflow = nil &#125; // 清除正在扩容的标志位 h.flags &amp;^= sameSizeGrow &#125;&#125; 搬迁的目的就是将老的 buckets 搬迁到新的 buckets。 应对条件 1，新的 buckets 数量是之前的一倍，所以要重新计算 key 的哈希，才能决定它到底落在哪个 bucket。例如，原来 B = 5，计算出 key 的哈希后，只用看它的低 5 位，就能决定它落在哪个 bucket。扩容后，B 变成了 6，因此需要多看一位，它的低 6 位决定 key 落在哪个 bucket。这称为 rehash 应对条件 2，新的 buckets 数量和之前相等。因此可以按序号来搬，比如原来在 0 号 bucktes，到新的地方后，仍然放在 0 号 buckets。 因此，某个 key 在搬迁前后 bucket 序号可能和原来相等，也可能是相比原来加上 2^B（原来的 B 值），取决于 hash 值 第 6 bit 位是 0 还是 1。 那么为什么遍历 map 是无序的？ 第一种情况插入时遍历：map 在扩容后，会发生 key 的搬迁，原来落在同一个 bucket 中的 key，可能就落在了不同的槽中 第二种情况不插入遍历：在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了 123456789101112//测试 map中存入6个值，则按照规则，只会有一个桶，通过十次遍历d := make(map[int]int)for i := 0; i &lt; 6; i++ &#123; d[i] = i&#125;for i := 0; i &lt; 10; i++ &#123; fmt.Printf(\"count:%d \\n\", i) for _, v := range d &#123; fmt.Print(v) &#125;&#125;//结果发现，每次开始的位置没有变化，但是整体的前后顺序不变 “迭代 map 的结果是无序的”这个特性是从 go 1.0 开始加入的 再明确一个问题：如果扩容后，B 增加了 1，意味着 buckets 总数是原来的 2 倍，原来 1 号的桶“裂变”到两个桶。 例如，原始 B = 2，1号 bucket 中有 2 个 key 的哈希值低 3 位分别为：010，110。由于原来 B = 2，所以低 2 位 10 决定它们落在 2 号桶，现在 B 变成 3，所以 010、110 分别落入 2、6 号桶 evacuate 函数每次只完成一个 bucket 的搬迁工作会有 2 层循环，外层遍历 bucket 和 overflow bucket，内层遍历 bucket 的所有 cell 源码里提到 X, Y part，桶的数量是原来的 2 倍，前一半桶被称为 X part，后一半桶被称为 Y part。 一个 bucket 中的 key 可能会分裂落到 2 个桶，一个位于 X part，一个位于 Y part。所以在搬迁一个 cell 之前，需要知道这个 cell 中的 key 是落到哪个 Part。很简单，重新计算 cell 中 key 的 hash，并向前“多看”一位，决定落入哪个 Part,如果 tophash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part 有一个特殊情况是：有一种 key，每次对它计算 hash，得到的结果都不一样。这个 key 就是 math.NaN() 的结果，它的含义是 not a number，类型是 float64。当它作为 map 的 key，在搬迁的时候，会遇到一个问题：再次计算它的哈希值和它当初插入 map 时的计算出来的哈希值不一样 你可能想到了，这样带来的一个后果是，这个 key 是永远不会被 Get 操作获取的！当我使用 m[math.NaN()] 语句的时候，是查不出来结果的。这个 key 只有在遍历整个 map 的时候，才有机会现身。所以，可以向一个 map 插入任意数量的 math.NaN() 作为 key 12345678910111213//通过tophash值与新算出来的哈希值进行运算得到if top&amp;1 != 0 &#123; // top hash 最低位为 1 // 新算出来的 hash 值的 B 位置 1 hash |= newbit&#125; else &#123; // 新算出来的 hash 值的 B 位置 0 hash &amp;^= newbit&#125;// hash 值的 B 位为 0，则搬迁到 x part// 当 B = 5时，newbit = 32，二进制低 6 位为 10 0000useX = hash&amp;newbit == 0 确定了要搬迁到的目标 bucket 后，搬迁操作就比较好进行了。将源 key/value 值 copy 到目的地相应的位置。 设置 key 在原始 buckets 的 tophash 为 evacuatedX 或是 evacuatedY，表示已经搬迁到了新 map 的 x part 或是 y part。新 map 的 tophash 则正常取 key 哈希值的高 8 位 下面通过图来宏观地看一下扩容前后的变化 扩容前，B = 2，共有 4 个 buckets，lowbits 表示 hash 值的低位。假设我们不关注其他 buckets 情况，专注在 2 号 bucket。并且假设 overflow 太多，触发了等量扩容（对应于前面的条件 2） 扩容完成后，overflow bucket 消失了，key 都集中到了一个 bucket，更为紧凑了，提高了查找的效率。 假设触发了 2 倍的扩容，那么扩容完成后，老 buckets 中的 key 分裂到了 2 个 新的 bucket。一个在 x part，一个在 y 的 part。依据是 hash 的 lowbits。新 map 中 0-3 称为 x part，4-7 称为 y part 注意，上面的两张图忽略了其他 buckets 的搬迁情况，表示所有的 bucket 都搬迁完毕后的情形。实际上，我们知道，搬迁是一个“渐进”的过程，并不会一下子就全部搬迁完毕。所以在搬迁过程中，oldbuckets 指针还会指向原来老的 []bmap，并且已经搬迁完毕的 key 的 tophash 值会是一个状态值，表示 key 的搬迁去向。 Map遍历 正常情况下，遍历所有的 bucket 以及它后面挂的 overflow bucket，然后挨个遍历 bucket 中的所有 cell。每个 bucket 中包含 8 个 cell，从有 key 的 cell 中取出 key 和 value，这个过程就完成了。 而扩容过程不是一个原子的操作，它每次最多只搬运 2 个 bucket，所以如果触发了扩容操作，那么在很长时间里，map 的状态都是处于一个中间态：有些 bucket 已经搬迁到新家，而有些 bucket 还待在老地方，它又是怎样遍历的呢？ 先是调用 mapiterinit 函数初始化迭代器，然后循环调用 mapiternext 函数进行 map 迭代，其中迭代器的结构体定义是： 1234567891011121314151617181920212223242526272829type hiter struct &#123; // key 指针 key unsafe.Pointer // value 指针 value unsafe.Pointer // map 类型，包含如 key size 大小等 t *maptype // map header h *hmap // 初始化时指向的 bucket buckets unsafe.Pointer // 当前遍历到的 bmap bptr *bmap overflow [2]*[]*bmap // 起始遍历的 bucet 编号 startBucket uintptr // 遍历开始时 cell 的编号（每个 bucket 中有 8 个 cell） offset uint8 // 是否从头遍历了 wrapped bool // B 的大小 B uint8 // 指示当前 cell 序号 i uint8 // 指向当前的 bucket bucket uintptr // 因为扩容，需要检查的 bucket checkBucket uintptr&#125; 之前说到每次遍历都是无序的 12345678910// 生成随机数 rr := uintptr(fastrand())if h.B &gt; 31-bucketCntBits &#123; r += uintptr(fastrand()) &lt;&lt; 31&#125;// 从哪个 bucket 开始遍历it.startBucket = r &amp; (uintptr(1)&lt;&lt;h.B - 1)// 从 bucket 的哪个 cell 开始遍历it.offset = uint8(r &gt;&gt; h.B &amp; (bucketCnt - 1)) 例如，B = 2，那 uintptr(1)&lt;&lt;h.B - 1 结果就是 3，低 8 位为 0000 0011，将 r 与之相与，就可以得到一个 0~3 的 bucket 序号；bucketCnt - 1 等于 7，低 8 位为 0000 0111，将 r 右移 2 位后，与 7 相与，就可以得到一个 0~7 号的 cell。 于是，在 mapiternext 函数中就会从 it.startBucket 的 it.offset 号的 cell 开始遍历，取出其中的 key 和 value，直到又回到起点 bucket，完成遍历过程。 假设我们有下图所示的一个 map，起始时 B = 1，有两个 bucket，后来触发了扩容（这里不要深究扩容条件，只是一个设定），B 变成 2。并且， 1 号 bucket 中的内容搬迁到了新的 bucket，1 号裂变成 1 号和 3 号；0 号 bucket 暂未搬迁。老的 bucket 挂在在 *oldbuckets 指针上面，新的 bucket 则挂在 *buckets 指针上面 这时，我们对此 map 进行遍历。假设经过初始化后，startBucket = 3，offset = 2。于是，遍历的起点将是 3 号 bucket 的 2 号 cell，下面这张图就是开始遍历时的状态 标红的表示起始位置，bucket 遍历顺序为：3 -&gt; 0 -&gt; 1 -&gt; 2。 因为 3 号 bucket 对应老的 1 号 bucket，因此先检查老 1 号 bucket 是否已经被搬迁过。判断方法就是： 1234func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; empty &amp;&amp; h &lt; minTopHash&#125; 如果 b.tophash[0] 的值在标志值范围内，即在 (0,4) 区间里，说明已经被搬迁过了 在本例中，老 1 号 bucket 已经被搬迁过了。所以它的 tophash[0] 值在 (0,4) 范围内，因此只用遍历新的 3 号 bucket。 依次遍历 3 号 bucket 的 cell，这时候会找到第一个非空的 key：元素 e。到这里，mapiternext 函数返回，这时我们的遍历结果仅有一个元素： 由于返回的 key 不为空，所以会继续调用 mapiternext 函数。 继续从上次遍历到的地方往后遍历，从新 3 号 overflow bucket 中找到了元素 f 和 元素 g。 遍历结果集也因此壮大： 新 3 号 bucket 遍历完之后，回到了新 0 号 bucket。0 号 bucket 对应老的 0 号 bucket，经检查，老 0 号 bucket 并未搬迁，因此对新 0 号 bucket 的遍历就改为遍历老 0 号 bucket。那是不是把老 0 号 bucket 中的所有 key 都取出来呢？ 并没有这么简单，回忆一下，老 0 号 bucket 在搬迁后将裂变成 2 个 bucket：新 0 号、新 2 号。而我们此时正在遍历的只是新 0 号 bucket（注意，遍历都是遍历的 *bucket 指针，也就是所谓的新 buckets）。所以，我们只会取出老 0 号 bucket 中那些在裂变之后，分配到新 0 号 bucket 中的那些 key。 因此，lowbits == 00 的将进入遍历结果集： 和之前的流程一样，继续遍历新 1 号 bucket，发现老 1 号 bucket 已经搬迁，只用遍历新 1 号 bucket 中现有的元素就可以了。结果集变成： 继续遍历新 2 号 bucket，它来自老 0 号 bucket，因此需要在老 0 号 bucket 中那些会裂变到新 2 号 bucket 中的 key，也就是 lowbit == 10 的那些 key。 这样，遍历结果集变成： 最后，继续遍历到新 3 号 bucket 时，发现所有的 bucket 都已经遍历完毕，整个迭代过程执行完毕。 顺便说一下，如果碰到 key 是 math.NaN() 这种的，处理方式类似。核心还是要看它被分裂后具体落入哪个 bucket。只不过只用看它 top hash 的最低位。如果 top hash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part。据此决定是否取出 key，放到遍历结果集里。 map 遍历的核心在于理解 2 倍扩容时，老 bucket 会分裂到 2 个新 bucket 中去。而遍历操作，会按照新 bucket 的序号顺序进行，碰到老 bucket 未搬迁的情况时，要在老 bucket 中找到将来要搬迁到新 bucket 来的 key Map插入 通过汇编语言可以看到，向 map 中插入或者修改 key，最终调用的是 mapassign 函数。 插入或修改 key 的语法是一样的，前者操作的 key 在 map 中不存在，而后者操作的 key 存在 map 中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; if h == nil &#123; panic(plainError(\"assignment to entry in nil map\")) &#125; if h.flags&amp;hashWriting != 0 &#123; //表示正在写 throw(\"concurrent map writes\") &#125; hash := t.hasher(key, uintptr(h.hash0)) //获取hash值 //在调用t.hasher之后设置hashWriting，因为t.hasher可能会出现紧急情况，在这种情况下，我们实际上并未执行写操作 h.flags ^= hashWriting //设置标记位，hashWriting //如果bucket数组一开始为空，则初始化 if h.buckets == nil &#123; h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) &#125;again: bucket := hash &amp; bucketMask(h.B) // 定位存储在哪一个bucket中 if h.growing() &#123; //如果现正在扩容，则扩容 growWork(t, h, bucket) &#125; //得到bucket的结构体 b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) //获取高八位hash值 var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointerbucketloop: for &#123; //死循环 for i := uintptr(0); i &lt; bucketCnt; i++ &#123; //循环bucket中的tophash数组 if b.tophash[i] != top &#123; //如果hash不相等 if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil &#123; //判断是否为空，为空则插入 inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) &#125; if b.tophash[i] == emptyRest &#123; //为空，且后续也没有非空cell break bucketloop &#125; continue &#125; //到这里说明高八位hash一样，获取已存在的key k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; //判断两个key是否相等，不相等就循环下一个 if !t.key.equal(key, k) &#123; continue &#125; // already have a mapping for key. Update it. if t.needkeyupdate() &#123; typedmemmove(t.key, k, key) &#125; //获取已存在的value elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done &#125; //如果上一个bucket没能插入，则通过overflow获取链表上的下一个bucket ovf := b.overflow(t) if ovf == nil &#123; break &#125; b = ovf &#125; //如果超过负载，则扩容 if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again &#125; //如果当前bmap满了，就新建一个 if inserti == nil &#123; // all current buckets are full, allocate a new one. newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) &#125; //存储元素 if t.indirectkey() &#123; kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem &#125; if t.indirectelem() &#123; vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem &#125; typedmemmove(t.key, insertk, key) *inserti = top h.count++done: //更新状态，返回元素 if h.flags&amp;hashWriting == 0 &#123; //如果是状态异常 throw(\"concurrent map writes\") &#125; h.flags &amp;^= hashWriting //设置标志位状态表示完成 if t.indirectelem() &#123; elem = *((*unsafe.Pointer)(elem)) &#125; return elem&#125; 整体来看，核心还是一个双层循环，外层遍历 bucket 和它的 overflow bucket，内层遍历整个 bucket 的各个 cell：对 key 计算 hash 值，根据 hash 值按照之前的流程，找到要赋值的位置（可能是插入新 key，也可能是更新老 key），对相应位置进行赋值。 注意： 函数首先会检查 map 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。 通过前文我们知道扩容是渐进式的，如果 map 处在扩容的过程中，那么当 key 定位到了某个 bucket 后，需要确保这个 bucket 对应的老 bucket 完成了迁移过程。即老 bucket 里的 key 都要迁移到新的 bucket 中来（分裂到 2 个新 bucket），才能在新的 bucket 中进行插入或者更新的操作。 上面说的操作是在函数靠前的位置进行的，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。 现在到了定位 key 应该放置的位置了，所谓找准自己的位置很重要。准备两个指针，一个（inserti）指向 key 的 hash 值在 tophash 数组所处的位置，另一个(insertk)指向 cell 的位置（也就是 key 最终放置的地址），当然，对应 value 的位置就很容易定位出来了。这三者实际上都是关联的，在 tophash 数组中的索引位置决定了 key 在整个 bucket 中的位置（共 8 个 key），而 value 的位置需要“跨过” 8 个 key 的长度。 在循环的过程中，inserti 和 insertk 分别指向第一个找到的空闲的 cell。如果之后在 map 没有找到 key 的存在，也就是说原来 map 中没有此 key，这意味着插入新 key。那最终 key 的安置地址就是第一次发现的“空位”（tophash 是 empty）。 如果这个 bucket 的 8 个 key 都已经放置满了，那在跳出循环后，发现 inserti 和 insertk 都是空，这时候需要在 bucket 后面挂上 overflow bucket。当然，也有可能是在 overflow bucket 后面再挂上一个 overflow bucket。这就说明，太多 key hash 到了此 bucket。 在正式安置 key 之前，还要检查 map 的状态，看它是否需要进行扩容。如果满足扩容的条件，就主动触发一次扩容操作。 这之后，整个之前的查找定位 key 的过程，还得再重新走一次。因为扩容之后，key 的分布都发生了变化。 最后，会更新 map 相关的值，如果是插入新 key，map 的元素数量字段 count 值会加 1；在函数之初设置的 hashWriting 写标志出会清零。 另外，有一个重要的点要说一下。前面说的找到 key 的位置，进行赋值操作，实际上并不准确。我们看 mapassign 函数的原型就知道，函数并没有传入 value 值，所以赋值操作是什么时候执行的呢？ 1func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer 答案还得从汇编语言中寻找。我直接揭晓答案，有兴趣可以私下去研究一下。mapassign 函数返回的指针就是指向的 key 所对应的 value 值位置，有了地址，就很好操作赋值了。 Map删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) &#123; //如果map为空，直接panic if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return &#125; //如果正在写，则异常并发读写 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map writes\") &#125; //计算hash值 hash := t.hasher(key, uintptr(h.hash0)) //设置标志位 h.flags ^= hashWriting //获取槽 bucket := hash &amp; bucketMask(h.B) if h.growing() &#123; //如果在扩容，直接触发扩容 growWork(t, h, bucket) &#125; b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) bOrig := b top := tophash(hash)search: for ; b != nil; b = b.overflow(t) &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; //双循环，如果topHash不一致，且不为emptyRest则继续 if b.tophash[i] == emptyRest &#123; //否则下一个overflow break search &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) k2 := k if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; if !t.key.equal(key, k2) &#123; continue &#125; // Only clear key if there are pointers in it. if t.indirectkey() &#123; *(*unsafe.Pointer)(k) = nil &#125; else if t.key.ptrdata != 0 &#123; memclrHasPointers(k, t.key.size) &#125; e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() &#123; *(*unsafe.Pointer)(e) = nil &#125; else if t.elem.ptrdata != 0 &#123; memclrHasPointers(e, t.elem.size) &#125; else &#123; memclrNoHeapPointers(e, t.elem.size) &#125; b.tophash[i] = emptyOne // If the bucket now ends in a bunch of emptyOne states, // change those to emptyRest states. // It would be nice to make this a separate function, but // for loops are not currently inlineable. if i == bucketCnt-1 &#123; if b.overflow(t) != nil &amp;&amp; b.overflow(t).tophash[0] != emptyRest &#123; goto notLast &#125; &#125; else &#123; if b.tophash[i+1] != emptyRest &#123; goto notLast &#125; &#125; for &#123; b.tophash[i] = emptyRest if i == 0 &#123; if b == bOrig &#123; break // beginning of initial bucket, we're done. &#125; // Find previous bucket, continue at its last entry. c := b for b = bOrig; b.overflow(t) != c; b = b.overflow(t) &#123; &#125; i = bucketCnt - 1 &#125; else &#123; i-- &#125; if b.tophash[i] != emptyOne &#123; break &#125; &#125; notLast: h.count-- //数量减1 break search &#125; &#125; if h.flags&amp;hashWriting == 0 &#123; throw(\"concurrent map writes\") &#125; //读写恢复 h.flags &amp;^= hashWriting&#125; 当然，我们只关心 mapdelete 函数。它首先会检查 h.flags 标志，如果发现写标位是 1，直接 panic，因为这表明有其他协程同时在进行写操作。 计算 key 的哈希，找到落入的 bucket。检查此 map 如果正在扩容的过程中，直接触发一次搬迁操作。 删除操作同样是两层循环，核心还是找到 key 的具体位置。寻找过程都是类似的，在 bucket 中挨个 cell 寻找。 找到对应位置后，对 key 或者 value 进行“清零”操作 最后，将 count 值减 1，将对应位置的 tophash 值置成 Empty Map类型 12345678910111213141516171819202122232425262728293031type maptype struct &#123; typ _type key *_type elem *_type bucket *_type // internal type representing a hash bucket // function for hashing keys (ptr to key, seed) -&gt; hash hasher func(unsafe.Pointer, uintptr) uintptr keysize uint8 // size of key slot elemsize uint8 // size of elem slot bucketsize uint16 // size of bucket flags uint32&#125;type _type struct &#123; size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; Map进阶 Key可以是float型 从语法上看，是可以的。Go 语言中只要是可比较的类型都可以作为 key。除开 slice，map，functions 这几种类型，其他类型都是 OK 的。具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。这些类型的共同特征是支持 == 和 != 操作符，k1 == k2 时，可认为 k1 和 k2 是同一个 key。如果是结构体，则需要它们的字段值都相等，才被认为是相同的 key chan是怎么比较的？ 删除Key之后，内存是直接释放吗？ 首先看看删除逻辑代码 1234567891011121314if t.indirectkey() &#123; //如果key是指针 *(*unsafe.Pointer)(k) = nil //则将key置位nil&#125; else if t.key.ptrdata != 0 &#123; //key中含有指针 memclrHasPointers(k, t.key.size)&#125;e := add(unsafe.Pointer(b),dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize))if t.indirectelem() &#123; //如果val是指针 *(*unsafe.Pointer)(e) = nil&#125; else if t.elem.ptrdata != 0 &#123; //如果val含有指针 memclrHasPointers(e, t.elem.size)&#125; else &#123; memclrNoHeapPointers(e, t.elem.size) //执行飞对指针&#125;b.tophash[i] = emptyOne 其中 1234567891011func memclrHasPointers(ptr unsafe.Pointer, n uintptr) &#123; bulkBarrierPreWrite(uintptr(ptr), 0, n) //添加写屏障 memclrNoHeapPointers(ptr, n) //clears n bytes starting at ptr 非堆&#125;func typedmemclr(typ *_type, ptr unsafe.Pointer) &#123; if writeBarrier.needed &amp;&amp; typ.ptrdata != 0 &#123; //类型含有指针字段 bulkBarrierPreWrite(uintptr(ptr), 0, typ.ptrdata) &#125; memclrNoHeapPointers(ptr, typ.size)&#125; 首先 memclrHasPointers 与 typedmemclr 的区别仅仅是 调用者是否直到清理的类型中是否含有堆指针 参考文献 如何设计并实现一个线程安全的Map https://www.kancloud.cn/kancloud/the-way-to-go/72489 https://blog.csdn.net/u010853261/article/details/99699350 https://cloud.tencent.com/developer/article/1468799 https://www.jianshu.com/p/7782d82f5154 https://qcrao.com/2019/05/22/dive-into-go-map/ https://github.com/golang/go/issues/20135","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Linux深入2-事件模型","slug":"Linux/Linux深入2-事件模型","date":"2021-07-04T14:39:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/04/Linux/Linux深入2-事件模型/","link":"","permalink":"http://xboom.github.io/2021/07/04/Linux/Linux%E6%B7%B1%E5%85%A52-%E4%BA%8B%E4%BB%B6%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"基础知识 同步与异步：关注的是消息通信机制 (synchronous communication/ asynchronous communication) 所谓同步，调用在发出后，在没有得到结果之前该调用不返回。一旦调用返回，就得到返回值 换句话说，就是由调用者主动等待这个调用的结果 所谓异步，调用在发出后，这个调用直接返回了，没有返回结果。而是被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用 阻塞与非阻塞：关注的是程序在等待调用结果消息返回值时的状态 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程 用户空间和内核空间 操作系统的核心是内核，可访问受保护的内存空间和底层硬件设备。为了保证用户进程不能直接操作内核（kernel），将虚拟空间划分为内核空间和用户空间。针对32位linux操作系统而言，寻址空间（虚拟存储空间）为4G（2的32次方），将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 补充：地址空间就是一个非负整数地址的有序集合。如{0,1,2…} 用户态与核心态的切换，一共有三种方式 系统调用：用户态进程通过系统调用申请使用操作系统的服务完成工作，系统调用其实是通过中断来实现 异常：当CPU在执行运行在用户态下的程序时，发生异常会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常 外围设备中断：当外围设备向CPU发出中断信号，CPU会暂停执行下一条即将要执行的指令转而去执行对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等 进程切换：内核挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行 保存处理机上下文，包括程序计数器和其他寄存器 更新PCB信息 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列 选择另一个进程执行，并更新其PCB 更新内存管理的数据结构。 恢复处理机上下文 系统中存放进程的管理和控制信息的数据结构称为进程控制块（PCB Process Control Block) 当进程被阻塞，它是不占用CPU资源的 文件描述符：文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统 缓冲IO：缓存IO又被称作标准IO，在Linux的缓存IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。导致数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作 基本类型 《UNIX网络编程：卷一》第六章——I/O复用，书中提及了5种类UNIX下可用的I/O模型： 阻塞式I/O 非阻塞式I/O I/O复用（select，poll，epoll…） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 阻塞IO模型 进程会一直阻塞，直到数据拷贝完成 应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。数据准备好后，从内核拷贝到用户空间，IO函数返回成功指示 非阻塞IO模型 通过进程反复调用IO函数，在数据拷贝过程中，进程是阻塞的 IO复用模型 主要是select和epoll。一个线程可以对多个IO端口进行监听，当socket有读写事件时分发到具体的线程进行处理 虽然I/O多路复用的函数也是阻塞的，但是其与以上两种还是有不同的，I/O多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的I/O系统调用如recvfrom之上。 select select本质是通过设置或检查存放fd标志位的数据结构来进行下一步处理。缺点是： 单个进程可监视的fd数量被限制，即能监听端口的大小有限。一般来说和系统内存有关，具体数目可以cat /proc/sys/fs/file-max察看。32位默认是1024个，64位默认为2048个 对socket进行扫描时是线性扫描，即采用轮询方法，效率低。当套接字比较多的时候，每次select()都要遍历FD_SETSIZE个socket来完成调度，不管socket是否活跃都遍历一遍。会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，就避免了轮询，这正是epoll与kqueue做的 需要维护一个用来存放大量fd的数据结构，会使得用户空间和内核空间在传递该结构时复制开销大 poll poll本质和select相同，将用户传入的数据拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或主动超时，被唤醒后又要再次遍历fd。它没有最大连接数的限制，原因是它是基于链表来存储的，但缺点是： 大量的fd的数组被整体复制到用户态和内核空间之间，不管有无意义 poll还有一个特点“水平触发”，如果报告了fd后，没有被处理，那么下次poll时再次报告该ffd。 epoll epoll支持水平触发和边缘触发，最大特点在于边缘触发，只告诉哪些fd刚刚变为就绪态，并且只通知一次。还有一特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一量该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。epoll的优点： 没有最大并发连接的限制。 效率提升，只有活跃可用的FD才会调用callback函数。 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递。 水平触发与垂直触发 select、poll、epoll区别总结 信号驱动IO模型 首先允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据 异步IO模型 相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的 5种IO模型比较 阻塞与非阻塞IO的区别是：调用阻塞IO后进程会一直等待对应的进程完成，而非阻塞IO不会等待对应的进程完成，在kernel还在准备数据的情况下直接返回 前四种I/O模型都是同步I/O操作，区别在于第一阶段，第二阶段是一样的：在数据从内核复制到应用缓冲区期间（用户空间），进程阻塞于recvfrom调用。相反，异步I/O模型在这两个阶段都要处理。 阻塞IO和非阻塞IO的区别：数据准备的过程中,进程是否阻塞。 同步IO和异步IO的区别：数据拷贝的过程中,进程是否阻塞 实现原理 123456789101112//创建socketint s = socket(AF_INET, SOCK_STREAM, 0); //绑定bind(s, ...)//监听listen(s, ...)//接受客户端连接int c = accept(s, ...)//接收客户端数据recv(c, ...);//将数据打印出来printf(...) 接收数据 网卡收到网线传来的数据 通过硬件电路传输，将数据写入到内存中的某个地址上 由硬件产生的信号CPU会中断掉正在执行的程序而做出响应。执行完毕再重新执行用户程序。中断过程和函数调用差不多。只不过函数调用是事先定好位置，而中断的位置由“信号”决定 以键盘为例，当用户按下键盘某个按键时，键盘会给cpu的中断引脚发出一个高电平。cpu能够捕获这个信号，然后执行键盘中断程序。 当网卡把数据写入到内存后，网卡向cpu发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据。 先将网络数据写入到对应socket的接收缓冲区里 再唤醒进程A，重新将进程A放入工作队列中 工作队列 当程序运行到recv时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态 计算机中运行着A、B、C三个进程，其中进程A执行着上述基础网络程序，一开始，这3个进程都被操作系统的工作队列所引用，处于运行状态，会分时执行 等待队列 当进程A执行创建socket语句时，操作系统会创建一个由文件系统管理的socket对象（如下图）。 socket对象包含了 发送缓冲区 接收缓冲区 等待队列：指向所有需要等待该socket事件的进程 当程序执行到recv时，操作系统会将进程A从工作队列移动到该socket的等待队列中（如下图）。由于工作队列只剩下了进程B和C，依据进程调度，cpu会轮流执行这两个进程的程序，不会执行进程A的程序。所以进程A被阻塞，不会往下执行代码，也不会占用cpu资源 操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。上图为了方便说明，直接将进程挂到等待队列之下 唤醒进程 当socket接收到数据后，操作系统将该socket等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。也由于socket的接收缓冲区已经有了数据，recv可以返回接收到的数据 问题1：操作系统如何知道网络数据对应于哪个socket？ 因为一个socket对应着一个端口号，而网络数据包中包含了ip和端口的信息，内核可以通过端口号找到对应的socket。当然，为了提高处理速度，操作系统会维护端口号到socket的索引结构，以快速读取 问题2：如何同时监视多个socket的数据？ 多路复用 监视多个socket select 1234567891011121314int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int fds[] = 存放需要监听的socketwhile(1)&#123; int n = select(..., fds, ...) for(int i=0; i &lt; fds.count; i++)&#123; if(FD_ISSET(fds[i], ...))&#123; //fds[i]的数据处理 &#125; &#125;&#125; 假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中 当任何一个socket收到数据后，中断程序将唤起进程。下图展示了sock2接收到了数据的处理流程 所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面。如下图所示 经由这些步骤，当进程A被唤醒后，它知道至少有一个socket接收了数据。程序只需遍历一遍socket列表，就可以得到就绪的socket 当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞 缺点： 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次 epoll epoll改进措施 措施一：功能分离 select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一，epoll将这两个操作分开，先用epoll_ctl维护等待队列，再调用epoll_wait阻塞进程 12345678910111213int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 措施二：就绪列表 select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据 创建epoll对象 当某个进程调用epoll_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列 维护监视列表 创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket。以添加socket为例，如下图，如果通过epoll_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中 当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程 接收数据 当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用。如下图展示的是sock2和sock3收到数据后，中断程序让rdlist引用这两个socket eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。 当程序执行到epoll_wait时，如果rdlist已经引用了socket，那么epoll_wait直接返回，如果rdlist为空，阻塞进程 阻塞和唤醒进程 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化 实现细节 就绪列表引用着就绪的socket，所以它应能够快速的插入数据。 程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。 所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist) 索引结构既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构 因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist并非直接引用socket，而是通过epitem间接引用，红黑树的节点也是epitem对象。同样，文件系统也并非直接引用着socket。为方便理解，本文中省略了一些间接结构 代码分析(还没看懂) epoll 1234567891011121314151617181920212223242526272829303132int init_reactor(int listen_fd,int worker_count)&#123; ...... // 创建多个epoll fd，以充分利用多核 for(i=0;i&lt;worker_count;i++)&#123; reactor-&gt;worker_fd = epoll_create(EPOLL_MAX_EVENTS); &#125; /* epoll add listen_fd and accept */ // 将accept后的事件加入到对应的epoll fd中 int client_fd = accept(listen_fd,(struct sockaddr *)&amp;client_addr,&amp;client_len))); // 将连接描述符注册到对应的worker里面 epoll_ctl(reactor-&gt;client_fd,EPOLL_CTL_ADD,epifd,&amp;event);&#125;// reactor的worker线程static void* rw_thread_func(void* arg)&#123; ...... for(;;)&#123; // epoll_wait等待事件触发 int retval = epoll_wait(epfd,events,EPOLL_MAX_EVENTS,500); if(retval &gt; 0)&#123; for(j=0; j &lt; retval; j++)&#123; // 处理读事件 if(event &amp; EPOLLIN)&#123; handle_ready_read_connection(conn); continue; &#125; /* 处理其它事件 */ &#125; &#125; &#125; ......&#125; eventpoll 核心结构 1234567891011121314151617181920// epoll的核心实现对应于一个epoll描述符 struct eventpoll &#123; spinlock_t lock; struct mutex mtx; wait_queue_head_t wq; // sys_epoll_wait() 等待在这里 // f_op-&gt;poll() 使用的, 被其他事件通知机制利用的wait_address wait_queue_head_t poll_wait; //已就绪的需要检查的epitem 列表 struct list_head rdllist; //保存所有加入到当前epoll的文件对应的epitem struct rb_root rbr; // 当正在向用户空间复制数据时, 产生的可用文件 struct epitem *ovflist; /* The user that created the eventpoll descriptor */ struct user_struct *user; struct file *file; //优化循环检查，避免循环检查中重复的遍历 int visited; struct list_head visited_list_link; &#125; epitem对应一个接入到epoll的文件 12345678910111213141516171819202122232425// 对应于一个加入到epoll的文件 struct epitem &#123; // 挂载到eventpoll 的红黑树节点 struct rb_node rbn; // 挂载到eventpoll.rdllist 的节点 struct list_head rdllink; // 连接到ovflist 的指针 struct epitem *next; /* 文件描述符信息fd + file, 红黑树的key */ struct epoll_filefd ffd; /* Number of active wait queue attached to poll operations */ int nwait; // 当前文件的等待队列(eppoll_entry)列表 // 同一个文件上可能会监视多种事件, // 这些事件可能属于不同的wait_queue中 // (取决于对应文件类型的实现), // 所以需要使用链表 struct list_head pwqlist; // 当前epitem 的所有者 struct eventpoll *ep; /* List header used to link this item to the &amp;quot;struct file&amp;quot; items list */ struct list_head fllink; /* epoll_ctl 传入的用户数据 */ struct epoll_event event; &#125;; eppoll_entry与一个文件上的一个wait_queue_head 相关联，因为同一文件可能有多个等待的事件 12345678910struct eppoll_entry &#123; // List struct epitem.pwqlist struct list_head llink; // 所有者 struct epitem *base; // 添加到wait_queue 中的节点 wait_queue_t wait; // 文件wait_queue 头 wait_queue_head_t *whead; &#125;; epoll_create() 返回一个文件描述符，此描述符挂载在anon_inode_fs(匿名inode文件系统)的根目录下面 1234567//先进行判断size是否&gt;=0，若是则直接调用epoll_create1SYSCALL_DEFINE1(epoll_create, int, size)&#123; if (size &lt;= 0) //当 size&lt;=0 的时候直接直接退出 return -EINVAL; return sys_epoll_create1(0); //参数也没有意义&#125; SYSCALL_DEFINE1是宏用于定义有一个参数的系统调用函数，展开后为： int sys_epoll_create(int size)，即epoll_create系统调用的入口。 为何要用宏而不是直接声明，主要是因为系统调用的参数个数、传参方式都有严格限制，受限于寄存器数量的限制，(80x86下的)kernel限制系统调用最多有6个参数 epoll_create1() 真正的系统调用 1234567891011121314151617181920212223242526272829303132333435363738394041SYSCALL_DEFINE1(epoll_create1, int, flags)&#123; int error, fd; struct eventpoll *ep = NULL; struct file *file; if (flags &amp; ~EPOLL_CLOEXEC) return -EINVAL; /* * Create the internal data structure (\"struct eventpoll\"). */ // kzalloc(sizeof(*ep), GFP_KERNEL),用的是内核空间 error = ep_alloc(&amp;ep); if (error &lt; 0) return error; //获取尚未被使用的文件描述符，即描述符数组的槽位 fd = get_unused_fd_flags(O_RDWR | (flags &amp; O_CLOEXEC)); if (fd &lt; 0) &#123; error = fd; goto out_free_ep; &#125; // 在匿名inode文件系统中分配一个inode,并得到其file结构体 // 且file-&gt;f_op = &amp;eventpoll_fops // 且file-&gt;private_data = ep; file = anon_inode_getfile(\"[eventpoll]\", &amp;eventpoll_fops, ep, O_RDWR | (flags &amp; O_CLOEXEC));//在fd_array中获取一个槽位 if (IS_ERR(file)) &#123; error = PTR_ERR(file); goto out_free_fd; &#125; ep-&gt;file = file; // 将file填入到对应的文件描述符数组的槽里面(下图中files_struct的fd_array) fd_install(fd, file); return fd;out_free_fd: put_unused_fd(fd);out_free_ep: ep_free(ep); return error;&#125; 最后epoll_create生成的文件描述符如下 所有的epoll系统调用都是围绕eventpoll结构体做操作,现简要描述下其中的成员: 123456789101112131415161718192021222324252627/* * 此结构体存储在file-&gt;private_data中 */struct eventpoll &#123; // 自旋锁，在kernel内部用自旋锁加锁，就可以同时多线(进)程对此结构体进行操作 // 主要是保护ready_list spinlock_t lock; // 这个互斥锁是为了保证在eventloop使用对应的文件描述符的时候，文件描述符不会被移除掉 struct mutex mtx; // epoll_wait使用的等待队列，和进程唤醒有关 wait_queue_head_t wq; // file-&gt;poll使用的等待队列，和进程唤醒有关 wait_queue_head_t poll_wait; // 就绪的描述符队列 双向链表 struct list_head rdllist; // 通过红黑树来组织当前epoll关注的文件描述符 红黑树 struct rb_root rbr; // 在向用户空间传输就绪事件的时候，将同时发生事件的文件描述符链入到这个链表里面 struct epitem *ovflist; // 对应的user struct user_struct *user; // 对应的文件描述符 struct file *file; // 下面两个是用于环路检测的优化 int visited; struct list_head visited_list_link;&#125;; epoll_ctl(add) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146/* * The following function implements the controller interface for * the eventpoll file that enables the insertion/removal/change of * file descriptors inside the interest set. */SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)&#123; int error; int full_check = 0; struct fd f, tf; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; struct eventpoll *tep = NULL; error = -EFAULT; if (ep_op_has_event(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event))) goto error_return; error = -EBADF; f = fdget(epfd); if (!f.file) goto error_return; /* Get the \"struct file *\" for the target file */ tf = fdget(fd); if (!tf.file) goto error_fput; /* The target file descriptor must support poll */ error = -EPERM; if (!tf.file-&gt;f_op-&gt;poll) goto error_tgt_fput; /* Check if EPOLLWAKEUP is allowed */ if (ep_op_has_event(op)) ep_take_care_of_epollwakeup(&amp;epds); /* * We have to check that the file structure underneath the file descriptor * the user passed to us _is_ an eventpoll file. And also we do not permit * adding an epoll file descriptor inside itself. */ error = -EINVAL; if (f.file == tf.file || !is_file_epoll(f.file)) goto error_tgt_fput; /* * At this point it is safe to assume that the \"private_data\" contains * our own data structure. */ ep = f.file-&gt;private_data; /* * When we insert an epoll file descriptor, inside another epoll file * descriptor, there is the change of creating closed loops, which are * better be handled here, than in more critical paths. While we are * checking for loops we also determine the list of files reachable * and hang them on the tfile_check_list, so we can check that we * haven't created too many possible wakeup paths. * * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when * the epoll file descriptor is attaching directly to a wakeup source, * unless the epoll file descriptor is nested. The purpose of taking the * 'epmutex' on add is to prevent complex toplogies such as loops and * deep wakeup paths from forming in parallel through multiple * EPOLL_CTL_ADD operations. */ /* 校验epfd是否是epoll的描述符 */ // 此处的互斥锁是为了防止并发调用epoll_ctl,即保护内部数据结构 // 不会被并发的添加修改删除破坏 mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (op == EPOLL_CTL_ADD) &#123; if (!list_empty(&amp;f.file-&gt;f_ep_links) || is_file_epoll(tf.file)) &#123; full_check = 1; mutex_unlock(&amp;ep-&gt;mtx); mutex_lock(&amp;epmutex); if (is_file_epoll(tf.file)) &#123; error = -ELOOP; if (ep_loop_check(ep, tf.file) != 0) &#123; clear_tfile_check_list(); goto error_tgt_fput; &#125; &#125; else list_add(&amp;tf.file-&gt;f_tfile_llink, &amp;tfile_check_list); mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (is_file_epoll(tf.file)) &#123; tep = tf.file-&gt;private_data; mutex_lock_nested(&amp;tep-&gt;mtx, 1); &#125; &#125; &#125; /* * Try to lookup the file inside our RB tree, Since we grabbed \"mtx\" * above, we can be sure to be able to use the item looked up by * ep_find() till we release the mutex. */ epi = ep_find(ep, tf.file, fd); error = -EINVAL; switch (op) &#123; case EPOLL_CTL_ADD: if (!epi) &#123; epds.events |= POLLERR | POLLHUP; // 插入到红黑树中 error = ep_insert(ep, &amp;epds, tf.file, fd, full_check); &#125; else error = -EEXIST; if (full_check) clear_tfile_check_list(); break; case EPOLL_CTL_DEL: if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; case EPOLL_CTL_MOD: if (epi) &#123; epds.events |= POLLERR | POLLHUP; error = ep_modify(ep, epi, &amp;epds); &#125; else error = -ENOENT; break; &#125; if (tep != NULL) mutex_unlock(&amp;tep-&gt;mtx); mutex_unlock(&amp;ep-&gt;mtx);error_tgt_fput: if (full_check) mutex_unlock(&amp;epmutex); fdput(tf);error_fput: fdput(f);error_return: return error;&#125; ep_insert 在ep_insert中初始化了epitem，然后初始化了本文关注的焦点,即事件就绪时候的回调函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129static int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd, int full_check)&#123; int error, revents, pwake = 0; unsigned long flags; long user_watches; struct epitem *epi; struct ep_pqueue epq; user_watches = atomic_long_read(&amp;ep-&gt;user-&gt;epoll_watches); if (unlikely(user_watches &gt;= max_user_watches)) return -ENOSPC; if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; /* Item initialization follow here ... */ INIT_LIST_HEAD(&amp;epi-&gt;rdllink); INIT_LIST_HEAD(&amp;epi-&gt;fllink); INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); epi-&gt;ep = ep; ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); epi-&gt;event = *event; epi-&gt;nwait = 0; epi-&gt;next = EP_UNACTIVE_PTR; if (epi-&gt;event.events &amp; EPOLLWAKEUP) &#123; error = ep_create_wakeup_source(epi); if (error) goto error_create_wakeup_source; &#125; else &#123; RCU_INIT_POINTER(epi-&gt;ws, NULL); &#125; /* Initialize the poll table using the queue callback */ /* 初始化epitem */ // &amp;epq.pt-&gt;qproc = ep_ptable_queue_proc epq.epi = epi; init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); /* * Attach the item to the poll hooks and get current event bits. * We can safely use the file* here because its usage count has * been increased by the caller of this function. Note that after * this operation completes, the poll callback can start hitting * the new item. */ revents = ep_item_poll(epi, &amp;epq.pt); // 在这里将回调函数注入 /* * We have to check if something went wrong during the poll wait queue * install process. Namely an allocation for a wait queue failed due * high memory pressure. */ error = -ENOMEM; if (epi-&gt;nwait &lt; 0) goto error_unregister; /* Add the current item to the list of active epoll hook for this file */ spin_lock(&amp;tfile-&gt;f_lock); list_add_tail_rcu(&amp;epi-&gt;fllink, &amp;tfile-&gt;f_ep_links); spin_unlock(&amp;tfile-&gt;f_lock); /* * Add the current item to the RB tree. All RB tree operations are * protected by \"mtx\", and ep_insert() is called with \"mtx\" held. */ ep_rbtree_insert(ep, epi); /* now check if we've created too many backpaths */ error = -EINVAL; if (full_check &amp;&amp; reverse_path_check()) goto error_remove_epi; /* We have to drop the new item inside our item list to keep track of it */ spin_lock_irqsave(&amp;ep-&gt;lock, flags); /* If the file is already \"ready\" we drop it inside the ready list */ // 如果当前有事件已经就绪，那么一开始就会被加入到ready list // 例如可写事件 // 另外，在tcp内部ack之后调用tcp_check_space,最终调用sock_def_write_space来唤醒对应的epoll_wait下的进程 if ((revents &amp; event-&gt;events) &amp;&amp; !ep_is_linked(&amp;epi-&gt;rdllink)) &#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); /* Notify waiting tasks that events are available */ // wake_up ep对应在epoll_wait下的进程 if (waitqueue_active(&amp;ep-&gt;wq)) wake_up_locked(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); atomic_long_inc(&amp;ep-&gt;user-&gt;epoll_watches); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return 0;error_remove_epi: spin_lock(&amp;tfile-&gt;f_lock); list_del_rcu(&amp;epi-&gt;fllink); spin_unlock(&amp;tfile-&gt;f_lock); rb_erase(&amp;epi-&gt;rbn, &amp;ep-&gt;rbr);error_unregister: ep_unregister_pollwait(ep, epi); /* * We need to do this because an event could have been arrived on some * allocated wait queue. Note that we don't care about the ep-&gt;ovflist * list, since that is used/cleaned only inside a section bound by \"mtx\". * And ep_insert() is called with \"mtx\" held. */ spin_lock_irqsave(&amp;ep-&gt;lock, flags); if (ep_is_linked(&amp;epi-&gt;rdllink)) list_del_init(&amp;epi-&gt;rdllink); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); wakeup_source_unregister(ep_wakeup_source(epi));error_create_wakeup_source: kmem_cache_free(epi_cache, epi); return error;&#125; 其中 tfile-&gt;f_op-&gt;poll的实现，向kernel更底层注册回调函数的是tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt)这一句，我们来看一下对于对应的socket文件描述符，其fd=&gt;file-&gt;f_op-&gt;poll的初始化过程 1234// 将accept后的事件加入到对应的epoll fd中int client_fd = accept(listen_fd,(struct sockaddr *)&amp;client_addr,&amp;client_len)));// 将连接描述符注册到对应的worker里面epoll_ctl(reactor-&gt;client_fd,EPOLL_CTL_ADD,epifd,&amp;event); epoll_wait 12345678910111213141516171819202122232425262728293031323334353637383940414243SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout)&#123; int error; struct fd f; struct eventpoll *ep; &#x2F;* The maximum number of event must be greater than zero *&#x2F; if (maxevents &lt;&#x3D; 0 || maxevents &gt; EP_MAX_EVENTS) return -EINVAL; &#x2F;* Verify that the area passed by the user is writeable *&#x2F; if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct epoll_event))) return -EFAULT; &#x2F;* Get the &quot;struct file *&quot; for the eventpoll file *&#x2F; f &#x3D; fdget(epfd); if (!f.file) return -EBADF; &#x2F;* * We have to check that the file structure underneath the fd * the user passed to us _is_ an eventpoll file. *&#x2F; error &#x3D; -EINVAL; if (!is_file_epoll(f.file)) goto error_fput; &#x2F;* * At this point it is safe to assume that the &quot;private_data&quot; contains * our own data structure. *&#x2F; ep &#x3D; f.file-&gt;private_data; &#x2F;* Time to fish for events ... *&#x2F; &#x2F;* 检查epfd是否是epoll\\_create创建的fd *&#x2F; &#x2F;&#x2F; 调用ep_poll error &#x3D; ep_poll(ep, events, maxevents, timeout);error_fput: fdput(f); return error;&#125; 紧接着，我们看下ep_poll函数: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980tatic int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res = 0, eavail, timed_out = 0; unsigned long flags; long slack = 0; wait_queue_t wait; ktime_t expires, *to = NULL; if (timeout &gt; 0) &#123; struct timespec end_time = ep_set_mstimeout(timeout); slack = select_estimate_accuracy(&amp;end_time); to = &amp;expires; *to = timespec_to_ktime(end_time); &#125; else if (timeout == 0) &#123; /* * Avoid the unnecessary trip to the wait queue loop, if the * caller specified a non blocking operation. */ timed_out = 1; spin_lock_irqsave(&amp;ep-&gt;lock, flags); goto check_events; &#125;fetch_events: // 获取spinlock spin_lock_irqsave(&amp;ep-&gt;lock, flags); if (!ep_events_available(ep)) &#123; /* * We don't have any available event to return to the caller. * We need to sleep here, and we will be wake up by * ep_poll_callback() when events will become available. */ // 将当前task_struct写入到waitqueue中以便唤醒 // wq_entry-&gt;func = default_wake_function; init_waitqueue_entry(&amp;wait, current); __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); for (;;) &#123; /* * We don't want to sleep if the ep_poll_callback() sends us * a wakeup in between. That's why we set the task state * to TASK_INTERRUPTIBLE before doing the checks. */ set_current_state(TASK_INTERRUPTIBLE); if (ep_events_available(ep) || timed_out) break; if (signal_pending(current)) &#123; res = -EINTR; break; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) timed_out = 1; spin_lock_irqsave(&amp;ep-&gt;lock, flags); &#125; __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); set_current_state(TASK_RUNNING); &#125;check_events: /* Is it worth to try to dig for events ? */ eavail = ep_events_available(ep); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* * Try to transfer events to user space. In case we get 0 events and * there's still timeout left over, we go trying again in search of * more luck. */ if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; !timed_out) goto fetch_events; return res;&#125; ep_send_events ep_send_events函数主要就是调用了ep_scan_ready_list,顾名思义ep_scan_ready_list就是扫描就绪列表: 12345678910111213141516static int ep_scan_ready_list(struct eventpoll *ep, int (*sproc)(struct eventpoll *, struct list_head *, void *), void *priv, int depth)&#123; ... // 将epfd的rdllist链入到txlist list_splice_init(&amp;ep-&gt;rdllist, &amp;txlist); ... /* sproc = ep_send_events_proc */ error = (*sproc)(ep, &amp;txlist, priv); ... // 处理ovflist,即在上面sproc过程中又到来的事件 ...&#125; 其主要调用了ep_send_events_proc: 123456789101112131415161718192021222324252627static int ep_send_events_proc(struct eventpoll *ep, struct list_head *head, void *priv)&#123; for (eventcnt = 0, uevent = esed-&gt;events; !list_empty(head) &amp;&amp; eventcnt &lt; esed-&gt;maxevents;) &#123; // 遍历ready list epi = list_first_entry(head, struct epitem, rdllink); list_del_init(&amp;epi-&gt;rdllink); // readylist只是表明当前epi有事件，具体的事件信息还是得调用对应file的poll // 这边的poll即是tcp_poll,根据tcp本身的信息设置掩码(mask)等信息 &amp; 上兴趣事件掩码，则可以得知当前事件是否是epoll_wait感兴趣的事件 revents = epi-&gt;ffd.file-&gt;f_op-&gt;poll(epi-&gt;ffd.file, NULL) &amp; epi-&gt;event.events; if(revents)&#123; /* 将event放入到用户空间 */ /* 处理ONESHOT逻辑 */ // 如果不是边缘触发，则将当前的epi重新加回到可用列表中，这样就可以下一次继续触发poll,如果下一次poll的revents不为0，那么用户空间依旧能感知 */ else if (!(epi-&gt;event.events &amp; EPOLLET))&#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); &#125; /* 如果是边缘触发，那么就不加回可用列表，因此只能等到下一个可用事件触发的时候才会将对应的epi放到可用列表里面*/ eventcnt++ &#125; /* 如poll出来的revents事件epoll_wait不感兴趣(或者本来就没有事件)，那么也不会加回到可用列表 */ ...... &#125; return eventcnt;&#125; eventpoll_init 文件系统初始化 1234567891011121314151617181920212223static int __init eventpoll_init(void) &#123; struct sysinfo si; si_meminfo(&amp;si); // 限制可添加到epoll的最多的描述符数量 max_user_watches = (((si.totalram - si.totalhigh) / 25) &lt;&lt; PAGE_SHIFT) / EP_ITEM_COST; BUG_ON(max_user_watches &lt; 0); // 初始化递归检查队列 ep_nested_calls_init(&amp;poll_loop_ncalls); ep_nested_calls_init(&amp;poll_safewake_ncalls); ep_nested_calls_init(&amp;poll_readywalk_ncalls); // epoll 使用的slab分配器分别用来分配epitem和eppoll_entry epi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem), 0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL); pwq_cache = kmem_cache_create(\"eventpoll_pwq\", sizeof(struct eppoll_entry), 0, SLAB_PANIC, NULL); return 0; &#125; 1.epoll_create中的size参数有什么作用？ 答：size这个参数其实没有任何用处，它只是为了保持兼容，因为之前的fd使用hash表保存，size表示hash表的大小，而现在使用红黑树保存，所以size就没用了。 2.LT和ET的区别（源码级别）？ 答：在源码中，两种模式的区别是一个if判断语句，通过ep_send_events_proc()函数实现，如果没有标上EPOLLET(即默认的LT)且“事件被关注”的fd就会被重新放回了rdllist。那么下次epoll_wait当然会又把rdllist里的fd拿来拷给用户了 参考链接 https://www.cnblogs.com/Yunya-Cnblogs/p/13246517.html https://mp.weixin.qq.com/s/4xqJGsjiSxRHdI80OERHmQ https://blog.csdn.net/baiye_xing/article/details/76352935 https://zhuanlan.zhihu.com/p/63179839 https://my.oschina.net/alchemystar/blog/3008840","categories":[{"name":"Linux Depth","slug":"Linux-Depth","permalink":"http://xboom.github.io/categories/Linux-Depth/"}],"tags":[{"name":"Event","slug":"Event","permalink":"http://xboom.github.io/tags/Event/"}]},{"title":"Go入门10-defer","slug":"Go/Go入门10-defer","date":"2021-07-03T15:07:15.000Z","updated":"2021-07-03T15:07:43.000Z","comments":true,"path":"2021/07/03/Go/Go入门10-defer/","link":"","permalink":"http://xboom.github.io/2021/07/03/Go/Go%E5%85%A5%E9%97%A810-defer/","excerpt":"","text":"defer原理 defer语句会进入一个栈，在函数return前按先进后出的顺序执行(原因是后面定义的函数可能会依赖前面的资源) 在defer函数定义时，对外部变量的引用是有两种方式的: 作为函数参数，则在defer定义时就把值传递给defer，并被cache起来； 作为闭包引用，则会在defer函数真正调用时根据整个上下文确定当前的值。 defer后的语句在执行的时候，函数调用的参数会被复制。如果此变量是一个“值”，那么就和定义的时候是一致的。如果此变量是一个“引用”，那么就可能和定义的时候不一致。 123456789func main() &#123; var whatever [3]struct&#123;&#125; for i := range whatever &#123; defer func() &#123; fmt.Println(i) &#125;() &#125;&#125; 执行结果为： 123222 因为闭包是根据上下文确定当前的值 123456789101112131415type number intfunc (n number) print() &#123; fmt.Println(n) &#125;func (n *number) pprint() &#123; fmt.Println(*n) &#125;func main() &#123; var n number defer n.print() defer n.pprint() defer func() &#123; n.print() &#125;() defer func() &#123; n.pprint() &#125;() n &#x3D; 3&#125; 执行结果是： 12343330 第四个defer语句是闭包，引用外部函数的n, 最终结果是3 第三个defer语句是闭包同第三个 第二个defer语句，n是引用，最终求值是3 第一个defer语句，对n直接求职，开始的时候n=0，所以最后是0 defer返回 返回语句 return xxx编译后编程三条命令 返回值 = xxx 调用defer函数 空的return 1234567func f() (r int) &#123; t :&#x3D; 5 defer func() &#123; t &#x3D; t + 5 &#125;() return t&#125; 可以看成 1234567891011121314func f() (r int) &#123; t :&#x3D; 5 &#x2F;&#x2F; 1. 赋值指令 r &#x3D; t &#x2F;&#x2F; 2. defer被插入到赋值与返回之间执行，这个例子中返回值r没被修改过 func() &#123; t &#x3D; t + 5 &#125; &#x2F;&#x2F; 3. 空的return指令 return&#125; 结果为5 第二例子： 123456func f() (r int) &#123; defer func(r int) &#123; r &#x3D; r + 5 &#125;(r) return 1&#125; 可以看成： 123456789101112func f() (r int) &#123; &#x2F;&#x2F; 1. 赋值 r &#x3D; 1 &#x2F;&#x2F; 2. 这里改的r是之前传值传进去的r，不会改变要返回的那个r值 func(r int) &#123; r &#x3D; r + 5 &#125;(r) &#x2F;&#x2F; 3. 空的return return&#125; 结果为 1 参考链接 https://qcrao.com/2019/02/12/how-to-keep-off-trap-of-defer/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门8-Channel","slug":"Go/Go入门8-Channel","date":"2021-07-03T07:41:44.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/07/03/Go/Go入门8-Channel/","link":"","permalink":"http://xboom.github.io/2021/07/03/Go/Go%E5%85%A5%E9%97%A88-Channel/","excerpt":"","text":"channel底层数据结构 12345678910111213141516171819202122232425type hchan struct &#123; // chan 里元素数量 qcount uint // chan 底层循环数组的长度 dataqsiz uint // 指向底层循环数组的指针 // 只针对有缓冲的 channel buf unsafe.Pointer // chan 中元素大小 elemsize uint16 // chan 是否被关闭的标志 closed uint32 // chan 中元素类型 elemtype *_type // element type // 已发送元素在循环数组中的索引(第几个位置 1 ~ len) sendx uint // send index // 已接收元素在循环数组中的索引第几个位置 1 ~ len) recvx uint // receive index // 等待接收的 goroutine 队列 recvq waitq // list of recv waiters // 等待发送的 goroutine 队列 sendq waitq // list of send waiters // 保护 hchan 中所有字段 lock mutex&#125; 其中需要注意的是： buf指向底层循环数组，如果是非缓冲则指向hchan地址 sendq，recvq 分别表示被阻塞的 goroutine，这些 goroutine 由于尝试读取 channel 或向 channel 发送数据而被阻塞(部分通过直接复制memmove的方式传输未被阻塞，也就不在链表中) waitq 是 sudog 的一个双向链表，而 sudog 实际上是对 goroutine 的一个封装 1234type waitq struct &#123; first *sudog last *sudog&#125; lock 用来保证每个读 channel 或写 channel 的操作都是原子的 chan初创建 chan的创建分为含有缓冲区和非缓冲区，都通过 func makechan(t *chantype, size int64) *hchan实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func makechan64(t *chantype, size int64) *hchan &#123; if int64(int(size)) != size &#123; panic(plainError(\"makechan: size out of range\")) &#125; return makechan(t, int(size))&#125;func makechan(t *chantype, size int) *hchan &#123; elem := t.elem //1. 安全检查 if elem.size &gt;= 1&lt;&lt;16 &#123; throw(\"makechan: invalid channel element type\") &#125; if hchanSize%maxAlign != 0 || elem.align &gt; maxAlign &#123; throw(\"makechan: bad alignment\") &#125; //2. 元素分配是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem &gt; maxAlloc-hchanSize || size &lt; 0 &#123; panic(plainError(\"makechan: size out of range\")) &#125; var c *hchan switch &#123; case mem == 0: //a. 非缓冲或者元素大小为0(struct&#123;&#125;) // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. c.buf = c.raceaddr() case elem.ptrdata == 0: //b. 不含有指针，buf指向数组起始阶段 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: //c. 元素含有指针 c = new(hchan) c.buf = mallocgc(mem, elem, true) &#125; c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(&amp;c.lock, lockRankHchan) ... return c&#125; 以上有几点注意： 针对创建不同的chan初始化过程 非缓冲chan 只需要分配一个hchanSize 缓冲区chan 不含有指针，则分配一个连续的内存 缓冲区chan 含有指针，分别分配hchan和 指针需要的内存大小 chan 分配在堆中，返回一个指针*hchan sendq 和 recvq 链表由goroutine初始化时候创建，不在这里创建 chan接收 接收的操作有两种写法 123456789// entry points for &lt;- c from compiled codefunc chanrecv1(c *hchan, elem unsafe.Pointer) &#123; chanrecv(c, elem, true)&#125;func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) &#123; _, received = chanrecv(c, elem, true) return&#125; received 反应channel是否被关闭 接收值会放到elem所指向的指针，如果忽略接收值，则elem为nil 第三个参数 都使用 true 表示阻塞模式， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/* 1. 如果 ep 是 nil，说明忽略了接收值。 2. 如果 block == false，即非阻塞型接收，在没有数据可接收的情况下，返回 (false, false) 3. 否则，如果 c 处于关闭状态，将 ep 指向的地址清零，返回 (true, false)// 否则，用返回值填充 ep 指向的内存地址。返回 (true, true)// 如果 ep 非空，则应该指向堆或者函数调用者的栈*///判断hchan是否有数据func empty(c *hchan) bool &#123; // dataqsiz 一旦初始化chan就不会变化 if c.dataqsiz == 0 &#123; //如果是非缓冲，当存储发送挂起协程的sendq链表为空，表示没有数据 return atomic.Loadp(unsafe.Pointer(&amp;c.sendq.first)) == nil &#125; //缓冲区则判断是否有数据 return atomic.Loaduint(&amp;c.qcount) == 0&#125;// chanbuf(c, i) 指向c的缓冲区第i个元素func chanbuf(c *hchan, i uint) unsafe.Pointer &#123; return add(c.buf, uintptr(i)*uintptr(c.elemsize))&#125;func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; //忽略debug //如果是 nil的channel if c == nil &#123; //如果不阻塞，直接返回 (false, false) if !block &#123; return &#125; //否则，接收一个nil的channel, goroutine 挂起 gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) //不会执行到这里 throw(\"unreachable\") &#125; // 非阻塞模式下，快速检查不用获取锁 if !block &amp;&amp; empty(c) &#123; if atomic.Load(&amp;c.closed) == 0 &#123; //如果chan没有关闭，返回(false, false) return &#125; //这里又做了一次为空判断，防止在关闭检查的时候收到的数据 if empty(c) &#123; // The channel is irreversibly closed and empty. if ep != nil &#123; //如果为空，则返回一个默认值 typedmemclr(c.elemtype, ep) &#125; return true, false &#125; &#125; lock(&amp;c.lock) //获取原子锁 //如果关闭了，且不含有缓冲数据，则ep指向默认值，返回(true, false) if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; unlock(&amp;c.lock) if ep != nil &#123; typedmemclr(c.elemtype, ep) &#125; return true, false &#125; //则获取sendq中阻塞的发送协程进行接收 if sg := c.sendq.dequeue(); sg != nil &#123; //找到一个等待的发送人。 如果缓冲区大小为 0，则接收值直接来自发送者。 //否则，从队列头接收并将发送者的值添加到队列的尾部 recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; //如果没有发送者，且当前缓冲区有数据， //则将数据复制给ep，且清理对应位置缓冲区数据，qcount-- if c.qcount &gt; 0 &#123; //1. 获取对应缓冲区数据 qp := chanbuf(c, c.recvx) //2. 将数据复制给ep if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; //3. 清理对应的缓冲区数据 typedmemclr(c.elemtype, qp) //4. 接收索引+1，如果 等于缓冲区大小，表示结束，从头开始 c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; //4. 缓冲区数目-1 并释放原子锁 c.qcount-- unlock(&amp;c.lock) return true, true &#125; if !block &#123; unlock(&amp;c.lock) return false, false &#125; //没有数据，也没有发送者，则将要阻塞这个协程 gp := getg() //获取当前协程指针 mysg := acquireSudog() // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. //保存代接收数据的地址 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil //进入等待链表中 c.recvq.enqueue(mysg) atomic.Store8(&amp;gp.parkingOnChan, 1) //将当前goroutine 挂起 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) //当goroutine被唤起 if mysg != gp.waiting &#123; //当前协程等待的不是创建的 mysg throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil gp.activeStackChans = false closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed&#125; 需要注意的是 同一个 chan 不能被重新开启 在缓冲区chan buf满了的情况下，发送协程阻塞，则接收者还是会优先处理缓冲区数据 12345678910111213141516171819202122232425262728293031323334353637383940//如果是非缓冲型的，就直接从发送者的栈拷贝到接收者的栈func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) &#123; src := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125;//接收数据的时候发现有协程阻塞func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; if c.dataqsiz == 0 &#123; if ep != nil &#123; //非缓冲直接从sender拷贝到ep recvDirect(c.elemtype, sg, ep) &#125; &#125; else &#123;//非缓冲区 //1. 获取缓冲区数据 qp := chanbuf(c, c.recvx) // 将数据从缓冲区拷贝给receiver if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 将sender的数据拷贝到缓冲区 typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ //索引+1，则刚才加了数据需要循环一遍才能拿到 if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz &#125; sg.elem = nil gp := sg.g // 解锁 unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒发送的 goroutine。需要等到调度器的光临 goready(gp, skip+1)&#125; chan发送 ch &lt;- 3 最终会转换成 chansend 函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105func full(c *hchan) bool &#123; if c.dataqsiz == 0 &#123; //如果是非缓冲区，且没有接收协程，则表示满了 return c.recvq.first == nil &#125; //如果是缓冲区，当缓冲数据等于缓冲区大小 则表示满了 return c.qcount == c.dataqsiz&#125;func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; //如果为空 if c == nil &#123; if !block &#123; //非阻塞模式下 return false &#125; //阻塞模式下，直接挂起 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\"unreachable\") &#125; //... // 如果 channel 未关闭且 channel 没有多余的缓冲空间。直接返回false if !block &amp;&amp; c.closed == 0 &amp;&amp; full(c) &#123; return false &#125; //1. 加锁 lock(&amp;c.lock) //如果channel 关闭，则直接panic if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError(\"send on closed channel\")) &#125; //如果接收协程阻塞存在，说明缓冲区没有数据，直接将sender数据拷贝给receiver if sg := c.recvq.dequeue(); sg != nil &#123; // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; //如果有缓冲空间 if c.qcount &lt; c.dataqsiz &#123; //1. 直接获取缓冲发送位置 qp := chanbuf(c, c.sendx) //2. 将数据直接拷贝进缓冲区 typedmemmove(c.elemtype, qp, ep) //3. 发送位置+1(表示下一个进来的数据存放的位置) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; //3. 数量+1 c.qcount++ unlock(&amp;c.lock) return true &#125; //非阻塞模式直接返回false if !block &#123; unlock(&amp;c.lock) return false &#125; //阻塞当前协程 gp := getg() mysg := acquireSudog() // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) //将协程加入到等待队列中 atomic.Store8(&amp;gp.parkingOnChan, 1) //挂起当前协程 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // 确保正在发送的值保持活动状态，直到接收者复制它 KeepAlive(ep) // 被唤起 if mysg != gp.waiting &#123; throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil gp.activeStackChans = false if gp.param == nil &#123; if c.closed == 0 &#123; throw(\"chansend: spurious wakeup\") &#125; // 被唤醒后，channel 关闭了。坑爹啊，panic panic(plainError(\"send on closed channel\")) &#125; gp.param = nil mysg.c = nil releaseSudog(mysg) return true&#125; 需要注意的是： KeepAlive 的作用和原理是? 123456789101112131415161718192021222324252627282930313233func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; //sg.elem 指向接收到的值存放的位置，如 val &lt;- ch，指的就是 &amp;val if sg.elem != nil &#123; // 直接拷贝内存（从发送者到接收者） sendDirect(c.elemtype, sg, ep) sg.elem = nil &#125; // sudog 上绑定的 goroutine gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒接收的 goroutine. skip 和打印栈相关，暂时不理会 goready(gp, skip+1)&#125;// 向一个非缓冲型的 channel 发送数据、从一个无元素的（非缓冲型或缓冲型但空）的 channel// 接收数据，都会导致一个 goroutine 直接操作另一个 goroutine 的栈// 由于 GC 假设对栈的写操作只能发生在 goroutine 正在运行中并且由当前 goroutine 来写// 所以这里实际上违反了这个假设。可能会造成一些问题，所以需要用到写屏障来规避func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) &#123; // src 在当前 goroutine 的栈上，dst 是另一个 goroutine 的栈 // 直接进行内存\"搬迁\" // 如果目标地址的栈发生了栈收缩，当我们读出了 sg.elem 后 // 就不能修改真正的 dst 位置的值了 // 因此需要在读和写之前加上一个屏障 dst := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125; 关闭chan 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func closechan(c *hchan) &#123; if c == nil &#123; //关闭一个空chan会panic panic(plainError(\"close of nil channel\")) &#125; lock(&amp;c.lock) if c.closed != 0 &#123; //关闭一个关闭的chan会panic unlock(&amp;c.lock) panic(plainError(\"close of closed channel\")) &#125; c.closed = 1 //修改chan状态为关闭 var glist gList // release all readers for &#123; //1. 所有接收协程出链表 sg := c.recvq.dequeue() if sg == nil &#123; break &#125; //2. 如果元素不为空，则清除并设置为空 if sg.elem != nil &#123; typedmemclr(c.elemtype, sg.elem) sg.elem = nil &#125; if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil glist.push(gp) &#125; // release all writers (they will panic) for &#123; //从发送链表中获取协程 sg := c.sendq.dequeue() if sg == nil &#123; break &#125; sg.elem = nil if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil //形成协程链表 glist.push(gp) &#125; unlock(&amp;c.lock) // Ready all Gs now that we've dropped the channel lock. for !glist.empty() &#123; // 取最后一个 gp := glist.pop() gp.schedlink = 0 //环形协程 goready(gp, 3) &#125;&#125; chan应用 针对不同的chan会有不同的效果： 控制并发数 12345678910111213var limit = make(chan int, 3)func main() &#123; // ………… for _, w := range work &#123; go func() &#123; limit &lt;- 1 w() &lt;-limit &#125;() &#125; // …………&#125; 参考链接 Go问题集 https://qcrao.com/2019/07/22/dive-into-go-channel/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门19-编程模式","slug":"Go/Go入门19-编程模式","date":"2021-06-18T17:05:45.000Z","updated":"2022-10-14T15:31:36.000Z","comments":true,"path":"2021/06/19/Go/Go入门19-编程模式/","link":"","permalink":"http://xboom.github.io/2021/06/19/Go/Go%E5%85%A5%E9%97%A819-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"深度比较 12345678910111213141516171819202122import ( \"fmt\" \"reflect\")func main() &#123; v1 := data&#123;&#125; v2 := data&#123;&#125; fmt.Println(\"v1 == v2:\",reflect.DeepEqual(v1,v2)) //prints: v1 == v2: true m1 := map[string]string&#123;\"one\": \"a\",\"two\": \"b\"&#125; m2 := map[string]string&#123;\"two\": \"b\", \"one\": \"a\"&#125; fmt.Println(\"m1 == m2:\",reflect.DeepEqual(m1, m2)) //prints: m1 == m2: true s1 := []int&#123;1, 2, 3&#125; s2 := []int&#123;1, 2, 3&#125; fmt.Println(\"s1 == s2:\",reflect.DeepEqual(s1, s2)) //prints: s1 == s2: true&#125; 接口编程 1234567891011121314151617type Shape interface &#123; Sides() int Area() int&#125;type Square struct &#123; len int&#125;func (s* Square) Sides() int &#123; return 4&#125;func main() &#123; s := Square&#123;len: 5&#125; fmt.Printf(\"%d\\n\",s.Sides())&#125;var _ Shape = (*Square)(nil) //接口实现校验 声明一个 _ 变量（没人用）会把一个 nil 的空指针从 Square 转成 Shape，这样，如果没有实现完相关的接口方法，编译器就会报错： cannot use (*Square)(nil) (type *Square) as type Shape in assignment: *Square does not implement Shape (missing Area method) 性能 如果需要把数字转换成字符串，使用 strconv.Itoa() 比 fmt.Sprintf() 要快一倍左右。 尽可能避免把String转成[]Byte ，这个转换会导致性能下降。 如果在 for-loop 里对某个 Slice 使用 append()，请先把 Slice 的容量扩充到位，这样可以避免内存重新分配以及系统自动按 2 的 N 次方幂进行扩展但又用不到的情况，从而避免浪费内存。 使用StringBuffer 或是StringBuild 来拼接字符串，性能会比使用 + 或 +=高三到四个数量级。 尽可能使用并发的 goroutine，然后使用 sync.WaitGroup 来同步分片操作。 避免在热代码中进行内存分配，这样会导致 gc 很忙。尽可能使用 sync.Pool 来重用对象。 使用 lock-free 的操作，避免使用 mutex，尽可能使用 sync/Atomic包（关于无锁编程的相关话题，可参看《无锁队列实现》或《无锁 Hashmap 实现》）。 使用 I/O 缓冲，I/O 是个非常非常慢的操作，使用 bufio.NewWrite() 和 bufio.NewReader() 可以带来更高的性能。 对于在 for-loop 里的固定的正则表达式，一定要使用 regexp.Compile() 编译正则表达式。性能会提升两个数量级。 需要更高性能的协议，就要考虑使用 protobuf 或 msgp 而不是 JSON，因为 JSON 的序列化和反序列化里使用了反射。 使用 Map 的时候，使用整型的 key 会比字符串的要快，因为整型比较比字符串比较要快 错误检查 1234567891011121314151617181920func parse(r io.Reader) (*Point, error) &#123; var p Point if err := binary.Read(r, binary.BigEndian, &amp;p.Longitude); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.Latitude); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.Distance); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationGain); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationLoss); err != nil &#123; return nil, err &#125;&#125; 太多的 if err != nil {} 12345678910111213141516171819202122func parse(r io.Reader) (*Point, error) &#123; var p Point var err error read := func(data interface&#123;&#125;) &#123; if err != nil &#123; //如果失败则不执行 return &#125; err = binary.Read(r, binary.BigEndian, data) &#125; read(&amp;p.Longitude) read(&amp;p.Latitude) read(&amp;p.Distance) read(&amp;p.ElevationGain) read(&amp;p.ElevationLoss) if err != nil &#123; return &amp;p, err &#125; return &amp;p, nil&#125; 上述代码存在一个变量err和一个内部函数 read 12345678910111213141516171819202122232425262728type Reader struct &#123; r io.Reader err error&#125;func (r *Reader) read(data interface&#123;&#125;) &#123; if r.err == nil &#123; r.err = binary.Read(r.r, binary.BigEndian, data) &#125;&#125;func parse(input io.Reader) (*Point, error) &#123; var p Point r := Reader&#123;r: input&#125; r.read(&amp;p.Longitude) r.read(&amp;p.Latitude) r.read(&amp;p.Distance) r.read(&amp;p.ElevationGain) r.read(&amp;p.ElevationLoss) if r.err != nil &#123; return nil, r.err &#125; return &amp;p, nil&#125; 主要应用于相同业务处理场景 错误包装 1234567891011121314import \"github.com/pkg/errors\" //第三方库//错误包装if err != nil &#123; return errors.Wrap(err, \"read failed\")&#125;// Cause接口switch err := errors.Cause(err).(type) &#123;case *MyError: // handle specificallydefault: // unknown error&#125; 多参数 1234567891011121314func main() &#123; slice1 := make([]interface&#123;&#125;, 0) slice2 := make([]interface&#123;&#125;, 1) //不要这样使用 会初始化1的nil slice1 = append(slice1, 1, \"2\") slice2 = append(slice2, 1, \"2\") test(slice1) //args [[1 2]] num:1 test(slice1...) //args [1 2] num:2 test(slice2) //args [[&lt;nil&gt; 1 2]] num:1 test(slice2...) //args [&lt;nil&gt; 1 2] num:3 &#125;func test(args ...interface&#123;&#125;) &#123; fmt.Printf(\"args %v num:%d \\n\", args, len(args))&#125; Map-Reduce 1234567891011121314151617181920212223242526272829303132333435363738func main() &#123; var list = []string&#123;\"Hao\", \"Chen\", \"MegaEase\"&#125; x := Reduce(list, func(s string) int &#123; return len(s) &#125;) fmt.Printf(\"%v\\n\", x) //15 var intset = []int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125; out := Filter(intset, func(n int) bool &#123; return n%2 == 1 &#125;) fmt.Printf(\"%v\\n\", out) //[1 3 5 7 9] out = Filter(intset, func(n int) bool &#123; return n &gt; 5 &#125;) fmt.Printf(\"%v\\n\", out) //[6 7 8 9 10]&#125;func Reduce(arr []string, fn func(s string) int) int &#123; sum := 0 for _, it := range arr &#123; sum += fn(it) &#125; return sum&#125;func Filter(arr []int, fn func(n int) bool) []int &#123; var newArray = []int&#123;&#125; for _, it := range arr &#123; if fn(it) &#123; newArray = append(newArray, it) &#125; &#125; return newArray&#125; 健壮的泛型Map 123456789101112131415161718192021222324func verifyFuncSignature(fn reflect.Value, types ...reflect.Type) bool &#123; //Check it is a funciton if fn.Kind() != reflect.Func &#123; return false &#125; // NumIn() - returns a function type's input parameter count. // NumOut() - returns a function type's output parameter count. if (fn.Type().NumIn() != len(types)-1) || (fn.Type().NumOut() != 1) &#123; return false &#125; // In() - returns the type of a function type's i'th input parameter. for i := 0; i &lt; len(types)-1; i++ &#123; if fn.Type().In(i) != types[i] &#123; return false &#125; &#125; // Out() - returns the type of a function type's i'th output parameter. outType := types[len(types)-1] if outType != nil &amp;&amp; fn.Type().Out(0) != outType &#123; return false &#125; return true&#125; 健壮的泛型Reduce 123456789101112131415161718192021222324252627282930313233func Reduce(slice, pairFunc, zero interface&#123;&#125;) interface&#123;&#125; &#123; sliceInType := reflect.ValueOf(slice) if sliceInType.Kind() != reflect.Slice &#123; panic(\"reduce: wrong type, not slice\") &#125; len := sliceInType.Len() if len == 0 &#123; return zero &#125; else if len == 1 &#123; return sliceInType.Index(0) &#125; elemType := sliceInType.Type().Elem() fn := reflect.ValueOf(pairFunc) if !verifyFuncSignature(fn, elemType, elemType, elemType) &#123; t := elemType.String() panic(\"reduce: function must be of type func(\" + t + \", \" + t + \") \" + t) &#125; var ins [2]reflect.Value ins[0] = sliceInType.Index(0) ins[1] = sliceInType.Index(1) out := fn.Call(ins[:])[0] for i := 2; i &lt; len; i++ &#123; ins[0] = out ins[1] = sliceInType.Index(i) out = fn.Call(ins[:])[0] &#125; return out.Interface()&#125; 健壮的泛型 Filter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func Filter(slice, function interface&#123;&#125;) interface&#123;&#125; &#123; result, _ := filter(slice, function, false) return result&#125;func FilterInPlace(slicePtr, function interface&#123;&#125;) &#123; in := reflect.ValueOf(slicePtr) if in.Kind() != reflect.Ptr &#123; panic(\"FilterInPlace: wrong type, \" + \"not a pointer to slice\") &#125; _, n := filter(in.Elem().Interface(), function, true) in.Elem().SetLen(n)&#125;var boolType = reflect.ValueOf(true).Type()func filter(slice, function interface&#123;&#125;, inPlace bool) (interface&#123;&#125;, int) &#123; sliceInType := reflect.ValueOf(slice) if sliceInType.Kind() != reflect.Slice &#123; panic(\"filter: wrong type, not a slice\") &#125; fn := reflect.ValueOf(function) elemType := sliceInType.Type().Elem() if !verifyFuncSignature(fn, elemType, boolType) &#123; panic(\"filter: function must be of type func(\" + elemType.String() + \") bool\") &#125; var which []int for i := 0; i &lt; sliceInType.Len(); i++ &#123; if fn.Call([]reflect.Value&#123;sliceInType.Index(i)&#125;)[0].Bool() &#123; which = append(which, i) &#125; &#125; out := sliceInType if !inPlace &#123; out = reflect.MakeSlice(sliceInType.Type(), len(which), len(which)) &#125; for i := range which &#123; out.Index(i).Set(sliceInType.Index(which[i])) &#125; return out.Interface(), len(which)&#125; 反射不适用于高性能的地方 类型检查 Type Assert 对变量进行 .(type)的转型操作返回两个值，分别是 variable 和 error variable 是被转换好的类型，error 表示如果不能转换类型，则会报错 12345678910111213141516171819202122232425//Container is a generic container, accepting anything.type Container []interface&#123;&#125;//Put adds an element to the container.func (c *Container) Put(elem interface&#123;&#125;) &#123; *c = append(*c, elem)&#125;//Get gets an element from the container.func (c *Container) Get() interface&#123;&#125; &#123; elem := (*c)[0] *c = (*c)[1:] return elem&#125;intContainer := &amp;Container&#123;&#125;intContainer.Put(7)intContainer.Put(42)// assert that the actual type is intelem, ok := intContainer.Get().(int)if !ok &#123; fmt.Println(\"Unable to read an int from intContainer\")&#125;fmt.Printf(\"assertExample: %d (%T)\\n\", elem, elem) 反射 1234567891011121314151617181920212223242526type Container struct &#123; s reflect.Value&#125;func NewContainer(t reflect.Type, size int) *Container &#123; if size &lt;=0 &#123; size=64 &#125; return &amp;Container&#123; s: reflect.MakeSlice(reflect.SliceOf(t), 0, size), &#125;&#125;func (c *Container) Put(val interface&#123;&#125;) error &#123; if reflect.ValueOf(val).Type() != c.s.Type().Elem() &#123; return fmt.Errorf(“Put: cannot put a %T into a slice of %s\", val, c.s.Type().Elem())) &#125; c.s = reflect.Append(c.s, reflect.ValueOf(val)) return nil&#125;func (c *Container) Get(refval interface&#123;&#125;) error &#123; if reflect.ValueOf(refval).Kind() != reflect.Ptr || reflect.ValueOf(refval).Elem().Type() != c.s.Type().Elem() &#123; return fmt.Errorf(\"Get: needs *%s but got %T\", c.s.Type().Elem(), refval) &#125; reflect.ValueOf(refval).Elem().Set( c.s.Index(0) ) c.s = c.s.Slice(1, c.s.Len()) return nil&#125; 参考文献 https://coolshell.cn/articles/21128.html https://github.com/robpike","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Redis入门0-设计与实现","slug":"Redis/Redis入门0-设计与实现","date":"2021-04-13T16:07:16.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2021/04/14/Redis/Redis入门0-设计与实现/","link":"","permalink":"http://xboom.github.io/2021/04/14/Redis/Redis%E5%85%A5%E9%97%A80-%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"前言 Redis的设计与实现主要关注以下几点： 高性能：线程模型(CPU)、数据结构(内存)、AOF(IO)、epoll网络框架(网络) 高可靠：主从复制、哨兵模式、RDB 高课拓展：数据分片，负载均衡 AOF AOF是写后日志，先执行命令，把数据写入内存，然后将命令已文本的形式追加到日志文件中(主线程执行) 优点： 只有命令能执行成功，才会被记录到日志中，否则直接返回错误，避免记录错误日志。 不会阻塞当前操作命令 缺点： 会阻塞下一个操作命令 如果执行完操作命令还没来得及写入文件就宕机，则会有丢失的风险 提供三种AOF文件的写回操作： Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘 可以做到基本不丢失数据，但太影响主线程性能 Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘 减少对系统性能的影响，如果发生宕机，上一秒内未落盘的命令操作仍然会丢失 No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 落盘的时机随机，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了 AOF重写 AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。 AOF文件过大存在的问题： 文件系统本身对文件大小有限制，无法保存过大的文件； 如果文件太大，之后再往里面追加命令记录的话，效率也会变低； 如果日志文件太大，整个恢复过程就会非常缓慢，会影响到 Redis 的正常使用 使用AOF重写过程解决文件过大的问题 虽然 AOF 重写后，日志文件会缩小，但把整个数据库的操作日志都写回磁盘是一个非常耗时的过程，所以AOF重写由后台子进程 bgrewriteaof 来完成的，过程为『一次拷贝，两处日志』 一个拷贝(内存)：每次执行重写，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，内存包含数据库最新数据，将其记入重写日志。 两处日志(写时复制)： 第一处日志指正在使用的 AOF 日志，新操作被写到它的缓冲区。即使宕机，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。 第二处日志指新的 AOF 重写日志。新操作也会被写到重写日志的缓冲区，以保证数据库最新状态的记录。此时就可以用新的 AOF 文件替代旧文件了 Redis机器上最好关闭内存大页机制(Huge Page，页面大小2M)，进程申请内存时阻塞的概率降低 缺点是：AOF记录的是操作命令，而不是实际的数据，所以故障恢复时，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。则使用内存快照RDB(Redis Database) RDB RDB记录的是某一时刻的数据(全量快照)，并不是操作。在数据恢复时，可直接把 RDB 文件读入内存完成恢复 RDB提供了两个命令来生成RDB文件：save和bgsave Save:在主线程中执行，会导致阻塞 bgsave:创建一个子进程，专门用于写入RDB文件(默认配置) 生成RDB文件是需要时间的，如果生成的这段时间时间里又有新数据怎么办？ 答：Redis 使用写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作 AOF与RDB混合 跟 AOF 相比，快照的恢复速度快，但快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销 使用参数：aof-use-rdb-preamble yes 过程如下： T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。这样快照不用频繁操作，AOF也只需记录两次快照之间的操作，避免重写开销","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Go入门2-Slice","slug":"Go/Go入门2-Slice","date":"2020-12-12T08:39:51.000Z","updated":"2023-03-12T02:09:38.431Z","comments":true,"path":"2020/12/12/Go/Go入门2-Slice/","link":"","permalink":"http://xboom.github.io/2020/12/12/Go/Go%E5%85%A5%E9%97%A82-Slice/","excerpt":"","text":"Go 语言的函数参数传递，只有值传递，没有引用传递 数组与切片 在 Go 中，与 C 数组变量隐式作为指针使用不同，Go 数组是值类型，赋值和函数传参操作都会复制整个数组数据 123456789101112131415161718192021222324252627func main() &#123; arrayA := [2]int&#123;100, 200&#125; var arrayB [2]int arrayB = arrayA fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB) arrayA[0] = 200 testArray(arrayA) fmt.Println(arrayA) fmt.Println(arrayB) testArray(arrayB)&#125;func testArray(x [2]int) &#123; fmt.Printf(\"func Array : %p , %p, %p, %v\\n\", &amp;x, &amp;x[0], &amp;x[1], x) x[0] = 300&#125;//运行结果为：arrayA : 0xc00010c010 , 0xc00010c010, 0xc00010c018, [100 200]arrayB : 0xc00010c020 , 0xc00010c020, 0xc00010c028, [100 200]func Array : 0xc00010c060 , 0xc00010c060, 0xc00010c068, [200 200][200 200][100 200]func Array : 0xc00010c0a0 , 0xc00010c0a0, 0xc00010c0a8, [100 200] 分析： 当直接使用 arrayB = arrayA 进行的是值复制，也就是说Go数组是值类型 将数组赋值给函数的时候，也是对整个数组进行复制，函数内改变值并不会改变外面的数组 上面的例子换成切片会怎么样？ 1234567891011121314151617181920212223242526func main() &#123; arrayA := []int&#123;100, 200, 300&#125; arrayB := arrayA[0:2] fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v, %d\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB, cap(arrayB)) arrayA[0] = 200 testArray(arrayB) fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v, %d\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB, cap(arrayB))&#125;func testArray(x []int) &#123; fmt.Printf(\"func Array : %p , %p, %p, %v %d\\n\", &amp;x, &amp;x[0], &amp;x[1], x, cap(x)) x[0] = 300 x = append(x, 400) fmt.Printf(\"func Array : %p , %p, %p, %v %d\\n\", &amp;x, &amp;x[0], &amp;x[1], x, cap(x))&#125;//运行结果为:arrayA : 0xc00000c060 , 0xc000014140, 0xc000014148, [100 200 300]arrayB : 0xc00000c080 , 0xc000014140, 0xc000014148, [100 200], 3func Array : 0xc00000c0e0 , 0xc000014140, 0xc000014148, [200 200] 3func Array : 0xc00000c0e0 , 0xc000014140, 0xc000014148, [300 200 400] 3arrayA : 0xc00000c060 , 0xc000014140, 0xc000014148, [300 200 400]arrayB : 0xc00000c080 , 0xc000014140, 0xc000014148, [300 200], 3 分析： 当直接使用 arrayB = arrayA 进行的是其实是值复制，地址空间不一致，但是指向的内存空间一致 切片做参数传递的时候,但函数内容修改切片内容并不会影响外部切片，改变底层数组会影响外部切片 切片数据结构 切片（slice）切片是一个引用类型，是对数组一个连续片段的引用。这个片段是由起始和终止索引标识的一些项的子集。终止索引标识的项不包括在切片内(左闭右开)。切片提供了一个与指向数组的动态窗口。 给定项的切片索引可能比相关数组的相同元素的索引小。和数组不同的是，切片的长度可以在运行时修改，最小为 0 最大为相关数组的长度：切片是一个长度可变的数组 12345type slice struct &#123; array unsafe.Pointer //指向数组的指针 len int //当前切片长度 cap int //当前切片容量&#125; uintptr is an integer type that is large enough to hold the bit pattern of any pointer unsafe.Pointer 通用指针,类似C中的 void * 切片创建 首先这里会有一个常见函数，math.MulUintptr 将两个参数相乘来判断是否溢出 123456789//MulUintptr返回a * b以及乘法是否溢出。在受支持的平台上，这是编译器固有的功能。//true 越界;false 没有越界func MulUintptr(a, b uintptr) (uintptr, bool) &#123; if a|b &lt; 1&lt;&lt;(4*sys.PtrSize) || a == 0 &#123; return a * b, false &#125; overflow := b &gt; MaxUintptr/a return a * b, overflow&#125; 解析： sys.PtrSize 在64位机器中为8 12const PtrSize = 4 &lt;&lt; (^uintptr(0) &gt;&gt; 63) // unsafe.Sizeof(uintptr(0)) but an ideal constconst MaxUintptr = ^uintptr(0) &lt; 1&lt;&lt;(4*sys.PtrSize)相当于&lt; 1&lt;&lt;32 ,而8位计算机中最大的是 2^64-1,所以判断a和b都小于2^32即可 否则 b &gt; MaxUintptr/a 如果 b 大于最大值除以a,则说明 a*b 超过最大值即越界 make与字面量切片 支持通过make或字面量的方式进行创建切片 12slice1 := make([]int, 4, 6) //makeslice2 := []int&#123;1,2,3,4&#125; //字面量，注意[]内不要写容量，否则是数组 再来看分片函数 makeslice 12345678910111213141516func makeslice(et *_type, len, cap int) unsafe.Pointer &#123; mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem &gt; maxAlloc || len &lt; 0 || len &gt; cap &#123; mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem &gt; maxAlloc || len &lt; 0 &#123; panicmakeslicelen() &#125; panicmakeslicecap() &#125; // 分配内存 // 小对象从当前P 的cache中空闲数据中分配 // 大的对象 (size &gt; 32KB) 直接从heap中分配 // runtime/malloc.go return mallocgc(mem, et, true)&#125; 解析： 根据 数据类型大小 x 切片容量 判断是否会越界 如果 越界或超过最大分配长度maxAlloc(32位与64位不一致)，长度大于容器。则尝试 根据 数据类型大小 x 切片长度 判断溢出 如果 长度溢出则 panicmakeslicelen: panic报错 “makeslice: len out of range” 如果 容量溢出则 panicmakeslicecap: panic报错 “makeslice: cap out of range” 否则分配内存： 1func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer &#123;&#125; 所以切片其实分配了容量大小的内存，只是访问不到，也被初始化 空切片与nil切片 123var slice []int //nil切片silce := make( []int , 0 ) //空切片slice := []int&#123; &#125; //空切片 空切片和 nil 切片的区别在于：空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素。 不管是使用 nil 切片还是空切片，对其调用内置函数 append，len 和 cap 的效果都是一样的 切片扩容 扩容原理 growslice用来处理在使用append时候的切片扩容，那么它的规则如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100func growslice(et *_type, old slice, cap int) slice &#123; if raceenabled &#123; callerpc := getcallerpc() racereadrangepc(old.array, uintptr(old.len*int(et.size)), callerpc, funcPC(growslice)) &#125; if msanenabled &#123; msanread(old.array, uintptr(old.len*int(et.size))) &#125; //当新容量比旧容量还有小的时候，直接panic报错 if cap &lt; old.cap &#123; panic(errorString(\"growslice: cap out of range\")) &#125; if et.size == 0 &#123; //如果切片元素大小为0，还调用了扩容方法，那么就新生成一个新的容量的切片返回 return slice&#123;unsafe.Pointer(&amp;zerobase), old.len, cap&#125; &#125; newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap &#123; //1. 当指定容量大于旧切片2倍，则使用指定容量 newcap = cap &#125; else &#123; //2. 指定容量小于旧切片2倍 if old.len &lt; 1024 &#123; newcap = doublecap //2.1. 如果旧切片长度小于1024，则使用容量为旧容量两倍 &#125; else &#123; //2.2. 如果旧切片长度大于1024且小于指定容量，则新容量循环增加自身的四分之一直到大于指定容量 for 0 &lt; newcap &amp;&amp; newcap &lt; cap &#123; newcap += newcap / 4 &#125; if newcap &lt;= 0 &#123; //2.3. 如果旧切片长度等于0，则新容量等于旧容量 newcap = cap &#125; &#125; &#125; var overflow bool var lenmem, newlenmem, capmem uintptr // Specialize for common values of et.size. // For 1 we don't need any division/multiplication. // For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch &#123; case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) &gt; maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) &gt; maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): var shift uintptr if sys.PtrSize == 8 &#123; // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) &amp; 63 &#125; else &#123; shift = uintptr(sys.Ctz32(uint32(et.size))) &amp; 31 &#125; lenmem = uintptr(old.len) &lt;&lt; shift newlenmem = uintptr(cap) &lt;&lt; shift capmem = roundupsize(uintptr(newcap) &lt;&lt; shift) overflow = uintptr(newcap) &gt; (maxAlloc &gt;&gt; shift) newcap = int(capmem &gt;&gt; shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) &#125; if overflow || capmem &gt; maxAlloc &#123; panic(errorString(\"growslice: cap out of range\")) &#125; var p unsafe.Pointer if et.ptrdata == 0 &#123; //先将 P 地址加上新的容量得到新切片容量的地址，然后将新切片容量地址后面的 capmem-newlenmem 个 bytes 这块内存初始化。为之后继续 append() 操作腾出空间 p = mallocgc(capmem, nil, false) // memclrNoHeapPointers clears n bytes starting at ptr memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) &#125; else &#123; // 重新申请 capmen 这个大的内存地址，并且初始化为0值 p = mallocgc(capmem, et, true) if lenmem &gt; 0 &amp;&amp; writeBarrier.enabled &#123; // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem-et.size+et.ptrdata) &#125; &#125; //将 lenmem 这个多个 bytes 从 old.array地址 拷贝到 p 的地址处 memmove(p, old.array, lenmem) return slice&#123;p, old.len, newcap&#125; //返回新切片&#125; 上述就是扩容的实现。主要需要关注的有两点: 扩容时候的策略 首先判断，如果新申请容量（cap）大于2倍的旧容量（old.cap），最终容量（newcap）就是新申请的容量（cap） 否则判断，如果旧切片的长度小于1024，则最终容量(newcap)就是旧容量(old.cap)的两倍，即（newcap=doublecap） 否则判断，如果旧切片长度大于等于1024，则最终容量（newcap）从旧容量（old.cap）开始循环增加原来的 1/4，即（newcap=old.cap,for {newcap += newcap/4}）直到最终容量（newcap）大于等于新申请的容量(cap)，即（newcap &gt;= cap） 如果最终容量（cap）计算值溢出，则最终容量（cap）就是新申请容量（cap） 扩容是生成全新的内存地址还是在原来的地址后追加 扩容实例 实例1： 123456789101112131415161718192021222324252627func main() &#123; s := make([]int, 0) oldCap := cap(s) for i := 0; i &lt; 2048; i++ &#123; s = append(s, i) newCap := cap(s) if newCap != oldCap &#123; fmt.Printf(\"[%d -&gt; %4d] cap = %-4d | after append %-4d cap = %-4d\\n\", 0, i-1, oldCap, i, newCap) oldCap = newCap &#125; &#125;&#125;//运行结果为:[0 -&gt; 0] cap = 1 | after append 1 cap = 2 [0 -&gt; 1] cap = 2 | after append 2 cap = 4 [0 -&gt; 3] cap = 4 | after append 4 cap = 8 [0 -&gt; 7] cap = 8 | after append 8 cap = 16 [0 -&gt; 15] cap = 16 | after append 16 cap = 32 [0 -&gt; 31] cap = 32 | after append 32 cap = 64 [0 -&gt; 63] cap = 64 | after append 64 cap = 128 [0 -&gt; 127] cap = 128 | after append 128 cap = 256 [0 -&gt; 255] cap = 256 | after append 256 cap = 512 [0 -&gt; 511] cap = 512 | after append 512 cap = 1024[0 -&gt; 1023] cap = 1024 | after append 1024 cap = 1280[0 -&gt; 1279] cap = 1280 | after append 1280 cap = 1696[0 -&gt; 1695] cap = 1696 | after append 1696 cap = 2304 分析： 1280/1024 = 1.25 , 1696/1024 = 1.325, 2304/1696=1.3584,为什么每次增加的倍数都不一样呢？ * 实例2： 1234567891011121314func main() &#123; slice := []int&#123;10, 20, 30, 40&#125; newSlice := append(slice, 50) fmt.Printf(\"Before slice %v, Pointer %p, Pointer0 %p, len %d, cap %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"Before newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf(\"After slice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"After newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice))&#125;Before slice [10 20 30 40], Pointer 0xc00000c060, Pointer0 0xc000014140, len 4, cap 4Before newSlice = [10 20 30 40 50], Pointer = 0xc00000c080, Pointer0 0xc000016140, len = 5, cap = 8After slice = [10 20 30 40], Pointer = 0xc00000c060, Pointer0 0xc000014140, len = 4, cap = 4After newSlice = [10 30 30 40 50], Pointer = 0xc00000c080, Pointer0 0xc000016140, len = 5, cap = 8 分析： 当旧切片容量小于1024，新切片容量直接翻倍 这里分配了一个新地址，修改新分片，旧分片并不会改变 实例2： 12345678910111213141516171819func main() &#123; array := [4]int&#123;10, 20, 30, 40&#125; slice := array[0:2] newSlice := append(slice, 50) fmt.Printf(\"Before array %v, Poninter %p, Poninter0 %p \\n\", array, &amp;array, &amp;array[0]) fmt.Printf(\"Before slice %v, Pointer %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"Before newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf(\"After slice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"After newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) fmt.Printf(\"After array = %v, Poninter = %p, Pointer0 %p \\n\", array, &amp;array, &amp;array[0])&#125;Before array [10 20 50 40], Poninter 0xc000014140, Poninter0 0xc000014140 Before slice [10 20], Pointer 0xc00000c060, Pointer0 0xc000014140, len = 2, cap = 4Before newSlice = [10 20 50], Pointer = 0xc00000c080, Pointer0 0xc000014140, len = 3, cap = 4After slice = [10 30], Pointer = 0xc00000c060, Pointer0 0xc000014140, len = 2, cap = 4After newSlice = [10 30 50], Pointer = 0xc00000c080, Pointer0 0xc000014140, len = 3, cap = 4After array = [10 30 50 40], Poninter = 0xc000014140, Pointer0 0xc000014140 分析： 与实例1进行对比：由于slice还有容量可以扩容，所以执行 append() 操作以后，会在原数组上直接操作，这种情况下，扩容以后的数组还是指向原来的数组 实例1中由于没有容量进行扩容，所以执行append之后指向的就是一个全新的数组，修改值并不会影响原来的数组 由于数组是值类型，而切片是应引用类型，所以看到 &amp;array 与 &amp;array[0]的地址是一样的。但是 &amp;slice 与 &amp;slice[0]的地址是不一样的。另外两者指向的第一个值地址都是一样的，指向同一片内存。 由于容量导致结果不一致，极易产生bug。 实例3： 123456789101112131415161718192021222324252627func main() &#123; s := make([]int, 0) oldCap := cap(s) for i := 0; i &lt; 2048; i++ &#123; s = append(s, i) newCap := cap(s) if newCap != oldCap &#123; fmt.Printf(\"[%d -&gt; %4d] cap = %-4d | after append %-4d cap = %-4d\\n\", 0, i-1, oldCap, i, newCap) oldCap = newCap &#125; &#125;&#125;//运行结果为:[0 -&gt; -1] cap = 0 | after append 0 cap = 1 [0 -&gt; 0] cap = 1 | after append 1 cap = 2 [0 -&gt; 1] cap = 2 | after append 2 cap = 4 [0 -&gt; 3] cap = 4 | after append 4 cap = 8 [0 -&gt; 7] cap = 8 | after append 8 cap = 16 [0 -&gt; 15] cap = 16 | after append 16 cap = 32 [0 -&gt; 31] cap = 32 | after append 32 cap = 64 [0 -&gt; 63] cap = 64 | after append 64 cap = 128 [0 -&gt; 127] cap = 128 | after append 128 cap = 256 [0 -&gt; 255] cap = 256 | after append 256 cap = 512 [0 -&gt; 511] cap = 512 | after append 512 cap = 1024[0 -&gt; 1023] cap = 1024 | after append 1024 cap = 1280[0 -&gt; 1279] cap = 1280 | after append 1280 cap = 1696[0 -&gt; 1695] cap = 1696 | after append 1696 cap = 2304 分析： 切片拷贝 拷贝原理 123456789101112131415161718192021222324252627282930313233343536func slicecopy(toPtr unsafe.Pointer, toLen int, fmPtr unsafe.Pointer, fmLen int, width uintptr) int &#123; if fmLen == 0 || toLen == 0 &#123; return 0 &#125; n := fmLen if toLen &lt; n &#123; n = toLen &#125; if width == 0 &#123; return n &#125; if raceenabled &#123; //竞争检测 callerpc := getcallerpc() pc := funcPC(slicecopy) racereadrangepc(fmPtr, uintptr(n*int(width)), callerpc, pc) racewriterangepc(toPtr, uintptr(n*int(width)), callerpc, pc) &#125; if msanenabled &#123; // 如果开启了 The memory sanitizer (msan) msanread(fmPtr, uintptr(n*int(width))) msanwrite(toPtr, uintptr(n*int(width))) &#125; size := uintptr(n) * width if size == 1 &#123; // common case worth about 2x to do here // TODO: is this still worth it with new memmove impl? //如果只有一个元素，那么指针直接转换即可 *(*byte)(toPtr) = *(*byte)(fmPtr) // known to be a byte pointer &#125; else &#123; //如果不止一个元素，那么就把 size 个 bytes 从 fm.array 地址开始，拷贝到 to.array 地址之后 memmove(toPtr, fmPtr, size) &#125; return n&#125; 拷贝实例 实例1： 12345678910111213func main() &#123; array := []int&#123;10, 20, 30, 40&#125; slice := make([]int, 6) slice1 := make([]int, 3) n := copy(slice, array) m := copy(slice1, array) fmt.Println(n, slice) fmt.Println(m, slice1)&#125;//运行结果4 [10 20 30 40 0 0]3 [10 20 30] 分析： copy返回复制数目，slicecopy 方法最终结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了 实例2： 1234567891011func main() &#123; slice := []int&#123;10, 20, 30, 40&#125; for index, value := range slice &#123; fmt.Printf(\"value = %d , value-addr = %x , slice-addr = %x\\n\", value, &amp;value, &amp;slice[index]) &#125;&#125;value = 10 , value-addr = c000018088 , slice-addr = c000014140value = 20 , value-addr = c000018088 , slice-addr = c000014148value = 30 , value-addr = c000018088 , slice-addr = c000014150value = 40 , value-addr = c000018088 , slice-addr = c000014158 分析： 如果用 range 的方式去遍历一个切片， Value 其实是切片里面的值拷贝。所以每次打印 Value 的地址都不变 由于 Value 是值拷贝而非引用传递，所以直接改 Value 是达不到更改原切片值的目的的，需通过 &amp;slice[index] 获取真实的地址 参考文献 https://halfrost.com/go_slice/ https://www.kancloud.cn/kancloud/the-way-to-go/72489","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门1-闭包","slug":"Go/Go入门1-闭包","date":"2020-12-08T14:58:29.000Z","updated":"2021-07-03T07:43:50.000Z","comments":true,"path":"2020/12/08/Go/Go入门1-闭包/","link":"","permalink":"http://xboom.github.io/2020/12/08/Go/Go%E5%85%A5%E9%97%A81-%E9%97%AD%E5%8C%85/","excerpt":"","text":"闭包概念 所谓闭包是指内层函数引用了外层函数中的变量或称为引用了自由变量的函数，其返回值也是一个函数 123闭包 = 函数 + 引用环境接受一个或多个函数作为输入 输出一个函数 函数 函数只是一段可执行代码，编译后就“固化”了，每个函数在内存中只有一份实例，得到函数的入口点便可以执行函数了。 在函数式编程语言中，函数是一等公民（First class value）：第一类对象，我们不需要像命令式语言中那样借助函数指针，委托操作函数，函数可以作为另一个函数的参数或返回值，可以赋给一个变量。函数可以嵌套定义，即在一个函数内部可以定义另一个函数，有了嵌套函数这种结构，便会产生闭包问题 匿名函数 函数可以像普通的类型（整型、字符串等）一样进行赋值、作为函数的参数传递、作为函数的返回值等。 匿名函数可以动态的创建，与之成对比的常规函数必须在包中编译前就定义完毕。匿名函数可以随时改变功能 Golang的函数只能返回匿名函数！ 闭包实例 实例1： 1234567891011121314151617181920212223242526272829303132//函数片段func add(base int) func(int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) //打印变量地址，可以看出来 内部函数时对外部传入参数的引用 f := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base += i return base &#125; return f&#125;//由 main 函数作为程序入口点启动func main() &#123; t1 := add(10) fmt.Println(t1(1), t1(2)) t2 := add(100) fmt.Println(t2(1), t2(2))&#125;//结果为0xc0000b40080xc0000b40080xc0000b400811 130xc0000b40200xc0000b40200xc0000b4020101 103 可以看出： 函数add返回一个函数，返回的这个函数就是闭包 调用同一个闭包函数，都是对同一个环境进行操作 函数add每进入一次，就形成了一个新的环境，对应的闭包中，函数都是同一个函数，环境却是引用不同的环境 实例2.1： 1234567891011121314151617//由 main 函数作为程序入口点启动func main() &#123; x, y := 1,2 defer func(a int)&#123; fmt.Println(\"defer x, y = \", a, y) //y为闭包引用 &#125;(x) //x值拷贝 调用时传入参数 x += 100 y += 200 fmt.Println(x, y)&#125;//结果为：101 202defer x, y = 1 202 实例2.2： 12345678910111213141516171819//由 main 函数作为程序入口点启动func main() &#123; for i := 0; i &lt; 3; i++ &#123; //多次注册延迟调用，相反顺序执行 defer func()&#123; fmt.Println(i) //闭包引用局部变量 &#125;() fmt.Print(i) if i == 2 &#123; fmt.Printf(\"\\n\") &#125; &#125;&#125;//结果为：012333 实例3： 123456789101112131415161718192021222324252627282930313233343536373839404142//返回加减函数，重点：内部函数时对外部变量的引用func calc(base int) (func(int) int, func(int) int) &#123; fmt.Printf(\"%p\\n\", &amp;base) add := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base += i return base &#125; sub := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base -= i return base &#125; return add, sub&#125;//由 main 函数作为程序入口点启动func main() &#123; f1, f2 := calc(100) fmt.Println(f1(1), f2(2)) //执行顺序：f1 f2 println fmt.Println(f1(3), f2(4)) fmt.Println(f1(5), f2(6)) fmt.Println(f1(7), f2(8))&#125;//结果为：0xc00001a0880xc00001a0880xc00001a088101 990xc00001a0880xc00001a088102 980xc00001a0880xc00001a088103 970xc00001a0880xc00001a088104 96 可以看出： 可利用go特性，同时返回两个闭包函数 相同父环境得两个闭包函数，使用的是相同的内存变量 实例4： 123456789101112131415161718//由 main 函数作为程序入口点启动func main() &#123; for i:=0; i&lt;5; i++ &#123; go func()&#123; fmt.Println(i) //i变量值也是引用.创建5个线程执行函数， for循环执行过程中可能执行完的时候，线程刚好处于i的某个值。 &#125;() &#125; time.Sleep(time.Second * 1)&#125;//结果为55555 闭包中的值是对源变量的引用。指向的是变量的当前值。当延迟调用函数体内某个变量作为defer匿名函数的参数，则在定义defer时已获得值拷贝，否则引用某个变量的地址(引用拷贝) 实例5(阻塞)： 123456789101112131415161718func main() &#123; //创建slice cs := make([](chan int), 10) for i := 0; i &lt; len(cs); i++ &#123; cs[i] = make(chan int) &#125; for i := range cs &#123; go func() &#123; cs[i] &lt;- i //创建线程，但是i是引用外部变量，不一定等线程执行的时候就是当前i值 &#125;() &#125; for i := 0; i &lt; len(cs); i++ &#123; t := &lt;-cs[i] //读取值的时候，可能会出现一只阻塞的情况 fmt.Println(t) &#125;&#125; 主要功能就是 创建10个线程执行函数，并向channal写入值。由于goroutine还没有开始，i的值已经跑到了最大9，使得这几个goroutine都取的i=9这个值，从而都向cs[9]发消息，导致执行t := &lt;-cs[i]时，cs[0]、cs[1]、cs[2] … 都阻塞起来了，从而导致了死锁 方案1(利用闭包传每次循环得参数)： 12345for i := range cs &#123; go func(index int) &#123; cs[index] &lt;- index &#125;(i)&#125; 方案2(利用阻塞管道，每次等待协程起来再执行下一次)： 12345678ch := make(chan int)for i := range cs &#123; go func() &#123; ch &lt;- 1 cs[i] &lt;- i &#125;() &lt;- ch&#125; 闭包原理 闭包是函数和它所引用的环境。像一个结构体 1234type Closure struct &#123; F func()() i *int&#125; 通过汇编查看闭包底层 1234567891011package mainfunc f(i int) func() int &#123; return func() int &#123; i++ return i &#125;&#125;//执行命令 go tool compile -S demo.go 得出结果为 可以看到LEAQ type.noalg.struct &#123; F uintptr; \"\".i *int &#125;(SB), CX 运行汇编命令得到得结果为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132➜ Go go tool compile -S demo.go&quot;&quot;.f STEXT size&#x3D;157 args&#x3D;0x10 locals&#x3D;0x20 0x0000 00000 (demo.go:3) TEXT &quot;&quot;.f(SB), ABIInternal, $32-16 0x0000 00000 (demo.go:3) MOVQ (TLS), CX 0x0009 00009 (demo.go:3) CMPQ SP, 16(CX) 0x000d 00013 (demo.go:3) PCDATA $0, $-2 0x000d 00013 (demo.go:3) JLS 147 0x0013 00019 (demo.go:3) PCDATA $0, $-1 0x0013 00019 (demo.go:3) SUBQ $32, SP 0x0017 00023 (demo.go:3) MOVQ BP, 24(SP) 0x001c 00028 (demo.go:3) LEAQ 24(SP), BP 0x0021 00033 (demo.go:3) FUNCDATA $0, gclocals·2589ca35330fc0fce83503f4569854a0(SB) 0x0021 00033 (demo.go:3) FUNCDATA $1, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB) 0x0021 00033 (demo.go:3) LEAQ type.int(SB), AX 0x0028 00040 (demo.go:3) MOVQ AX, (SP) 0x002c 00044 (demo.go:3) PCDATA $1, $0 0x002c 00044 (demo.go:3) CALL runtime.newobject(SB) 0x0031 00049 (demo.go:3) MOVQ 8(SP), AX 0x0036 00054 (demo.go:3) MOVQ AX, &quot;&quot;.&amp;i+16(SP) 0x003b 00059 (demo.go:3) MOVQ &quot;&quot;.i+40(SP), CX 0x0040 00064 (demo.go:3) MOVQ CX, (AX) 0x0043 00067 (demo.go:4) LEAQ type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;(SB), CX 0x004a 00074 (demo.go:4) MOVQ CX, (SP) 0x004e 00078 (demo.go:4) PCDATA $1, $1 0x004e 00078 (demo.go:4) CALL runtime.newobject(SB) 0x0053 00083 (demo.go:4) MOVQ 8(SP), AX 0x0058 00088 (demo.go:4) LEAQ &quot;&quot;.f.func1(SB), CX 0x005f 00095 (demo.go:4) MOVQ CX, (AX) 0x0062 00098 (demo.go:4) PCDATA $0, $-2 0x0062 00098 (demo.go:4) CMPL runtime.writeBarrier(SB), $0 0x0069 00105 (demo.go:4) JNE 131 0x006b 00107 (demo.go:4) MOVQ &quot;&quot;.&amp;i+16(SP), CX 0x0070 00112 (demo.go:4) MOVQ CX, 8(AX) 0x0074 00116 (demo.go:4) PCDATA $0, $-1 0x0074 00116 (demo.go:4) MOVQ AX, &quot;&quot;.~r1+48(SP) 0x0079 00121 (demo.go:4) MOVQ 24(SP), BP 0x007e 00126 (demo.go:4) ADDQ $32, SP 0x0082 00130 (demo.go:4) RET 0x0083 00131 (demo.go:4) PCDATA $0, $-2 0x0083 00131 (demo.go:4) LEAQ 8(AX), DI 0x0087 00135 (demo.go:4) MOVQ &quot;&quot;.&amp;i+16(SP), CX 0x008c 00140 (demo.go:4) CALL runtime.gcWriteBarrierCX(SB) 0x0091 00145 (demo.go:4) JMP 116 0x0093 00147 (demo.go:4) NOP 0x0093 00147 (demo.go:3) PCDATA $1, $-1 0x0093 00147 (demo.go:3) PCDATA $0, $-2 0x0093 00147 (demo.go:3) CALL runtime.morestack_noctxt(SB) 0x0098 00152 (demo.go:3) PCDATA $0, $-1 0x0098 00152 (demo.go:3) JMP 0 0x0000 65 48 8b 0c 25 00 00 00 00 48 3b 61 10 0f 86 80 eH..%....H;a.... 0x0010 00 00 00 48 83 ec 20 48 89 6c 24 18 48 8d 6c 24 ...H.. H.l$.H.l$ 0x0020 18 48 8d 05 00 00 00 00 48 89 04 24 e8 00 00 00 .H......H..$.... 0x0030 00 48 8b 44 24 08 48 89 44 24 10 48 8b 4c 24 28 .H.D$.H.D$.H.L$( 0x0040 48 89 08 48 8d 0d 00 00 00 00 48 89 0c 24 e8 00 H..H......H..$.. 0x0050 00 00 00 48 8b 44 24 08 48 8d 0d 00 00 00 00 48 ...H.D$.H......H 0x0060 89 08 83 3d 00 00 00 00 00 75 18 48 8b 4c 24 10 ...&#x3D;.....u.H.L$. 0x0070 48 89 48 08 48 89 44 24 30 48 8b 6c 24 18 48 83 H.H.H.D$0H.l$.H. 0x0080 c4 20 c3 48 8d 78 08 48 8b 4c 24 10 e8 00 00 00 . .H.x.H.L$..... 0x0090 00 eb e1 e8 00 00 00 00 e9 63 ff ff ff .........c... rel 5+4 t&#x3D;17 TLS+0 rel 36+4 t&#x3D;16 type.int+0 rel 45+4 t&#x3D;8 runtime.newobject+0 rel 70+4 t&#x3D;16 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0 rel 79+4 t&#x3D;8 runtime.newobject+0 rel 91+4 t&#x3D;16 &quot;&quot;.f.func1+0 rel 100+4 t&#x3D;16 runtime.writeBarrier+-1 rel 141+4 t&#x3D;8 runtime.gcWriteBarrierCX+0 rel 148+4 t&#x3D;8 runtime.morestack_noctxt+0&quot;&quot;.f.func1 STEXT nosplit size&#x3D;19 args&#x3D;0x8 locals&#x3D;0x0 0x0000 00000 (demo.go:4) TEXT &quot;&quot;.f.func1(SB), NOSPLIT|NEEDCTXT|ABIInternal, $0-8 0x0000 00000 (demo.go:4) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (demo.go:4) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (demo.go:4) MOVQ 8(DX), AX 0x0004 00004 (demo.go:5) MOVQ (AX), CX 0x0007 00007 (demo.go:5) INCQ CX 0x000a 00010 (demo.go:5) MOVQ CX, (AX) 0x000d 00013 (demo.go:6) MOVQ CX, &quot;&quot;.~r0+8(SP) 0x0012 00018 (demo.go:6) RET 0x0000 48 8b 42 08 48 8b 08 48 ff c1 48 89 08 48 89 4c H.B.H..H..H..H.L 0x0010 24 08 c3 $..go.cuinfo.packagename. SDWARFINFO dupok size&#x3D;0 0x0000 6d 61 69 6e main&quot;&quot;..inittask SNOPTRDATA size&#x3D;24 0x0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0010 00 00 00 00 00 00 00 00 ........runtime.memequal64·f SRODATA dupok size&#x3D;8 0x0000 00 00 00 00 00 00 00 00 ........ rel 0+8 t&#x3D;1 runtime.memequal64+0runtime.gcbits.01 SRODATA dupok size&#x3D;1 0x0000 01 .type..namedata.*struct &#123; F uintptr; i *int &#125;- SRODATA dupok size&#x3D;32 0x0000 00 00 1d 2a 73 74 72 75 63 74 20 7b 20 46 20 75 ...*struct &#123; F u 0x0010 69 6e 74 70 74 72 3b 20 69 20 2a 69 6e 74 20 7d intptr; i *int &#125;type.*struct &#123; F uintptr; &quot;&quot;.i *int &#125; SRODATA dupok size&#x3D;56 0x0000 08 00 00 00 00 00 00 00 08 00 00 00 00 00 00 00 ................ 0x0010 23 f3 08 44 08 08 08 36 00 00 00 00 00 00 00 00 #..D...6........ 0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0030 00 00 00 00 00 00 00 00 ........ rel 24+8 t&#x3D;1 runtime.memequal64·f+0 rel 32+8 t&#x3D;1 runtime.gcbits.01+0 rel 40+4 t&#x3D;5 type..namedata.*struct &#123; F uintptr; i *int &#125;-+0 rel 48+8 t&#x3D;1 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0runtime.gcbits.02 SRODATA dupok size&#x3D;1 0x0000 02 .type..namedata..F- SRODATA dupok size&#x3D;5 0x0000 00 00 02 2e 46 ....Ftype..namedata.i- SRODATA dupok size&#x3D;4 0x0000 00 00 01 69 ...itype.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125; SRODATA dupok size&#x3D;128 0x0000 10 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 ................ 0x0010 b7 43 25 9a 02 08 08 19 00 00 00 00 00 00 00 00 .C%............. 0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0040 02 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 ................ 0x0050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0070 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 ................ rel 32+8 t&#x3D;1 runtime.gcbits.02+0 rel 40+4 t&#x3D;5 type..namedata.*struct &#123; F uintptr; i *int &#125;-+0 rel 44+4 t&#x3D;6 type.*struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0 rel 48+8 t&#x3D;1 type..importpath.&quot;&quot;.+0 rel 56+8 t&#x3D;1 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+80 rel 80+8 t&#x3D;1 type..namedata..F-+0 rel 88+8 t&#x3D;1 type.uintptr+0 rel 104+8 t&#x3D;1 type..namedata.i-+0 rel 112+8 t&#x3D;1 type.*int+0gclocals·2589ca35330fc0fce83503f4569854a0 SRODATA dupok size&#x3D;10 0x0000 02 00 00 00 02 00 00 00 00 00 ..........gclocals·9fb7f0986f647f17cb53dda1484e0f7a SRODATA dupok size&#x3D;10 0x0000 02 00 00 00 01 00 00 00 00 01 ..........gclocals·33cdeccccebe80329f1fdbee7f5874cb SRODATA dupok size&#x3D;8 0x0000 01 00 00 00 00 00 00 00 ........ 参考链接 https://segmentfault.com/a/1190000019753885 https://www.cnblogs.com/landv/p/11589074.html","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"TCP-5-拥塞控制","slug":"TCP/TCP-5-拥塞控制","date":"2020-11-06T14:46:37.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/06/TCP/TCP-5-拥塞控制/","link":"","permalink":"http://xboom.github.io/2020/11/06/TCP/TCP-5-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","excerpt":"","text":"引言 有了流量控制为什么还要有拥塞控制？ 流量控制是避免 发送方 的数据填满 接收方 的缓存。 在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大…. 所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。 拥塞控制 是避免 发送方 的数据填满整个网络。 在 发送方 调节所要发送数据的量，定义了 拥塞窗口 拥塞窗口和发送窗口有什么区别？ 拥塞窗口 cwnd 是发送方维护的一个 的状态变量，会根据网络的拥塞程度动态变化的。 前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，加入拥塞窗口概念后，发送窗口的值是swnd = min(cwnd, rwnd)，即拥塞窗口和接收窗口中的最小值。 拥塞窗口 cwnd 变化的规则：只要网络中没有出现拥塞， cwnd 就会增大； 但网络中出现了拥塞， cwnd 就减少； 怎么知道网络出现了阻塞？ 只要 发送方 没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就认为网络 出现了拥塞。 拥塞控制主要是四个算法： 慢启动 拥塞避免 拥塞发生 快速恢复 慢启动 TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据 包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？ 慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，就拥塞窗口 cwnd 的大小就会加 1。 假定拥塞窗口 cwnd 和发送窗口 swnd 相等，下面举个栗子： 连接建立完成后，一开始初始化 cwnd = 1 ，表示可以传一个 MSS 大小的数据。 当收到 1 个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个 当收到 2 个的 ACK 确认应答后，cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个 当收到 4 个的 ACK 确认到来的时候，4 个确认 cwnd 增加 4，于是就可以 比之前多发 4 个，所以这一次能够发送 8 个 可以看出慢启动算法，发包的个数是指数性的增长。 那慢启动涨到什么时候是个头呢？ 有一个叫慢启动门限 ssthresh （slow start threshold）状态变量 当 cwnd &lt; ssthresh 时，使用慢启动算法 当 cwnd &gt;= ssthresh 时，就会使用 拥塞避免算法 拥塞避免 当拥塞窗口 cwnd &gt;= ssthresh 慢启动门限 就会进入拥塞避免算法 一般来说 ssthresh 的大小是 65535 字节 = 2^16 - 1 那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd 接上前面的慢启动的例子，现假定 ssthresh 为 8 ： 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次 能够发送 9 个 MSS 大小的数据，变成了线性增长 拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶 段，但是增长速度缓慢了一些。 就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失 的数据包进行重传。 当触发了重传机制，也就进入了 拥塞发生算法 拥塞发生 当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种： 超时重传 快速重传 发生超时重传则就会使用拥塞发生算法。 这个时候，sshresh 和 cwnd 的值会发生变化： ssthresh 设为 cwnd/2 cwnd 重置为 1 接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦超时重传 ，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。 还有更好的方式 快速重传算法： 当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。 TCP 认为这种情况不严重，因为大部分没丢，只丢了小部分，则 ssthresh 和 cwnd 变化如下： cwnd = cwnd/2 设置为原来的一半; ssthresh = cwnd 进入快速恢复算法 快速恢复 快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也 不那么糟糕，所以没有必要像 RTO 超时那么强烈。 进入快速恢复之前， cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，设置为原来的一半 ssthresh = cwnd 然后进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了） 重传丢失的数据包 如果再收到重复的 ACK，那么 cwnd 增加 1 如果收到新数据的 ACK 后，设置 cwnd 为 ssthresh，接着就进入了拥塞避免算法 也就是没有像 超时重传 一夜回到解放前，而是还在比较高的值，后续呈线性增长","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-4-流量控制","slug":"TCP/TCP-4-流量控制","date":"2020-11-06T14:02:09.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/06/TCP/TCP-4-流量控制/","link":"","permalink":"http://xboom.github.io/2020/11/06/TCP/TCP-4-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/","excerpt":"","text":"引言 如果发送方一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。 流量控制可以让发送方根据接收方的实际接收能力控制发送 的数据量 如果窗口固定 客户端是接收方，服务端是发送方， 假设接收窗口和发送窗口相同都为 200 假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响 根据上图的流量控制，说明下每个过程(本次是把服务端作为发送方，所以没有画出服务端的接收窗口)： 客户端向服务端发送请求数据报文 服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 Usable 减少为 120 字 节，同时 SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。 客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节， RCV.NXT 也就指向 321，这意味着 客户端期望的下一个报文的序列号是 321，接着发送确认报文给服务端。 服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法在继续发送数据。 客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节， RCV.NXT 也就指向 441，接着 发送确认报文给服务端。 服务端收到对 80 字节数据的确认报文后， SND.UNA 指针往右偏移后指向 321，于是可用窗口 Usable 增大到 80。 服务端收到对 120 字节数据的确认报文后， SND.UNA 指针往右偏移后指向 441，于是可用窗口 Usable 增大到 200。 服务端可以继续发送了，于是发送了 160 字节的数据后， SND.NXT 指向 601，于是可用窗口 Usable 减少到 40。 客户端收到 160 字节后，接收窗口往右移动了 160 字节， RCV.NXT 也就是指向了 601，接着发送 确认报文给服务端。 服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 SND.UNA 指针 偏移了 160 后指向 601，可用窗口 Usable 也就增大至了 200。 操作系统缓冲区与滑动窗口的关系 前面的流量控制例子假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中 所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。 当应用进程没法及时读取缓冲区的内容时，也会对缓冲区造成影响。 操作系统的缓冲区，是如何影响发送窗口和接收窗口的呢？ 缓冲区与滑动窗口 考虑以下场景： 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 360 ； 服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。 根据上图的流量控制，说明下每个过程： 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140） 服务端收到 140 字节数据，但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100），最后发送确认信息时，将窗口大小通过给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 260 客户端发送 180 字节数据，此时可用窗口减少到 80 服务端收到 180 字节数据，但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区， 于是接收窗口收缩到了 80 （260 - 180），并在发送确认信息时，通过窗口大小给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 80 客户端发送 80 字节数据后，可用窗口耗尽 服务端收到 80 字节数据，但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是 接收窗口收缩到了 0，并在发送确认信息时，通过窗口大小给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 0。 可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变 缩小缓冲区 当服务端系统资源非常紧张，操心系统可能会直接减少接收缓冲区大小，应用程序会因为无法及时读取缓存数据，可能会出现丢包 说明下每个过程： 客户端发送 140 字节的数据，于是可用窗口减少到了 220 服务端因为现在非常的繁忙，操作系统把接收缓存减少了 100 字节，当收到对端 140 数据确认报文后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大 小从 360 收缩成了 100，最后发送确认信息时，通告窗口大小给对方 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户 端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40 服务端收到了 180 字节数据时，发现数据大小超过了接收窗口的大小，于是就把数据包丢失了 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗 口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。 所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。所以TCP 规定不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段间再减少缓存 窗口关闭 TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控 制。 如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭 窗口关闭存在潜在危险：接收方向发送方通告窗口大小时，是通过 ACK 报文来通告的。 当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个ACK报文丢失呢？ 这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，就会出现相互等待的死锁现象 TCP 是如何解决窗口关闭时，潜在的死锁现象呢？ 为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 窗口探测 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器； 如果接收窗口不是 0，那么死锁的局面就可以被打破了。 窗口探查探测的次数一般为 3 此次，每次次大约 30-60 秒（不同的实现可能会不一样，如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接 糊涂窗口综合症 如果接收方太忙了，来不及取走接收窗口里的数据，就会导致发送方的发送窗口越来越小。 到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。 要知道，我们的 TCP + IP 头有 40 个字节，如果只是为了传几个字节不太划算 考虑以下场景： 接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下： 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据； 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节； 每个过程的窗口大小不断减少了，并且发送的数据都是比较小的了 糊涂窗口综合症的现象： 接收方可以通告一个小的窗口 而发送方可以发送小数据 解决糊涂窗口综合症的办法是： 让接收方不通告小窗口给发送方：当 窗口大小 小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会 向发送方通告窗口为 0 ，也就阻止了发送方再发数据过来。 等到接收方处理了一些数据后，窗口大小 &gt;= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。 让发送方避免发送小数据：使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据： 要等到窗口大小 &gt;= MSS 或是 数据大小 &gt;= MSS 收到之前发送数据的 ack 回包 只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件 Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。 可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每 个应用自己的特点来关闭） 1setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&amp;value, sizeof(int))","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-3-滑动窗口","slug":"TCP/TCP-3-滑动窗口","date":"2020-11-02T15:53:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/02/TCP/TCP-3-滑动窗口/","link":"","permalink":"http://xboom.github.io/2020/11/02/TCP/TCP-3-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"","text":"引言 TCP 是每发送一个数据，都要进行一次确认应答。如果上一个数据包收到应答再发送下一个。数据包往返时间越长，通信的效率就越低。 为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，也不会降低网络通信的效率 有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值 窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。 假设窗口大小为 3 个 TCP 段，那么发送方就可以连续发送 3 个 TCP 段，若中途若有 ACK 丢失，可以通过下一个确认应答进行确认 图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通话下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据接收方都收到了。这个模式就叫累计确认或者累计应答 窗口的大小由哪一方决定？ TCP 头里有一个字段叫 Window ，即窗口大小。 接收端告知发送端自己还有多少缓冲区，发送方数据大小不能超过这个窗口大小，否则接收方无法正常接收数据 发送方的滑动窗口 下图发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口： #1 是已发送并收到 ACK确认的数据：1~31 字节 #2 是已发送但未收到 ACK确认的数据：32~45 字节 #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节 #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后 在下图，当发送方把数据全部都一下发送出去后，可用窗口为 0 ，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据 在下图，当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 52~56 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了 程序是如何表示发送方的四个部分的呢？ TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移） SND.WND：表示发送窗口的大小（大小是由接收方指定的） SND.UNA：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节 SND.NXT：是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的 第一个字节 指向 #4 的第一个字节是个相对指针，它需要 SND.NXT 指针加上 SND.WND 大小的偏移量，就可以指向 #4 的第一个字节了 可用窗口大小的计算就是： 可用窗口大 = SND.WND -（SND.NXT - SND.UNA） 接收方的滑动窗口 接收窗口相对简单一些，根据处理的情况划分成三个部分： #1 + #2 是已成功接收并确认的数据（等待应用进程读取）； #3 是未收到数据但可以接收的数据； #4 未收到数据并不可以接收的数据； 其中三个接收部分，使用两个指针进行划分: RCV.WND ：表示接收窗口的大小，它会通告给发送方 RCV.NXT ：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节 指向 #4 的第一个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND 大小的偏移量，就可以指向 #4 的第一个字节 接收窗口和发送窗口的大小是相等的吗？ 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。 因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发 送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。 滑动窗口是固定的吗？ 如果是固定窗口，就会存在两个问题： 如果说窗口过小，当传输比较大的数据的时候需要不停的对数据进行确认，会造成很大的延迟。 如果说窗口的大小定义的过大。假设发送方一次发送100个数据。但接收方只能处理50个数据。这样每次都会只对这50个数据进行确认。发送方下一次还是发送100个数据，但是接受方还是只能处理50个数据。这样就有不必要的数据来拥塞我们的链路。 窗口左边沿向右移动称为窗口合拢，发生在数据被发送和确认后(重复ACK会会被丢弃，左边沿不会左移)。 窗口右边沿向右移动称为窗口张开，发生在另一端的接收进程读取已经确认的数据并释放了TCP的接收缓存时 窗口右边沿向左移动称为窗口收缩。RFC建议不使用这种方式。但TCP必须能够在某一端这种情况时进行处理 如果左边沿到达右边沿，则称其为一个零窗口，此时发送方不能够发送任何数据","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-2-超时与重传","slug":"TCP/TCP-2-超时与重传","date":"2020-11-01T03:00:54.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/01/TCP/TCP-2-超时与重传/","link":"","permalink":"http://xboom.github.io/2020/11/01/TCP/TCP-2-%E8%B6%85%E6%97%B6%E4%B8%8E%E9%87%8D%E4%BC%A0/","excerpt":"","text":"引言 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。针对TCP数据包丢失情况，常用以下四种重传机制解决： 超时重传 快速重传 SACK D-SACK 超时重传 在发送时设置一个定时器，当定时器溢出时还没收到ACK确认，它就重传该数据。TCP会在以下两种情况发生超时重传 数据包丢失 确认应答丢失 那么怎样决定超时间隔和重传的频率? RTT(Round-Trip Time 往返时延):表示数据往返两端一次所需要的时间 RTO(Retransmission Timeout 超时重传时间):表示数据包发送超过该时间还没有收到回复，则进行超时重传 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发 所以 超时重传时间 RTO 的值应该略大于报文往返 RTT 的值 由于网络是波动的，报文往返 RTT 是经常波动变化的，那系统是怎么确定RTO的值的呢？ 系统通过采样的方式来估计往返时间RTT： 采样 RTT 的时间进行加权平均，算出一个 SRTT(smoothed round-trip time)的值，而且这个值是不断变化的，因为网络状况不断地变化。 采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况 [RFC6298] 建议使用 Jacobaon/Karels 算法 计算 RTO 123456789# 第一次计算RTO,其中R1为第一次测量的RTT值, SRTT是计算平滑的RTT, DevRTR 是计算平滑的 RTT 与最新 RTT 的差距。SRTT = R1DevRTT = R1/2RTO = μ * SRTT + ∂ * DevRTT = μ * R1 + ∂ * R1 / 2# 后续计算RTO,其中R2为最新测量的RTTSRTT = SRTT + α * (RTT - SRTT) = R1 + α * (R2 - R1)DevRTT = (1 - β) * DevRTT + β * (|RTT - STRR|) = (1 - β) * R1 / 2 + β * (|R2 - R1|)RTO = μ * SRTT + ∂ * DevRTT 在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4 通过大量实验调试出来的 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍：即每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。 快速重传 超时触发重传存在的问题是，超时周期可能相对较长。 就有了快速重传机制来解决超时重发的时间等待 发送方发出了 1，2，3，4，5 份数据 第一份 Seq1 先送到了，于是就 Ack 回 2(表示2以前的都收到了，其他发送方接下来从2开始发送) 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2; 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到; 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失 的Seq2 最后，接收到收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段 SACK **快速重传( Selective Acknowledgment 选择性确认)**重传的时候，怎么知道是重传之前的一个，还是重传所有。发送端并不清楚这连续的三个 Ack 2 是由于哪个请求而传回来的。比如快速重传例子，是重传 Seq2 还是重传 Seq2、Seq3、Seq4、Seq5 。为了解决不知道重传哪些 TCP 报文，就有 SACK。 在 TCP 头部 Option 字段里加一个 SACK 的东西，将缓存的地图(几组不连续块的第一个序列号与不连续块的最后一个序列号)发送给发送方， 这样发送方就可以知道哪些数据收到了，哪些数据没收到，只重传丢失的数据。 如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现 只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。 如果要支持 SACK ，必须双方都要支持。在 Linux 下，通过 net.ipv4.tcp_sack 参数打开这个功能(Linux 2.4 后默认打开) Duplicate SACK Duplicate SACK 又称 D-SACK ，使用 **SACK 来通知发送方有哪些数据被重复接收了 ** 比如1,ACK丢包： 接收方发给发送方的两个 ACK 确认应答都丢失了，发送方超时后，重传第一个数据包(3000 ~ 3499) 接收方发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉发送方 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据 都已收到，所以这个 SACK 就代表着 D-SACK 。 这样发送方就知道了，数据没有丢，是接收方的 ACK 确认报文丢了 比如2,网络延时： 数据包(1000~1499) 被网络延迟了，导致发送方没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但重传后，被延迟的数据包(1000~1499)又到了接收方; 所以接收方回了一个SACK=1000~1500，因为ACK已经到了3000，所以这个SACK是D- SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而 是因为网络延迟了。 可见，D-SACK 有这么几个好处: 可以让发送方知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以判断是不是发送方的数据包被网络延迟了; 可以判断网络中是不是把发送方的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能(Linux 2.4 后默认打开)","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"Linux新手1-内核入门","slug":"Linux/Linux新手1-内核入门","date":"2020-10-25T02:24:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/10/25/Linux/Linux新手1-内核入门/","link":"","permalink":"http://xboom.github.io/2020/10/25/Linux/Linux%E6%96%B0%E6%89%8B1-%E5%86%85%E6%A0%B8%E5%85%A5%E9%97%A8/","excerpt":"","text":"编译内核代码 下载内核代码 从国内镜像网站下载最新的内核源码linux-5.9.1，并进行解压 12xz -d linux-5.9.1.tar.xztar xvf linux-5.9.1.tar 配置内核 对内核进行配置是为了得到内核配置文件.config。通过对内核进行配置，可以使未来编译成功的内核增加或减少对一些内核特性的支持。对内核进行配置有多种方法：基于文本/图形等，这里采用make menuconfig方式： 由于该配置方式基于ncurses库，所以在启动配置界面前要先安装ncurses库 12yum install -y ncurses-devel # make menuconfig requires the ncurses librariesmake menuconfig 配置界面成功如下 对内核按照默认的配置方式进行编译，因此当配置菜单启动后直接退出并保存即可。此时就在内核源码根目录下生成了.config文件 编译内核 编译内核包含两部分的工作， 其一是编译内核，即编译配置选项中标记为Y的那部分，这部分内核最终形成bzIamge镜像文件； 其二是编译内核模块，即编译配置选项中标记为M的那部分内核，这部分形成以.ko结尾的内核模块目标文件。 上述两部分编译工作可以依次通过make bzImage和make modules完成，也可以通过一条make命令直接完成。 编译内核的整个过程比较漫长，可以对make加-j参数来提高编译的效率。在make时使用该选项会为编译过程分配n个并发任务，这样可以缩短编译时间。n的取值为cpu个数的二倍。 1make -j4 如果不想看到垃圾信息又不想错过告警和错误信息，可以使用重定向文件写入，编译信息会写到文件中，同时错误和告警信息都会在屏幕上显示 1make &gt; ../detritus 安装内核 安装过程分为两部分，首先对内核模块进行安装，这个过程会将刚刚编译内核模块时生成的内核模块复制到/lib/modules/5.9.1/目录下，其中5.9.1为对应的内核版本。使用的命令如下： 1sudo make modules_install 接着使用下述命令安装编译好的内核： 1sudo make install 安装内核的过程主要完成了以下的工作： 将编译内核时生成的内核镜像bzImage拷贝到/boot目录下，并将这个镜像命名为vmlinuz-5.9.1。如果使用x86的cpu，则该镜像位于arch/x86/boot/目录下（处于正在编译的内核源码下）。 将~/linux-5.9.1/目录下的System.map拷贝到/boot/目录下，重新命名为System.map-5.9.1。该文件中存放了内核的符号表。 将~/linux-5.9.1/目录下的.config拷贝到/boot/目录下，重新命名为config-5.9.1。 创建initrd.img文件 initrd.img即为初始化的ramdisk文件，它是一个镜像文件，将一些最基本的驱动程序和命令工具打包到镜像文件里。该镜像文件的作用是在系统还没有挂载根分区前，系统需要执行一些操作，比如挂载scsi驱动，此时将initrd文件释放到内存中，作为一个虚拟的根分区，然后执行相关脚本，运行insmod命令加载需要的模块。 1sudo mkinitramfs 5.9.1 -o /boot/initrd.img-5.9.1 更新grub 最后一步则是更新grub启动菜单，使用下面的命令则可以自动更新启动菜单： 1sudo update-grub2 这样会将刚才编译好的内核放在启动菜单的首位，如果需要修改启动菜单中默认系统的启动顺序，则修改/boot/grub/grub.cfg文件中的set default=的值即可 内核就这样编译完毕了 Hello Kernel 编写一个Hello.c文件 1234567891011121314151617181920212223242526272829#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;//必选//模块许可声明MODULE_LICENSE(\"GPL\");//模块加载函数static int hello_init(void)&#123; printk(KERN_ALERT \"hello,I am edsionte\\n\"); return 0;&#125;//模块卸载函数static void hello_exit(void)&#123; printk(KERN_ALERT \"goodbye,kernel\\n\");&#125;//模块注册module_init(hello_init);module_exit(hello_exit);//可选MODULE_AUTHOR(\"xboom dove\");MODULE_DESCRIPTION(\"This is a simple example!\\n\");MODULE_ALIAS(\"A simplest example\"); 一个模块程序的中，模块加载函数，模块卸载函数以及模块许可声明是必须有的 编写了模块加载函数后，还必须用module_init(mode_name);的形式注册这个函数。当接下来用insmod加载模块时，内核会自动去寻找并执行内核加载函数，完成一些初始化工作。类似的当使用rmmod命令时，内核会自动去执行内核卸载函数。 注意这里的printk函数，可以简单的理解为它是内核中的printf函数，初次使用很容易将其打成printf 编写Makefile文件 12345678910111213141516171819obj-m += hello.o#generate the pathCURRENT_PATH:=$(shell pwd)#the current kernel version numberLINUX_KERNEL:=$(shell uname -r)#the absolute pathLINUX_KERNEL_PATH:=/lib/modules/$(LINUX_KERNEL)/build#complie objectall: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules#cleanclean: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean 在当前目录下运行make命令，就会生成hello.ko文件，即模块目标文件。接下来： 使用insmod hello.ko 添加该模块 如果报错 “Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel” Centos: yum install elfutils-libelf-devel Ubuntu： 12apt install libelf-devapt install libssl-dev 查看已经加载的模块 lsmod 12[root@localhost linuxDemo]# lsmod |grep hellohello 16384 0 使用rmmod hello.ko 卸载该模块 使用 dmesg命令查看printk中显示的语句 12345[339722.818996] hello: loading out-of-tree module taints kernel.[339722.819088] hello: module verification failed: signature and/or required key missing - tainting kernel[339722.819609] hello,I am edsionte[339764.633084] goodbye,kernel[339786.267993] hello,I am edsionte linux内核从3.7 开始加入模块签名检查机制，如果内核选项CONFIG_MODULE_SIG和CONFIG_MODULE_SIG_FORCE打开的话， 当加载模块时内核会检查模块的签名，如果签名不存在或者签名内容不一致，会强制退出模块的加载 提示&quot;module verification failed: signature and/or required key missing - tainting kernel&quot; 关闭方式： 在Makefile中添加 CONFIG_MODULE_SIG=n 然后重新重新编译内核 其实第一步insmod还有另外一种办法： 将hello.ko文件拷贝到/lib/module/#uname -r#/目录下， #uname -r#意思是，在终端中输入 uname -r后显示的内核版本及名称，例如mini2440中#uname -r#就是2.6.32.2-FriendlyARM。 然后depmod（会在/lib/modules/#uname -r#/目录下生成modules.dep和modules.dep.bb文件，表明模块的依赖关系） 最后modprobe hello（注意这里无需输入.ko后缀）即可 两种方法的区别： modprobe和insmod类似，都是用来动态加载驱动模块的，区别在于modprobe可以解决load module时的依赖关系，它是通过/lib/modules/#uname -r/modules.dep(.bb)文件来查找依赖关系的；而insmod不能解决依赖问题。 参考文档 Linux内核新手区 《Linux内核设计与实现》","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"Linux性能优化-0-Tools","slug":"Linux/Linux性能优化-0-Tools","date":"2020-09-30T16:00:38.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/10/01/Linux/Linux性能优化-0-Tools/","link":"","permalink":"http://xboom.github.io/2020/10/01/Linux/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-0-Tools/","excerpt":"","text":"前言 Linux性能分析 CPU性能 磁盘I/O性能 内存性能 网络性能 Linux大神Brendan D.Gregg的Linux Performance Tools Virtual Memory vmstat vmstat：Virtual Meomory Statistics(虚拟内存统计)，可对操作系统的虚拟内存、进程、CPU活动进行监控 vmstat 命令详解 使用 man vmstat 查看 vmstat 的使用说明 123456789101112131415161718192021222324252627vmstat [options] [delay [count]]# 参数详解delay 表示间隔更新的时间。如果不指定，则只会打印一次从系统启动到现在的平均值count 更新次数，在没有计数的情况下，定义延迟时，默认值为无限-a, --active -f, --forks 显示从启动到现在forks次数-m, --slabs 显示内核slab缓冲区信息-n, --one-header 标题只显示一次而不是定期显示-s, --state 显示各种事件计数器和内存统计信息的表。此显示不重复-d, --disk 显示磁盘统计-D, --disk-sum 显示磁盘摘要信息-p, --partition device 显示分区统计信息-S, --unit character 在1000(k),1024(k),1000000(m)或1048576(m)字节之间切换输出。不会更改交换(si/so)或块(bi/bo)字段-t, --timestamp 每一行显示时间戳-w, --wide 宽输出模式（适用于内存量较大的系统，其中默认输出模式会出现不需要的列中断）。输出宽度超过80个字符每行NOTES 1. vmstat does not require special permissions. 2. These reports are intended to help identify system bottlenecks. Linux vmstat does not count itself as a running process. 3. All linux blocks are currently 1024 bytes. Old kernels may report blocks as 512 bytes, 2048 bytes, or 4096 bytes. 4. Since procps 3.1.9, vmstat lets you choose units (k, K, m, M). Default is K (1024 bytes) in the default mode. FILES /proc/meminfo /proc/stat /proc/*/stat vmstat常用命令 vmstat 2 1 间隔2s采集一次服务器状态，1表示只采集一次 12345678910111213141516171819202122232425262728293031323334353637[xboom@localhost ~]$ vmstat 2 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 698692 70384 80 136748 159 827 4004 965 250 511 5 4 91 0 0#FIELD DESCRIPTION FOR VM MODEProcs r: The number of runnable processes (running or waiting for run time). b: The number of processes in uninterruptible sleep. Memory swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. inact: the amount of inactive memory. (-a option) active: the amount of active memory. has been recently userd (-a option) Swap si: Amount of memory swapped in from disk (kb/s) so: Amount of memory swapped to disk (kb/s) IO bi: Blocks received from a block device (blocks/s). default 1024bytes per block bo: Blocks sent to a block device (blocks/s).System in: The number of interrupts per second, including the clock. cs: The number of context switches per second.CPU These are percentages of total CPU time. us: Time spent running non-kernel code. (user time, including nice time) sy: Time spent running kernel code. (system time) id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time. wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle. st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown. Active memory: This information is currently in memory, and has been recently used. Inactive memory: This information in memory is not actively being used, but was recently used. For example, if you’ve been using Mail and then quit it, the RAM that Mail was using is marked as Inactive memory. This Inactive memory is available for use by another application, just like Free memory. However, if you open Mail before its Inactive memory is used by a different application, Mail will open quicker because its Inactive memory is converted to Active memory, instead of loading Mail from the slower hard dis vmstat -d 查看磁盘信息 123456789101112131415161718192021222324[xboom@localhost ~]$ vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur secsda 73171 8499 7041491 19482 12371 181181 1645664 10221 0 23sr0 52 0 2125 55 0 0 0 0 0 0dm-0 46855 0 6696761 13980 5743 0 143320 2504 0 17dm-1 32794 0 266008 6924 187764 0 1502112 308321 0 10#FIELD DESCRIPTION FOR DISK MODEReads total: Total reads completed successfully merged: grouped reads (resulting in one I/O) sectors: Sectors read successfully ms: milliseconds spent readingWrites total: Total writes completed successfully merged: grouped writes (resulting in one I/O) sectors: Sectors written successfully ms: milliseconds spent writingIO cur: I/O in progress s: seconds spent for I/O vmstat -p dev 查看分区情况 12345678[root@localhost ~]# vmstat -p /dev/sda1sda1 reads read sectors writes requested writes 343 68386 34 456#FIELD DESCRIPTION FOR DISK PARTITION MODE reads: Total number of reads issued to this partition,分区的读的次数 read sectors: Total read sectors for partition,分区的读扇区的次数 writes : Total number of writes issued to this partition,分区写的次数 requested writes: Total number of write requests made for partition,分区写请求次数 vmstat -m 查看系统slab信息 123456789101112[root@localhost ~]# vmstat -mCache Num Total Size Pagesisofs_inode_cache 92 92 704 46fuse_inode 36 36 896 36nf_conntrack 102 102 320 51#FIELD DESCRIPTION FOR SLAB MODE cache: Cache name num: Number of currently active objects total: Total number of available objects size: Size of each object pages: Number of pages with at least one active object slab:由于内核会有许多小对象，这些对象构造销毁十分频繁，比如i-node，dentry，这些对象如果每次构建的时候就向内存要一个页(4kb),这样就会非常浪费，为了解决这个问题，就引入了一种新的机制来处理在同一个页框中如何分配小存储区，而slab可以对小对象进行分配,这样就不用为每一个对象分配页框，从而节省了空间，内核对一些小对象创建析构很频繁，slab对这些小对象进行缓冲,可以重复利用,减少内存分配次数 vmstat -f显示从系统启动至今的fork(创建进程的系统调用)数量 12[root@localhost ~]# vmstat -f 4319 forks Block Device Int-块设备IO接口 pidstat pidstat命令用于监视Linux内核当前正在管理的各个任务 pidstat 命令详解 使用 man pidstat 查看 pidstat 的使用说明, 进程(任务)相关统计 1234567891011121314151617181920212223242526272829303132333435363738394041pidstat [ -d ] [ -H ] [ -h ] [ -I ] [ -l ] [ -R ] [ -r ] [ -s ] [ -t ] [ -U [ username ] ] [ -u ] [ -V ] [ -v ] [ -w ] [ -C comm ] [ -G process_name ] [ --human ] [ -p &#123; pid [,...] | SELF | ALL &#125; ] [ -T &#123; TASK | CHILD | ALL &#125; ] [ interval [ count ] ] [ -e program args ]#描述 1. 未指定 -p &lt;pid&gt; 将默认使用-p ALL，仅显示活动任务(统计值非0的任务) 2. 使用 -T 查看子进程信息 3. interval参数表示每次报告的时间间隔，0(或没有参数)表示自系统启动以来的的任务统计信息。如果没有设置为0，则可以与 count 一起使用，count表示指定间隔时间之后生成的报告次数。如果没有则是连续报告#参数详解-C comm 仅显示命令名称包含字符串comm的任务。 该字符串可以是正则表达式-d 报告I/O统计信息（仅内核2.6.20及更高版本）-e program args 给定参数args执行程序，并使用pidstat对其进行监视。当程序终止时，pidstat停止-G process_name 仅显示命令名称包含字符串process_name的进程。该字符串可以是正则表达式。 如果选项-t与选项-G一起使用，则属于该进程的线程为也会显示， 即使其命令名称不包含字符串process_name-H 以秒为单位显示时间戳-h 所有活动都单行显示，报告末尾没有平均统计信息。目的是使它易于被其他程序解析--human 以人类可读的格式打印尺寸（例如1.0k，1.2M等），设置与指标关联的其他任何默认单位（例如，千字节，扇区...）。-I 在多处理器环境中，指示任务CPU使用率（如选项-u所示）应除以处理器总数.-l 显示完成的进程命令和它的参数-p &#123; pid [,...] | SELF | ALL &#125; 选择要报告其统计信息的任务。 pid 指定进程标识号 SELF 报告pidstat进程本身的统计信息 ALL 报告系统管理的所有任务的统计信息。-R 报告实时优先级和调度策略信息 prio 任务的优先级 policy 任务的调度策略-r 报告页面错误和内存利用率。当报告单个任务的统计信息时，可能会显示以下值：-s 报告堆栈利用率-T &#123; TASK | CHILD | ALL &#125; 此选项指定pidstat命令必须监视的内容。 TASK 表示将报告单个任务的统计信息（这是默认选项） CHILD 表示报告所选任务及其子进程 ALL 表示要报告单个任务的统计信息，并针对所选全局报告任务和子进程 注意：任务及其所有子项的全局统计信息不适用于pidstat的所有选项 同样，这些统计信息不一定与当前时间间隔有关：子进程的统计信息仅在完成或被杀死时才收集进程-t 显示与所选任务关联的线程的统计信息 TGID 线程组标识符 TID 线程标识符-U [ username ] 显示正在监视的任务的真实用户名，而不是UID。如果指定了用户名，则仅显示属于指定用户的任务 -u 显示CPU利用率-v 内核表信息-w 显示进程上下文切换情况 pidstat常用命令 pidstat 2 5 每两秒钟显示一次系统中每个活动任务的CPU统计信息的五个报告 1234567891011121314151617181920212223[xboom@localhost ~]$ pidstat 2 5Linux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时10分13秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分16秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分16秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分18秒 0 5569 0.00 0.50 0.00 0.00 0.50 0 kworker/0:1-ata_sff23时10分18秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分18秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分20秒 0 4130 0.50 0.00 0.00 0.00 0.50 0 sssd_kcm23时10分20秒 1000 5605 0.50 0.00 0.00 0.00 0.50 0 pidstat23时10分20秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分22秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分22秒 UID PID %usr %system %guest %wait %CPU CPU Command平均时间: UID PID %usr %system %guest %wait %CPU CPU Command平均时间: 0 4130 0.10 0.00 0.00 0.00 0.10 - sssd_kcm平均时间: 0 5569 0.00 0.10 0.00 0.00 0.10 - kworker/0:1-ata_sff平均时间: 1000 5605 0.10 0.30 0.00 0.00 0.40 - pidstat pidstat -d 报告I/O统计信息（仅内核2.6.20及更高版本） 123456789101112131415[xboom@localhost ~]$ pidstat -dLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时11分56秒 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command23时11分56秒 1000 2050 0.99 0.00 0.00 12 dbus-daemon23时11分56秒 1000 2052 0.81 0.00 0.00 0 gdm-wayland-ses23时11分56秒 1000 2056 28.68 0.07 0.00 6 gnome-session-b23时11分56秒 1000 2104 197.84 0.05 0.00 199 gnome-shell23时11分56秒 1000 2167 4.47 0.00 0.00 6 XwaylandkB_rd/s 每秒从磁盘读取任务的千字节数kB_wr/s 每秒磁盘写入任务的千字节数kB_ccwr/s 任务已取消其写入磁盘的千字节数。 当任务截断一些脏页缓存时，可能会发生这种情况。 在这种情况下，一些IO另一个任务已被考虑将不会发生iodelay 任务的块I/O延迟，以时钟周期为单位。该指标包括等待同步块I/O完成和交换块I/O完成 pidstat -r 报告页面错误和内存利用率 1234567891011121314151617181920[xboom@localhost ~]$ pidstat -rLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时16分42秒 UID PID minflt/s majflt/s VSZ RSS %MEM Command23时16分42秒 0 1 3.02 0.23 245432 5032 0.62 systemd23时16分42秒 0 725 0.19 0.19 104716 2700 0.33 systemd-journal23时16分42秒 0 757 0.65 0.02 118924 420 0.05 systemd-udevd23时16分42秒 32 885 0.05 0.00 67124 144 0.02 rpcbind23时16分42秒 0 888 0.03 0.01 143108 48 0.01 auditd23时16分42秒 0 890 0.03 0.00 48488 20 0.00 sedispatch23时16分42秒 998 927 6.24 0.12 1634948 2484 0.31 polkitdminflt/s 任务每秒发生的次要错误，不需要从磁盘中加载页majflt/s 任务每秒发生的主要错误，需要从磁盘中加载页VSZ 虚拟地址大小，虚拟内存的使用KBRSS 常驻集合大小，非交换区五里内存使用KB%MEM 任务当前使用的可用物理内存份额当报告任务及其所有子任务的全局统计信息时，可能会显示以下值：minflt-nr 任务及其所有子项所产生的次要错误总数majflt-nr 任务及其所有子项所产生的主要错误总数 pidstat -s 报告堆栈利用率 123456789101112[xboom@localhost ~]$ pidstat -sLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时18分36秒 UID PID StkSize StkRef Command23时18分36秒 1000 2016 132 16 systemd23时18分36秒 1000 2037 132 4 pulseaudio23时18分36秒 1000 2050 132 12 dbus-daemon23时18分36秒 1000 2052 132 4 gdm-wayland-ses23时18分36秒 1000 2056 132 8 gnome-session-bStkSize 为任务保留为堆栈的内存量（以KB为单位），但不一定使用。StkRef 任务引用的用作堆栈的内存量（以KB为单位） pidstat -u 报告CPU利用率 12345678910111213141516171819[xboom@localhost ~]$ pidstat -uLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时20分26秒 UID PID %usr %system %guest %wait %CPU CPU Command23时20分26秒 0 1 0.00 0.03 0.00 0.07 0.03 0 systemd23时20分26秒 0 2 0.00 0.00 0.00 0.00 0.00 0 kthreadd23时20分26秒 0 9 0.00 0.01 0.00 0.03 0.01 0 ksoftirqd/023时20分26秒 0 10 0.00 0.00 0.00 0.81 0.00 0 rcu_sched%usr 用户太CPU占比，不包含虚拟处理器(虚拟处理器)所花费时间%system 内核态CPU占比 %guest 虚拟机CPU占比 %wait 等待IO的CPU占比 %CPU 总的CPU利用率，如果在多处理器环境中，添加参数 -I,该值将除以总的CPU个数CPU 进程运行的CPU id当报告任务及其所有子任务的全局统计信息时，可能会显示以下值：usr-ms 用户态任务及其所有子项花费的毫秒总数system-ms 内核态任务及其所有子项花费的毫秒总数guest-ms 虚拟处理器任务及其所有子项花费的毫秒总数 pidstat -v 内核表信息 12345678910[xboom@localhost ~]$ pidstat -vLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时22分03秒 UID PID threads fd-nr Command23时22分03秒 1000 2016 1 25 systemd23时22分03秒 1000 2037 3 36 pulseaudio23时22分03秒 1000 2050 2 71 dbus-daemonthreads 当前任务关联线程数fd-nr 当前任务关联文件描述符 pidstat -w 显示上下文切换 123456789101112131415[xboom@localhost ~]$ pidstat -wLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时23分28秒 UID PID cswch/s nvcswch/s Command23时23分28秒 0 1 0.79 0.27 systemd23时23分28秒 0 2 0.03 0.00 kthreadd23时23分28秒 0 3 0.00 0.00 rcu_gp23时23分28秒 0 4 0.00 0.00 rcu_par_gp23时23分28秒 0 6 0.00 0.00 kworker/0:0H-kblockdcswch/s 每秒执行的任务的自愿上下文切换总数。 自愿的上下文切换：进程无法获取所需资源导致,如 I/O、内存等系统系统资源不足。nvcswch/s 每秒执行的任务的非自愿上下文切换总数。 非自愿的上下文切换：是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。 比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换 I/O Controller iostat iostat命令用于通过观察设备活动的时间及其平均传输速率来监视系统输入/输出设备的负载。 iostat命令生成可用于更改系统配置的报告，以更好地平衡物理磁盘之间的输入/输出负载 iostat 命令详解 使用 man iostat 查看 iostat 的使用说明, 进程(任务)相关统计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667iostat [ -c ] [ -d ] [ -h ] [ -k | -m ] [ -N ] [ -s ] [ -t ] [ -V ] [ -x ] [ -y ] [ -z ] [ -j &#123; ID | LABEL | PATH | UUID | ... &#125; ] [ -o JSON ] [ [ -H ] -g group_name ] [ --human ] [ -p [ device [,...] | ALL ] ] [ device [...] | ALL ] [ interval [ count ] ]#描述 1.iostat命令生成的第一个报告提供有关自系统启动以来的时间的统计信息，除非使用了-y选项（在这种情况下，将忽略此第一个报告）。 2.每个后续报告涵盖自上一个报告以来的时间。每次运行iostat命令时，都会报告所有统计信息。 该报告由一个CPU标头行和随后的CPU统计信息行组成。 在多处理器系统上，CPU统计信息是系统范围内所有处理器的平均值。将显示设备标题行，后跟配置的每个设备的一行统计信息。 3.interval参数指定每个报告之间的时间间隔（以秒为单位）。可以与interval参数一起指定count参数。如果指定了count参数，count的值将确定间隔间隔秒生成的报告数。如果没有指定count参数而未指定count参数，则iostat命令将连续生成报告。# iostat命令生成两种类型的报告，即CPU利用率报告和设备利用率报告。# CPU使用率报告iostat命令生成的第一个报告是CPU利用率报告。对于多处理器系统，CPU值是所有处理器之间的全局平均值。 该报告具有以下格式： #磁盘利用率报告设备报告提供了基于每个物理设备或分区的统计信息。 如果未输入设备或分区，则将显示系统使用的每个设备的统计信息，并提供内核维护的统计信息。 如果在命令行上指定了ALL关键字，则将显示系统定义的每个设备的统计信息，包括从未使用过的设备。 除非设置了环境变量POSIXLY_CORRECT，否则默认情况下传输速率以1K块为单位，在这种情况下，将使用512字节的块。 sec/s (kB/s, MB/s): 每秒从设备读取或写入设备的扇区数（千字节，兆字节） rqm/s: 每秒排队到设备中的 I/O 请求数 areq-sz: 发出给设备的 I/O 请求的平均大小（以千字节为单位）注意：在以前的版本中，此字段称为avgrq-sz，以扇区表示 await: 发出给要服务的设备的 I/O 请求的平均时间（毫秒）。 这包括队列中的请求所花费的时间以及为请求服务所花费的时间 # 参数详解-c 显示CPU利用率-d 显示设备利用率-g group_name &#123; device [...] | ALL &#125; 显示一组设备的统计信息 iostat命令报告列表中每个设备的统计信息， 然后报告显示为group_name并由列表中所有设备组成的组的全局统计信息。 ALL关键字表示系统定义的所有块设备都应包含在组中。-H 此选项必须与选项-g一起使用，并指示仅显示该组的全局统计信息，而不显示该组中各个设备的统计信息。-h 使设备使用情况报告更易于人阅读。--human使用此选项隐式启用。--human 以可读格式打印的尺寸（例如KB, M等）。使用此选项显示的单位将取代与度量标准关联的任何其他默认单位（例如千字节，扇区...）-j &#123; ID | LABEL | PATH | UUID | ... &#125; [ device [...] | ALL ] 显示永久设备名称。 选项ID，LABEL等指定持久名称的类型。这些选项不受限制，只有先决条件是 /dev/disk 中必须存在具有所需持久名称的目录。 （可选）可以在所选的持久名称类型中指定多个设备。 因为持久性设备名称通常很长，所以选择-k 显示统计信息（KB/s）-m 显示统计信息（M/s）。-N 显示任何设备映射器设备的注册设备映射器名称。用于查看LVM2统计信息。-o JSON 以JSON（JavaScript对象表示法）格式显示统计信息。JSON输出字段顺序未定义，将来可能会添加新字段。-p [ &#123; device [,...] | ALL &#125; ] 显示系统使用的块设备及其所有分区的统计信息。 如果在命令行上输入了设备名称，则对其进行统计并显示其所有分区。 ALL关键字指示必须显示系统定义的所有块设备和分区的统计信息，包括那些从未使用过的 如果在此选项之前定义了选项-j，则可以使用所选的持久名称类型指定在命令行上输入的设备。-s 在80个字符的宽屏幕中显示报告的简短（窄版）版本。-t 打印显示的每个报告的时间。时间戳格式可能取决于S_TIME_FORMAT环境变量的值（请参见下文）。-V 打印版本号然后退出-x 显示扩展统计信息。-y 如果以给定的时间间隔显示多个记录，则自系统启动以来忽略带有统计信息的第一个报告。-z 告诉iostat忽略在采样期间没有任何活动的任何设备的输出。#环境变量：iostat命令考虑以下环境变量：POSIXLY_CORRECT 设置此变量后，传输速率以512字节块而不是默认的1K块显示。#BUG /proc filesystem must be mounted for iostat to work. Kernels older than 2.6.x are no longer supported. The average service time (svctm field) value is meaningless, 1. I/O statistics are now calculated at block level 2， we don't know when the disk driver starts to process a request.#FILES /proc/stat contains system statistics. /proc/uptime contains system uptime. /proc/diskstats contains disks statistics. /sys contains statistics for block devices. /proc/self/mountstats contains statistics for network filesystems. /dev/disk contains persistent device names. iostat常用命令 iostat自启动报告以来，显示所有CPU和设备的单个历史记录 123456Device: 此列提供 /dev 目录中列出的设备（或分区）名称tps: 表示每秒发送到设备的I/O请求次数。多个逻辑请求可以组合成对设备的单个I/O请求。传输的大小不确定。Blk_read/s(kB_read/s, MB_read/s): 表示从设备读取的数据量，以每秒的块数（千字节，兆字节）表示。块等同于扇区，因此大小为512BBlk_wrtn/s(kB_wrtn/s, MB_wrtn/s): 表示写入设备的数据量，以每秒的块数（千字节，兆字节）表示。Blk_read (kB_read, MB_read): 读取的总块数（千字节，兆字节）。Blk_wrtn (kB_wrtn, MB_wrtn): 写入的总块数（千字节，兆字节）。 iostat -x 显示IO拓展信息 123456789101112131415161718Device: 此列提供 /dev 目录中列出的设备（或分区）名称r/s: 每秒设备完成的读取请求数（合并后）w/s: 每秒设备完成的写请求数（合并后）rsec/s (rkB/s, rMB/s): 每秒从设备读取的扇区数（千字节，兆字节）wsec/s (wkB/s, wMB/s): 每秒写入设备的扇区数（千字节，兆字节）rrqm/s: 每秒合并到设备中的读取请求数wrqm/s: 每秒合并到设备中的写入请求数%rrqm: 读取请求(已合并)的百分比%wrqm: 写入请求(已合并)的百分比r_await: 发送给要服务的设备的读取请求的平均时间（以毫秒为单位）包括队列中的请求所花费的时间以及为请求服务所花费的时间w_await: 发布给要服务的设备的写请求的平均时间（以毫秒为单位）包括队列中的请求所花费的时间以及为请求服务所花费的时间aqu-sz: 发出到设备的请求的平均队列长度。注意：在以前的版本中 此字段称为avgqu-szrareq-sz: 发出给设备的读请求的平均大小（以千字节为单位）wareq-sz: 发出给设备的写请求的平均大小（以千字节为单位）svctm: 发出给设备的 I/O 请求的平均服务时间（以毫秒为单位）警告！不再信任此字段。 将来的sysstat版本中将删除此字段%util: 向设备发出 I/O 请求的经过时间的百分比（设备的带宽利用率） 当该值接近100％时，发生设备饱和用于串行服务请求的设备 对于并行处理请求的设备（例如RAID阵列和现代SSD）此数字并不反映其性能限制 参考链接 Linux vmstat命令详解","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"TCP-1-传输控制协议","slug":"TCP/TCP-1-传输控制协议","date":"2020-07-26T09:25:09.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/07/26/TCP/TCP-1-传输控制协议/","link":"","permalink":"http://xboom.github.io/2020/07/26/TCP/TCP-1-%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"引言 TCP提供一种面向连接的，可靠的字节流服务 面向连接的：指通信通过三次握手建立TCP连接 可靠的： 三次握手建立连接，四次挥手释放连接； ARQ协议(Automatic Repeat-reQuest)即自动重传请求来保证数据传输的正确性； 使用分组窗口和滑动窗口协议来保证接收方能够及时处理接收到的数据，进行流量控制； 使用慢开始、拥塞避免、拥塞发生、快恢复进行拥塞控制 字节流：不在字节流中添加边界标识符 如何唯一确定一个TCP连接？ 答：源地址+源端口+目的地址+目的端口，源地址和目的地址的字段(32位)是在IP头部，源端口和目的端口的字段(16位)在TCP头部。 有一个IP的服务器监听一个端口，它的TCP最大连接数是多少？ 答：首先，存在文件描述符限制，Socket都是文件，所以通过ulimit配置文件描述符的限制；其次，存在内存限制，每个TCP连接都要占用一定内存，操作系统的内存是有限的 TCP数据结构 源端口(Source Port, 2B = 16bits)和目的端口(Destination Port, 2B = 16bit)加上IP层源IP和目的IP唯一确定一个TCP连接，一个IP地址和一个端口号称为一个插口(socket)。 序列号(Sequence Number, 4B = 32bits)标识了TCP发送端到TCP接收端的数据流的一个字节，该字节代表着包含该序列号的报文段的数据报文段的第一个字节(想象两程序直接一个方向上流动的数据流，TCP给每个字节赋予了一个序列号)。如果TCP报文中flags标志位为SYN，该序列号表示初始化序列号(ISN)，此时第一个数据应该是从序列号ISN+1开始(因为SYN标识消耗了一个序号，同理FIN标识也要占用一个序号)。序号是32bits的无符号数，序号到达最大值 2^32 - 1后又从0开始。 初始序列号可被视为一个32位的计数器，该计数器数值每4微妙加1，防止出现与其他连接的序列号重叠 确认序列号(Acknowledgment Number, 4B = 32bits)表示TCP发送确认的一端所期望接受的下一个数据分片的序列号。该序号在TCP分片中Flags标志位为ACK时生效。 TCP为应用层提供全双工服务。这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据序号。 首部长度(Data Offset/Header Length, 4bits)也叫数据偏移，该字段表示 TCP 所传输的数据部分应该从 TCP 包的哪个位置开始计算，可以看作是 TCP 首部的长度。它表示TCP头包含了多少4字节的Words。因为4bits在十进制中能表示的最大值为15，32bits表示4个字节，那么Data Offset的最大可表示15*4=60个字节。所以TCP报头长度最大为60字节。如果options fields为0的话，报文头长度为20个字节。 预留字段(Reserved field, 6bits)值全为零。预留给以后使用。 标志位(Flags, 6bits)表示TCP包特定的连接状态。一个标签位占1bit，从低位到高位值依次为 FIN: 表示发送者已经发送完数据。通常用在发送者发送完数据的最后一个包中 SYN: 在连接建立时用来同步序号。当SYN=1而ACK=0时，表明这是一个连接请求报文。对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此,SYN置1就表示这是一个连接请求或连接接受报文。 RST: 表示复位TCP连接 PSH: 通知接收端处理接收的报文，而不是将报文缓存到buffer中 ACK: 只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 URG: 表示紧急指针字段有效 补充的标志还有ECE,CWR占用保留字段2bits ECE: Explicit Congestion Notification。ECN回显。 CWR: Congestion Window Reduced。拥塞窗口减(发送方降低它的发送速率)，CWR 标志与后面的 ECE 标志都用于 IP 首部的 ECN 字段，ECE 标志为 1 时，则通知对方已将拥塞窗口缩小。 详见RFC3168。 窗口大小(Window, 2B = 16bits)表示滑动窗口的大小，用来告诉发送端接收端的buffer space的大小。接收端buffer大小用来控制发送端的发送数据数率，从而达到流量控制。最大值为2^16 = 65535B。若窗口为 0，则表示可以发送窗口探测，以了解最新的窗口大小，但这个数据必须是 1 个字节。 校验和(Checksum, 2B = 16bits)奇偶校验是对整个的 TCP 报文段(包括 TCP 头部和 TCP 数据，以 16 位字进行计算所得)由发送端计算和存储，并由接收端进行验证。 紧急指针(Urgent pointer, 2B = 16bits)只有在 URG 控制位为 1 时有效。该字段的数值表示本报文段中紧急数据的指针。从数据部分的首位到紧急指针所在的位置为止是紧急数据。 选项和填充(Option和pading)可变长度。表示TCP可选选项以及填充位。当选项不足32bits时，填充字段加入额外的0填充。最常见的可选字段: 最长报文大小MSS(Maximum Segment Size)在通信的第一个报文段(为建立连接而设置SYN标志为1的那个段)中指明这个选项，表示本端所能接受的最大报文段的长度(不包含TCP与IP头部)。选项长度不一定是32位的整数倍，所以要加填充位，即在这个字段中加入额外的零，以保证TCP头是32的整数倍 最大段大小的均值为1460，加上40字节(TCP头部和IP头部)组成1500字节 最大传输单元典型值 由于IPv6的头部比IPv4的头部长20字节，最大报文大小也减少20字节 TCP选择确认(SACK)选项 窗口缩放选项 时间戳选项和防回绕序列号 用户超时选项 认证选项 数据(Data)长度可变。用来存储上层协议的数据信息。可以为空。比如在连接建立和连接中止时。 TCP三次握手 第一次握手：建立连接时，客户端发送syn包(syn=x)到服务器，并进入SYN_SENT状态，等待服务器确认； 第二次握手：服务器收到syn包，确认客户的SYN(ack=x+1)，同时自己也发送一个SYN包(syn=y)，即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(seq=x+1, ack=y+1），此包发送完毕，客户端和服务器进入ESTABLISHED(TCP连接成功）状态，完成三次握手。 第三次握手是可以带数据的，前两次的握手不能带数据 TCP四次挥手 第一次挥手：客户端进程发出连接释放请求FIN(seq=u)，并且停止发送数据。客户端进入FIN-WAIT-1(终止等待1)状态。FIN报文段即使不携带数据，也要消耗一个序号。 第二次挥手： 服务器收到连接释放请求，发出确认报文ACK(ack=u+1)，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。整个TCP处于半关闭状态，即客户端已经没数据需要发送，但服务器若发送数据，客户端依然要接受。 客户端收到服务器的确认请求后ACK后，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。 第三次挥手： 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。 第四次挥手：服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。 套接字API还提供半关闭操作，使用shutdown()代替close()可以实现TCP半关闭 MSL与MTT的区别？ TCP状态转换 箭头表示因报文段传输、接收以及计时器超时而引发的状态转换 红色箭头表示客户端行为 虚线箭头表示服务器行为 TIME_WAIT状态又称为2MSL等待状态，在此状态TCP会等待2倍的最大段生存期(Maximum Segment Lifetime, MSL)的时间。这样做的目的是： 让TCP重新发送最终的ACK已避免丢失的情况。重新发送最终的ACK并不是因为TCP重传了ACK，而是因为通信另一方重传了它的FIN。TCP总是重传FIN，直到它收到一个最终的ACK。 当TCP处于TIME_WAIT状态的时候，通信双方将该连接(四元组)定义为不可重新使用。只有当2MSL等待结束，或一条新链接使用的初始序列号超过了连接之前的实例所使用的最高序列号时，或允许使用时间戳选项来区分之前连接实例的报文段以避免混淆时，这条连接才能重新使用。 当一个连接处于TIME_WAIT状态时，任何延迟到达的报文都会被丢弃。 FIN_WAIT_2状态可以一直保持这个状态，也就是对端一直处于CLOSE_WAIT状态，在Linux中可以通过调整变量net.ipv4.tcp_fin_timeout来设置计时器的秒数(默认60s) 问题解答 为什么在TCP首部的开始便是源和目的的端口号？ 答：一个ICMP差错报文必须至少返回引起差错的IP数据报中除了IP首部的前8个字节。当TCP收到一个ICMP差错报文时，它需要检查两个端口号以决定差错对应于哪个连接。因此，端口号必须包含在TCP首部的前8个字节里 首部校验和是怎么计算的? 为什么连接的时候是三次握手，关闭的时候却是四次握手？ 答：当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，只有等到Server端所有报文都发送完，才能发送FIN报文，因此不能一起发送。 为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？ 答：网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间，一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 为什么不能用两次握手进行连接？ 答：3次握手完成三个重要的功能 三次握手才可以阻止重复历史连接的初始化(主要原因) 三次握手可以同步双方的初始序列号 三次握手才可以避免资源浪费 两次握手可能存在死锁的情况：计算机S和C之间的通信，假定C给S发送一个连接请求分组，S收到了这个分组，并发送了确认应答分组。按照两次握手的协定，S认为连接已经成功地建立了，可以开始发送数据分组。可是，C在S的应答分组在传输中被丢失的情况下，将不知道S是否已准备好，不知道S建立什么样的序列号，C甚至怀疑S是否收到自己的连接请求分组。在这种情况下，C认为连接还未建立成功，将忽略S发来的任何数据分组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。 如果已经建立了连接，但是客户端突然出现故障了怎么办？ 答：TCP设有保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 TCP与UDP的区别 目标和源端口：告诉UDP协议应该把报文发往哪个进城 包长度：保存了UDP首部的长度和数据的长度之和 校验和：校验和是为了提供可靠的UDP首部和数据 答：如下 连接 TCP是面向链接的传输层协议，传输数据前先要建立连接 UDP是不需要连接，即刻传输数据 服务对象 TCP是一对一的两点服务，即一条连接只有两个端点 UDP支持一对一、一对多、多对多的交互方式 可靠性 TCP是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达 UDP是尽最大努力交付，不保证可靠交付(什么叫尽最大努力？？) 拥塞控制、流量控制 TCP有拥塞控制和流量控制机制，保证数据的可靠性 UDP则没有 首部开销 TCP首部长度较长、会有一定的开销，没有使用选项字段是20字节 UDP首部只有8个字节，并且固定不变 传输方式 TCP是流式传输、没有边界，但保证顺序和可靠 UDP是一个包一个包的发送，是有边界的，可能丢失或乱序 分片不同 TCP的数据大小如果大于MSS大小，则会在传输层进行分片。中途丢失分片，也只会重传分片 UDP的数据大小如果大于MTU大小，则会在IP层分片并在IP层组装。如果中途丢失分片。则需要传输所有，所以UDP包大小最好小于MTU 应用场景 TCP面向链接，保证数据可靠。如 FTP文件传输、HTTP/HTTPS UDP面向无连接，可随时发送。如 DNS、SNMP、视频/音频等多媒体通信、广播通信 为什么UDP头部有包长度 字段，而TCP头部长度则没有 答：TCP数据的长度 = IP总长度 - IP首部 - TCP头部。UDP为啥不用这个公式，可能是为了保证数据包长度为4字节的整数倍 如何在Linux系统中查看TCP状态 答：通过命令 netstat -antp 参考链接 TCP:传输控制协议 TCP的三次握手和四次挥手 TCP状态转换图解析 TCP状态转换解析","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]}],"categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"},{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"},{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"},{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"},{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"},{"name":"Interesting Algorithm","slug":"Interesting-Algorithm","permalink":"http://xboom.github.io/categories/Interesting-Algorithm/"},{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"},{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"},{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"},{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"},{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/categories/Docker/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"},{"name":"Linux Depth","slug":"Linux-Depth","permalink":"http://xboom.github.io/categories/Linux-Depth/"},{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"},{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"},{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"},{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"},{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"SMB","slug":"SMB","permalink":"http://xboom.github.io/tags/SMB/"},{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"},{"name":"TLS","slug":"TLS","permalink":"http://xboom.github.io/tags/TLS/"},{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"},{"name":"Quic","slug":"Quic","permalink":"http://xboom.github.io/tags/Quic/"},{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"},{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/tags/Docker/"},{"name":"HTTP","slug":"HTTP","permalink":"http://xboom.github.io/tags/HTTP/"},{"name":"Memory","slug":"Memory","permalink":"http://xboom.github.io/tags/Memory/"},{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"},{"name":"Event","slug":"Event","permalink":"http://xboom.github.io/tags/Event/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]}