{"meta":{"title":"XBoom Dove","subtitle":"","description":"","author":"XBoom Dove","url":"http://xboom.github.io","root":"/"},"pages":[{"title":"关于","date":"2023-08-09T14:37:17.070Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"about/index.html","permalink":"http://xboom.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2023-08-09T14:37:17.072Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"categories/index.html","permalink":"http://xboom.github.io/categories/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2023-08-09T14:37:17.016Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"repository/index.html","permalink":"http://xboom.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-08-09T14:37:17.071Z","updated":"2020-09-02T15:24:35.000Z","comments":false,"path":"tags/index.html","permalink":"http://xboom.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"TCPIP 网络编程-08-套接字选项","slug":"Unix NetWrok/TCPIP 网络编程-08-套接字选项","date":"2024-01-19T15:00:00.000Z","updated":"2024-01-24T15:03:29.685Z","comments":true,"path":"2024/01/19/Unix NetWrok/TCPIP 网络编程-08-套接字选项/","link":"","permalink":"http://xboom.github.io/2024/01/19/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-08-%E5%A5%97%E6%8E%A5%E5%AD%97%E9%80%89%E9%A1%B9/","excerpt":"","text":"套接字选项支持对套接字进行设置 1234567891011121314151617181920212223#include &lt;sys/socket.h&gt;/** * @brief 查看套接字选项 * sock 用于查看选项套接字文件描述符 * level 可查看的可选项的协议层 * optname 要查看的可选项名 * optval 保存查看结果的缓存地址值 * optlen 第四个参数 optval 的参缓存大小，返回四个参数的字节数 * @return 成功返回 0，失败返回-1*/int getsockopt(int sock, int level, int optname, void *optval, socklen_t *optlen);/** * @brief 查看套接字选项 * sock 用于更改选项套接字文件描述符 * level 可更改的可选项的协议层 * optname 要更改的可选项名 * optval 保存要更改的选项信息的缓存地址值 * optlen 第四个参数 optval 的字节数 * @return 成功返回 0，失败返回-1*/int setsockopt(int sock, int level, int optname, const void *optval, socklen_t oplen); 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/11/ 来看看 应用如下： 查看套接字类型，运行结果如下 123[root]#./client [11/client.c:23 main info] get tcp[1] sock opt :1 4 [11/client.c:27 main info] get udp[2] sock opt :2 4 但是！！！ 套接字类型只能在创建时设置，以后不能再改变 查看缓冲区大小 1234[11/client.c:33 main info] tcp send buff size:16384 [11/client.c:36 main info] tcp recv buff size:131072 [11/client.c:39 main info] udp send buff size:212992 [11/client.c:42 main info] udp recv buff size:212992 time-wait 重用 1234int option = 1;optlen = sizeof(option);ret = setsockopt(tcp_sock, SOL_SOCKET, SO_REUSEADDR, (void *)&amp;option, optlen); time_wait 在四次挥手过程中会等待 2msl 时间(大多数系统是 120s)，是为了等待最后的 ack 未收到。如果服务端发起 Fin，那么最后会因为 time_wait 而等待端口释放，那么再异常重启状态下，就需要等待大概 4 分钟端口才能恢复(客户端因为端口是随机分配的，所以不太在意立即释放)。 1net.ipv4.tcp_tw_reuse # 1 表示启用了端口重用，即允许在 TIME_WAIT 状态下的连接所使用的端口被新的连接使用。 0 表示禁止 Nagle算法 Nagle算法是一种用于减少小包传输的网络优化算法。它的目标是减少网络上的小型数据包数量，提高网络效率。会等待当前缓冲区中的数据被确认之后才发送新的数据。这有助于避免发送大量的小数据包，从而提高网络的效率 Nagle算法的原理如下： 如果长度达到 MSS，则允许发送 如果数据包中包含FIN（断开连接标致），则允许发送 当设置了TCP_NODELAY选项，则允许发送，禁止Nagle算法 未设置TCP_CORK选项时，若所有发送的小数据包（包含长度小于MSS）均被确认，则设置该选项后，内核会尽力把小的数据包拼成一个大的数据包（一个MTU）在发送出去，若到指定时间，一般为200ms，仍未组成一个MTU，也要立刻发送。 123//禁用 Nagleint flag = 1;setsockopt(socket_fd, IPPROTO_TCP, TCP_NODELAY, (char*)&amp;flag, sizeof(int)); 在某些情况下，Nagle算法可能会导致一定的延迟，特别是对于那些需要低延迟的应用程序。为了避免这种情况，可以通过设置TCP_NODELAY选项来禁用Nagle算法，强制立即发送数据(默认是开启的) 粘包 问题的缘由可能发生在发送端也可能发生在接收端： 由 Nagle 算法造成的发送端的粘包：当提交一段数据给 TCP 发送时，TCP 并不立刻发送此段数据，而是等待一小段时间看看在等待期间是否还有要发送的数据，若有则会一次把这好几段数据发送出去。 接收端接受不及时造成的接收端粘包：TCP 会把接收到的数据存在自己的缓冲区中，然后通应用层取数据。当应用层由于某些原因不能及时地把 TCP 的数据取出来，就会造成 TCP 缓冲区中存放了几段数据 解决粘包 定长消息：协议提前约定好包的长度为多少，每当接收端接收到固定长度的字节就确定一个包； 消息分隔符：利用特殊符号标志着消息的开始或者结束，例如 HTTP 协议中的换行符； 长度前缀：先发送N个字节代表包的大小（注意大端和小端问题），后续解析也按长度读取解析。 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-07-域名地址转换","slug":"Unix NetWrok/TCPIP 网络编程-07-域名地址转换","date":"2024-01-14T15:00:00.000Z","updated":"2024-01-24T15:04:02.604Z","comments":true,"path":"2024/01/14/Unix NetWrok/TCPIP 网络编程-07-域名地址转换/","link":"","permalink":"http://xboom.github.io/2024/01/14/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-07-%E5%9F%9F%E5%90%8D%E5%9C%B0%E5%9D%80%E8%BD%AC%E6%8D%A2/","excerpt":"","text":"访问服务端的时候不能总是通过 IP 访问而是通过域名，于是就有了域名与地址转换 12345678910111213141516171819202122232425#include &lt;netdb.h&gt;/*** @brief 通过传递字符串格式的域名获取IP地址* @return 成功时返回hostent结构体地址，失败时返回 NULL指针*/struct hostent *gethostbyname(const char* hostname);struct hostent&#123; char * h_name; //官网域名，代表着某一个主页(可能很多注明的域名并未注册官方域名) char ** h_aliases; //通过多个域名访问同一个主页。同一个IP可以绑定多个域名，所以除了官方域名还可以指定其他域名 int h_addrtype; //地址类型 IPv4则是AF_INET int h_length; //保存IP地址长度(单位B)。若是IPv4地址，则是4; IPv6 则是16 char ** h_addr_list; //以证书形式保存域名对应的IP地址，可能多个IP分配给同一个域名&#125;/*** 利用IP地址获取域名* addr 含有IP地址信息的in_addr结构体指针。为了同时传递IPv4地址之外的其他细心，该变量类型声明为char指针* len 第一个参数的字节数，IPv4时为4，IPv6 时为16* family 传递协议族信息，IPv4时为AF_INET，IPv6 时为AF_INET6* 成功返回hostent结构体变量地址值，失败返回NULL指针*/struct hostent *gethostbyaddr(const char *addr, socklen_t len, int family); 有了上面的函数解析 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/10/ 来看看 1234567891011121314[root]#./client www.naver.com[10/client.c:12 main info] www.naver.com get host name: [10/client.c:13 main info] h_name:www.naver.com.nheos.com [10/client.c:14 main info] h_aliases: [10/client.c:17 main info] [0]:www.naver.com [10/client.c:19 main info] type:IPv4 [10/client.c:20 main info] h_length:4 [10/client.c:21 main info] h_addr_list: [10/client.c:24 main info] [0]:223.130.200.104 [10/client.c:24 main info] [1]:223.130.200.107 [root]#./server 223.130.200.104[10/server.c:14 main error] gethostbyaddr failed Unknown host [root]#./server 223.130.200.107[10/server.c:14 main error] gethostbyaddr failed Unknown host 并不能总是通过 IP 地址获取到域名 通过抓去端口 53 的报文(DNS)可以得到 通过 UDP 去发送 DNS 请求，一次返回多个 IP 地址，而无法获取域名则如下返回 No such name 需要注意的就是 inet_ntoa(*(struct in_addr *)addr_info-&gt;h_addr_list[i]) 为什么是一个结构体，而要使用 char* 进行存储 这是因为 这个结构体并不一定有 IPv4，也有可能是 IPv6；另外在定义这个标准之前还没有 void * 标准，所以使用了 char* 定义 不过目前比较常用的是 getaddrinfo 更加灵活 123456789101112131415161718192021#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;netdb.h&gt;int getaddrinfo(const char *node, const char *service, const struct addrinfo *hints, struct addrinfo **res);// node 参数是要解析的主机名或 IP 地址，可以为 NULL。// service 参数是服务名或端口号的字符串表示，可以为 NULL。// hints 参数是一个指向 struct addrinfo 结构的指针，用于提供解析的提示信息。// res 参数是一个指向 struct addrinfo 结构链表的指针，用于保存解析结果。struct addrinfo &#123; int ai_flags; /* AI_PASSIVE, AI_CANONNAME, AI_NUMERICHOST */ int ai_family; /* PF_xxx */ int ai_socktype; /* SOCK_xxx */ int ai_protocol; /* 0 or IPPROTO_xxx for IPv4 and IPv6 */ socklen_t ai_addrlen; /* length of ai_addr */ char *ai_canonname; /* canonical name for hostname */ struct sockaddr *ai_addr; /* binary address */ struct addrinfo *ai_next; /* next structure in linked list */&#125;; 最终输出结果为 123getaddrinfo: [10/client.c:55 main info] ip: 182.61.200.7 [10/client.c:55 main info] ip: 182.61.200.6 最后还有两个小知识点 hstrerror(h_errno) ：h_errno 是一个全局错误码，而 hstrerror 用来解析 host相关的错误 inet_ntop（Internet Network TO Presentation）二进制字符串转换 1234567#include &lt;arpa/inet.h&gt;const char *inet_ntop(int af, const void *src, char *dst, socklen_t size);// af 是地址族，可以是 AF_INET（IPv4）或 AF_INET6（IPv6）。// src 是指向存储二进制形式地址的指针。// dst 是一个用于存储字符串形式地址的缓冲区。// size 是缓冲区的大小 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-06-套接字断开","slug":"Unix NetWrok/TCPIP 网络编程-06-套接字断开","date":"2024-01-08T15:00:00.000Z","updated":"2024-01-24T15:04:09.388Z","comments":true,"path":"2024/01/08/Unix NetWrok/TCPIP 网络编程-06-套接字断开/","link":"","permalink":"http://xboom.github.io/2024/01/08/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-06-%E5%A5%97%E6%8E%A5%E5%AD%97%E6%96%AD%E5%BC%80/","excerpt":"","text":"close 是关闭套接字，调用之后无法再通过 write 与 read 读写数据。但是通过查看 TCP 的四次挥手我们知道 在 close 之后，对端可能还会发送一部分数据，这部分数据其实是丢失的。所以就引入了半关闭，只能发送数据但是不能接收 或者 只能接收数据但是不能发送。 这里提到了一个需求：客户端向服务器发起请求，然后服务端将文件传输给客户端之后，客户端接收完文件，然后告知服务端接收完毕了，客户端自动退出。 这里有一个疑问，客户端怎么知道接收完文件了？ 从 read 函数返回-1表示接收结束，但是read 返回-1 表示对端关闭了连接，那么需求中接收完文件还要发送响应就不行了 写一个标志位，告知这个标志位就表示文件结束。但是这样的话就必须让文件中没有与标志符一样的内容 这个时候就用到了半关闭 1234567891011/** * @brief 半关闭 * @param * sock 要被关闭的套接字 * howto 传递断开的方式 * SHUT_RD: 断开输入流 * SHUT_WR: 断开输出流 * SHUT_RDWR: 同时断开输入输出流 * @return 成功返回 0；失败返回 -1 */int shutdown(int sock, int howto); 知道了 shutdown 函数之后，看看是如何实现的 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/09/ 运行结果如下 1234[root]#./server 5555[09/server.c:57 main info] server recv recv all file [root]#./client 127.0.0.1 5555 另外通过抓包可以看出跟平时的四次挥手不一样的地方(5555 表示服务端) 首先通过三次握手(1~3) 服务端第一次发送数据给客户端，客户端确认(4~5) 服务端第二次发送数据给客户端，客户端确认(6、8) 在服务端第二次发送数据的时候，同时发送了FIN 结束(7)，也就是调用的 shutdown(client_sock, SHUT_WR); 客户端接着向服务端发送数据，客户端发送响应(9-10)，发现客户端调用的 shutdown(server_sock, SHUT_RD); 并没有作用，因为已经收到了服务端的 FIN 报文 发送结束之后，客户端通过 close 发起 FIN 对于两个 shutdown 可以看成下图这样 shotdown 分别对应的是两个流，当一端关闭之后就不需要再次关闭了。而一条流的关闭也不会影响另外一条流 那么如果改变一下服务端的关闭流程，先关闭读，再关闭写。改成如下 12345678910//关闭读shutdown(client_sock, SHUT_RD);memset(buff, 0 , sizeof(buff));while(read(client_sock, buff, sizeof(buff)))&#123; LOG_INFO(\"server recv %s\", buff);&#125;//关闭写shutdown(client_sock, SHUT_WR); 程序运行结果如下，好像服务端没有什么 1234[root]#./server 5555 [root]#./client 127.0.0.1 5555[root]# 看看交互报文 服务端发送完数据之后，关闭了读 shutdown(client_sock, SHUT_RD); 客户端接收完数据之后也关闭了读，导致客户端与服务端都阻塞等待 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-05-基于UDP的CS","slug":"Unix NetWrok/TCPIP 网络编程-05-基于UDP的CS","date":"2024-01-02T15:00:00.000Z","updated":"2024-01-24T15:04:18.804Z","comments":true,"path":"2024/01/02/Unix NetWrok/TCPIP 网络编程-05-基于UDP的CS/","link":"","permalink":"http://xboom.github.io/2024/01/02/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-05-%E5%9F%BA%E4%BA%8EUDP%E7%9A%84CS/","excerpt":"","text":"在 《TCPIP 网络编程-02-套接字类型与协议设置》 中通过 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/03/学习了基于UDP的CS通信，可以发现 Client 是不需要发起connect的，直接使用 sendto 进行发送 Server 不需要 listen/accept，而是直接通过recvfrom进行数据接收 新增了两个函数 sendto 与 recvfrom 123456789101112131415161718192021222324252627282930#include &lt;sys/socket.h&gt;/** * @brief 使用UDP套接字进行发送 * @param * sock 用于传输数据的UDP套接字文件描述符 * buff 保存待传输数据的缓冲地址值 * nbytes 待传输的数据长度，以B为单位 * flags 可选参数、没有则为0 * to 存有目的地址信息的sockaddr解耦提变量的地址值 * addrlen 参数to的地址值结构体变量长度 * @return 成功返回传输的字节数，失败时返回-1*/ssize_t sendto(int sock, void *buff, size_t nbytes, int flags, struct sockaddr *to, socklen_t addrlen);/** * @brief 使用UDP套接字接收数据 * @param * sock 用于接收数据的UDP套接字文件描述符 * buff 保存接收数据的缓冲地址值 * nbytes 可接收的最大字节数，故无法超过参数buff所指的缓冲大小 * flags 可选项参数，若没有则传入0 * from 存有发送端地址信息的sockaddr结构体变量的地址值 * addrlen 保存参数from的结构体变量长度的变量值 * @return 成功返回传输的字节数，失败时返回-1 */ssize_t recvfrom(int sock, void *buff, size_t nbytes, int flags, struct sockaddr *from, socklen_t *addrlen); 写一个基于UDP的回声CS，示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/07/。运行结果如下 1234567[root]#./client 127.0.0.1 555512345[07/client.c:28 main info] client send 12345 to 127.0.0.1 [07/client.c:36 main info] client recv 12345 from 127.0.0.1 [root]#./server 5555[07/server.c:33 main info] recv message 12345 from 127.0.0.1 从上面可以看出，**CS都不需要连接，**那么客户端地址又是如何分配的，答案是在 sendto的时候，首次调用 sendto 的时候会自动给套接字分配 IP 与端口，然后一直沿用到套接字关闭。 所以 客户端其实可以使用 connect 当然此处的作用就不是连接，而是分配地址(TCP 也是在这个时候分配的地址); 又因为套接字有了地址之后，就可以不用 sendto 而直接使用 write 进行发送; 而且CS均只需要一个套接字(即只需要一个套接字就能和多个主机通信) 所以最后将示例 7 修改为 示例8：https://github.com/XBoom/network-ip.git 中的 apps/socket/08/，效果一致。通过 connect 进行地址分配，通过 read/write 进行消息发送接收。运行结果如下： 123456[root]#./server 5555[08/server.c:33 main info] recv message 12345 from 127.0.0.1 [root]#./client 127.0.0.1 555512345[08/client.c:36 main info] recv server response:12345 TIPS: sendto 函数主要分为三个阶段 向 UDP 套接字注册 IP 地址与端口 传输数据 删除 UDP 套接字中注册的 IP 地址与套接字 sendto 函数每次调用都执行上述三个阶段，将未注册 IP 地址与端口的称为未连接的 UDP 套接字。用于向不同的目标发送数据 相反，通过 connect 将 IP 地址与端口注册到 UDP 套接字，称为 已连接的 UDP 套接字。用于向同一个目标发送数据 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-04-基于TCP的CS","slug":"Unix NetWrok/TCPIP 网络编程-04-基于TCP的CS","date":"2023-12-27T15:00:00.000Z","updated":"2024-01-24T15:04:30.498Z","comments":true,"path":"2023/12/27/Unix NetWrok/TCPIP 网络编程-04-基于TCP的CS/","link":"","permalink":"http://xboom.github.io/2023/12/27/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-04-%E5%9F%BA%E4%BA%8ETCP%E7%9A%84CS/","excerpt":"","text":"有了地址的设置函数之后，接下来就是开始进行连接 服务端存在两个函数 1234567891011121314151617#include &lt;sys/socket.h&gt;/** * 通过调用 listen 函数进入等待连接请求状态。(只有调用 listen，客户端才能发起 connect) * sock 希望进入等待连接请求的状态的套接字文件描述 * backlog 连接请求等待队列(Queue)的长度，若位 5，表示最多使5个连接请求进入队列*/int listen(int sock, int backlog);/** * 受理客户端连接请求，返回一个套接字连接到发起请求的客户端 * sock 服务端套接字的文件描述符 * addr 保存发起连接请求的客户端地址信息的变量地址值 * addrlen 第二个参数 addr 结构体的长度 * 成功返回创建的套接字文件描述符，失败返回-1*/int accept(int sock, struct sockaddr * addr, socklen_t * addr_len); accept 函数受理连接请求等待队列中待处理的客户端连接请求 客户端使用 connect发起连接请求 1234567/** * 向服务端发起请求连接 * sock 客户端套接字文件描述符 * servaddr 保存目标服务端地址信息的变量地址值 * addrlen 第二个参数结构体的长度*/int connect(int sock, struct sockaddr * server_addr, socklen_t addr_len); 客户端调用 connect 之后，可能发生以下情况之一才返回(完成函数调用) 服务端接收连接请求 发生断网导致一场情况而中断连接请求(阻塞的) 客户端connect的时候并没有指定客户端的地址，那么是什么时候分配的ip 与端口？其实是自动分配的，所以客户端无需调用标记的 bind 函数进行分配 TIPS: 接收连接并不意味着服务端调用 accept 函数，而是服务端把连接请求信息记录到等待队列。因此 connect 函数返回之后并不立即进行数据交换，那么问题来了，客户端既然不知道服务端是否已经 accept 了，那么如果客户端立即 write 发送数据，数据到服务端会是一个什么状态 答：如果服务器端尚未调用 accept，连接处于已建立但未接受的状态。在这种情况下，发送的数据将保留在等待队列中，等待服务器端调用 accept 来接受连接。一旦服务器端调用了 accept，则可以开始在客户端和服务器端之间进行数据交换 所以函数的调用关系如下 这章要求是写一个回写 CS，代码：https://github.com/XBoom/network-ip.git 中的 apps/socket/06/执行结果，服务端接收到消息之后恢复给客户端，有两种情况，每一次写入都原样写回；第二种是每次的写入收集起来一次写回 要求： 服务器端在同一时刻只能与一个客户端相连，并提供回声服务 服务器端一次向5个客户端提供服务并退出 客户端接收用户输入的字符串并发送到服务器端 服务器端将接收的字符串数据传回客户端，即“回声服务” 服务器端与客户端之间的字符串回声一致执行到客户端输入Q 1234567891011121314151617[root]#./server 5555[06/server.c:44 main info] accept socket 4 [06/server.c:49 main info] recv message:client1 [06/server.c:55 main info] reponse len 7 message:client1 [06/server.c:44 main info] accept socket 4 [06/server.c:49 main info] recv message:client2 [06/server.c:55 main info] reponse len 7 message:client2 [06/server.c:44 main info] accept socket 4 [06/server.c:49 main info] recv message:client3 [06/server.c:55 main info] reponse len 7 message:client3 [06/server.c:44 main info] accept socket 4 [06/server.c:49 main info] recv message:client4 [06/server.c:55 main info] reponse len 7 message:client4 [06/server.c:44 main info] accept socket 4 [06/server.c:49 main info] recv message:client5 [06/server.c:55 main info] reponse len 7 message:client5 [06/server.c:60 main info] reponse message end 读写操作 read/write 由于是无边界，数据存储在IO缓存区等待读写 IO缓存区有以下特点 IO缓冲在每个TCP套接字中单独存在 IO缓冲在创建套接字时自动生成 关闭套接字会继续传递输出缓冲中遗留的数据 关闭套接字会丢失输入缓冲中的数据 TIPS: 输出不丢失是因为系统自动进行发送操作，而输入丢失是因为套接字关闭了输入缓冲中的数据也没有人能够读到了，也就丢掉 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-03-地址族与数据序列","slug":"Unix NetWrok/TCPIP 网络编程-03-地址族与数据序列","date":"2023-12-22T15:00:00.000Z","updated":"2024-01-24T15:04:38.465Z","comments":true,"path":"2023/12/22/Unix NetWrok/TCPIP 网络编程-03-地址族与数据序列/","link":"","permalink":"http://xboom.github.io/2023/12/22/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-03-%E5%9C%B0%E5%9D%80%E6%97%8F%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97/","excerpt":"","text":"上一章《套接字类型与协议设置》是构建套接字，这一章就是给地址分配 IP 地址和端口号 首先是结构体 123456789101112struct sockaddr_in&#123; sa_family_t sin_family; //地址族(Address Family) __uint8_t uint16_t sin_port; //16 位 TCP/UDP 端口号 struct in_addr sin_addr; //32 位 IP 地址 char sin_zero[8]; //不使用&#125;struct in_addr&#123; in_addr_t s_addr; //32 位 IPv4 地址 (__uint32_t) &#125; 其中 sin_zero 是为了 使 struct sockaddr_in 与 struct sockaddr 结构大小一致而插入的成员，必须填充为0 struct sockaddr 的结构如下 12345struct sockaddr&#123; sa_family_t sin_family; //协议族 char sa_data[14]; //地址信息&#125; sa_data 保存的地址信息包含了地址与端口号，剩余部分填充为0 tips: 服务端套接字虽然要绑定一个端口，但是同一个端口 TCP套接字和UDP套接字是可以共用的（虽然一般不共用） 字节序转换有以下内容 网络字节序规定统一使用大端 如何记住大小端：端(即 开端)，地址从低到高，低字节在低地址就是小端，高字节在低地址就是大端 即使系统本身是大端，最好也调用一下，反正也不会有任何变化。提升代码兼容性 字节序转换函数 1234unsigned short htons(unsigned short);unsigned short ntohs(unsigned short);unsigned long ntohl(unsigned long);unsigned long ntohl(unsigned long); 网络地址的转换 12345678#include &lt;arpa/inet.h&gt;//成功时返回32位大端序整数型值，失败返回INADDR_NONE(检查无效IP地址)in_addr_t inet_addr(const char * string);//与上面功能一样，成功返回1，失败返回0int inet_aton(const char * string, struct in_addr *addr);//将网络字节序地址转换为字符串形式(注意返回结果的释放)char *inet_ntoa(struct in_addr *addr); ntoa 可以理解为 network to address，aton 同理 重写代码地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/05/执行结果 123456789[05/server.c:16 main info] host ordered port: 0x1234 [05/server.c:17 main info] net ordered addr: 0x3412 [05/server.c:18 main info] host ordered post: 0x12345678 [05/server.c:19 main info] net ordered post: 0x12345678 [05/server.c:31 main info] inet addr 127.0.0.1 success 0x100007f [05/server.c:37 main error] inet addr 127.0.0.258 failed [05/server.c:53 main info] inet_aton 127.0.0.1 success 0x100007f [05/server.c:58 main error] inet_aton 127.0.0.258 failed [05/server.c:69 main info] inet ntoa 127.0.0.1 127.0.0.1 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-02-套接字类型与协议设置","slug":"Unix NetWrok/TCPIP 网络编程-02-套接字类型与协议设置","date":"2023-12-13T15:00:00.000Z","updated":"2024-01-24T15:04:50.121Z","comments":true,"path":"2023/12/13/Unix NetWrok/TCPIP 网络编程-02-套接字类型与协议设置/","link":"","permalink":"http://xboom.github.io/2023/12/13/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-02-%E5%A5%97%E6%8E%A5%E5%AD%97%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%8D%8F%E8%AE%AE%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"第一章《TCPIP 网络编程-01-入门》讲到了 socket 函数，这里将详细看看套接字逻辑 12345678#include &lt;sys/socket.h&gt;/** * @brief 创建套接字 * domain 套接字中使用的协议族信息 * type 套接字数据传输类型的信息 * protocol 计算机见通信中使用的协议信息*/int socket(int domain, int type, int protocol); domain参数代表的协议族分类可分为以下几类 PF_INET: IPv4 互联网协议族 PF_INET6: IPv6 互联网协议族 PF_LOCAL: 本地通信的 UNIX 协议族 PF_PACKET: 底层套接字的协议族 PF_IPX: IPX Novell 协议族 type 参数代表的套接字数据的传输方式 SOCK_STREAM: 面向连接的套接字(TCP)。 面向连接的特点，传输过程中数据不会消失，按序传输数据，传输的数据不存在数据边界(面向连接的可靠字节流) 可靠的(不丢失) 按序传输数据 传输的数据没有边界 SOCK_DGRAM: 面向消息的套接字(UDP)。有以下特点 强调快速而非传输顺序 传输的数据可能丢失也可能损毁 传输数据有边界 限制每次传输的数据大小 protocol 参数代表决定了最终的协议，为什么前两个参数无法决定？ 因为同一协议族中可能存在多个数据传输方式相同的协议。所以一般可以传一个 0 表示使用默认协议 Tips: 为了接收数据，套接字内部有字节数组缓冲区。调用 read 函数从缓冲区读取部分数据，所以缓冲区并不总是满的。即使读的比较慢，缓冲区满了，套接字无法再接受数据，也不会发生数据丢失。因为套接字会根据接收端的状态传输数据(滑动窗口)，传输错误也会有重传服务 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/03/ 运行结果如下： 1234[03/server.c:44 main info] server recv 48 len msssage:client message client message client message 可以看到，即使一个一个字符的读取也是能读到所有的内容(注意字符串结束符会影响打印) 问题 1：既然数据是没有边界的，那么上层是如何获取对应的数据结构的？ 固定长度的消息：每次读取固定长度，既浪费空间也不灵活 消息头包含长度信息：在消息头包含一个字段，表示整个消息的长度。接收方先读消息头，然后根据长度信息读取对应长度的数据 使用消息格式协议：使用 JSON、 XML 等消息格式协议 使用消息起始标志与结束标志：通过检测这些标志来确定消息的边界 问题 2：如何使用 SOCK_DGRAM 又如何编程？ 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/04/ 区别在于 客户端并不需要执行连接请求，只需要在发送的时候指定发送地址。而服务端也不需要 bind/listen 12345678for(int i = 0; i &lt; 3; i++) &#123; char message[MAX_BUFF_LEN] = \"client message \\n\"; ret = sendto(server_sock, message, strlen(message) + 1, 0, (const struct sockaddr*)&amp;server_addr, server_addr_len); CHECK_RET(ret == -1, \"sendto failed\");&#125; 而服务端也不需要监听，直接读取数据 123456while((read_len = recvfrom(server_sock, &amp;message[str_len], MAX_BUFF_LEN, 0, (struct sockaddr *)&amp;client_addr, &amp;clietn_addr_len)) &gt; 0)&#123; LOG_INFO(\"recv %d %d\", str_len, read_len); str_len += read_len;&#125; 而且数据也不一个一个读取，这是读取 MAX_BUFF_LEN 的结果 123[04/server.c:32 main info] recv 0 17 [04/server.c:32 main info] recv 17 17 [04/server.c:32 main info] recv 34 17 而这是一次读取一个的情况 123[04/server.c:32 main info] recv 0 1 [04/server.c:32 main info] recv 1 1 [04/server.c:32 main info] recv 2 1 每次读取一个之后整个数据其实就丢了 最后来看看正常情况下是不是实现了 UDP 通信 可以看到 UDP 并不需要三次握手也就不需要 connect 以及 listen 这类操作直接进行数据发送与接收，详细的过程后续在介绍 UDP 在详细查看 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-00-简介","slug":"Unix NetWrok/TCPIP 网络编程-00 简介","date":"2023-12-07T15:00:00.000Z","updated":"2024-01-24T15:05:00.874Z","comments":true,"path":"2023/12/07/Unix NetWrok/TCPIP 网络编程-00 简介/","link":"","permalink":"http://xboom.github.io/2023/12/07/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-00%20%E7%AE%80%E4%BB%8B/","excerpt":"","text":"学习《TCP&amp;IP 网络编程》，将书中的代码结合自己的项目结构重新写一遍 项目地址在 https://github.com/XBoom/network-ip.git 目录 apps/socket/ 对应章节如下 01 第一章 理解网络编程和套接字 02 第一章 理解网络编程-从文件读写发送 编译方法只需要在 apps/socket/ 目录下执行 make DIR=&lt;directory&gt; 即可编译对应目录下的文件 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"TCPIP 网络编程-01-入门","slug":"Unix NetWrok/TCPIP 网络编程-01-入门","date":"2023-12-07T15:00:00.000Z","updated":"2024-01-24T15:04:57.569Z","comments":true,"path":"2023/12/07/Unix NetWrok/TCPIP 网络编程-01-入门/","link":"","permalink":"http://xboom.github.io/2023/12/07/Unix%20NetWrok/TCPIP%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-01-%E5%85%A5%E9%97%A8/","excerpt":"","text":"带着问题看世界： 套接字在网络编程中的作用是什么？为何称为套接字 比较说明 listen 函数和 accept 函数的作用 创建套接字后一般会为它分配地址，为什么？为了完成地址分配需要调用哪些函数 入门例子就是写一个客户端与服务器通信，示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/01/ 1234567891011121314151617181920212223242526272829303132#include &lt;sys/socket.h&gt;/** * 类似打电话的过程 * 1. 拥有一个电话机 * 2. 准备自己的电话号码 * 3. 插上电话线准备监听 * 4. 拿起电话进行对话 * 1. 有人打电话*///构建套接字，成功返回文件描述符，失败返回-1int socket(int domain, int type, int protocol);//给套接字分配地址(IP PORT)，成功返回0，失败返回-1int bind(int sockfd, struct sockaddr *myaddr, socklen_t addrlen);//开始监听(将套接字转为可接受请求状态)，成功返回 0，失败返回-1int listen(int sockfd, int backlog);//处理连接请求，成功返回文件描述符，失败时返回-1int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);//发起套接字请求int connect(int sockfd, struct sockaddr *serv_addr, socklen_t addrlen);#include &lt;arpa/inet.h&gt;//用于将点分十进制的IP地址字符串转换为网络字节序（big-endian）的32位IPv4地址in_addr_t inet_addr(const char *cp); sockaddr 与 sockaddr_in 的区别 sockaddr 用来代表套接字地址，IPv4 or IPv6 sockaddr_in 用来特指 IPv4 所以套接字接口使用的是 struct sockaddr，如果定义结构如果使用的是 struct sockaddr_in 则注意强制转换 为了对比，这里也把书中的文件例子重写了一遍，从client.txt读取文件然后写入到另外一个文件server.txt 中 示例地址：https://github.com/XBoom/network-ip.git 中的 apps/socket/02/ 12345678910111213141516171819202122232425262728293031323334/** * 打开文件 成功时返回文件描述符，失败时返回-1 * path 文件名的字符串地址 * flag 文件打开模式信息 * O_CREAT 必要时创建文件 * O_TRUNC 删除全部现有文件 * O_APPEND 维持现有数据，保存到其后面 * O_RDONLY 只读打开 * O_WRONLY 只写打开 * O_RDWR 读写打开*/int open(const char *path, int flag);/** * 关闭文件 * 成功时返回 0，失败时返回-1*/int close(int fd);/** * 数据写入文件 * fd 文件描述符 * buf 数据的缓冲地址值 * nbytes 要传输的字节数*/ssize_t write(int fd, const void *buf, size_t nbytes);/** * 读取文件中的数据 * fd 文件描述符 * buf 数据的缓冲地址值 * nbytes 要传输的字节数*/ssize_t read(int fd, void *buf, size_t nbytes); 服务端运行结果 1234[root]#./server 5555[02/server.c:9 write_file info] ./02/server.txt TCP/IP 网络编程 client.txt [02/server.c:18 write_file info] wirte success [02/server.c:66 main info] write file success 客户端结果 123[root]#./client 127.0.0.1 5555[02/client.c:50 main info] client send msg success [02/client.c:56 main info] receive server reponse:write file success 参考文档 《TCPIP 网络编程》","categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"}]},{"title":"数据结构-02-Heap","slug":"Data Structure/数据结构-02-Heap","date":"2023-09-28T16:16:00.000Z","updated":"2023-09-28T16:22:33.072Z","comments":true,"path":"2023/09/29/Data Structure/数据结构-02-Heap/","link":"","permalink":"http://xboom.github.io/2023/09/29/Data%20Structure/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-02-Heap/","excerpt":"","text":"在阅读 libevent 库中另外一个用来存储事件的第二个结构就是小根堆，这里重新学习一下它的实现 代码路径:minheap-internal.h 12345typedef struct min_heap&#123; struct event** p; size_t n, a; // n 表示堆的元素数目，a 表示当前分配的内存空间大小&#125; min_heap_t; struct event** p 区别与使用数组进行存储，这样就可以实现动态扩容 堆初始化 一开始没有元素，而要做的就是初始化一个最小堆对象 min_heap_t 1234//event.c:650 event_base_new_with_configmin_heap_ctor_(&amp;base-&gt;timeheap);void min_heap_ctor_(min_heap_t* s) &#123; s-&gt;p = 0; s-&gt;n = 0; s-&gt;a = 0; &#125; 初始化堆，这里有一个小技巧就是将 s-&gt;p= 0 , 就是不指向任何对象，相当于 NULL，min_heap_t* a = 0;，同时堆的分配内容大小 a 一开始并没指定**(懒加载形式)** 其他辅助函数包括 123456int min_heap_empty_(min_heap_t* s) &#123; return 0 == s-&gt;n; &#125; //堆是否为空size_t min_heap_size_(min_heap_t* s) &#123; return s-&gt;n; &#125; //堆的节点数量struct event* min_heap_top_(min_heap_t* s) &#123; return s-&gt;n ? *s-&gt;p : 0; &#125; //堆顶节点void min_heap_dtor_(min_heap_t* s) &#123; if (s-&gt;p) mm_free(s-&gt;p); &#125; //释放堆//节点为堆中最大索引(最后一个节点)void min_heap_elem_init_(struct event* e) &#123; e-&gt;ev_timeout_pos.min_heap_idx = EV_SIZE_MAX; &#125; 添加节点 1234//event.c:2217 event_assignmin_heap_elem_init_(ev);void min_heap_elem_init_(struct event* e) &#123; e-&gt;ev_timeout_pos.min_heap_idx = EV_SIZE_MAX; &#125; 在添加节点的时候，首先指定节点在堆中的索引是 EV_SIZE_MAX，表示并没有加入到堆中，接着加入到堆中 1234567int min_heap_push_(min_heap_t* s, struct event* e)&#123; if (min_heap_reserve_(s, s-&gt;n + 1)) return -1; min_heap_shift_up_(s, s-&gt;n++, e); return 0;&#125; 这里就发生了两步操作 因为可能堆已经满了，所以需要将堆进行扩容 min_heap_reserve_ 堆化，需要调整成小顶堆 扩容 首次分配的容量大小是8，后续依次加倍 问题1：如果节点新增很多，会不会导致内存分配暴涨？ golang 的 map 有一个 2倍增长到了一定数量再1.25倍增长的过程 123456789101112131415int min_heap_reserve_(min_heap_t* s, size_t n)&#123; if (s-&gt;a &lt; n) //判断容量是否不够 &#123; struct event** p; size_t a = s-&gt;a ? s-&gt;a * 2 : 8; // 小于8则使用8，否则加倍增长 if (a &lt; n) //如果仍然小于n, 则直接使用 n a = n; if (!(p = (struct event**)mm_realloc(s-&gt;p, a * sizeof *p))) return -1; s-&gt;p = p; s-&gt;a = a; &#125; return 0;&#125; 当容量不够的时候，需要进行扩容操作，不需要考虑 golang map 那种因为装载量过大而影响查询效率的情况 容量首先是8，然后依次递增，当等待事件比较多，注意内存占用(一大块内存) 使用 realloc 操作，直接保留了原来的内容(realloc 的底层并不一定在原有的基础上，因为无法保证内存后面是否仍然有足够的空闲空间） 向上调整 元素入堆的基本流程是： 将元素添加到堆尾(堆最后一个元素) 将元素与父节点进行比较 a. 如果比父节点大则与父节点交换位置 然后重复 1～2 操作 b. 如果不比父节点大则停止入堆，完成元素添加 接着看它的实际操作 12345678910111213//min_heap_shift_up_(s, s-&gt;n++, e);void min_heap_shift_up_(min_heap_t* s, size_t hole_index, struct event* e)&#123; //找到节点的父节点 size_t parent = (hole_index - 1) / 2; while (hole_index &amp;&amp; min_heap_elem_greater(s-&gt;p[parent], e)) &#123; (s-&gt;p[hole_index] = s-&gt;p[parent])-&gt;ev_timeout_pos.min_heap_idx = hole_index; hole_index = parent; parent = (hole_index - 1) / 2; &#125; (s-&gt;p[hole_index] = e)-&gt;ev_timeout_pos.min_heap_idx = hole_index;&#125; hole_index 一开始为堆的最后一个元素的索引，即当前事件 e 的位置 找到最后一个节点的父节点 (hole_index - 1) / 2; 如果当前位置 hole_index 还不是堆顶(hole_index != 0 )且 父节点超时时间比自己大 当前元素等于父节点，索引位置更新为 hole_index 当前索引 hole_index 为父节点索引，hole_index = parent; 找到父节点位置 重复操作 3 更新当前节点为e，设置索引为 hole_index 比较逻辑则是使用超时时间进行对比 12345678#define min_heap_elem_greater(a, b) \\ (evutil_timercmp(&amp;(a)-&gt;ev_timeout, &amp;(b)-&gt;ev_timeout, &gt;))//如果秒相同就是用毫秒#define evutil_timercmp(tvp, uvp, cmp) \\ (((tvp)-&gt;tv_sec == (uvp)-&gt;tv_sec) ? \\ ((tvp)-&gt;tv_usec cmp (uvp)-&gt;tv_usec) : \\ ((tvp)-&gt;tv_sec cmp (uvp)-&gt;tv_sec)) 弹出节点 有增加就有删除，出堆常规操作是 将堆顶与堆尾元素进行交换 将堆尾元素出堆 从对顶元素开始从上到下，与子节点比较交换 看看它是如何做到的 1234567891011struct event* min_heap_pop_(min_heap_t* s)&#123; if (s-&gt;n) &#123; struct event* e = *s-&gt;p; min_heap_shift_down_(s, 0, s-&gt;p[--s-&gt;n]); e-&gt;ev_timeout_pos.min_heap_idx = EV_SIZE_MAX; return e; &#125; return 0;&#125; --s-&gt;n 因为要弹出，所以 数量减 1，s-&gt;p[--s-&gt;n] 指向堆尾 如果堆元素数量大于 0，记录堆顶事件。否则返回 0(与前面一样，0 指向指针表示 NULL) 向下调整，注意最后一个节点指向的是最后一个节点 更新出堆事件的索引(表示不在堆里面) 向下调整 将 hole_index 与 struct event* e 进行交换 1234567891011121314void min_heap_shift_down_(min_heap_t* s, size_t hole_index, struct event* e)&#123; size_t min_child = 2 * (hole_index + 1); while (min_child &lt;= s-&gt;n) &#123; min_child -= min_child == s-&gt;n || min_heap_elem_greater(s-&gt;p[min_child], s-&gt;p[min_child - 1]); if (!(min_heap_elem_greater(e, s-&gt;p[min_child]))) break; (s-&gt;p[hole_index] = s-&gt;p[min_child])-&gt;ev_timeout_pos.min_heap_idx = hole_index; hole_index = min_child; min_child = 2 * (hole_index + 1); &#125; (s-&gt;p[hole_index] = e)-&gt;ev_timeout_pos.min_heap_idx = hole_index;&#125; 找到节点的右子节点 min_child 如果右节点存在(不应该超过数目上线) 找到左右节点种较大的 如果当前超时时间比子节点小则停止 为什么有 min_child == s-&gt;n 的比较操作，感觉没有必要！！因为 while (min_child &lt;= s-&gt;n) 已经判断了 当前节点为子节点，并更新索引 更新当前节点位置 找到当前节点的右子节点 当前位置设置为 e，并更新事件的索引 删除元素 删除元素常用于事件发生异常，需要从堆中直接删除而不是从堆顶，这个时候就会出现堆空洞的情况 123456789101112131415int min_heap_erase_(min_heap_t* s, struct event* e)&#123; if (EV_SIZE_MAX != e-&gt;ev_timeout_pos.min_heap_idx) &#123; struct event *last = s-&gt;p[--s-&gt;n]; size_t parent = (e-&gt;ev_timeout_pos.min_heap_idx - 1) / 2; if (e-&gt;ev_timeout_pos.min_heap_idx &gt; 0 &amp;&amp; min_heap_elem_greater(s-&gt;p[parent], last)) min_heap_shift_up_unconditional_(s, e-&gt;ev_timeout_pos.min_heap_idx, last); else min_heap_shift_down_(s, e-&gt;ev_timeout_pos.min_heap_idx, last); e-&gt;ev_timeout_pos.min_heap_idx = EV_SIZE_MAX; return 0; &#125; return -1;&#125; 如果 e-&gt;ev_timeout_pos.min_heap_idx != EV_SIZE_MAX 表示已经不在堆中了，添加和弹出的时候都会更新这个索引 获取堆尾元素 获取事件的父节点 如果事件不在堆顶而且父结点大于最后一个节点 向上调整 如果事件在堆顶，类似 min_heap_pop_ 向下调整 向上调整2 这里为了防止空洞，出现了一个不一样的操作 min_heap_shift_up_unconditional_ 1234567891011void min_heap_shift_up_unconditional_(min_heap_t* s, size_t hole_index, struct event* e)&#123; size_t parent = (hole_index - 1) / 2; do &#123; (s-&gt;p[hole_index] = s-&gt;p[parent])-&gt;ev_timeout_pos.min_heap_idx = hole_index; hole_index = parent; parent = (hole_index - 1) / 2; &#125; while (hole_index &amp;&amp; min_heap_elem_greater(s-&gt;p[parent], e)); (s-&gt;p[hole_index] = e)-&gt;ev_timeout_pos.min_heap_idx = hole_index;&#125; 首先找到当前元素的父节点 将父节点更新到当前位置，并跟新索引 设置当前位置为父节点，找到父节点的父节点 parent2 如果 parent2 不是堆顶，而且 parent2 大于 last，那么重复 2~3操作（感觉不会，因为是小根堆） 更新 last 到合适的位置 也就是说通过 将 last 填补到要删除元素的位置，解决空洞问题。 问题1：min_heap_shift_up_unconditional_ 与 min_heap_shift_up_ 不同的是，后者先判断了 当前位置以及 当前位置事件与 e 的大小，为什么要做这个区别？ 答：是因为前者的位置肯定不是堆顶(走乡下条) 问题2：min_heap_erase_ 删除元素中为什么会出现向下调整与向上调整 答：首先需要明白向上调整与向下调整的作用分别是什么， 向下调整：被用于在删除堆顶元素后，将最后一个元素移到堆顶并逐步将其“下沉”至合适的位置，以保持堆的有序性质。 向上调整：被用于在向堆中插入新元素后，将其逐步“上浮”至合适的位置，以保持堆的有序性质 如果删除位置不是堆顶元素，且父节点大于 last 。那么根据小根堆的性质，last 应该从这个位置开始向上调整 如果删除位置是堆顶元素或者父节点小于last。那么根据小根堆的性质，last应该在删除位置的下发（或不变） 参考链接 https://www.hello-algo.com/chapter_heap/heap/ https://www.hello-algo.com/chapter_sorting/heap_sort/","categories":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://xboom.github.io/categories/Data-Structure/"}],"tags":[{"name":"Heap","slug":"Heap","permalink":"http://xboom.github.io/tags/Heap/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"http://xboom.github.io/tags/Data-Structure/"},{"name":"Libevent","slug":"Libevent","permalink":"http://xboom.github.io/tags/Libevent/"}]},{"title":"算法-01-Heap","slug":"Algorithm/算法-01-Heap","date":"2023-09-28T16:15:00.000Z","updated":"2023-09-28T16:21:25.689Z","comments":true,"path":"2023/09/29/Algorithm/算法-01-Heap/","link":"","permalink":"http://xboom.github.io/2023/09/29/Algorithm/%E7%AE%97%E6%B3%95-01-Heap/","excerpt":"","text":"堆(Heap)是具有以下性质的完全二叉树 每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆 每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆 完全二叉树：除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列 **问题 **：那左右子节点有谁大谁小？没有要求！！！ 大根堆，每个子结点都要小于父结点，不区分左右儿子谁大谁小，也不必保证某个 孙子结点 一定要小于另一个 儿子结点 小根堆，每个子结点都要大于父结点，不区分左右儿子谁大谁小，**也不必保证某个 孙子结点 一定要大于另一个 儿子结点 ** 也就是说，即使组成了 大小根堆，它们的排列还不是有序的，只是保证 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] 小顶堆：arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2] 实现原理 首先明白两个概念：向下调整 与 向上调整 向下调整：指的是为了满足堆的性质而向下不断比较调整父子节点的关系，一般用于堆删除一个元素 从堆的根节点开始(通常是数组的第一个元素)，比较当前节点与其子节点的值。 如果当前节点的值小于其子节点中的一个或多个节点的值，那么将当前节点与最大的子节点进行交换，以满足大顶堆的性质。 重复上述步骤，直到节点没有子节点或者节点的值大于等于其子节点的值 这里删除堆顶元素之后还向下调整的原因是，会将最后一个元素放置到堆顶 向上调整：指的是为了满足堆的性质而向上不断比较调整父子节点的关系，一般用于堆插入一个元素 在堆的最后一个位置(通常是数组的最后一个元素)插入新元素。 比较新插入的元素与其父节点的值。 如果新元素的值大于其父节点的值（对于大顶堆），则交换新元素与父节点，以满足堆性质。 重复上述步骤，直到新元素的值小于等于其父节点的值或者新元素已经成为根节点 实际应用中还有一种情况，比如需要从堆中直接删除一个元素(元素已经失效，需要释放该元素重新调整小根堆的结构)，此时会出现一个空洞 将最后一个元素 last 放到被删除的位置 如果删除位置不是堆顶元素，且父节点大于 last 。那么根据小根堆的性质，last 应该从这个位置开始向上调整 如果删除位置是堆顶元素或者父节点小于last。那么根据小根堆的性质，last应该在删除位置的下发(或不变) 所以上面其实就是堆的插入和删除过程了，原理就是只在头尾处理，然后进行向上或者向下调整，保证大根堆、小根堆的性质 问题 1：为什么不在删除对顶元素之后，直接将第二大元素放到堆顶呢？ 答：这样的后出现空洞的情况，就需要进一步处理 所以，这里改变思路：将最后一个节点与堆顶交换，然后删除最后一个元素，最后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法 问题 2：知道了插入和删除，它首要条件是有一个堆，所以堆从哪里来 答：方案有两种 从前往后，尽管数组中包含 n 个数据，假设起初堆中只包含一个数据，就是下标为 0 的数据。然后不断的插入将下标从 1 到 n - 1 的数据依次插入到堆中。每插入一个元素都向上调整到合适位置 从后往前，依次与子节点进行比较，因为叶子节点无法与子节点比较，所以就需要从最后一个非叶子节点比较。每次都向下调整到合适的位置 问题 3：最后一个非叶子节点如何表示 答：假设最后一个非叶子节点下表是 i (i 从 0 开始) 如果它的左子节点是最后一个节点，那么最后一个节点下标就是 2i + 1。整个二叉树的长度就是 len = 2i + 2 如果他的右子节点是最后一个节点，那么最后一个节点下标就是 2i + 2。整个二叉树的长度就是 len = 2i + 3 所以最后一个非叶子节点的下标就是 i = len / 2 - 1 问题 4：建堆的过程有两种，哪种更好 答：第二种方式可以节省一半的时间 第一种方式需要对n-1个节点进行堆化，每个节点堆化的时间复杂度为 O(logn)，所以它精确的时间复杂度为(n-1)*O(logn) 第二种方式从非叶子节点开始，需要对 n/2个节点进行堆化，精确的时间复杂度为(n/2)*O(logn) 问题 5：既然不关心左右节点大小，那样怎么样做到整个有序呢？ 答：就像删除堆顶元素一样。当堆顶元素移除之后，把下标为 n 的元素放到堆顶，然后向下调整，让剩下的 n−1 个元素重新构建成堆。堆化完成之后，再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。 一个包含 n 个节点的完全二叉树，树的高度不会超过 log2n，堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。 插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn) 因为每个元素都需要进行删除的过程，所以堆排序整体的时间复杂度为 O(nlogn) 代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include&lt;bits/stdc++.h&gt;using namespace std;void print(int a[], int n)&#123; int i; for(i = 0; i &lt; n; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; \" \"; &#125; cout &lt;&lt; endl;&#125; //heapAdjust 作用从上往下构建，保证数组 a[] 中 第low个元素为堆顶的元素构成的结构为大顶堆void heapAdjust(int a[], int low, int high) //调整的过程&#123; int pivotKey = a[low - 1]; //注意数组的索引从0开始 int i; //当前节点下标 for(i = 2 * low; i &lt;= high; i *= 2) &#123; if(i &lt; high &amp;&amp; a[i - 1] &lt; a[i]) //找到左右子节点中较大的那个 &#123; i++; //i指向较大值 &#125; if(pivotKey &gt;= a[i - 1]) //如果大于等于子节点说明已经是大顶堆 &#123; break; &#125; a[low - 1] = a[i - 1]; //否则将子节点更换到父节点的位置 low = i; //然后继续判断子节点与孙子节点的大小,保证子节点与起构成的子节点也是大堆顶 &#125; a[low - 1] = pivotKey; //将最开始的堆顶放到合适的位置&#125; void heapSort(int a[], int n)&#123; int i, tmp; //堆化 for(i = n/2; i &gt; 0; i--) //从非叶子节点开始，开始调整(最后一个非叶子节点是 len/2 - 1) &#123; heapAdjust(a, i, n); &#125; for(i = n; i &gt; 1; i--) //将堆顶元素放置最后，开始调整 &#123; //调整最后一个元素于第一个元素的位置 tmp = a[i -1]; a[i - 1] = a[0]; a[0] = tmp; //向下调整 heapAdjust(a, 1, i - 1); print(a, n); &#125;&#125;int main()&#123; int a[] = &#123;30, 20, 40, 10, 0, 60, 80, 70&#125;; int n = sizeof(a) / sizeof(a[0]); //计算数组长度 heapSort(a, n); print(a, n); return 0;&#125; 堆排序应用 Top K问题 从堆的定义来说，很容易想到堆顶元素是TOP 1，那么如何求经典的TOP K问题? 如果求最大的Top K 元素，是建立大顶堆，还是小顶堆 如果用大顶堆，堆顶是最大的元素，新加入的元素必须和所有的堆中元素做比对，显然不够划算； 如果用小顶堆，那堆顶元素是最小的元素，新加入的元素只要和堆顶元素做对比，如果比堆顶元素大，就可以把堆顶的元素删除，将新加入的元素加入到堆中 每次求TopK的问题的时候，只需要直接返回堆中所有元素即可，每次堆化的算法复杂度为O(logK),那么N个元素堆化的时间为O(nlogK),如果不用堆，每次新增元素都要重新排序，再取前面N个 定时器应用 独立线程，以超时时间建立了一个小顶堆，如果堆顶元素为2分钟，清理连接的线程的休眠时间设置为2分钟，2分钟后取堆顶元素，执行连接关闭操作。 求中位数和99%的响应时间 动态数组取中位数： 如果n为偶数，前n/2数据存储在大顶堆中，n/2存储小顶堆中，则大顶堆的堆顶元素为要找的中位数 如果n为奇数，可以将前n/2+1个数据存储在大顶堆中，后n/2存储在小顶堆中 如果添加新数据，则将数据与大顶堆中的堆顶元素比较 如果小于等于大顶堆中的元素，就插入到大顶堆中 如果比大顶堆的堆顶大，那就插入到小顶堆中 如果插入数据后不满足要求两个堆的数量为n/2和n/2 或n/2 和n+1/2 的要求，需要调整两个堆的大小，从大顶堆中删除堆顶元素，或小顶堆中删除堆顶元素，移动到另外一个堆中即可。 99%的响应时间：如果一个接口的有100个访问请求，分别耗时1ms，2ms，3ms…100ms，那么按照访问时间从小到大排列，排在第99位的访问时间，就是99%的访问时间，我们维护两个堆，大顶堆的元素个数为n * 99%，小顶堆的元素个数为n*1%，那么大顶堆中的堆顶元素即是所求的99%的响应时间，和中位数的应用一样，只是中位数中的应用更特殊一点 优先级队列 优先级队列和通常的栈和队列一样，只不过里面的每一个元素都有一个&quot;优先级”，在处理的时候，首先处理优先级最高的。如果两个元素具有相同的优先级，则按照他们插入到队列中的先后顺序处理 参考链接 https://www.cnblogs.com/chengxiao/p/6129630.html https://mp.weixin.qq.com/s/OqpSdFfK12NhPpcsXTMvtA https://blog.csdn.net/weixin_45728685/article/details/105115912","categories":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/categories/Algorithms/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/tags/Algorithms/"},{"name":"Heap","slug":"Heap","permalink":"http://xboom.github.io/tags/Heap/"}]},{"title":"数据结构-01-List","slug":"Data Structure/数据结构-01-List","date":"2023-09-18T14:58:29.000Z","updated":"2023-09-28T16:22:28.149Z","comments":true,"path":"2023/09/18/Data Structure/数据结构-01-List/","link":"","permalink":"http://xboom.github.io/2023/09/18/Data%20Structure/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-01-List/","excerpt":"","text":"在阅读 Libevent 库的时候看到 TAILQ_ENTRY的宏定义有点特别，这里记录一下，代码路径 libevent/compat/sys/queue.h 首先它实现的类似泛型的功能 123456#define TAILQ_ENTRY(type) \\struct &#123; \\ struct type *tqe_next; /* next element */ \\ struct type **tqe_prev; /* address of previous next element */ \\&#125;#endif /* !TAILQ_ENTRY */ C11 中还能通过 _Generic关键字 实现泛型编程 但是有一个奇怪的地方，tqe_prev 为什么是指针的指针，这里可能跟平时的好像不太一样? 平时定义的双向链表节点 123456// 定义一个结构体，表示链表中的节点struct Node &#123; int data; struct Node* next; // 后一个节点的指针 struct Node* prev; // 前一个节点的指针&#125;; 由它构建的双向链表结构 删除节点的操作 1234567891011121314151617181920// 在链表中删除指定值的节点void deleteNode(struct Node** head, int target) &#123; struct Node* current = *head; // 找到要删除的节点 while (current != NULL &amp;&amp; current-&gt;data != target) &#123; current = current-&gt;next; &#125; // 如果节点存在，则删除它 if (current != NULL) &#123; if (current-&gt;prev != NULL) &#123;//头节点不用调整 prev 指向 current-&gt;prev-&gt;next = current-&gt;next; //前一个节点的 next 指向下一个节点 &#125; if (current-&gt;next != NULL) &#123; //尾节点不用调整 next 指向 current-&gt;next-&gt;prev = current-&gt;prev; //下一个节点的前一个节点指向当前的前一个节点 &#125; free(current); &#125;&#125; 而平时定义的单向链表 1234struct node &#123; int data; struct Node* next; //指向下一个节点&#125;; 由它构建的单向链表 而单链表的删除操作是这样的 12345678910111213141516171819202122// 在链表中删除指定值的节点void deleteNode(struct Node** head, int target) &#123; struct Node* current = *head; struct Node* prev = NULL; // 找到要删除的节点 while (current != NULL &amp;&amp; current-&gt;data != target) &#123; prev = current; current = current-&gt;next; &#125; // 如果节点存在，则删除它 if (current != NULL) &#123; if (prev == NULL) &#123; // 要删除的节点是头节点 *head = current-&gt;next; &#125; else &#123; prev-&gt;next = current-&gt;next; &#125; free(current); &#125;&#125; 于是我又去看了 linux 内核中的链表结构，发现了它的使用 代码路径：libevent/compat/sys/queue.h 1234567891011struct list_head &#123; struct list_head *next, *prev;&#125;;struct hlist_head &#123; struct hlist_node *first;&#125;;struct hlist_node &#123; struct hlist_node *next, **pprev;&#125;; 它在定义双向链表的同时，还定义了一个hlist_node用于 hash 表的桶链表，头节点使用 hlist_head表示。它的删除操作 123456789static inline void __hlist_del(struct hlist_node *n)&#123; struct hlist_node *next = n-&gt;next; struct hlist_node **pprev = n-&gt;pprev; WRITE_ONCE(*pprev, next); if (next) WRITE_ONCE(next-&gt;pprev, pprev);&#125; 其中 WRITE_ONCE的宏是 Linux 内核中用于确保写入操作是原子的宏，也就是说 WRITE_ONCE(*pprev, next); 相当于原子操作 *pprev = next, 那么这个双向链表的结构是 跟自己定义的区别就是将 prev 的指针指向的是前一个节点的 next 指针，为什么这样做其实从删除操作可以看出来，不需要额外判断是否是头节点与尾节点 回到刚才的宏定义，有个双向链表节点，看它是如何完整的实现双向链表的 首先是初始化一个头部节点 123456struct event_list events;TAILQ_INIT(&amp;events);#define TAILQ_INIT(head) do &#123; \\ (head)-&gt;tqh_first = NULL; \\ (head)-&gt;tqh_last = &amp;(head)-&gt;tqh_first; \\&#125; while (/*CONSTCOND*/0) 也就是说 head 是一个包含 tqh_first 与 tqh_last 的节点 123456789101112131415TAILQ_HEAD (event_list, event);#define TAILQ_HEAD(name, type) \\struct name &#123; \\ struct type *tqh_first; \\ struct type **tqh_last; \\&#125;#endifTAILQ_ENTRY(event)#define TAILQ_ENTRY(type) \\struct &#123; \\ struct type *tqe_next; /* next element */ \\ struct type **tqe_prev; /* address of previous next element */ \\&#125; TAILQ_HEAD 定义了一个名字为 event_list 的双向链表，记录了第一个节点与最后一个节点 它的删除操作 12345678#define TAILQ_REMOVE(head, elm, field) do &#123; \\ if (((elm)-&gt;field.tqe_next) != NULL) \\ (elm)-&gt;field.tqe_next-&gt;field.tqe_prev = \\ (elm)-&gt;field.tqe_prev; \\ else \\ (head)-&gt;tqh_last = (elm)-&gt;field.tqe_prev; \\ *(elm)-&gt;field.tqe_prev = (elm)-&gt;field.tqe_next; \\&#125; while (0) 其中 (elm)-&gt;field 表示的是 elm 指针的成员字段 field 而 (elm)-&gt;field.tqe_next-&gt;field.tqe_prev = (elm)-&gt;field.tqe_prev; 与上面的 WRITE_ONCE(next-&gt;pprev, pprev); 效果一样 那么 *(elm)-&gt;field.tqe_prev = (elm)-&gt;field.tqe_next; 理解为 WRITE_ONCE(*pprev, next);，所以这里的 *(elm) 而不是 elm 这里不同的是，头节点记录的链表的头节点与尾节点，所以它的结构是 这样的好处是 可以同时在双向链表的头尾都进行增删操作，变成了一个双向队列，那为什么 linux 中的 hash 表没有弄成一个双向队列，因为新增的值直接加入到链表头，不需要头尾都都操作","categories":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://xboom.github.io/categories/Data-Structure/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://xboom.github.io/tags/Data-Structure/"},{"name":"List","slug":"List","permalink":"http://xboom.github.io/tags/List/"},{"name":"Libevent","slug":"Libevent","permalink":"http://xboom.github.io/tags/Libevent/"}]},{"title":"Protocol-04-DHCP","slug":"Protocol/Protocol-04-DHCP","date":"2023-09-07T02:25:38.000Z","updated":"2023-09-08T15:38:23.323Z","comments":true,"path":"2023/09/07/Protocol/Protocol-04-DHCP/","link":"","permalink":"http://xboom.github.io/2023/09/07/Protocol/Protocol-04-DHCP/","excerpt":"","text":"动态主机配置协议DHCP(Dynamic Host Configuration Protocol) 是一种网络管理协议，用于集中对用户 IP 地址进行动态管理和配置。 DHCP协议由 RFC 2131 定义，采用 Client /服务器通信模式，由 客户端(DHCP Client) 向 **服务器(DHCP Server)**提出配置申请，Server为网络上的每个设备动态分配 IP 地址、子网掩码、默认网关地址，域名服务器(DNS)地址等参数 相关协议(按时间从远到近排序) RFC1531 、RFC1541、RFC2131 、RFC3396 DHCP好处 网络中，每个连接Internet的设备都需要分配唯一的 IP。DHCP 使网络管理员能从中心结点监控和分配IP地址，从以下几个方面降低了配置和部署设备的时间 减少 IP 地址冲突：每个连接的设备都必须有一个IP地址。但是，每个地址只能使用一次，重复的地址将导致无法连接一个或两个设备的冲突。 IP地址管理的自动化：如果没有 DHCP，网络管理员将需要手动分配和撤消地址。手动需要随时知道设备何时需要访问网络以及何时需要离开网络。 高效的变更管理：DHCP 的使用使更改地址，范围或端点变得非常简单。例如，组织可能希望将其IP寻址方案从一个范围更改为另一个范围。Server配置有新信息，该信息将传播到新端点。同样，如果升级并更换了网络设备，则不需要网络配置 DHCP 工作原理 DHCP 协议采用 UDP 作为传输协议，Client 发送请求消息到 Server 的 Port 67，Server 回应应答消息给 Client 的 Port 68，只有跟 Client 在同一个网段的 Server 才能收到 Client 广播的 DISCOVER报文。当 Client 与 Server 不在同一个网段时，必须部署DHCP中继来转发 Client 和Server之间的DHCP报文 非中继 在没有部署 DHCP中继 的场景下，首次接入网络 Client 与 Server 的报文交互过程，该过程称为DHCP报文四步交互 第一步：发现阶段 首次接入网络的 Client 不知道 Server 的IP地址，为了学习到 Server 的IP地址，Client 以广播方式发送DISCOVER报文(目的IP地址为255.255.255.255)给同一网段内的所有设备(包括Server或中继)。DISCOVER报文中携带了 Client 的MAC地址等信息 第二步：提供阶段 与 Client 位于同一网段的 Server 都会接收到 DISCOVER报文，Server 选择跟接收 DISCOVER报文接口的IP地址处于同一网段的地址池，并且从中选择一个可用的IP地址，然后通过OFFER报文发送给Client，Server 在地址池中为 Client 分配IP地址的顺序如下： Server 上已配置的与 Client MAC地址静态绑定的 IP 地址。 Client 发送的 DISCOVER报文中 Option50(请求IP地址选项)指定的地址 地址池内查找 Expired 状态的IP地址，即曾经分配给 Client 的超过租期的IP地址 在地址池内随机查找一个 Idle 状态的IP地址。 如果未找到可供分配的 IP 地址，则地址池依次自动回收超过租期的 Expired 和处于冲突状态 Conflict 的IP地址。回收后如果找到可用的IP地址，则进行分配；否则，Client 等待应答超时后，重新发送DHCP DISCOVER报文来申请IP地址 通常，Server 的地址池中会指定IP地址的租期，如果 Client 发送的 DISCOVER报文 中携带了期望租期，服务器会将 Client 请求的期望租期与其指定的租期进行比较，选择其中时间较短的租期 为了防止分配出去的IP地址跟网络中其他 Client 的IP地址冲突，比如Client所在网段已经手工配置了一个地址池中的地址用于DNS服务器，Server 在发送DHCP OFFER报文前通过发送 Source IP = ServerIP地址、Target IP 为预分配出去IP地址的 ICMP ECHO REQUEST报文 对分配的IP地址进行地址冲突探测。 如果在指定的时间内没有收到应答报文，表示网络中没有 Client 使用这个IP地址，可以分配给 Client ； 如果指定时间内收到应答报文，表示网络中已经存在使用此IP地址的 Client ，则把此地址列为冲突地址，然后等待重新接收到DHCP DISCOVER报文后重新选择（TODO 这个时间会不会很长，为什么不直接找一个IP地址分配） 此阶段 Server 分配给 Client 的IP地址不一定是最终确定使用的IP地址，因为DHCP OFFER报文发送给 Client 等待16秒 后如果没有收到 Client 的响应，此地址就可以继续分配给其他 Client 。 第三步：选择阶段 在发现阶段 DISCOVER报文 是广播报文，也就是说有可能有多个 Server 向 Client 回应 DHCP OFFER报文，则 Client 一般只接收第一个收到的 DHCP OFFER报文，然后以广播方式发送 DHCP REQUEST报文，该报文中包含 Client 想选择的 Server标识符(即Option50，填充了接收的DHCP OFFER报文中yiaddr字段的IP地址) Client 广播发送 DHCP REQUEST报文通知所有的Server，它将选择某个Server提供的IP地址，其他Server可以重新将曾经分配给 Client 的IP地址分配给其他 Client 。 Server怎么判断这个报文是来请求自己分配出去的IP，还是 Client 用来告诉 Server 不用这个地址（两个DHCP 服务器分配的IP地址可能还都一样，不能通过IP判断是否是自己给的）？ 第四步：确认阶段 当Server收到 Client 发送的DHCP REQUEST报文后，Server回应 DHCP ACK报文，表示 DHCP REQUEST报文 中请求的IP地址(Option50 填充的)分配给 Client 使用。 Client 收到 DHCP ACK报文，会广播发送免费ARP报文，探测本网段是否有其他终端使用服务器分配的IP地址 如果在指定时间内没有收到回应，表示 Client 可以使用此地址。 如果收到了回应，说明有其他终端使用了此地址， Client 会向服务器发送 DHCP DECLINE报文，并重新向服务器请求IP地址，同时，服务器会将此地址列为冲突地址。当服务器没有空闲地址可分配时，再选择冲突地址进行分配，尽量减少分配出去的地址冲突 DECLINE 是广播报文吗， 如果发送失败或者 Server无法分配其他地址呢？ 当 Server 收到 Client 发送的 DHCP REQUEST报文 后，如果Client 由于某些原因（例如协商出错或者由于发送REQUEST过慢导致服务器已经把此地址分配给其他 Client )无法分配 DHCP REQUEST报文中 Option50 填充的IP地址，则发送DHCP NAK报文作为应答，通知 Client 无法分配此IP地址。** Client 需要重新发送DHCP DISCOVER报文来申请新的IP地址** 中继 有DHCP中继的场景中，首次接入网络的 Client 和Server的工作原理与无中继场景时 Client 首次接入网络的工作原理相同。主要差异是DHCP中继在Server和 Client 之间转发DHCP报文，以保证Server和 Client 可以正常交互。在 Client 看来，DHCP中继就像Server；在Server看来，DHCP中继就像 Client 如下图所示，在部署DHCP中继的场景下，首次接入网络 Client 与Server的报文交互过程 第一步：发现阶段 DHCP中继接收到 Client 广播发送的 DHCP DISCOVER报文后，进行如下处理： 检查DHCP报文中的hops字段，如果大于16，则丢弃DHCP报文；否则，将hops字段加1(表明经过一次DHCP中继)，并继续下面的操作。 检查DHCP报文中的giaddr字段。如果是0，将giaddr字段设置为接收DHCP DISCOVER报文的接口IP地址。如果不是0，则不修改该字段，继续下面的操作。 记录 giaddr 的目的就是在 DHCP 地址池中分配相同网段的 IP 地址 将DHCP报文的目的IP地址改为Server或下一跳中继的IP地址，源地址改为中继连接 Client 的接口地址，通过路由转发将DHCP报文单播发送到Server或下一跳中继 如果 Client 与Server之间存在多个DHCP中继，后面的中继接收到DHCP DISCOVER报文的处理流程同前面所述 第二步：提供阶段 Server接收到DHCP DISCOVER报文后，选择与报文中giaddr字段为同一网段的地址池，并为 Client 分配IP地址等参数，然后向giaddr字段标识的DHCP中继单播发送DHCP OFFER报文。 DHCP中继收到DHCP OFFER报文后，会进行如下处理： 检查报文中的giaddr字段，如果不是接口的地址，则丢弃该报文；否则，继续下面的操作。 DHCP 服务器怎么知道是不是中继过来的？通过 hops 吗？ DHCP中继检查报文的广播标志位。如果广播标志位为1，则将DHCP OFFER报文广播发送给 Client ；否则将DHCP OFFER报文单播发送给 Client 。 广播标志位是什么时候设置的？ 为什么 DHCP OFFER 需要官博发送给 DHCP Client 第三步：选择阶段 中继接收到来自 Client 的DHCP REQUEST报文的处理过程同无中继场景下的选择阶段。 第四步：确认阶段 中继接收到来自服务器的DHCP ACK报文的处理过程同无中继场景下的确认阶段。 续租 DHCP提供了两种地址分配机制，可以根据网络需求为不同的主机选择不同的分配策略。 动态分配机制：通过DHCP为主机分配一个有使用期限的IP地址 静态分配机制：网络管理员通过DHCP为指定的主机分配固定的IP地址。 Client 向 Server 申请地址时可以携带期望租期，将分配租期与期望租期比较，**分配其中一个较短的租期给 Client **。租期到期或者 Client 下线释放地址后，服务器会收回该IP地址，收回的IP地址可以继续分配给其他 Client 使用。这种机制可以提高IP地址的利用率，避免 Client 下线后IP地址继续被占用。如果 Client 希望继续使用该地址，需要更新IP地址的租期(如延长IP地址租期)，更新租期过程如下 当租期达到50%(T1)时，Client 会自动以单播的方式向 Server 发送 DHCP REQUEST报文，请求更新IP地址租期 如果收到 Server 回应的 DHCP ACK报文，则租期更新成功(即租期从0开始计算)； 如果收到 DHCP NAK报文，则重新发送 DHCP DISCOVER报文请求新的IP地址。 当租期达到87.5%(T2)时，如果仍未收到Server的应答，Client 会自动以广播的方式向 Server 发送 DHCP REQUEST报文，请求更新IP地址租期。后续处理逻辑同上 如果租期时间到时都没有收到服务器的回应， Client 停止使用此IP地址，重新发送DHCP DISCOVER报文请求新的IP地址。 Client 在租期时间到之前，如果不想使用分配的IP地址(例如 Client 网络位置需要变更)，会触发 Client 向 Server 发送 DHCP RELEASE报文，通知 Server 释放IP地址的租期。Server 会保留这个 Client 的配置信息，将IP地址列为曾经分配过的IP地址中，以便后续重新分配给该 Client 或其他 Client 。 Client 可以通过发送 DHCP INFORM报文向服务器请求更新配置信息 部署DHCP中继时，更新租期的过程与上述过程相似。 **如果不是首次呢，比如客户端重启了 ？**如果客户端重启后，会自动发送 DHCP REQUEST广播 给 Server 以便请求继续租用原来使用的IP地址， 如果服务器收到这个消息，确认客户端可以使用该地址，则回答一个DHCP ACK确认消息 如果此IP地址已经无法再分配，则返回一个 DHCP NCK否认信息，客户端收到这个信息以后，则必须重新发送 DHCP Discovery消息获取新的地址(启动首次登录的流程)。 若没有得到响应，则尝试与网关通信，如果通信正常，这个租期还没到期的话，则可以继续使用，但是不能和网关通信的话，则会获得169.254.0.1~169.254.255.254之间的IP地址，然后每隔5min尝试更新租约 DHCP 报文 Server 与 Client 之间通过DHCP报文进行通信。DHCP报文是基于UDP协议传输的。Client 使用的端口号为68，Server 使用的端口号为67。目前DHCP定义了如下八种类型报文，每种报文的格式相同，只是某些字段的取值不同 报文名称 说明 DHCP DISCOVER Client 首次登录网络时进行DHCP交互过程发送的第一个报文，用来寻找Server。 DHCP OFFER Server用来响应DHCP DISCOVER报文，此报文携带了各种配置信息。 DHCP REQUEST 此报文用于以下三种用途。1. Client 初始化后，发送广播的 DHCP REQUEST报文来回应服务器的DHCP OFFER报文。2. Client 重启后，发送广播的 DHCP REQUEST报文来确认先前被分配的IP地址等配置信息。3. 当 Client 已经和某个IP地址绑定后，发送 DHCP REQUEST单播或广播报文来更新IP地址的租约。 DHCP ACK Server 对 Client 的DHCP REQUEST报文的确认响应报文，Client收到此报文后，才真正获得了IP地址和相关的配置信息。 DHCP NAK 服务器对Client 的DHCP REQUEST报文的拒绝响应报文，例如Server收到DHCP REQUEST报文后，没有找到相应的租约记录，则发送DHCP NAK 报文作为应答，告知 Client 无法分配合适IP地址。 DHCP DECLINE 当 Client 发现服务器分配给它的IP地址发生冲突时会通过发送此报文来通知服务器，并且会重新向服务器申请地址。 DHCP RELEASE Client 可通过发送此报文主动释放服务器分配给它的IP地址，当服务器收到此报文后，可将这个IP地址分配给其它的 Client 。 DHCP INFORM Client 获取IP地址后，如果需要向Server获取更为详细的配置信息(网关地址、DNS服务器地址)，则向Server发送DHCP INFORM请求报文。 报文地址：https://wiki.wireshark.org/uploads/moin_import/attachments/Development/PcapNg/dhcp.pcapng DHCP 四步交互如下 服务端的端口是 67，客户端的端口是 68 这里有一个疑问就是 192.168.0.10 是从哪里来的，DHCP 服务器分配之后直接写进去了 Discover DHCP 报文格式一样，只是最后的 Option 不同，前面包含的字段如下： Message type：报文的操作类型。分为请求报文和响应报文，1为请求报文，2为响应报文。具体报文类型在option字段中标识 Hardware type：DHCP客户端的硬件地址类型。1表示ethernet地址 Hardeare address length：DHCP客户端的硬件地址长度。Ethernet地址为6 Hops：DHCP报文经过的DHCP中继的数目。初始为0，报文每经过一个DHCP中继，该字段就会增加1 Transaction ID: 客户端发起一次请求时选择的随机数，用来标识一次地址请求过程 Seconds elapsed: Client 开始DHCP请求后所经过的时间。目前尚未使用，固定取0 Bootp flags：DHCP服务器响应报文是采用单播还是广播方式发送。只使用第0比特位，0表示采用单播方式，1表示采用广播方式，其余比特保留不用 Client IP address: DHCP客户端的IP地址 Your(client) IP address: DHCP服务器分配给客户端的IP地址 Next server IP address：DHCP客户端获取IP地址等信息的服务器IP地址 Relay agent IP address: DHCP客户端发出请求报文后经过的第一个DHCP中继的IP地址 Client MAC address: DHCP客户端的硬件地址 Server host name: DHCP客户端获取IP地址等信息的服务器名称 Boot file name: DHCP服务器为DHCP客户端指定的启动配置文件名称及路径信息 Magic cookie: 标识和解释DHCP消息的元素，有助于确保不同设备之间的协议兼容性，并提供了消息类型和选项格式的关键信息 Option：可选变长选项字段，包含报文的类型、有效租期、DNS服务器的IP地址和WINS服务器的IP地址等配置信息。 因为是广播报文，所以目的地址是 255.255.255.255，其中 Option(53): 表示 DHCP 消息类型，Discover(1) Option(50): 用来记录请求的 IP Option(55): 用来请求其他参数 Option(61): 客户端标识符 Offer 这里也包含了很多 Option: Option(53): DHCP 消息类型，Offer(2) Option(1): 子网掩码， 255.255.255.0 Option(58): 续租时间 T1，一般为租期的 50% Option(59): 重新发起T2, 租约响应超时，重新发起广播时间，一般位租期的 87.5% Option(51): 租约 Option(54): DHCP服务器标识符选项 Request 这里也包含了很多 Option: Option(53): DHCP 消息类型，Request(3) Option(61): 客户段标识 Option(50): 请求的 IP Option(54): 服务端标识 Option(55): 请求的参数 ACK 这里也包含了很多 Option: Option(53): DHCP 消息类型，Ack(5) Option(1): 子网掩码， 255.255.255.0 Option(58): 续租时间 T1，一般为租期的 50% Option(59): 重新发起T2, 租约响应超时，重新发起广播时间，一般位租期的 87.5% Option(51): 租约 Option(54): DHCP服务器标识符选项 DHCP Snooping DHCP Snooping 是DHCP的一种安全特性，用于保证 DHCP客户端从合法的 DHCP服务器获取IP地址，并记录DHCP客户端IP地址与MAC地址等参数的对应关系。DHCP Snooping 用来抵御网络中针对DHCP的各种攻击 DHCP攻击有哪些 它的原理是什么 DHCP 攻击 1. DHCP Server仿冒者攻击 由于 Server 和 Client 之间没有认证机制，而 DHCP Discover报文 是以广播形式发送，如果在网络上随意添加一台 DHCP服务器，它就可以为 Client 分配IP地址以及其他网络参数。 如果此时 DHCP Server仿冒者 回应给 Client 仿冒信息，如错误的网关地址、错误的DNS(Domain Name System)服务器、错误的IP等信息。Client 将无法获取正确的IP地址和相关信息，导致合法客户无法正常访问网络或信息安全受到严重威胁。 解决办法：为了防止DHCP Server仿冒者攻击，可配置设备接口的 信任(Trusted)/非信任(Untrusted)工作模式 将与合法DHCP服务器直接或间接连接的接口设置为信任接口，其他接口设置为非信任接口。此后，从非信任(Untrusted)接口上收到的DHCP回应报文将被直接丢弃，这样可以有效防止DHCP Server仿冒者的攻击 2. 防止非DHCP用户攻击 在DHCP网络中，静态获取IP地址的用户（非DHCP用户）对网络可能存在多种攻击，譬如仿冒DHCP Server、构造虚假DHCP Request报文等。这将为合法DHCP用户正常使用网络带来了一定的安全隐患 解决办法：开启设备根据 DHCP Snooping绑定表 生成接口的静态MAC表项功能，设备将根据接口下所有的DHCP用户对应的DHCP Snooping绑定表项自动执行命令生成这些用户的静态MAC表项，并同时关闭接口学习动态MAC表项的能力。此时，只有源MAC与静态MAC表项匹配的报文才能够通过该接口，否则报文会被丢弃。因此对于该接口下的非DHCP用户，只有管理员手动配置了此类用户的静态MAC表项其报文才能通过，否则报文将被丢弃。 动态MAC表项是设备自动学习并生成的，静态MAC表项则是根据命令配置而成的。MAC表项中包含用户的MAC、所属VLAN、连接的接口号等信息，设备可根据MAC表项对报文进行二层转发 3. 防止仿冒DHCP报文攻击 已获取到IP地址的合法用户通过向服务器发送 DHCP Request 或 DHCP Release 报文用以续租或释放IP地址。 如果攻击者冒充合法用户不断向DHCP Server发送DHCP Request报文来续租IP地址，会导致这些到期的IP地址无法正常回收，以致一些合法用户不能获得IP地址； 而若攻击者仿冒合法用户的DHCP Release报文发往DHCP Server，将会导致用户异常下线 解决办法：利用DHCP Snooping绑定表的功能。设备通过将DHCP Request续租报文和DHCP Release报文与绑定表进行匹配操作能够有效的判别报文是否合法(主要是检查报文中的VLAN、IP、MAC、接口信息是否匹配动态绑定表)，若匹配成功则转发该报文，匹配不成功则丢弃 4. 防止DHCP Server服务拒绝攻击 若设备接口if1 下存在大量攻击者恶意申请IP地址，会导致DHCP Server中IP地址快速耗尽而不能为其他合法用户提供IP地址分配服务。 另一方面，DHCP Server通常仅根据DHCP Request报文中的(Client Hardware Address)字段来确认客户端的MAC地址。如果某一攻击者通过不断改变CHADDR字段向DHCP Server申请IP地址，同样将会导致DHCP Server上的地址池被耗尽，从而无法为其他正常用户提供IP地址 解决办法： 为了抑制大量 Client 恶意申请IP地址，在使能设备的DHCP Snooping功能后，可配置设备或接口允许接入的最大DHCP用户数，当接入的用户数达到该值时，则不再允许任何用户通过此设备或接口成功申请到IP地址。 通过改变DHCP Request报文中的CHADDR字段方式的攻击，可使能设备检测DHCP Request报文帧头MAC与DHCP数据区中CHADDR字段是否一致功能，此后设备将检查上送的DHCP Request报文中的帧头MAC地址是否与CHADDR值相等 5. LDRA功能感知用户位置信息 LDRA称为轻量级DHCPv6中继代理，该中继代理能够记录用户位置信息并将其发送到 DHCPv6 Server，从而使得DHCPv6 Server能够获取到用户详细的物理位置信息，以实现对用户客户端部署诸如地址分配、计费、接入控制等策略 解决办法：在使能Switch的DHCP Snooping功能之后，可使能其LDRA功能。Switch既能够获取用户详细的位置信息并将其发送到DHCPv6 Server。DHCPv6 Server即可根据用户的详细位置信息为其部署地址分配策略或其他安全策略 LDRA功能仅是记录了DHCPv6用户的详细位置信息并通过 RELAY-FORW报文 将该信息发送给DHCPv6 Server，对不同的用户部署诸如地址分配、计费、接入控制等策略，由DHCPv6 Server实现 实现原理 DHCP Snooping分为DHCPv4 Snooping和DHCPv6 Snooping，两者实现原理相似。开启 DHCP Snooping 的接口会根据 DHCP 请求与响应生成 DHCP Snoop 绑定表，后续设备再从盖接口接受用户发送的 DHCP 报文时，会进行匹配检查 如下图所示的DHCP场景中，连接在二层接入设备上的PC配置为自动获取IP地址。PC作为DHCP客户端通过广播形式发送DHCP请求报文，使能了DHCP Snooping功能的二层接入设备将其通过信任接口转发给DHCP服务器。最后DHCP服务器将含有IP地址信息的DHCP ACK报文通过单播的方式发送给PC。在这个过程中，二层接入设备收到DHCP ACK报文后，会从该报文中提取关键信息（包括PC的MAC地址以及获取到的IP地址、地址租期），并获取与PC连接的使能了DHCP Snooping功能的接口信息（包括接口编号及该接口所属的VLAN，根据这些信息生成DHCP Snooping绑定表。 DHCP Snooping绑定表 根据DHCP租期进行老化或根据用户释放IP地址时发出的DHCP Release报文自动删除对应表项 由于DHCP Snooping绑定表记录了DHCP客户端IP地址与MAC地址等参数的对应关系，故通过对报文与DHCP Snooping绑定表进行匹配检查，能够有效防范非法用户的攻击。 为了保证设备在生成DHCP Snooping绑定表时能够获取到用户MAC等参数，DHCP Snooping功能需应用于二层网络中的接入设备或第一个DHCP Relay上。 在DHCP中继使能DHCP Snooping场景中，DHCP Relay设备不需要设置信任接口。因为DHCP Relay收到DHCP请求报文后进行源目的IP、MAC转换处理，然后以单播形式发送给指定的合法DHCP服务器，所以DHCP Relay收到的DHCP ACK报文都是合法的，生成的DHCP Snooping绑定表也是正确的 总结来说，是有一个 DHCP Snooping 绑定表用来记录以及校验 DHCP 报文，另外还有校验 二层 MAC 地址与 DHCP 的 Client mac address 的功能 DHCPv6 546号端口用于 DHCPv6 Client，而不用于DHCPv4，是为DHCP failover服务，这是需要特别开启的服务，DHCP failover是用来做双机热备的 参考链接 https://info.support.huawei.com/info-finder/encyclopedia/zh/DHCP.html https://support.huawei.com/hedex/hdx.do?docid=EDOC1100087046&amp;id=ZH-CN_CONCEPT_0176371515 https://support.huawei.com/hedex/hdx.do?docid=EDOC1100087046&amp;id=ZH-CN_CONCEPT_0176371535 https://info.support.huawei.com/info-finder/encyclopedia/zh/ICMP.html https://wiki.wireshark.org/DHCP.md https://info.support.huawei.com/info-finder/encyclopedia/zh/DHCP+Snooping.html","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"DHCP","slug":"DHCP","permalink":"http://xboom.github.io/tags/DHCP/"},{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/tags/Protocol/"}]},{"title":"Go-27-SyncPool","slug":"Go/Go-27-SyncPool","date":"2023-07-15T05:32:45.000Z","updated":"2023-07-15T05:38:19.201Z","comments":true,"path":"2023/07/15/Go/Go-27-SyncPool/","link":"","permalink":"http://xboom.github.io/2023/07/15/Go/Go-27-SyncPool/","excerpt":"","text":"前面一篇讲解了Sync.Pool的底层数据结构 poolDequeue，接着看看Sync.Pool的具体实现原理。如果想看看 Sync.Pool 的使用 可以看看我的 Go 入门 26-Issue，使用的时候最好分配固定大小的对象否则注意清理 带着问题看世界 Sync.Pool 与 Goroutine 的关系 Sync.Pool 是如何释放的(并没有主动释放接口) 代码：src/sync/pool.go，1.20版本 首先来看内存池的说明，翻译过来就是就是： 存放在Pool 中的元素任何时候都有可能在没有被其他引用的情况下释放掉 Pool 是并发安全的 使用 Pool 之后不能再复制它。假设缓存池对象 A 被对象 B 拷贝了，如果 A 被清空，B 的缓存对象指针指向的对象将会不可控 先来看看整体的结构图 全局变量 1234567891011121314var poolRaceHash [128]uint64var ( allPoolsMu Mutex //互斥锁，用于更新 allPools，因为所有的 sync.pool 都存放在里面 // allPools is the set of pools that have non-empty primary // caches. Protected by either 1) allPoolsMu and pinning or 2) // STW. allPools []*Pool // oldPools is the set of pools that may have non-empty victim // caches. Protected by STW. oldPools []*Pool) allPoolsMu：用于对allPools的更新进行保护 allPools: []*Pool切片，存储具有非空私有缓存的对象池。可以被多个 goroutine 访问 oldPools：[]*Pool类型的切片，用于存储可能具有非空受害者缓存的对象池，由于只有在 STW 时候才会更新，不会被并发访问 基本结构 123456789101112type Pool struct &#123; noCopy noCopy // 提示不要复制 local unsafe.Pointer // 指向每个 P(处理器) 的本地池的指针。它的类型是 [P]poolLocal，其中P是处理器数量 localSize uintptr // 本地数组的大小 victim unsafe.Pointer // GC上一个周期中的本地池的指针。在当前周期中，这个本地池变成了受害者。它的类型是 unsafe.Pointer victimSize uintptr // 受害者缓存数组的大小，以字节为单位 //可选地指定一个函数，当 Get 方法本来会返回 nil 时，可以用于生成一个值。 New func() any&#125; noCopy 用于提示不要进行对象复制 local 与 victim 的关系在后面 内存池清理 中说明 New 则是自定义的分配对象函数 noCopy noCopy 支持 使用 go vet 检查对象是否被复制，它是一个内置的空结构体类型，当然也可以自行实现类似功能 12345678910111213141516171819type noCopy struct&#123;&#125;func (n noCopy) Lock() &#123; //TODO implement me panic(\"implement me\")&#125;func (n noCopy) Unlock() &#123; //TODO implement me panic(\"implement me\")&#125;var _ sync.Locker = (*noCopy)()type Person struct &#123; noCopy age int height int&#125; 它第一次使用后不能被复制，其实代码是能够编译通过运行的，只是在 go vet 或者 部分编辑器会提示而已。可以看看没有成为标准的原因 https://golang.org/issues/8005#issuecomment-190753527 poolLocal 每个处理器 P 都有一个 poolLocal 的本地池对象 12345678910111213// 每个 P 的本地对象池type poolLocalInternal struct &#123; private any // 仅能被当前 P 进行读写 shared poolChain // 共享，当前 P 能对其进行 pushHead/popHead; 任何 P 都能执行popTail&#125;type poolLocal struct &#123; poolLocalInternal // Prevents false sharing on widespread platforms with // 128 mod (cache line size) = 0 . pad [128 - unsafe.Sizeof(poolLocalInternal&#123;&#125;)%128]byte&#125; private 是一个仅用于当前 P 进行读写的字段(即没有并发读写的问题) shared 可以在多个 P 之间进行共享读写，是一个 poolChain 链式队列结构， 当前 P 上可以进行 pushHead 和 popHead 操作(队头读写)， 在所有 P 上都可以进行 popTail (队尾出队)操作 pad 用于 伪共享 保证 poolLocal 的大小是 128 字节的倍数 runtime_procPin 12345678910111213//go:nosplitfunc procPin() int &#123; gp := getg() //获取当前goroutine的指针 mp := gp.m //当前goroutine 对应的 协程代表 mp.locks++ //这个变量用于跟踪当前线程(m)持有的锁的数量 return int(mp.p.ptr().id) //代表当前线程(m)执行的处理器(p)的唯一标识符&#125;func procUnpin() &#123; gp := getg() gp.m.locks--&#125; locks:通过增加锁的计数，表明当前线程被固定（pinned）在处理器上 mp.p表示当前线程所绑定的处理器，.ptr()方法返回处理器的指针，.id表示处理器的唯一标识符 pinSlow 将当前的goroutine绑定到Pool中的一个poolLocal上，并返回该poolLocal及其索引 12345678910111213141516171819202122232425262728293031323334353637func (p *Pool) pinSlow() (*poolLocal, int) &#123; //1. 解除当前goroutine与之前绑定的poolLocal的绑定关系，使当前goroutine可以进行重新绑定 //使用 mutex 时候 P 必须可抢占 runtime_procUnpin() //2. 获取池的全局锁 allPoolsMu.Lock() defer allPoolsMu.Unlock() //3. 将当前的goroutine绑定到一个poolLocal上，并返回其索引pid。 //再次固定 P 时 poolCleanup 不会被调用 pid := runtime_procPin() //获取未越界返回poolLocal s := p.localSize l := p.local if uintptr(pid) &lt; s &#123; return indexLocal(l, pid), pid &#125; // 如果数组为空，将其添加到 allPools，垃圾回收器从这里获取所有 Pool 实例 if p.local == nil &#123; allPools = append(allPools, p) &#125; // 根据 P 数量创建 slice，如果 GOMAXPROCS 在 GC 间发生变化 // 我们重新分配此数组并丢弃旧的 size := runtime.GOMAXPROCS(0) // 获取当前的GOMAXPROCS值，即当前系统的最大并发数 local := make([]poolLocal, size) //创建本地池 // 将底层数组起始指针保存到 p.local，并设置 p.localSize atomic.StorePointer(&amp;p.local, unsafe.Pointer(&amp;local[0])) runtime_StoreReluintptr(&amp;p.localSize, uintptr(size)) // 返回所需的 pollLocal return &amp;local[pid], pid&#125; 这个逻辑的前提是当前 P 已经发生了动态调整，需要重新计算localPool 首先解除 goroutine 与 Process 的绑定，让 goroutine 可以重新绑定 获取 allPools的全局锁 将当前 goroutine 与 Process 重新绑定 如果Process 未发生变化，返回 Process 的 localPool 如果没有变化则 如果 Pool.local为空，则需要将 Pool 加入到 allPools 中，用于 GC 扫描回收 重新创建 [p]poolLocal 重新将 Pool.local 指向 [p]poolLocal pin 获取当前 Process 中的 poolLocal，将当前的goroutine绑定到一个特定的Process上，禁用抢占并返回Process的poolLocal本地池和P的标识 12345678910111213func (p *Pool) pin() (*poolLocal, int) &#123; // 1. 将当前的goroutine绑定到一个P上，并返回P的标识pid。 pid := runtime_procPin() s := runtime_LoadAcquintptr(&amp;p.localSize) // load-acquire l := p.local // load-consume //可能存在动态的 P（运行时调整 P 的个数）procresize/GOMAXPROCS，如果 P.id 没有越界，则直接返回 if uintptr(pid) &lt; s &#123; return indexLocal(l, pid), pid &#125; return p.pinSlow()&#125; 尝试通过加载local和localSize字段的方式来判断是否可以直接返回一个可用的poolLocal，如果不满足条件，则调用pinSlow方法来重新分配并返回一个新的poolLocal。调用者在使用完poolLocal之后，必须调用runtime_procUnpin()来解除与P的绑定关系。 1234func indexLocal(l unsafe.Pointer, i int) *poolLocal &#123; lp := unsafe.Pointer(uintptr(l) + uintptr(i)*unsafe.Sizeof(poolLocal&#123;&#125;)) return (*poolLocal)(lp)&#125; 内存池清理 在使用 init 仅执行了一个逻辑，就是注册内存池回收机制 123func init() &#123; runtime_registerPoolCleanup(poolCleanup)&#125; poolCleanup 用于实现内存池的清理 12345678910111213141516171819202122func poolCleanup() &#123; // Because the world is stopped, no pool user can be in a // pinned section (in effect, this has all Ps pinned). // Drop victim caches from all pools. for _, p := range oldPools &#123; p.victim = nil p.victimSize = 0 &#125; // Move primary cache to victim cache. for _, p := range allPools &#123; p.victim = p.local p.victimSize = p.localSize p.local = nil p.localSize = 0 &#125; // The pools with non-empty primary caches now have non-empty // victim caches and no pools have primary caches. oldPools, allPools = allPools, nil&#125; 垃圾回收的策略就是 将 oldPools 中也就是所有localPool的 victim 对象丢弃 将 allPools 的 local 复制给victim，并 local 重置 最后将 allPools 复制给 oldPools，allPools 置空 Get 整体流程如下 首先获取当前 Process 的 poolLocal，也就是说当 Goroutine 在哪个 Process 运行的时候就会从哪个 Process 的 localPool中获取对象 优先从 private 中选择对象，并将 private = nil 若取不到，则尝试从 shared 队列的队头进行读取 若取不到，则尝试从其他的 Process 中进行偷取 getSlow（跨 Process 读写） 若还是取不到，则使用自定义的 New 方法新建对象 获取对象代码如下操作 1234567891011121314151617181920212223func (p *Pool) Get() any &#123; //...race //1. 获取一个poolLocal l, pid := p.pin() //2. 先从private获取对象 x := l.private l.private = nil if x == nil &#123; // 尝试从 localPool 的 shared 队列队头读取 x, _ = l.shared.popHead() if x == nil &#123; // 如果取不到，则获取新的缓存对象 x = p.getSlow(pid) &#125; &#125; runtime_procUnpin() //...race // 如果 getSlow 还是获取不到，则 New 一个 if x == nil &amp;&amp; p.New != nil &#123; x = p.New() &#125; return x&#125; 其中的 getSlow 就是从 其他 Process 或者 victim 中获取 1234567891011121314151617181920212223242526272829303132333435363738func (p *Pool) getSlow(pid int) any &#123; //1. 加载本地池 size := runtime_LoadAcquintptr(&amp;p.localSize) // load-acquire locals := p.local // load-consume // Try to steal one element from other procs. //2. 尝试从其他 process 偷取元素 for i := 0; i &lt; int(size); i++ &#123; l := indexLocal(locals, (pid+i+1)%int(size)) if x, _ := l.shared.popTail(); x != nil &#123; return x &#125; &#125; //3. 从其他 process 也没有偷到 //那么判断是否 process数量发生了变化 size = atomic.LoadUintptr(&amp;p.victimSize) if uintptr(pid) &gt;= size &#123; return nil &#125; //4. 从 victim 中获取 locals = p.victim l := indexLocal(locals, pid) if x := l.private; x != nil &#123; l.private = nil return x &#125; for i := 0; i &lt; int(size); i++ &#123; l := indexLocal(locals, (pid+i)%int(size)) if x, _ := l.shared.popTail(); x != nil &#123; return x &#125; &#125; //到这里说明没有了，那么就将 victimSize 设置为 0后续不会访问 atomic.StoreUintptr(&amp;p.victimSize, 0) return nil&#125; Put Put 的操作如下 存放策略是： 如果存放 nil 直接返回 获取当前 Process 的 poolLocal 如果 private == nil 则放到 private 中 如果private != nil 则将起放入到 链表头部 12345678910111213141516func (p *Pool) Put(x any) &#123; if x == nil &#123; return &#125; //...race // 获得一个 localPool l, _ := p.pin() // 优先放入private if l.private == nil &#123; l.private = x &#125; else &#123; l.shared.pushHead(x) &#125; runtime_procUnpin() //...race&#125; 总结 Pool 本质是为了提高临时对象的复用率； Pool 使用两层回收策略（local + victim）避免性能波动； Pool 本质是一个杂货铺属性，啥都可以放，Pool 池本身不做限制； Pool 池里面 cache 对象也是分层的，一层层的 cache，取用方式从最热的数据到最冷的数据递进； Pool 是并发安全的，但是内部是无锁结构，原理是对每个 P 都分配 cache 数组（ poolLocalInternal 数组），这样 cache 结构就不会导致并发； 永远不要 copy 一个 Pool，明确禁止，不然会导致内存泄露和程序并发逻辑错误； 代码编译之前用 go vet 做静态检查，能减少非常多的问题； 每轮 GC 开始都会清理一把 Pool 里面 cache 的对象，注意流程是分两步，当前 Pool 池 local 数组里的元素交给 victim 数组句柄，victim 里面 cache 的元素全部清理。换句话说，引入 victim 机制之后，对象的缓存时间变成两个 GC 周期； 不要对 Pool 里面的对象做任何假定，有两种方案：要么就归还的时候 memset 对象之后，再调用 Pool.Put ，要么就 Pool.Get 取出来的时候 memset 之后再使用； 参考文档 https://mp.weixin.qq.com/s/dLzWAqM9lCln83jhkvmtMw https://geektutu.com/post/hpg-sync-pool.html https://golang.design/under-the-hood/zh-cn/part1basic/ch05sync/pool/ https://xie.infoq.cn/article/55f28d278cccf0d8195459263","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-26-Issue","slug":"Go/Go-26-Issue","date":"2023-07-13T14:59:45.000Z","updated":"2023-07-19T15:34:46.967Z","comments":true,"path":"2023/07/13/Go/Go-26-Issue/","link":"","permalink":"http://xboom.github.io/2023/07/13/Go/Go-26-Issue/","excerpt":"","text":"Issue-23199 在阅读 sync.Pool 在 fmt 中的使用时候看到了这样一段代码 1234567891011121314151617func (p *pp) free() &#123; // Proper usage of a sync.Pool requires each entry to have approximately // the same memory cost. To obtain this property when the stored type // contains a variably-sized buffer, we add a hard limit on the maximum // buffer to place back in the pool. If the buffer is larger than the // limit, we drop the buffer and recycle just the printer. // // See .https://golang.org/issue/23199 if cap(p.buf) &gt; 64*1024 &#123; p.buf = nil &#125; else &#123; p.buf = p.buf[:0] &#125; //... //存入内存池 ppFree.Put(p)&#125; 上述英文解释的大致意思是 sync.Pool在使用的时候每个对象的内存消耗应该大致相同。当存储的类型包含可变大小的缓冲区时，需要对最大缓冲区设置一个硬限制，以确保如果缓冲区的大小超过限制，将丢弃缓冲区，并仅回收 因为 fmt 使用内存池分配的对象大小不是固定的，如下 buf 其实是一个缓冲区 12345678910var ppFree = sync.Pool&#123; New: func() any &#123; return new(pp) &#125;,&#125;type pp struct &#123; buf buffer //...&#125;type buffer []byte 这里做一个测试就会明白这样的处理 123456789a := make([]int, 100)b := make([]int, 100)fmt.Println(cap(a), len(a)) //100 100fmt.Println(cap(b), len(b)) //100 100a = nilb = b[:0]fmt.Println(cap(a), len(a)) //0 0fmt.Println(cap(b), len(b)) ///100 0 使用 b[:0] 之后只是切片长度变化，容量并不会变，也就是内存仍然占用那么多 所以，如果内存池大小不固定的时候注意主动释放，防止额外占用空间而不被释放。再来看看它的测试结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package mainimport ( \"bytes\" \"fmt\" \"runtime\" \"sync\" \"time\")func finalizer(buffer *bytes.Buffer) &#123; fmt.Printf(\"GC %d buffer \\n\", buffer.Cap())&#125;func main() &#123; var stats runtime.MemStats runtime.ReadMemStats(&amp;stats) fmt.Printf(\"start %d B\\n\", stats.Alloc) pool := sync.Pool&#123;New: func() interface&#123;&#125; &#123; buf := new(bytes.Buffer) runtime.SetFinalizer(buf, finalizer) return buf &#125;&#125; processRequest := func(size int) &#123; b := pool.Get().(*bytes.Buffer) time.Sleep(500 * time.Millisecond) // Simulate processing time b.Grow(size) pool.Put(b) time.Sleep(1 * time.Millisecond) // Simulate idle time &#125; // 模拟一组大规模写入 for i := 0; i &lt; 10; i++ &#123; go func() &#123; processRequest(1 &lt;&lt; 20) // 256MiB &#125;() &#125; time.Sleep(time.Second) // Let the initial set finish // 模拟小规模写入且不会停 for i := 0; i &lt; 10; i++ &#123; go func() &#123; for &#123; processRequest(1 &lt;&lt; 10) // 1KiB &#125; &#125;() &#125; // 每次GC之后查看分配的内存 for i := 0; ; i++ &#123; runtime.ReadMemStats(&amp;stats) fmt.Printf(\"Cycle %d: %dB\\n\", i, stats.Alloc) time.Sleep(time.Second) runtime.GC() &#125;&#125; 通过 runtime.SetFinalizer 大概说明对象的回收时间，会发现大对象并不是立即回收的，而是经过了一段时间，内存才趋于稳定。高并发场景下，比如处理网络请求的时候，可能导致大量内存的占用而没有及时释放 解决办法: 就在在判断超过一定大小的时候直接丢弃 参考链接 https://golang.org/issue/23199 https://go-review.googlesource.com/c/go/+/136035/1/src/encoding/json/encode.go","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-25-PoolDequeue","slug":"Go/Go-25-PoolDequeue","date":"2023-07-01T08:51:45.000Z","updated":"2023-07-15T05:38:08.054Z","comments":true,"path":"2023/07/01/Go/Go-25-PoolDequeue/","link":"","permalink":"http://xboom.github.io/2023/07/01/Go/Go-25-PoolDequeue/","excerpt":"","text":"在学习Golang 中 的内存池 Sync.Pool 之前，底层是使用的 一个无锁的 动态扩容 的双端队列支持 单生产者 多消费者 的 Sync.Pool 内存分配模式，它是一个私有结构体，所以外部无法访问它，这里来看看如何实现的 代码：src/sync/poolqueue.go 带着问题看世界 如何动态扩容的 为什么内存池要是用这样的 数组 + 链表 poolDequeue 首先看一下基本结构结构体，使用数组实现了一个 ring buffer 的结构 123456789101112type poolDequeue struct &#123; // headTail 包含了 32bit的头部索引与32位尾部索引指向 vals // head 高 32 位, 指向下一个存放对象的索引 // tail 低 32 位, 指向队列中最早(下一个读取)的对象索引 // 索引区间 tail &lt;= i &lt; head, 是消费者可以在该区间不断获取对象，直至获取到的对象为 nil headTail uint64 // vals 表示队列元素容器，大小必须为 2 的 N 次幂 // 容器会在初始化时指定容量，实现数据元素内存预初始化 // 队列会将未使用的槽位设置为nil vals []eface&#125; 为什么要将 head 和 tail 合并到一个变量里面？ 当队列中存在多个对象，两边同时操作貌似没什么问题，但为了防止队列中仅剩一个对象时，就需要锁住两个索引进行操作，所以利用了 atomic 包的提供的 CAS 操作，完成两个字段的 lock free 无锁编程 更新 head 和 tail 两个字段的时候，也是通过 CAS + 位运算 进行操作的。更新逻辑如下 123456789101112131415//dequeueNil 是用于在 poolDequeue 中表示 interface&#123;&#125;(nil) 的类型。由于使用 nil 来表示空槽位，所以需要一个特殊值来表示 nil。type dequeueNil *struct&#123;&#125;func (d *poolDequeue) unpack(ptrs uint64) (head, tail uint32) &#123; const mask = 1&lt;&lt;dequeueBits - 1 //获取掩码 head = uint32((ptrs &gt;&gt; dequeueBits) &amp; mask) //获取头部索引 tail = uint32(ptrs &amp; mask) //获取尾部索引 return&#125;func (d *poolDequeue) pack(head, tail uint32) uint64 &#123; const mask = 1&lt;&lt;dequeueBits - 1 return (uint64(head) &lt;&lt; dequeueBits) | uint64(tail&amp;mask)&#125; unpack 与 pack 实现了在 poolDequeue 结构中进行索引值的打包和解包操作。使得在 poolDequeue 结构中使用单个 64 位整数来同时存储头部索引和尾部索引 所以整体就如下图所示 接着是对队列的操作，分为 头部写入、头部弹出、尾部弹出 pushHead 头部写入返回成功失败 1234567891011121314151617181920212223242526272829303132func (d *poolDequeue) pushHead(val any) bool &#123; //1. 原子操作获取索引 ptrs := atomic.LoadUint64(&amp;d.headTail) //2. 解析头尾索引 head, tail := d.unpack(ptrs) //3. (当尾部索引 + 队列长度)&amp; 掩码 == 头部索引，说明队列满了 if (tail+uint32(len(d.vals)))&amp;(1&lt;&lt;dequeueBits-1) == head &#123; // Queue is full. return false &#125; //4. 没有满则获取头部位置 slot := &amp;d.vals[head&amp;uint32(len(d.vals)-1)] //5. 判断头部是否已经被释放 typ := atomic.LoadPointer(&amp;slot.typ) if typ != nil &#123; // Another goroutine is still cleaning up the tail, so // the queue is actually still full. return false &#125; // The head slot is free, so we own it. //如果存放的是nil，那么使用 dequeueNil(nil) 表示存放的是一个nil类型 if val == nil &#123; val = dequeueNil(nil) &#125; *(*any)(unsafe.Pointer(slot)) = val //因为头部索引是高32位，所以 增加 1&lt;&lt;dequeueBits，同理尾部索引是低32，直接减1 atomic.AddUint64(&amp;d.headTail, 1&lt;&lt;dequeueBits) return true&#125; 如果 type == nil，则表示 slot 已经被释放，如果 value = dequeueNil(nil) 表示存放的是一个 nil (但注意，Sync.Pool 的Put(nil) 会直接返回，而不会真的存放一个 nil) popHead 返回头部索引位置 123456789101112131415161718192021222324252627282930313233func (d *poolDequeue) popHead() (any, bool) &#123; var slot *eface //使用 CAS 不断尝试获取头部值 for &#123; //加载并解析索引获取头部位置，判断是否为空 ptrs := atomic.LoadUint64(&amp;d.headTail) head, tail := d.unpack(ptrs) if tail == head &#123; // Queue is empty. return nil, false &#125; //计算更新后的索引值 head-- ptrs2 := d.pack(head, tail) //CAS 更新 if atomic.CompareAndSwapUint64(&amp;d.headTail, ptrs, ptrs2) &#123; //获取头部位置 slot = &amp;d.vals[head&amp;uint32(len(d.vals)-1)] break &#125; // 如果 不是 ptrs 所以已经发生了变化那么重试 &#125; val := *(*any)(unsafe.Pointer(slot)) if val == dequeueNil(nil) &#123; //如果存放的是nil，那么返回nil val = nil &#125; //将槽置位 zero，为什么不会与pushHead冲突 *slot = eface&#123;&#125; return val, true&#125; popTail 与 popHead差不多，最后的赋值有区别 12slot.val = nil //直接赋值空值atomic.StorePointer(&amp;slot.typ, nil) //表示为空 为什么 popTail 与 popHead 处理方式不一样，是因为设计的就是单个生产者与多个消费者，所以popHead并不会有竞争的问题，最后都是存放的 slot{} poolChainElt poolChainElt 是一个双向链表的节点，包含林一个每个节点 poolDequeue 结构 以及 双向指针 1234type poolChainElt struct &#123; poolDequeue next, prev *poolChainElt&#125; poolChain 有了双向链表节点，那么就有双向链表 poolChain 1234567type poolChain struct &#123; //由于只有生产者访问该字段，因此不需要同步操作 head *poolChainElt //由于消费者访问该字段，因此读写必须是原子操作 tail *poolChainElt&#125; poolChain 是一个双向链表队列，其中每个 poolDequeue 的大小是前一个 poolDequeue 的两倍。一旦一个 poolDequeue 填满，就会分配一个新的 poolDequeue，并且只会将数据推入到最新的 poolDequeue。弹出操作发生在列表的另一端，当一个 poolDequeue 被耗尽后，它会从链表中移除。 通过这种设计，poolChain 实现了一个可以动态增长的队列，以适应对象池中的元素数量变化。它能够高效地管理多个不同大小的队列，并提供生产者和消费者之间的并发访问。生产者将数据推入到最新的 poolDequeue，而消费者从链表的另一端弹出数据。这样可以避免竞争条件和锁等待，并提高并发性能 pushHead 有了双向链表之后，再来看它是如何写入一个值的 12345678910111213141516171819202122232425262728293031func (c *poolChain) pushHead(val any) &#123; //1. 获取双向链表头部节点 d := c.head if d == nil &#123; //2. 如果头部节点是空的，说明整个链表都是空的，那么就需要初始化一个链表节点 const initSize = 8 // 默认大小是8，必须是2的倍数 d = new(poolChainElt) d.vals = make([]eface, initSize) //节点popDequeue的默认大小是8 c.head = d //双向链表的的头部指针指向这个节点 storePoolChainElt(&amp;c.tail, d) //将节点追加到双向链表尾部 &#125; //将值写入头部节点的内存池队列中，如果成功就结束 if d.pushHead(val) &#123; return &#125; //如果到这里，说明插入失败了，那么就会创建一个更大(2倍)的内存池队列 poolDequeue newSize := len(d.vals) * 2 if newSize &gt;= dequeueLimit &#123; //最大不能超过 1 &lt;&lt; 32 / 4 = 1 &lt;&lt; 30 newSize = dequeueLimit &#125; d2 := &amp;poolChainElt&#123;prev: d&#125; //新建节点前驱是 d d2.vals = make([]eface, newSize) //构建指定大小的切片 c.head = d2 //将d2设置为头部节点 storePoolChainElt(&amp;d.next, d2) //d的后继为d2 d2.pushHead(val) &#125; 按照效果如下 1[8] -- [16] -- [32] -- head popHead 12345678910111213func (c *poolChain) popHead() (any, bool) &#123; //获取头部节点 d := c.head for d != nil &#123; //获取队列头部节点 if val, ok := d.popHead(); ok &#123; return val, ok &#125; // 到这里是没有获取到，然后指向前驱继续获取 d = loadPoolChainElt(&amp;d.prev) &#125; return nil, false&#125; popTail 123456789101112131415161718192021222324252627282930func (c *poolChain) popTail() (any, bool) &#123; //1. 加载尾部队列节点，如果没有表示链表为空则返回 false d := loadPoolChainElt(&amp;c.tail) if d == nil &#123; return nil, false &#125; for &#123; // 记录尾部的后驱 d2 := loadPoolChainElt(&amp;d.next) // 弹出队列尾部值 if val, ok := d.popTail(); ok &#123; return val, ok &#125; // 如果 d 没有了，d2 也为空，说明没有值 if d2 == nil &#123; return nil, false &#125; // CAS 尝试获取 d if atomic.CompareAndSwapPointer((*unsafe.Pointer)(unsafe.Pointer(&amp;c.tail)), unsafe.Pointer(d), unsafe.Pointer(d2)) &#123; //如果获取到那么去掉前继 storePoolChainElt(&amp;d2.prev, nil) &#125; //更新尾部指针 d = d2 &#125;&#125; 总结： 无锁的原因是队列形式，单个生产者在头部操作，消费者从队尾消费 并不是固定大小，而是作为双向链表节点的队列最大长度是 1 &lt;&lt; 32/ 4 ，并且用环形缓冲区 ring buffer 实现 Pool 底层 使用 数组 + 链表的形式的原因是由 Pool 的特性决定的，它需要频繁的内存分配，所以数组是一个好的选择。又为了解决扩容的问题，使用链表来连接数组 参考文档 https://mp.weixin.qq.com/s/dLzWAqM9lCln83jhkvmtMw https://geektutu.com/post/hpg-sync-pool.html https://studygolang.com/articles/28386 https://juejin.cn/post/7213257917255385149","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Protocol-03-Raft","slug":"Protocol/Protocol-03-Raft","date":"2023-06-26T02:25:38.000Z","updated":"2023-09-08T15:38:09.211Z","comments":true,"path":"2023/06/26/Protocol/Protocol-03-Raft/","link":"","permalink":"http://xboom.github.io/2023/06/26/Protocol/Protocol-03-Raft/","excerpt":"","text":"在学习分布式系统过程中，经常能碰到一致性算法，即使出现部分节点故障，网络延时等情况，也不影响整体可用性，进而提高系统的整体可用性。Raft协议是一种分布式一致性算法(共识算法)，共识就是多个节点对某一个事件达成一致的算法，常见的一致性协议有Raft、Paxos、ZAB，根据 《ETCD 技术内幕》 学习 Raft 协议 基本定义 Raft 为了实现共识，会选举出主从节点，任意节点都处于下面3类角色： Leader(领袖)：领袖由群众投票选举得出，每次选举，只能选出一名领袖； Candidate(候选人)：当没有领袖时，某些群众可以成为候选人，然后去竞争领袖的位置； Follower(群众)：相对于 Leader 来说，在非选举过程中的其他节点就是 Follower，任何节点都可能通过 Candidate 成为 Leader 在进行选举过程中，还有几个重要的概念： Leader Election(领导人选举)：简称选举，就是指选出 Leader； Term(任期)：其实是一个单独递增的连续数字，每一次任期就会重新发起一次 Leader Election，每个节点都会记录当前的任期值 Election Timeout(选举超时)：就是一个超时时间，当 Follower 超时未收到 Leader 的心跳时，会重新进行选举 任期(Term)：实际上是一个全局的、连续递增的整数。在raft中，每进行一次选举，任期就会+1 节点存在三种状态分别是： Leader节点：可读可写、处理日志复制、保证⼀致性、维护⼼跳 Follower节点：可进行读操作，写操作需要路由到Leader处理；参与投票，⼀个term任期内只能投⼀次; 当触发 election timeout 时，晋升为 Candidate Candidate节点：集群的候选者，会发起投票试图当选leader ; 通过 RequestVote 通知其他节点来投票; 集群中任意一个节点都处于这三个状态之一 如上图所示 当 Etcd节点刚启动的时候，节点初始化状态为 Follower 为了进行 Leader Election 有两个时间 选举超时时间(Election timeout): 每个 Follower 节点在接收不到 Leader 节点的心跳消息之后，并不会立即发起新一轮选举，而是需要等待一段时间之后才切换成 Candidate 状态发起新一轮选举。是一个一定范围的随机数，为了保证各节点不是同时发起选举请求 心跳超时时间(Heartbeat timeout): Leader节点向集群中其他Follower节点发送心跳消息的时间间隔 Leader选举 能成为 Leader 的条件有 当前集群⽆可用 leader 触发 Election timeout Term 任期最新 Log 日志最新 获取多数投票 流程可以查看：http://thesecretlivesofdata.com/raft/ 需要注意的是： 超过集群半数节点应答，才将日志写入WAL中，也是就如果没有等到半数应答就挂了，那么数据就丢了 Follower 收到 Leader 的 复制日志 则直接将日志写入 WAL 中 当Follower收到 Vote Request 也会重置 election timeout 定时器(降低同时出现两个候选人的概率) Term 将持续直到某个节点因为心跳超时而从新发起选举 出现多个候选人都没有称为 Leader，那么选举失败，切换为Follower，等待下一个选举超时 Leader 宕机后，其他节点重新选出新 Leader。等到旧Leader 恢复，会因为收到新Leader心跳 而自己的 Term 落后，切换成 Follower，更新自己的Term，然后重新投出自己的选票 基本流程 将设ETCD集群中有三个节点(A、B、C) 初始化，所有节点起初都是 Follower 状态(Term = 0， 得票数 = 0) 节点 A 由于长时间未收到 Leader 的心跳消息，就会切换成为 Candidate 状态并发起选举 节点 A 的选举计时器 Election Timer 己被重置 节点 A 首先会将自己的选票投给自己，并会向集群中其他节点发送选举请求 **Request Vote **以获取其选票，此时节点会有几种状态 当前状态是 Leader(初始化的时候不会) 当前状态是 Candidate，当处于 Candidate 状态而收到 Request Vote 的时候，如果接收到 Term 值比自身记录的 Term 值大的请求时，节点会切换成 Follower 状态井更新自身记录的 Term 值 当前状态是 Follower，此时的节点 B 和节点 C 还都是处于 Term=0 的任期之中，且都是 Follower 状态，均未投出 Term=1 任期中的选票，所以节点 B 和节点 C 在接收到节点 A 的选举请求后会将选票投给节点 A，任期变为1 在节点 A 收到节点 B、 C 的投票之后，其收到了集群中超过半数的选票，所以在 Term = 1 的任期中，该集群的 Leader 节点就是节点 A，其他节点将切换成 Follower 状态， 节点除了记录当期任期号(CurrentTerm)，还记录在该任期中当前节点的投票结果(VoteFor) 成为 Term = 1 任期的 Leader 节点之后，节点 A 会定期向集群中的其他节点发送心跳消息，防止节点 B 和节点 C 中的选举计时器Election timer 超时而触发新一轮的选举(重置定时器)，此时当前节点的状态 当前状态是 Follower，当节点 B 和节点 C (Follower) 收到节点 A 的心跳消息之后会重置选举计时器，更新 Term 当前状态是 Candidate， 当处于 Candidate 状态而收到 Request Vote 的时候，如果接收到 Term 值比自身记录的 Term 值大的请求时，节点会切换成 Follower 状态井更新自身记录的 Term 值 心跳超时时间（heartbeat timeout）需要远小于选举超时时间(election timeout) Candidate 发起选举前,会先尝试连接集群中的其他节点。如果连接不上，放弃本次选举。这个状态称之为：Prevote 带着问题看世界 如果Leader节点频繁宕机，或者选举反复进行，怎么办？ 答：要求 心跳超时时间 &lt;&lt; 选举超时时间 &lt;&lt; 平均故障间隔时间 心跳超时时间：节点直接发送心跳信息的完整返回时间 Hearthbeat timeout：0.5ms~50ms 选举超时时间：election timer：200ms~1s 故障间隔时间：两次故障的平均时间：1个月或更多(保证节点的不要经常出故障) 是不是谁先发起了选举请求，谁就得到了Leader？ 答：不是，除了看先后顺序，还取决于Candidate节点日志是不是最新最全的日志，否则拒绝投票，防止出现日志(即数据)丢失的情况 如果两个节点同时成为 Candidate 发起选举，刚好它们的日志 都是最新的是如何选举的 答：虽然每个节点的 Election Timer 都不同，但也是不能避免两个节点同时触发选举。比如有 4 个节点 异常情况 场景 1：假设A、B同时触发选举，A 的Request Vote先抵达节点 C ，B 的Request Vote先抵达节点 D，A、B 除了得到自身的选票之外，还分别得到了节点 C 和节点 D 的 Vote且票数相同没有超过半数 在这种情况下， Term = 4 这个任期会以选举失败结束，随着时间的流逝，当任意节点的 Election Timer 到期之后，会再次发起新一轮的选举。由于 election timeout 是在一个时间区间内取的随机数，所以在配置合理的时候，像上述情况多次出现的概率并不大 场景 2：假设选举已经完成， Leader 在运行过程中 Down 掉了 当 A 恢复之后，会收到节点 D 发来的心跳消息，该消息中携带的任期号 Term=6 &gt; 节点 A 前记录的任期号 Term=5 ，A 切换成 Follower 并更新自身的 Term，同时重置远举计时器 场景 3：当节点与其他节点断开连接(出现网络分区)，不断触发选举超时，在恢复的时候因为 Term 比较大又成了 Leader 为了防止选举超时而不断的增加 Term ，当恢复的时候因为 Term 而变成 Leader，所以采取了 Prevote 措施，变为 candidate 的条件 询问其他节点是否有可用 leader 可连通绝⼤数节点 日志复制 Leader 节点除了向 Follower 节点发送心跳消息，还会处理客户端的请求，并将客户端的写操作 以消息(Append Entries 消息)的形式发送到集群中所有Follower节点 步骤如下： Leader 节点接到 client 请求后 set a=10,将本请求计入本地log Leader 节点向 Follower1 和 Follower2 发送Append Entries消息 set a=10 Follower1 和 Follower2 将此信息计入本地log，并返回给 Leader Leader 将日志信息置为 已提交(Commited),然后状态机处理。 响应 client 请求 向 Follower1 和 Follower2 发送消息，该信息已提交 Follower1 和 Follower2 接到消息后，修改日志状态，交给自己的状态机处理 索引 集群中各个节点都会维护一个本地 Log 用于记录更新操作，还会维护 commitlndex 和lastApplied 两个值，它们是本地Log 的索引值 commitlndex 表示的是当前节点已知的、最大的、己提交的日志索引值 lastApplied 表示的是当前节点最后一条被应用到状态机中的日志索引值。 当节点中的 commitlndex 值大于 lastApplied 值时，会将 lastApplied + 1 ，并将 lastApplied 对应的日志应用到其状态机中。 Leader 还需要了解集群中其他 Follower 节点的这些信息，而决定下次发送 Append Entries 消息中包含哪些日志记录。为此， Leader 节点会维护 nextlndex[]和 matchlndex[]两个数组，这两个数组中记录的都是日志索引值 nextlndex[] 记录了需要发送给 Follower 节点的下一条日志的索引值 matchlndex[] 记录了己经复制给每个Follower 节点的最大的日志索引值 一个数组不就可以表达了吗？ 答：不行，nextIndex用于指示Leader节点将要发送给Follower节点的下一个日志条目的位置，帮助Leader节点推进复制进度；matchIndex用于指示Leader节点已经复制到Follower节点上的最高日志条目的位置，帮助Leader节点确定已经被大多数节点确认的日志条目，可以进行提交 Leader 节点中 matchlndex 大于等于该 Follower 节点对应的 nextlndex 值，那么通过 Append Entries 消息发送从nextlndex 开始的所有日志。之后，Leader节点会检测该 Follower 节点返回的相应响应， 如果成功则更新相应该 Follower 节点对应的 nextlndex 值和matchlndex 值； 如果因为日志不一致而失败，则减少 nextlndex 值重试 A 作为 Leader 节点记录了 nextlnde 和 matchlndex，所以 A 应该知道向 B 、C 节点发送哪日志 Raft 协议采用批量发送的方式，当B、C 收到 Append Entries 消息后将日志记录到本地 Log 中，然后向 Leader 节点返回追加日志成功的响应，Leader 节点收到响应之后会递增节点对应的nextlndex 、matchlnde 这样，Leader 节点就知道下次发送日志的位置 上面是 Leader 的情况，当 Leader 从 A 切换到 B，B 并不知道 Leader 节点记录 nextlndex、matchlndex 信息 ，所以新 Leader 节点会重置 nextlndex、matchlnd ，其中会将 nextlndex 全部重置为其自身 Log 的最后一条己提交日志的 Index，而 matchlndex 全部重置为 0 新任期中的 Leader 节点向其他节点发送 Append Entrie 消息，拥有了当前 Leader 全部日志记录，会返回追加成功的响应并等后续的日志，而 C 没有 Index=2 Index=3 两条日志，所以追加日志失败的响应， Leader 节点会将 nextindex 前移 然后新 Leader 会再次尝试发送 append entries 消息，循环往复，不断减小 nextlndex值，直至节点C 返回追加成功的响应，之后就进入了正常追加消息记录的流程 日志判断 那么又是如何判断两条日志是相同的呢，可能两个节点的 log index 相同但是内容并不相同 raft 中的每个日志记录都带有 term 和 log index 来唯⼀标识，日志记录具有两个特性 如果两条不同节点的某两个日志记录具有相同的 term 和 index 号，则两条记录⼀定是完全相同的 ; 如果两条不同节点的某两个日志记录具有相同的 term 和 index 号，则两条记录之前的所有记录也⼀定是完全相同的 ; 以此来判断 leader与 follower 之间的日志是否相同 leader 从来不会覆盖，删除或者修改其日志 ; 若 follower 对比其前⼀条 log 不⼀致，则会拒绝 leader 发来的请求. 此时 leader 就将其在 nextIndex[] 中的对应值减⼀ leader 按照自身日志顺序将日志正常复制给 follower，并不断将 nextIndex[] 对应 值 +1，直到对应值 “追上” 自身日志的 index 为⽌; 问题 1：为什么要一个一个对比，而不直接找到对应的位置，批量复制？ 答：通过数组存储的是索引，但是日志比较是通过 index 与 term 所以，在选举过程中，Follower 节点还需要比较该 Candidate 节点的日志记录与自身的日志记录，拒绝那些日志没有自己新的 Candidat 节点发来的投票请求，确保将选票投给包含了全部己提交(Commited)日志记录的 Candidate 节点 Raft 协议通过较两节点日志中的最后一条日志记录的索引值和任期号，以决定谁的日志比较新 首先比较最后一条日志记录的任期号，如果最后的日志记录的任期号不同，那么任期号大的日志记录比较新： 如果最后一条日志记录的任期号相同，那么日志索引较大的比较新 复制异常 场景1：两个Follower 节点不可用，Leader 如何处理请求 leader 通过⼼跳已知 follower 已挂, 则直接返回错误 ; leader 不知节点已挂, 同步数据时得知异常; 触发异常触发超时 场景2： 提交给Leader后，发生了crash leader 本地已记录已提交日志, 重启后强制同步新 leader 数据 ; leader 还没记录未提交日志就crash, 丢了就丢了; 场景3：复制给 follower-1 后, leader就发⽣了 crash ? leader 发⽣了重启，集群触发新的选举 ; 由于 follower-1 的数据较新, 那么该节点会晋升 leader ; follower-1 会把 uncommited 的 v=3 同步给其他节点 ; follower-1 收到其他节点接收确认后, 进⾏提交日志 ; 场景4：follower 返回确认消息时, leader 发⽣了 crash ? 三个节点的数据已经⼀致, 都为 uncommited v=3 ; 这时谁当 Leader 都可以 ; 新leader当选后, 需要进⾏同步提交通知 ; 脑裂问题 脑裂问题是原本一个集群，被分成了两个集群，同时出现了两个“大脑”，这就是所谓的“脑裂”现象 出现脑裂的情况： 两边都得到过半票数而选举出Leader的情况，不可能出现 两边都无法得到过半票数而无法选举出Leader的情况，则服务端会返回客户端集群不可用 第三种情况：Leader 与 其他节点通信异常，导致其他节点重新选出 Leader 由于 E 能收到超过半数的节点选举票 3 而成为新的 Leader，而 A 、B 会发起 PreVote 因为无法获取半数响应，所以不会触发选举 客户端交互 集群中只有 Leader 点可以处理客户端发来的请求，当 follower 节点收到客户端的请求时，也须将 Leader 信息告知客户端，然后由 Leader 点处理其请求，具体步骤如下： 当客户端初次 接到集群时， 会随机挑选个服务器节点进行通信 如果客户端第一次挑选的节点不是 Leader 节点 ，那么该节点会拒绝客户端的请求，并且将它所知道的 Leader 节点的信息返回给客户端。 当客户端连接到 Leader 节点之后，即可发送消息进行交互 如果在交互过程中 Leader 节点宕机，那么客户端的请求会超时，客户端会再次随挑选集群中的节点，并从步骤2重新开始执行 异常 1：发生脑裂 与节点之间发生网络分区之后，客户端发往节点 A 请求将会超时，这是因为节点 A 无法将请求发送到集群中超过半数的节点 ，该请求相应的日志记录也就无法提交，从而导致无法给客户端返回相应的响应 异常 2：Leader 与其他节点通信异常 由于 leader A ⽆法同步数据到多数节点, 造成 client 写请求失败 ; ⽹络分区后 follower c 长时间未收到⼼跳, 则触发选举且当选 ; client 按照策略⼀直尝试跟可用的节点进⾏请求 ; 当⽹络恢复后, leader A B 将降为 follower, 并且强制同步 Leader C 的数据 ; 参考链接 《ETCD技术内幕》 http://www.xuyasong.com/?p=1706 http://thesecretlivesofdata.com/raft/","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://xboom.github.io/tags/Distributed-System/"},{"name":"Raft","slug":"Raft","permalink":"http://xboom.github.io/tags/Raft/"}]},{"title":"Go-15-SyncMap","slug":"Go/Go-15-SyncMap","date":"2023-04-04T13:01:05.349Z","updated":"2023-07-15T05:37:13.807Z","comments":true,"path":"2023/04/04/Go/Go-15-SyncMap/","link":"","permalink":"http://xboom.github.io/2023/04/04/Go/Go-15-SyncMap/","excerpt":"","text":"一开始以为 Sync.Map 仅仅是封装的了一层，后来在与同事的讨论中发现并不是这样，这里重新学习一下源码 线程安全 首先搞清楚一个问题，什么是并发安全。由于没有完整的解释，我找了一段Java中的表达 当多个线程访问某个类时，不管运行环境采用何种调度方式或者这些进程将如何交替执行，并且在主调代码中不需要任何额外的协同或者同步，这个类都能表现出正确的行为，那么这个类是线程安全的 大致抓住两点 无论如何交替执行 表现出正确的行为，有点抽象，反过来问 什么情况会导致线程不安全？怎样才能保证线程安全？ 当前的一个操作可能不是原子的，执行过程中会被打断，其他线程有能力修改共享变量的值，同时存在线程修改的值不是立即对其他线程可见的，因为线程有自己的执行空间，另外一点就是存在程序可能存在乱序执行的情况，单线程没问题，但是多个线程同时执行，线程共享的数据会出现错乱。那么保证线程安全的三个特征： 原子性：提供互斥访问、同一时刻只能有一个线程在操作 可见性：一个线程对主内容的修改可以及时被其他线程看到 有序性：程序在执行的时候，程序的代码执行顺序和语句的顺序是一致的有序性 多个协程并发执行，那么几个协程之间是没有有序性的，但是协程里面的代码是有序的 什么情况下代码的执行顺序与语句顺序不一致？比如指令重排序，将多次访问主存合并到一起执行 这样就好理解为啥普通的map会存在并发安全问题了，因为一个协程的修改对于另外一个协程可能是不可见的。另外当map并发读写时候触发的panic是map内部实现的，结构体的并发读写只会报错不会panic。 使用 普通的map并发处理实践 1234var counter = struct&#123; sync.RWMutex m map[string]int&#125;&#123;m: make(map[string]int)&#125; 使用 sync.Map 则可以 1var counter sync.Map 实现原理 源码：src/sync/map.go 为了实现并发安全的Map，Sync.Map实现了一种读写分离的方式 空间换时间。 通过冗余的两个数据结构(read、dirty),实现加锁对性能的影响。 动态调整，miss次数多了之后，将dirty数据提升为read。 double-checking，因为是判断与获取并不是原子操作，所以会出现 double-checking 延迟删除。 删除一个键值只是打标记，只有在提升dirty的时候才清理删除的数据。 优先从read读取、更新、删除，因为对read的读取不需要锁。 问题1：如何在并发情况下修改或删除已经存在的Key？ 答：通过将 key 映射到 val 的地址，然后 val 指针指向真正的值，那么只需要保证 address 指针的原子操作，就能解决并发安全问题 key -&gt; &amp;地址 --&gt; 真正的值 问题2：如何在写的同时保证读的准确性 答：读写指向的是同一个指针，首先在写时会通过互斥锁防止并发操作 dirty，其次就是用到了atomic原子操作value指针 问题3：如果读写分离，如何将写入的数据同步到读中 答： 1. 第一种是当misses == len(dirty)的时候就会升级dirty到read中；第二种就是在遍历的如果dirty有read中没有的数据； 第三其实也不是读写分离，而是乐观锁，因为如果read中已经有key/val，也会尝试去更新read，发现已经删除，则去dirty中更新 最后的实现就变成了下面这个样子 问题4：指向的是同一个entry，不还是一样需要加锁吗？内部 的mutex 与 atomic.pointer的区别跟作用又是什么？ mutex 与 atomic 的区别是：mutex 通过阻塞其他协程起到了原子操作的功能，是用来保护 dirty 的读写的。而atomic是通过CPU指令，来达到值操作的原子性，所以 atomic与mutex并不是一个层面的东西，atomic也比mutex更快。 所以才会出现 mutex 保护的是 dirty，而 atomic 保护的是 entry 具体值 数据结构 定义的 Map 中 除普通的map 与 互斥锁，还增加了两个字段 12345678910111213type Map struct &#123; //当涉及到dirty的操作的时候，需要使用这个锁 mu Mutex // 一个只读结构(实际也会更新这个数据的entries，标记为未删除的unexpunger) read atomic.Pointer[readOnly] // dirty包含最新的entries(包括read中未删除的数据,虽有冗余，但也提升dirty字段为read的时候非常快，不用一个一个的复制，而是直接将这个数据结构作为read字段的一部分),有些数据还可能没有移动到read字段中。 // 对于dirty的操作需要加锁，因为对它的操作可能会有读写竞争。 // 当dirty为空的时候， 比如初始化或者刚提升完，下一次的写操作会复制read字段中未删除的数据到这个数据中。 dirty map[any]*entry // 当从Map中读取entry的时候，如果read中不包含这个entry,会尝试从dirty中读取，这个时候会将misses加一， // 当misses累积到 dirty的长度的时候， 就会将dirty提升为read,避免从dirty中miss太多次。因为操作dirty需要加锁 misses int&#125; 其中存在两个结构 1234type readOnly struct &#123; m map[any]*entry amended bool // 为true 表示dirty中存在一些新增的键值对&#125; 基本 read 与 dirty 的转换 entry的操作 entry entry 代表的是每个 key 在Map中对应的一个 entry，并且还记录了 其他信息 12345type entry struct &#123; p atomic.Pointer[any]&#125;var expunged = new(any) // p 一共记录林 entry 的三种状态 nil: entry已被删除了，并且m.dirty为nil expunged: entry已被删除了，并且m.dirty不为nil，而且这个entry不存在于m.dirty中 其它： entry是一个正常的值 构建一个 entry 如下： 12345678910111213func newEntry(i any) *entry &#123; e := &amp;entry&#123;&#125; e.p.Store(&amp;i) return e&#125;func (e *entry) load() (value any, ok bool) &#123; p := e.p.Load() if p == nil || p == expunged &#123; //表示不存在 return nil, false &#125; return *p, true //表示存在&#125; entry 使用 atomic.Pointer[n] 的作用在后面才体现出来, atomic.Pointer 原子地读取和存储指针类型的值 1234567891011func (e *entry) trySwap(i *any) (*any, bool) &#123; for &#123; p := e.p.Load() if p == expunged &#123; //如果已经被删除则返回false return nil, false &#125; if e.p.CompareAndSwap(p, i) &#123; //否则不断的CAS更新 return p, true &#125; &#125;&#125; 操作 操作与正常的map类似，包含 存储、获取、删除、遍历。除此之外还增加的 交换、比较交换、比较删除 获取 12345678910111213141516171819202122232425func (m *Map) Load(key any) (value any, ok bool) &#123; // 1.首先从m.read中得到只读readOnly,从它的map中查找，不需要加锁 read := m.loadReadOnly() e, ok := read.m[key] //2. 如果没找到，并且m.dirty中有新数据，需要从m.dirty查找，这个时候需要加锁 if !ok &amp;&amp; read.amended &#123; // a 操作 m.mu.Lock() //b 操作 //双检查，避免加锁的时候m.dirty提升为m.read,这个时候m.read可能被替换了。 read = m.loadReadOnly() e, ok = read.m[key] // 如果m.read中还是不存在，并且m.dirty中有新数据 if !ok &amp;&amp; read.amended &#123; // 从m.dirty查找 e, ok = m.dirty[key] // 不管m.dirty中存不存在，都将misses计数加一 // missLocked()中满足条件后就会提升m.dirty m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125; 这里有两个注意事项 出现double-check，是因为在执行 if !ok &amp;&amp; read.amended 的过程中，dirty可能升级为read，所以还是需要锁操作 不管 m.dirty中是否存在，都需要misses+1，表明 read中不存在才到 dirty中获取 123456789func (m *Map) missLocked() &#123; m.misses++ if m.misses &lt; len(m.dirty) &#123; return &#125; m.read.Store(&amp;readOnly&#123;m: m.dirty&#125;) //直接将dirty升级到read中，然后 m.dirty = nil m.misses = 0&#125; 什么时候讲dirty提升为 read，当 misses &gt;= len(m.dirty) 存储 从接口实现可以看出，存储 k/v 内部使用的其实 Swap 接口 123func (m *Map) Store(key, value any) &#123; _, _ = m.Swap(key, value)&#125; 直接查看 Swap 的实现原理 根据描述，Swap 的作用是 将 newValue 更新入 key 所对应的 value中，并将 oldValue 返回。另外一个返回值loaded表示 key 是否存在 1234567891011121314151617181920212223242526272829303132333435363738func (m *Map) Swap(key, value any) (previous any, loaded bool) &#123; // 如果m.read存在这个键，并且这个entry没有被标记删除，尝试直接存储。 // 因为m.dirty也指向这个entry,所以m.dirty也保持最新的entry。 read := m.loadReadOnly() if e, ok := read.m[key]; ok &#123; //如果只读存在 if v, ok := e.trySwap(&amp;value); ok &#123; //尝试去更新value，如果未 标记删除 if v == nil &#123; //value为空 return nil, false &#125; return *v, true //更新成功 返回 true &#125; &#125; m.mu.Lock() // 加锁，如果`m.read`不存在或者已经被标记删除 read = m.loadReadOnly() if e, ok := read.m[key]; ok &#123; //只读存在 if e.unexpungeLocked() &#123; //如果未标记删除则删除read,然后存入dirty(写数据到dirty中) m.dirty[key] = e //存入 dirty &#125; if v := e.swapLocked(&amp;value); v != nil &#123; //未标记删除则直接更新read loaded = true previous = *v &#125; &#125; else if e, ok := m.dirty[key]; ok &#123; //只读不存在但是dirty存在 if v := e.swapLocked(&amp;value); v != nil &#123; loaded = true previous = *v &#125; &#125; else &#123; //都不存在 if !read.amended &#123; //dirty中没有新值 m.dirtyLocked() //如果dirty为空，从 m.read中复制未删除的数据 m.read.Store(&amp;readOnly&#123;m: read.m, amended: true&#125;) //read中存储 &#125; m.dirty[key] = newEntry(value) //dirty 存储一个新的 value地址 &#125; m.mu.Unlock() return previous, loaded&#125; 这里 dirtyLocked的作用是将未删除的复制到dirty中 123456789101112131415161718192021222324func (m *Map) dirtyLocked() &#123; if m.dirty != nil &#123; //如果不为空直接返回 return &#125; read := m.loadReadOnly() m.dirty = make(map[any]*entry, len(read.m)) for k, e := range read.m &#123; if !e.tryExpungeLocked() &#123; m.dirty[k] = e &#125; &#125;&#125;func (e *entry) tryExpungeLocked() (isExpunged bool) &#123; p := e.p.Load() //加载entry中的地址 for p == nil &#123; //如果为空 if e.p.CompareAndSwap(nil, expunged) &#123; //尝试使用 expunged更新状态 return true &#125; p = e.p.Load() &#125; return p == expunged&#125; 删除 12345678910111213141516171819202122232425func (m *Map) Delete(key any) &#123; m.LoadAndDelete(key)&#125;func (m *Map) LoadAndDelete(key any) (value any, loaded bool) &#123; //先从read中加载 read := m.loadReadOnly() e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; //read没有且dirty中有新数据 //开启锁 m.mu.Lock() read = m.loadReadOnly() e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; //如果read没有，则dirty中有新数据 e, ok = m.dirty[key] delete(m.dirty, key) m.missLocked() //当misses数量达到 len(m.dirty),dirty升级为read &#125; m.mu.Unlock() &#125; if ok &#123; return e.delete() //如果read存在则标记删除 &#125; return nil, false&#125; 如果read中存在，那么仅会标记删除 1234567891011func (e *entry) delete() (value any, ok bool) &#123; for &#123; p := e.p.Load() if p == nil || p == expunged &#123; //如果为空或者标记删除，返回没有数据 return nil, false &#125; if e.p.CompareAndSwap(p, nil) &#123; //标记为nil，并不会直接删除 return *p, true &#125; &#125;&#125; 遍历 因为是读写分离，所以在遍历的过程中 如果dirty中没有新数据，那么就使用read遍历 如果dirty中有新数据，那么就将dirty升级为read，然后遍历read 12345678910111213141516171819202122232425func (m *Map) Range(f func(key, value any) bool) &#123; //判断dirty是否有新值 read := m.loadReadOnly() if read.amended &#123; m.mu.Lock() //开锁 read = m.loadReadOnly() if read.amended &#123; //dirty是否有新值 read = readOnly&#123;m: m.dirty&#125; m.read.Store(&amp;read) //升级dirty为read m.dirty = nil m.misses = 0 &#125; m.mu.Unlock() &#125; //遍历read for k, e := range read.m &#123; v, ok := e.load() if !ok &#123; continue &#125; if !f(k, v) &#123; break &#125; &#125;&#125; 总结： 该map仅针对特定的场景 Key 读多写少的情况，例如只增长的缓存 多个协程针对不同的key合集进行读写更新时候可以用 sync.Map 不能在第一次使用后复制 由于dirty达到一定条件就需要升级为read，所以适用读多写少的场景 参考文档 https://www.cnblogs.com/qcrao-2018/p/12833787.html https://colobu.com/2017/07/11/dive-into-sync-Map/#sync-Map的性能","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-24-协程池问题","slug":"Go/Go-24-协程池问题","date":"2023-03-05T06:46:45.000Z","updated":"2023-07-15T05:38:02.316Z","comments":true,"path":"2023/03/05/Go/Go-24-协程池问题/","link":"","permalink":"http://xboom.github.io/2023/03/05/Go/Go-24-%E5%8D%8F%E7%A8%8B%E6%B1%A0%E9%97%AE%E9%A2%98/","excerpt":"","text":"之前在整理 GoZero4-协程池的过程中发现 字节跳动开源的协程池在使用中需要注意的地方： ​ 使用双向链表存储任务，表示它理论上支持无限个任务。后面的任务可能存在长时间等待的情况，并不存在任务过期处理逻辑 没想到在实际应用中真的没注意踩了一遍，这里记录一下这个原因与后续的解决思路，排查过程就不赘述了。 伪代码如下： 1234567891011121314151617181920212223242526//伪代码func Send(conn *grpc.ClientConn) &#123; c := pb.NewGreeterClient(conn) ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) gopool.CtxGo(ctx, func() &#123; defer cancel() //1. 通信消息自定义处理 Handle(ctx) //2. 发送消息 _, err := c.SayHello(ctx, &amp;pb.HelloRequest&#123;Name: *name&#125;) if err != nil &#123; log.Fatalf(\"could not greet: %v\", err) &#125; &#125;) select &#123; case &lt;-ctx.Done(): //log.Printf(\"end success\") return case &lt;-time.After(time.Second * 5): log.Printf(\"time out\") return &#125;&#125; 问题原因：负责底层通信的 grpc-go(进行过二次开发) 使用了上述的协程池进行发送接口的超时控制。当突然大量的消息进入，消息接受速率 大于 消息发送-响应速率。导致过量的消息全部堆积到这个双向链表中，而本身消息是有超时时间的，就导致堆积的消息越来越多，消息等到发送的时候已经超时了，所以就出现了服务OOM，消息也无法发送出去。 类似上图所示，模拟客户端通过协程池的发送协程 work 实现超时控制消息任务 t 的发送 第一阶段，发送协程 work 的消费速率 大于 消息任务 t 的生产速率，一切正常 第二阶段，发送协程 work 的消费速率 小于 消息任务 t 的生产速率的时候，消息开始在双向链表中堆积，出现内存快速增长 第三阶段，当 消息任务 t 的堆积 超过自定义超时时间的时候，所有消息都发送超时失败 第四阶段，当 双向链表中的任务堆积导致内存超时服务上限的时候，服务OOM 这里产生了如下几个思考 能否在任务双向链表中限制数量大小当发送超过限制的时候直接失败? 为什么不直接使用管道来代替底层的双向链表？ 能否自动丢弃超时的消息并返回结果，注意这个时候是在协程里面运行的 如何像linux一样保存 pod因为OOM而产生的core方便后续分析 第一个问题是可以解决的，协程池对象存在 taskCount 记录当前任务数量，也就是说可以增加协程池容量判断来达到限制目的 增加协程池任务容量字段 taskCap 增加接口，协程失败返回错误 error 添加协程池容量判断 1234567func (p *pool) CtxGo2(ctx context.Context, f func()) error &#123; cur := atomic.AddInt32(&amp;p.taskCount, 1) if cur &gt; p.taskCap &#123; //当前任务容量判断 return errors.New &#125; //....&#125; 第二个问题管道的缺点是管道数量是固定的，也就是达不到根据任务数量动态扩容 work 的功效 第三个问题由于任务放入协程中相当于异步处理，并不能直接将任务待执行超时的情况返回给客户端，可以通过自定义处理函数或者内部的panic处理任务超时的情况；但又能如何发现任务超时，时间轮貌似是个好东西 第四个问题找到一篇实践，实操之后再单独记录 总结： 我在使用过程中忽视了双向队列任务堆积的情况，令牌桶的使用也可能存在这样的情况(正常情况不会配置速率上千万的情况)，注意异常情况下的配置范围 上述需要改进与实操的部分尝试解决之后再来补充这边文章","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"算法-E10C-最大公约数","slug":"Algorithm/算法-E10C-最大公约数","date":"2023-02-02T12:25:25.494Z","updated":"2023-09-28T16:21:03.132Z","comments":true,"path":"2023/02/02/Algorithm/算法-E10C-最大公约数/","link":"","permalink":"http://xboom.github.io/2023/02/02/Algorithm/%E7%AE%97%E6%B3%95-E10C-%E6%9C%80%E5%A4%A7%E5%85%AC%E7%BA%A6%E6%95%B0/","excerpt":"","text":"今天碰到一道有意思的题目，学到了很多东西，所以记录一下 检查好数组 碰到之后完全不知道如何入手，最后通过看解答才弄明白，但是中间有很多知识点可以记录一下 首先 需要知道裴蜀定理，又称贝祖定理，是一个最大公约数定理：设 a, b 是不全为零的整数，则存在整数 x , y , 使得 ax+by=gcd(a,b)ax + by = gcd(a, b) ax+by=gcd(a,b) 若任何一个等于0，则 gcd(a,b)==agcd(a, b) == agcd(a,b)==a 对任何整数a、b和它们的最大公约数d，关于未知数x和y的线性丢番图方程 ax+by=max + by = m ax+by=m 有解当前仅当 m 是 d 的倍数 这里有一个奇特的点 0 因为没有因数，所以 0 不与其他数存在最大公约数，等我看了 类欧几里德算法 再补充 所以 ax+by=1ax + by = 1ax+by=1 有解，那么 gcd(a,b)=1gcd(a, b) = 1gcd(a,b)=1 其次 我们如何求一个两个数的最大公约数，经典的 辗转相除法(又称 欧几里德算法) 1234//也可以使用现有函数 gcd(a, b)int GCD(int a, int b) &#123; return b ? GCD(b, a % b) : a;&#125; 证明 gcd(a,b)=gcd(b,a%b)gcd(a, b) = gcd(b, a \\% b)gcd(a,b)=gcd(b,a%b) 过程如下 设 a=bk+ca = bk + ca=bk+c，所以 c=a%bc = a \\% bc=a%b 设 d 是 a , b 的公约数 所以 a/d=b/d∗k+c/da / d = b / d * k + c / da/d=b/d∗k+c/d 因为 a/da / da/d 与 b/d∗kb / d * kb/d∗k 是整数，所以 c/dc / dc/d 也是整数 将 c=a%bc = a \\% bc=a%b 带入第3步得 a/d−b/d∗k=a%b/da / d - b / d * k = a \\% b / da/d−b/d∗k=a%b/d，因为 左边是整数，所有等式右边也是整数 那么 a/d=a%b/d+b/d∗ka / d = a \\% b / d + b / d * ka/d=a%b/d+b/d∗k，那么 d 也是 b, a%ba \\% ba%b 的约数，也是 a, b 的约数 尽然两式的公约数是相同的，那么最大公约数也会相同 所以 gcd(a,b)=gcd(b,amodb)gcd(a, b) = gcd(b, a mod b)gcd(a,b)=gcd(b,amodb) 最后就是这个题目的解答 1234567891011121314class Solution &#123;public: int Gcd(int a, int b) &#123; return b ? Gcd(b, a % b) : a; &#125; bool isGoodArray(vector&lt;int&gt;&amp; nums) &#123; int v = nums[0]; for(int i = 1; i &lt; nums.size(); i++) &#123; if(v == 1) return true; //1与任意正整数的最大公约数是1 v = Gcd(nums[i], v); &#125; return v == 1; &#125;&#125;; 参考链接 https://oi-wiki.org/math/number-theory/gcd/ https://leetcode.cn/problems/check-if-it-is-a-good-array/ https://oi-wiki.org/math/number-theory/bezouts/","categories":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/categories/Algorithms/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/tags/Algorithms/"}]},{"title":"Go-23-arena","slug":"Go/Go-23-arena","date":"2023-01-10T08:00:20.000Z","updated":"2023-07-15T05:37:56.756Z","comments":true,"path":"2023/01/10/Go/Go-23-arena/","link":"","permalink":"http://xboom.github.io/2023/01/10/Go/Go-23-arena/","excerpt":"","text":"最近 Go1.20 更新，中间讲到了一个特性 arena ，这里看看加入 arena 的作用 在Go的内存管理中，arena 其实就是所谓的堆区，然后将这个区域分割成 8KB 大小的页，组合起来称为 mspan，mspan就是Go中内存管理的基本单元，是由一片连续的 8KB 的页组成的大块内存。其实 mspan 是一个包含起始地址、规格、页的数量等内容的双端链表 虽然 Go 的垃圾回收机制能够正常的进行内存管理，但是存在以下问题 垃圾回收机制需要花费大量CPU进行垃圾回收操作 垃圾回收机制有一定的延迟性，导致花费的内存比实际的内存要大 而 arena 的优势 允许从连续的内存空间中分配对象并一次性进行释放 注意：在 github arena 的话题中，有一个最新的笔记提醒, 即处在测试阶段的 arena 功能随时可能被去除掉 1Note, 2023-01-17. This proposal is on hold indefinitely due to serious API concerns. The GOEXPERIMENT=arena code may be changed incompatibly or removed at any time, and we do not recommend its use in production. 带着问题看世界： 它跟内存池的区别 由于需要连续的内存空间，那么当需要分配的内存比较多，没有这么大的连续内存空间怎么办？ 如果它分配的对象有一部分存在内存逃逸，那么该如何处理？ 能支持并发吗 基础功能 由于是实验功能，所以需要配置环境变量 GOEXPERIMENT=arenas 123456789101112131415161718192021import \"arena\"type T struct&#123; Foo string Bar [16]byte&#125;func processRequest(req *http.Request) &#123; // Create an arena in the beginning of the function. mem := arena.NewArena() // Free the arena in the end. defer mem.Free() // Allocate a bunch of objects from the arena. for i := 0; i &lt; 10; i++ &#123; obj := arena.New[T](mem) &#125; // Or a slice with length and capacity. slice := arena.MakeSlice[T](mem, 100, 200)&#125; 如果想在arena释放时候继续使用它分配的对象，则可以通过 Clone 从堆中浅拷贝一个对象 12345obj1 := arena.New[T](mem) // arena-allocatedobj2 := arena.Clone(obj1) // heap-allocatedfmt.Println(obj2 == obj1) // falsemem.Free() 其他接口包括 NewArena：创建一个新的 arena 内存空间。 Free：释放 arena 及其关联对象。 New：基于 arena，创建新对象。 MakeSlice：基于 arena，创建新切片。 Clone：克隆一个 arena 的对象，并移动到内存堆上。只能是指针、slice或者字符串 实现原理 代码路径： src/runtime/arena.go src/arena/arena.go 123456789//arena 表示多个Go一起分配与释放的内存集合，当其中的对象不在被引用那么将会自动释放type Arena struct &#123; a unsafe.Pointer&#125;// NewArena allocates a new arena.func NewArena() *Arena &#123; return &amp;Arena&#123;a: runtime_arena_newArena()&#125;&#125; 根据描述这里注意点有两个： arena 分配的对象需要及时释放 既然是自动释放，然后在使用中 defer arena.Free() 可以任务是，不用等到二次垃圾回收，直接将资源释放，并将可重复使用的mspan放入reused中 查看Arena 内部结构 12345678910111213type userArena struct &#123; // 指向一个链表，表示一系列没有足够空闲内存的 mspan(内存段)。当该内存管理区域被释放时，这些 mspan 也会被释放 fullList *mspan //内存组件 mspan // 指向一个 mspan，表示未满的内存段。这个 mspan 中还有可用的内存可以分配 active *mspan //一个指向 unsafe.Pointer 类型的切片，用于引用当前内存管理区域的对象。这可以防止在仍然有引用对象时释放该内存管理区域 refs []unsafe.Pointer //一个原子布尔类型的变量，用于标记内存管理区域是否已经被释放。如果为 true，表示该内存管理区域已经被释放，以避免重复释放 defunct atomic.Bool&#125; Arena 如果重复释放也没有关系，判断释放过则直接结束 **第一步：**分配一个 Arena 12345678910111213141516171819202122232425262728293031323334353637// newUserArena creates a new userArena ready to be used.func newUserArena() *userArena &#123; a := new(userArena) SetFinalizer(a, func(a *userArena) &#123; //g // If arena handle is dropped without being freed, then call // free on the arena, so the arena chunks are never reclaimed // by the garbage collector. a.free() &#125;) a.refill() return a&#125;func (a *userArena) refill() *mspan &#123; // If there's an active chunk, assume it's full. s := a.active //... var x unsafe.Pointer // Check the partially-used list. lock(&amp;userArenaState.lock) if len(userArenaState.reuse) &gt; 0 &#123; //当前存在可重用的就使用重用的 &#125; unlock(&amp;userArenaState.lock) if s == nil &#123; // Allocate a new one. 否则分配一个新的mspan x, s = newUserArenaChunk() if s == nil &#123; throw(\"out of memory\") &#125; &#125; a.refs = append(a.refs, x) //记录mspan.base()，报活mspan a.active = s //记录当前使用的mspan return s&#125; SetFinalizer 函数可参考文章，当gc检测到unreachable对象有关联的SetFinalizer函数时，会执行关联的SetFinalizer函数， 同时取消关联。 这样当下一次gc的时候，对象重新处于unreachable状态并且没有SetFinalizer关联， 就会被回收。 **第二步：**从 arena 中分配具体类型的对象 1234567891011121314151617181920212223242526272829303132333435func (a *userArena) refill() *mspan &#123; // If there's an active chunk, assume it's full. s := a.active //上次分配了mspan if s != nil &#123; if s.userArenaChunkFree.size() &gt; userArenaChunkMaxAllocBytes &#123; // It's difficult to tell when we're actually out of memory // in a chunk because the allocation that failed may still leave // some free space available. However, that amount of free space // should never exceed the maximum allocation size. throw(\"wasted too much memory in an arena chunk\") &#125; s.next = a.fullList //将这个mspan放到fullList的链表头部 a.fullList = s a.active = nil //active置为空 s = nil &#125; var x unsafe.Pointer // Check the partially-used list. lock(&amp;userArenaState.lock) if len(userArenaState.reuse) &gt; 0 &#123; // //如果有可以重用的mspan则放到s中 &#125; unlock(&amp;userArenaState.lock) if s == nil &#123; // Allocate a new one. x, s = newUserArenaChunk() //否则新分配一个新的 if s == nil &#123; throw(\"out of memory\") &#125; &#125; a.refs = append(a.refs, x) a.active = s return s&#125; **第三步：**释放的核心是这块代码 123456789s := a.fullList //获取这个mspani := len(a.refs) - 2for s != nil &#123; //不为空 a.fullList = s.next //指向下一个节点 s.next = nil freeUserArenaChunk(s, a.refs[i]) //释放这个mspan s = a.fullList //指向下一个节点 i--&#125; 释放的时候仅仅是将fullList中所有的都释放掉了，而active中的则会进去到全局reuse对象中用于下次使用 这个全局变量就是 userArenaState 用于存放可重复使用的mspan以及回收的mspan 123456789var userArenaState struct &#123; lock mutex //可重复使用 reuse []liveUserArenaChunk //回收释放的 fault []liveUserArenaChunk&#125; 对比Sync.Pool arena 与 Sync.Pool 同样都是为了解决频繁分配对象和大量对象GC带来的开销 Sync.Pool相同类型的对象，使用完后暂时缓存不GC，下次再有相同的对象分配时直接用之前的缓存的对象，避免频繁创建大量对象。不承诺这些缓存对象的生命周期，GC时会释放之前的缓存，适合解决频繁创建相同对象带来的压力，短时间(两次GC之间)大量创建可能还是会有较大冲击，使用相对简单，但只能用于相同结构创建，不能创建slice等复杂结构 arena手动管理连续内容并统一释放，对象的生命周期完全自己控制，使用相对复杂，支持slice等复杂结构，也不是一个真正意义的连续超大空间，而是通过管理不同的mspan实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package mainimport ( \"arena\" \"sync\" \"testing\")type MyObj struct &#123; Index int&#125;func BenchmarkCreateObj(b *testing.B) &#123; b.ReportAllocs() var p *MyObj for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; 1000; j++ &#123; p = new(MyObj) p.Index = j &#125; &#125;&#125;var ( objPool = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return &amp;MyObj&#123;&#125; &#125;, &#125;)func BenchmarkCreateObj_SyncPool(b *testing.B) &#123; b.ReportAllocs() var p *MyObj for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; 1000; j++ &#123; p = objPool.Get().(*MyObj) p.Index = 23 objPool.Put(p) &#125; &#125;&#125;func BenchmarkCreateObj_Arena(b *testing.B) &#123; b.ReportAllocs() var p *MyObj a := arena.NewArena() defer a.Free() for i := 0; i &lt; b.N; i++ &#123; for j := 0; j &lt; 1000; j++ &#123; p = arena.New[MyObj](a) p.Index = 23 &#125; &#125;&#125; 性能对比的结果 1234cpu: Intel(R) Core(TM) i7-8559U CPU @ 2.70GHzBenchmarkCreateObj-8 100518 11370 ns/op 8000 B/op 1000 allocs/opBenchmarkCreateObj_SyncPool-8 110017 11523 ns/op 0 B/op 0 allocs/opBenchmarkCreateObj_Arena-8 80409 15340 ns/op 8032 B/op 0 allocs/op Sync.Pool 不需要重复分配且每次操作时间短，而Arena执行时间会长一点且每次还是需要分配内存的，因为需要引入新的 mspan 总结 Arena 不支持并发，可以看出操作同一个 arena的时候并不存在锁操作 Arena 强制 Free()之后的对象无法继续使用 优点： 一旦被释放但仍然被访问则会显示的导致程序错误 arena 地址空间除非没有指针指向，否则将不能被重用 arena 永远不会被垃圾回收机制回收(如果GC不可达它会执行 SetFinalizer 自己释放掉，那我们手动free的意义在哪里 --&gt; 也就是构建arena的目的，提前释放内存，降低GC扫描频率) 参考文档 https://uptrace.dev/blog/golang-memory-arena.html https://colobu.com/2022/10/17/a-first-look-at-arena/ https://zhuanlan.zhihu.com/p/604686258","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-21-SetFinalizer","slug":"Go/Go-21-SetFinalizer","date":"2023-01-04T13:45:13.344Z","updated":"2023-07-15T05:37:47.256Z","comments":true,"path":"2023/01/04/Go/Go-21-SetFinalizer/","link":"","permalink":"http://xboom.github.io/2023/01/04/Go/Go-21-SetFinalizer/","excerpt":"","text":"在阅读Go 1.20 新特性 arena 的时候，看到在构建 arena 对象的时候使用了 SetFinalizer 这样一个函数 带着问题看世界 它有什么用 它怎么用 它有什么缺点导致不是随处可见这种用法 这里来详细看看它的作用，备注如下 12SetFinalizer sets the finalizer associated with obj to the provided finalizer function. When the garbage collector finds an unreachable block with an associated finalizer, it clears the association and runs finalizer(obj) in a separate goroutine. This makes obj reachable again,but now without an associated finalizer. Assuming that SetFinalizer is not called again, the next time the garbage collector sees that obj is unreachable, it will free obj. 大意是：为对象提供一个析构函数，当GC发现不可达对象带有析构函数的时候，会单独使用协程执行这个析构函数。这样对GC来说对象是可达但没有了析构函数，下次GC发现对象不可达就会释放掉对象 有一些协程的生命周期是与整个服务一致的，比如定时清理机制，它的好处是自动处理一些业务而不需要人工调用，但如果是在一些与业务完全分离的场景。比如为业务提供一个缓存池，缓存池中为了清理过期的缓存而设计了一个常驻协程。是否可以使用 SetFinalizer 的过期删除机制 首先看一个栗子： 一般情况我们会提供对象一个 Close()函数用于业务在不需要的时候清理对象，这里就可以用到这个 SetFinalizer，如 os.NewFile 就注册了 SetFinalizer逻辑 1234567891011121314151617181920func newFile(fd uintptr, name string, kind newFileKind) *File &#123; fdi := int(fd) if fdi &lt; 0 &#123; return nil &#125; f := &amp;File&#123;&amp;file&#123; pfd: poll.FD&#123; Sysfd: fdi, IsStream: true, ZeroReadIsEOF: true, &#125;, name: name, stdoutOrErr: fdi == 1 || fdi == 2, &#125;&#125; //... runtime.SetFinalizer(f.file, (*file).close) return f&#125; 调用方式 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( \"fmt\" \"runtime\" \"time\")type Foo struct &#123; name string num int&#125;func finalizer(f *Foo) &#123; fmt.Println(\"a finalizer has run for \", f.name, f.num)&#125;var counter intfunc MakeFoo(name string) (a_foo *Foo) &#123; a_foo = &amp;Foo&#123;name, counter&#125; counter++ runtime.SetFinalizer(a_foo, finalizer) return&#125;func Bar() &#123; f1 := MakeFoo(\"one\") f2 := MakeFoo(\"two\") fmt.Println(\"f1 is: \", f1.name) fmt.Println(\"f2 is: \", f2.name)&#125;func main() &#123; for i := 0; i &lt; 3; i++ &#123; Bar() time.Sleep(time.Second) runtime.GC() &#125; fmt.Println(\"done.\")&#125; 执行结果 12345678910111213f1 is: onef2 is: twoa finalizer has run for two 1a finalizer has run for one 0f1 is: onef2 is: twoa finalizer has run for two 3a finalizer has run for one 2f1 is: onef2 is: twoa finalizer has run for two 5a finalizer has run for one 4done. 注意事项 obj 必须是指针 SetFinalizer 执行顺序按照类似对象的出栈顺序 可以通过 SetFinalizer(obj, nil) 清理对象的析构器 栗子2，它的实际效果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( \"fmt\" \"math/rand\" \"runtime\" \"runtime/debug\" \"strconv\" \"time\")type Foo struct &#123; a int&#125;func main() &#123; debug.SetGCPercent(-1) var ms runtime.MemStats runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects) for i := 0; i &lt; 1000000; i++ &#123; f := NewFoo(i) _ = fmt.Sprintf(\"%d\", f.a) &#125; runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects) runtime.GC() time.Sleep(time.Second) runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects) runtime.GC() time.Sleep(time.Second) runtime.ReadMemStats(&amp;ms) fmt.Printf(\"Allocation: %f Mb, Number of allocation: %d\\n\", float32(ms.HeapAlloc)/float32(1024*1024), ms.HeapObjects)&#125;//go:noinlinefunc NewFoo(i int) *Foo &#123; f := &amp;Foo&#123;a: rand.Intn(50)&#125; runtime.SetFinalizer(f, func(f *Foo) &#123; _ = fmt.Sprintf(\"foo \" + strconv.Itoa(i) + \" has been garbage collected\") &#125;) return f&#125; 运行结果 1234Allocation: 0.121063 Mb, Number of allocation: 140Allocation: 29.111671 Mb, Number of allocation: 1899990Allocation: 128.025635 Mb, Number of allocation: 4382420Allocation: 0.122147 Mb, Number of allocation: 155 可以看出，正如它功能说的一样，在第二次GC之后，分配的内存被释放 它也有缺点： SetFinalizer 最大的问题是延长了对象生命周期。在第一次回收时执行 Finalizer 函数，且目标对象重新变成可达状态，直到第二次才真正 “销毁”。这对于有大量对象分配的高并发算法，可能会造成很大麻烦 指针构成的 “循环引⽤” 加上 runtime.SetFinalizer 会导致内存泄露 SetFinalizer只在GC 发现对象不可达之后的任意时间执行，所以如果程序正常结束或者发生错误，而对象还没有被GC选中那么 SetFinalizer 也不会执行。 所以保险起见还是提供了 Close() 逻辑供业务调用 参考链接 https://zhuanlan.zhihu.com/p/76504936 https://go.dev/play/p/jWhRSPNvxJ https://medium.com/a-journey-with-go/go-finalizers-786df8e17687 runtime/mfinal.go","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"算法之美-八数码问题","slug":"Algorithms To Live By/算法之美-八数码问题","date":"2022-12-11T09:47:28.000Z","updated":"2023-09-18T15:53:58.319Z","comments":true,"path":"2022/12/11/Algorithms To Live By/算法之美-八数码问题/","link":"","permalink":"http://xboom.github.io/2022/12/11/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E5%85%AB%E6%95%B0%E7%A0%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"八数码问题也称为九宫问题。在3×3的棋盘，摆有八个棋子，每个棋子上标有1至8的某一数字，不同棋子上标的数字不相同。棋盘上还有一个空格，与空格相邻的棋子可以移到空格中。要求解决的问题是：给出一个初始状态和一个目标状态，找出一种从初始转变成目标状态的移动棋子步数最少的移动步骤 所谓一个状态就是棋子在棋盘上的一种摆法。棋子移动后，状态就会发生改变。实际上就是找出从初始状态到达目标状态所经过的一系列中间过渡状态 首先这个九宫格的状态数量是 9!，那么存在问题 是否存在从一个状态转移到另外一个状态无解的情况？ 如何计算或者衡量从一个转移到另外一个状态的需要走多少步？ 排列的性质 为了搞清楚上述问题，需要知道几个基本定义与引理 把n个不同的元素按一定的顺序排列成一行，成为这n个元素的一个排列。n个不同元素的排列共有 n! 种 对于n个自然数的一个排列，如果一个大数排在一个小数之前，就称这两个数构成一个逆序。一个排列的逆序总和称为该排列的逆序对，记为τ(j1j2⋯jn)∗τ∗(∗j∗1∗j∗2⋯∗j∗∗n∗)τ(j1j2⋯jn)*τ*(*j*1*j*2⋯*j**n*)τ(j1j2⋯jn)∗τ∗(∗j∗1∗j∗2⋯∗j∗∗n∗) 如5阶排列31542的逆序是(3,1),(3,2),(5,4),(5,2),(4,2),故 τ(31542)=5∗τ∗(31542)=5τ(31542)=5*τ*(31542)=5τ(31542)=5∗τ∗(31542)=5 逆序对为奇数的排列称为奇排列。逆序对为偶数的排列称为偶排列。自然排列 123⋯n的逆序对为0，故它是偶排列 在一个排列中，把某两个数的位置互换（其他数不动）变成另一个排列的变动称为一个对换，将相邻的两个数对换称为相邻对换 性质的证明 性质1：一个排列中的任意两个数对换后，排列改变奇偶性。即经过一次对换，奇排列变成偶排列，偶排列变成奇排列 证明： 先证明相邻对换的情形 设排列 a1a2...asabb1b2...bta_1 a_2 ... a_s a b b_1 b_2 ... b_ta1​a2​...as​abb1​b2​...bt​，对换a 与 b 的排列变为 a1a2...asbab1b2...bta_1 a_2 ... a_s b a b_1 b_2 ... b_ta1​a2​...as​bab1​b2​...bt​ a 与 b 的 对换不影响 a1a2...asa_1 a_2 ... a_sa1​a2​...as​， b1b2...btb_1 b_2 ... b_tb1​b2​...bt​ 与 其他数的关系 但a与b的序关系变为： 当 a &lt; b 时，在新排列中 a 、b 构成逆序 当 a &gt; b 时，在新排列中 a、b 不构成逆序 因此 a1a2...asabb1b2...bta_1 a_2 ... a_s a b b_1 b_2 ... b_ta1​a2​...as​abb1​b2​...bt​ 比 a1a2...asbab1b2...bta_1 a_2 ... a_s b a b_1 b_2 ... b_ta1​a2​...as​bab1​b2​...bt​ 的逆序多1或者少1 再证一般对换情形 设存在排列 a1a2…asab1b2…bmbc1c2…cta_1 a_2 … a_s a b_1 b_2 … b_m b c_1 c_2 … c_ta1​a2​…as​ab1​b2​…bm​bc1​c2​…ct​ 将 b 做 m 次相邻对换变为 a1a2…asabb1b2…bmc1c2…cta_1 a_2 … a_s a b b_1 b_2 … b_m c_1 c_2 … c_ta1​a2​…as​abb1​b2​…bm​c1​c2​…ct​ 再将 a 做 (m + 1) 次相邻对换变为 a1a2…asbb1b2…bmac1c2…cta_1 a_2 … a_s b b_1 b_2 … b_m a c_1 c_2 … c_ta1​a2​…as​bb1​b2​…bm​ac1​c2​…ct​ 所以 经过 (2m + 1) 次相邻兑换，可以把排列 a1a2…asab1b2…bmbc1c2…cta_1 a_2 … a_s a b_1 b_2 … b_m b c_1 c_2 … c_ta1​a2​…as​ab1​b2​…bm​bc1​c2​…ct​转变成a1a2…asbb1b2…bmac1c2…cta_1 a_2 … a_s b b_1 b_2 … b_m a c_1 c_2 … c_ta1​a2​…as​bb1​b2​…bm​ac1​c2​…ct​，这两个排列的奇偶性相反 性质2：在全部的 n(n≥2)阶排列中，奇偶排列各占一半，各有 n!/2 个 证明： 假设在全部n级排列中共有t个奇排列，s个偶排列 将t个奇排列中的前两个数字对换，得到t个互不相同的偶排列。因此 t≤s 同理可证 s≤t 于是 s=t，即奇、偶排列的总数相等，各有 n!/2个 性质3：任意一个n阶排列都可以经过一系列对换变成自然排列，并且所作对换的次数与这个排列有相同的奇偶性 证明(归纳法)： 1阶排列只有一个，结论显然成立 假设对n-1阶排列已经成立，证对n阶排列的情形结论也成立 设j1j2...jnj_1 j_2 ... j_nj1​j2​...jn​ 是一个n阶排列 如果 jn=nj_n=njn​=n ，假设n-1级排列 j1j2…jn−1j_1 j_2 … j_n−1j1​j2​…jn​−1 可以经过一系列变换变成自然序列，即 1 2 … n−1，于是这一系列对换也就把 j1j2⋯jn 变成 12⋯n 这种自然序列的形式。 如果 jn≠njn≠njn​=n，那么对 j1j2…jnj_1 j_2 … j_nj1​j2​…jn​ 作 jnj_njn​ 和 n 的对换，它就变成 j1j2…jn−1nj_1 j_2 … j_n−1 nj1​j2​…jn​−1n ，这就归结成上面的情形，因此 性质4：奇偶性与可达性关系与证明 必要性证明：排列的奇偶性不同则对应在八数码问题中不可达 在满足上述约定的八数码问题中，空格与相邻棋子的交换不会改变棋局中棋子数列的逆序对的奇偶性 空格与左右棋子交换：是不改变棋子数列的逆序对的(因为数列并没有改变) 空格与上下棋子交换：也是不改变棋子数列的逆序对的 假设交换棋子为c[i]=X 原数列p=c[1]… X c[i+1]c[i+2]…c[8]将变为新数列q=c[1]…c[i+1]c[i+2]X …c[8]（注意：在棋盘中，上下相邻的两棋格之间隔有两个棋格）。可以解释为用X与c[i+1]、 c[i+2]先后进行两次相邻交换而完成状态转变。 由p状态到q状态并不会改变改变棋子数列的逆序对的奇偶性。同理可证空格与下方棋子交换也不会改变棋子数列的逆序对的奇偶性。所以，空格与相邻棋子的交换不会改变棋局中棋子数列的逆序对的奇偶性 得出 定理1：对于任意两个状态映射的序列，如果这两个状态等价，那么它们的逆序数相同 充分性证明：排列的奇偶性相同则对应在八数码问题中也可达 首先明确几个定义： 状态(state)：八数码中8个数字与0的排列被定义为八数码的状态，比如： 状态空间(state space)：八数码所有状态的合集被称为状态空间 完全态(completeness)：当状态空间的子集中任何两个状态都能通过一定步骤得到，那么称这个状态空间的子集是完全态 状态映射(a sequence mapped by a state)：一个状态{}可以映射(忽略0)成一个排序，那么这个排列就称为这个状态的映射 标准格式(a standard form)：如果 0 在正中间，那么称这个状态为标准格式 区域(field)：对于任意状态(state)，4个位置中任意两个都相邻，那么称之为 区域，特别的，当两个区域有相同的两个位置，则称为 同边区域 转圈(circle moving)：0在一个区域内移动称 转圈 引理1：对于标准格式中的任意区域，转圈可以得到两个等价的区域 考虑到 标准格式 的对称性，只需要考虑一个区域的的变化 第一种情况(顺时针方向部分先后): a b d 第二种情况(顺时针方向不分先后): a d b 接着考虑在一个标准格式中，在两个具有相同边的区域交换数据，由于标准格式的对称性，所以只考虑上半部分 这个图形转换根据上述原则手动绘制交换流程更容易理解 得出的规律1: m 属于 {a ,d}，n 属于 {c, e}，p 属于 {a, d} 但 p != m，q 属于 {c, e}但 q != n，那么交换 m n，则 n 来到 m 之前的位置，而 m 来到 q 之前的位置，q 来到 n 之前的位置 得出的规律2：abc 可以经过转换编程 cab 或者 bac，不影响其他行且逆序对不变 对于标准格式，如果它们的逆序对的奇偶性相同，那么它们是等价的 证明： 首先，将九宫格分为A、B、C、D分为四个区域 第一步，将 h 移动到位置 位置 9，这是肯定可以的 第二步，将 g、e 移动到区域D 为什么不考虑g、e的顺序，因为可以在不影响h的情况下 g 在 位置 6 与 位置8 任意切换 2.1 如果 g、e 已经在区域D，那么不需要移动 2.2 如果其中 g 在 区域D，而e 在其他区域 2.2.1 e 在区域A 2.2.2 e 在区域B 2.2.3 e 在区域C 第三步，将 b、c 移动到 B区域 3.1 如果 b、c 都在区域 B，那么不需要移动 3.2 如果 b、c 有一个在区域B，那么 b 、c 的位置也同样可以忽略，如果b、c是正常顺序，那么也可以在不影响 g e h 位置的情况下调整 b c 的顺序 3.1.1 如果在位置3 3.1.2 如果在位置2，那么先将c移动位置3再执行上述步骤 如果 b、c 都不在区域B，同理将c 移动到位置3，然后在执行上述3.1.1 步骤 第四步，在不影响bcehg的情况下调整adf的顺序 规律二在 此场景下仍然适用，所以首先将防止到位置7 根据定理1，如果两者等价，那么它们的逆序对是相同的。而如果位置1与位置3交换其他位置不变，那边整个序列的逆序对是变化的。所以a一定在位置1，d在位置4 所以，如果奇偶性相同，那么两个状态都能转换成一个相同的标准状态，那么两个状态之间是可达的 得出结论：两个排列的逆序对奇偶性相同，那么在八数码中必可达 参考链接 https://blog.csdn.net/u011008379/article/details/40144147 https://chengfeng96.com/blog/2018/05/26/利用BFS，DFS，A-解决八数码难题/ http://www.huaying1988.com/blogs/8_Puzzle_StrictProof_SolutionAlgorithm_AndOthers/detail.html 《A constructive proof for the subsets completeness of 8-Puzzle state space》","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"Go-22-Goland激活","slug":"Go/Go-22-Goland激活","date":"2022-12-09T08:00:20.000Z","updated":"2023-07-15T05:37:52.316Z","comments":true,"path":"2022/12/09/Go/Go-22-Goland激活/","link":"","permalink":"http://xboom.github.io/2022/12/09/Go/Go-22-Goland%E6%BF%80%E6%B4%BB/","excerpt":"","text":"Goland好用但是激活太贵，网上找了一个使用激活服务器的办法， Goland 2022.2.2 激活成功 最好支持正版！！！ 第一步：首先打开网址 https://search.censys.io 第二步：搜索信息 services.http.response.headers.location: account.jetbrains.com/fls-auth 第三步：点击搜索到的网址信息 第四步：找到状态为302的信息网址信息，如果没有找到请重复第三步 第五步：复制上一步信息中的Details信息，这里是 https://188.210.42.106 第六步：将地址信息填入 goland 中的 Licence Service 并点击 Acitve 激活 第七步：如果激活成功将出现下图所示，否则重复第三步 参考文档 https://dushusir.com/jetbrains/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Grpc-14-健康检查","slug":"Grpc/Grpc-14-健康检查","date":"2022-11-04T14:58:29.000Z","updated":"2023-05-27T10:56:13.039Z","comments":true,"path":"2022/11/04/Grpc/Grpc-14-健康检查/","link":"","permalink":"http://xboom.github.io/2022/11/04/Grpc/Grpc-14-%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/","excerpt":"","text":"前面学习了保持链接，这里看一下健康检查 带着问题看世界 健康检查的作用 健康检查的原理 保持链接与健康检查的区别 首先看一下源码例子中的ReadMe说明，gRPC提供了一个健康检查库，用于向客户端通报系统的健康状况。它通过使用health/v1 API提供服务定义。通过使用健康检查库，客户端可以在遇到问题时优雅地避免使用服务器。大多数语言都提供了现成的实现，这使得它在不同系统之间可以互操作。内置健康检查的优势 健康检查的格式与普通的RPC一样 重用现有的配额等机制，内部对健康检查有完全的控制权 基本使用 内部定义了健康检查的 proto 1234567891011121314151617181920212223syntax = \"proto3\";package grpc.health.v1;message HealthCheckRequest &#123; string service = 1;&#125;message HealthCheckResponse &#123; enum ServingStatus &#123; UNKNOWN = 0; SERVING = 1; NOT_SERVING = 2; SERVICE_UNKNOWN = 3; // Used only by the Watch method. &#125; ServingStatus status = 1;&#125;service Health &#123; rpc Check(HealthCheckRequest) returns (HealthCheckResponse); rpc Watch(HealthCheckRequest) returns (stream HealthCheckResponse);&#125; 一共有两种方式进行健康检查 Check 探测服务器的健康状态 Watch 观察服务端变化，这里注意 是一个服务端流，也就是客户端不断会收到服务端的状态更新 一般，客户端不需要手动执行 Check操作，而是通过配置 healthCheckConfig ，它会在内部自动执行 Watch操作 客户端 123456789101112// import grpc/health to enable transparent client side checking import _ \"google.golang.org/grpc/health\"// set up appropriate service configserviceConfig := grpc.WithDefaultServiceConfig(`&#123; \"loadBalancingPolicy\": \"round_robin\", \"healthCheckConfig\": &#123; \"serviceName\": \"\" &#125;&#125;`)conn, err := grpc.Dial(..., serviceConfig) 服务端 启动一个协程，来模拟服务端状态变化，核心逻辑就是设置服务的状态 1234567891011121314151617181920212223242526272829303132333435363738394041424344var ( port = flag.Int(\"port\", 50051, \"the port to serve on\") sleep = flag.Duration(\"sleep\", time.Second*5, \"duration between changes in health\") system = \"\" // empty string represents the health of the system)//...func main() &#123; flag.Parse() lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port)) if err != nil &#123; log.Fatalf(\"failed to listen: %v\", err) &#125; s := grpc.NewServer() healthcheck := health.NewServer() healthpb.RegisterHealthServer(s, healthcheck) //健康检查 pb.RegisterEchoServer(s, &amp;echoServer&#123;&#125;) //健康状态更新 go func() &#123; // asynchronously inspect dependencies and toggle serving status as needed next := healthpb.HealthCheckResponse_SERVING for &#123; healthcheck.SetServingStatus(system, next) if next == healthpb.HealthCheckResponse_SERVING &#123; next = healthpb.HealthCheckResponse_NOT_SERVING &#125; else &#123; next = healthpb.HealthCheckResponse_SERVING &#125; time.Sleep(*sleep) &#125; &#125;() if err := s.Serve(lis); err != nil &#123; log.Fatalf(\"failed to serve: %v\", err) &#125;&#125; 健康服务器可以返回四种状态：UNKNOWN、SERVING、NOT_SERVING和SERVICE_UNKNOWN。 UNKNOWN 表示当前状态尚未知晓。在服务器实例启动时经常会看到这种状态。 SERVING 表示系统健康，准备好提供服务请求。 NOT_SERVING 表示系统当前无法处理请求。 SERVICE_UNKNOWN 表示客户端请求的服务名未被服务器所知。此状态仅由 Watch() 调用报告 实现原理 客户端 客户端默认初始化Watch函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364func init() &#123; internal.HealthCheckFunc = clientHealthCheck&#125;const healthCheckMethod = \"/grpc.health.v1.Health/Watch\" //指定健康检查方法名称func clientHealthCheck(ctx context.Context, newStream func(string) (interface&#123;&#125;, error), setConnectivityState func(connectivity.State, error), service string) error &#123; tryCnt := 0retryConnection: for &#123; //退避重试 if tryCnt &gt; 0 &amp;&amp; !backoffFunc(ctx, tryCnt-1) &#123; return nil &#125; tryCnt++ if ctx.Err() != nil &#123; return nil &#125; setConnectivityState(connectivity.Connecting, nil) //设置状态 rawS, err := newStream(healthCheckMethod) //构建流链接 if err != nil &#123; continue retryConnection &#125; s, ok := rawS.(grpc.ClientStream) if !ok &#123; setConnectivityState(connectivity.Ready, nil) //服务端健康 return fmt.Errorf(\"newStream returned %v (type %T); want grpc.ClientStream\", rawS, rawS) &#125; if err = s.SendMsg(&amp;healthpb.HealthCheckRequest&#123;Service: service&#125;); err != nil &amp;&amp; err != io.EOF &#123; // Stream should have been closed, so we can safely continue to create a new stream. continue retryConnection &#125; s.CloseSend() resp := new(healthpb.HealthCheckResponse) for &#123; //不断接收服务端响应 err = s.RecvMsg(resp) // if status.Code(err) == codes.Unimplemented &#123; //表示服务端未支持健康检查 setConnectivityState(connectivity.Ready, nil) return err &#125; // 其他错误 if err != nil &#123; setConnectivityState(connectivity.TransientFailure, fmt.Errorf(\"connection active but received health check RPC error: %v\", err)) continue retryConnection &#125; //收到消息那么 tryCnt = 0 if resp.Status == healthpb.HealthCheckResponse_SERVING &#123; setConnectivityState(connectivity.Ready, nil) &#125; else &#123; setConnectivityState(connectivity.TransientFailure, fmt.Errorf(\"connection active but health check failed. status=%s\", resp.Status)) &#125; &#125; &#125;&#125; 根据策略启动健康检查，针对的是同一个域名对应的多个子地址 1234567891011121314151617181920212223242526272829303132333435//开启健康检查func (ac *addrConn) startHealthCheck(ctx context.Context) &#123; var healthcheckManagingState bool defer func() &#123; if !healthcheckManagingState &#123; ac.updateConnectivityState(connectivity.Ready, nil) &#125; &#125;() //... 判断健康检查是否开启 currentTr := ac.transport newStream := func(method string) (interface&#123;&#125;, error) &#123; ac.mu.Lock() if ac.transport != currentTr &#123; ac.mu.Unlock() return nil, status.Error(codes.Canceled, \"the provided transport is no longer valid to use\") &#125; ac.mu.Unlock() return newNonRetryClientStream(ctx, &amp;StreamDesc&#123;ServerStreams: true&#125;, method, currentTr, ac) &#125; //更新连接状态 setConnectivityState := func(s connectivity.State, lastErr error) &#123; ac.mu.Lock() defer ac.mu.Unlock() if ac.transport != currentTr &#123; return &#125; ac.updateConnectivityState(s, lastErr) &#125; // 独立协程去检查健康状态 go func() &#123; err := ac.cc.dopts.healthCheckFunc(ctx, newStream, setConnectivityState, healthCheckConfig.ServiceName) //.... &#125;()&#125; 这里可以通过健康检查更新连接状态，但不是唯一更新连接状态的方式 服务端 维护 首先需要明白服务端是如何维护状态的 SetServingStatus 其实还是需要用户自定义状态 123456789101112131415161718192021func (s *Server) SetServingStatus(service string, servingStatus healthpb.HealthCheckResponse_ServingStatus) &#123; //.... s.setServingStatusLocked(service, servingStatus)&#125;func (s *Server) setServingStatusLocked(service string, servingStatus healthpb.HealthCheckResponse_ServingStatus) &#123; s.statusMap[service] = servingStatus for _, update := range s.updates[service] &#123; // Clears previous updates, that are not sent to the client, from the channel. // This can happen if the client is not reading and the server gets flow control limited. select &#123; case &lt;-update: default: &#125; // Puts the most recent update to the channel. update &lt;- servingStatus &#125;&#125;statusMap map[string]healthpb.HealthCheckResponse_ServingStatusupdates map[string]map[healthgrpc.Health_WatchServer]chan healthpb.HealthCheckResponse_ServingStatus 这里的状态更新有两种 更新 statusMap[service] 对应的服务状态 将状态写入到服务对应的隧道中，隧道中的状态则是通过Watch函数不断地通知客户端(不同的状态才会通知) 这里有两种特殊的情况可以直接使用内置函数，就是服务恢复正常与停止服务 1234567891011121314151617func (s *Server) Shutdown() &#123; s.mu.Lock() defer s.mu.Unlock() s.shutdown = true for service := range s.statusMap &#123; s.setServingStatusLocked(service, healthpb.HealthCheckResponse_NOT_SERVING) &#125;&#125;func (s *Server) Resume() &#123; s.mu.Lock() defer s.mu.Unlock() s.shutdown = false for service := range s.statusMap &#123; s.setServingStatusLocked(service, healthpb.HealthCheckResponse_SERVING) &#125;&#125; 这里会统一更新服务的状态，而不需要单独设置某个服务的状态更新，一般用于服务启动与恢复。 接着就是接收客户端的健康检查请求 Check 当客户端请求服务状态的时候，直接从状态服务中返回结果 statusMap[in.Service] 12345678910func (s *Server) Check(ctx context.Context, in *healthpb.HealthCheckRequest) (*healthpb.HealthCheckResponse, error) &#123; s.mu.RLock() defer s.mu.RUnlock() if servingStatus, ok := s.statusMap[in.Service]; ok &#123; return &amp;healthpb.HealthCheckResponse&#123; Status: servingStatus, &#125;, nil &#125; return nil, status.Error(codes.NotFound, \"unknown service\")&#125; Watch watch 其实就是服务端不断地将状态的变化通知给客户端 1234567891011121314151617181920212223242526272829303132333435363738394041424344// Watch implements `service Health`.func (s *Server) Watch(in *healthpb.HealthCheckRequest, stream healthgrpc.Health_WatchServer) error &#123; service := in.Service // update channel is used for getting service status updates. update := make(chan healthpb.HealthCheckResponse_ServingStatus, 1) s.mu.Lock() // Puts the initial status to the channel. if servingStatus, ok := s.statusMap[service]; ok &#123; update &lt;- servingStatus //如果服务状态存在，那么写入隧道中 &#125; else &#123; //如果不存在，则更新未知 update &lt;- healthpb.HealthCheckResponse_SERVICE_UNKNOWN &#125; // 注册状态 if _, ok := s.updates[service]; !ok &#123; s.updates[service] = make(map[healthgrpc.Health_WatchServer]chan healthpb.HealthCheckResponse_ServingStatus) &#125; s.updates[service][stream] = update defer func() &#123; s.mu.Lock() delete(s.updates[service], stream) s.mu.Unlock() &#125;() s.mu.Unlock() var lastSentStatus healthpb.HealthCheckResponse_ServingStatus = -1 for &#123; select &#123; case servingStatus := &lt;-update: //监听 update if lastSentStatus == servingStatus &#123; //与上一次的状态相同，那么就不会响应 continue &#125; //否则发送状态 lastSentStatus = servingStatus err := stream.Send(&amp;healthpb.HealthCheckResponse&#123;Status: servingStatus&#125;) if err != nil &#123; return status.Error(codes.Canceled, \"Stream has ended.\") &#125; case &lt;-stream.Context().Done(): return status.Error(codes.Canceled, \"Stream has ended.\") &#125; &#125;&#125; 总结 心跳检查是为了客户端可以根据服务端状态进行自定义操作 心跳检查启动的时候启动了退避算法，gRPC默认的退避算法是 123456var DefaultConfig = Config&#123; BaseDelay: 1.0 * time.Second, Multiplier: 1.6, Jitter: 0.2, MaxDelay: 120 * time.Second,&#125; 在Watch中如果与上一次的状态没有变化，则不会通知客户端 健康检查针对的是具有相同地址的多个服务，一个服务异常整体连接在不会断开，而是更新连接状态 针对不能提供服务的服务器端，客户端会将链接状态更新为TransientFailure，但是链接不会断的；当该连接的状态重新更新为Ready时，还可以继续创建流，传输数据。 针对可以对外提供服务的服务器端，客户端会将链接状态更新为Ready，生成Picker，即将此链接缓存到平衡器里，并且将链接状态更新为Ready，接下来，就可以创建流，传输数据了 健康检查可以应用会更新连接状态，且不能自定义 withHealthCheckFunc 如果在传输过程中收到服务端通知，由于数据发送存在重试机制，所以还是可以从平衡器中选择 Ready 的连接重新传输数据 健康检查与保持链接的区别 keepalive和healthcheck都是用于确保通信的可用性和健康状态的机制。 keepalive是一种保持连接活动的机制，用于检测连接是否处于空闲状态，并在需要时发送ping帧以防止连接关闭。keepalive机制确保长时间的空闲连接不会被关闭，以避免重新建立连接的开销 healthcheck是一种检查服务可用性的机制，用于检测服务器是否可用，以及在服务器不可用时采取相应的措施。healthcheck机制通过发送特定的RPC请求来检查服务器的可用性，并根据响应的状态码和错误信息来确定服务器的状态。 参考链接 https://github.com/grpc/grpc/blob/master/doc/health-checking.md https://blog.csdn.net/u011582922/article/details/120052706","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-13-保持链接","slug":"Grpc/Grpc-13-保持链接","date":"2022-10-31T14:58:29.000Z","updated":"2023-05-27T10:56:04.414Z","comments":true,"path":"2022/10/31/Grpc/Grpc-13-保持链接/","link":"","permalink":"http://xboom.github.io/2022/10/31/Grpc/Grpc-13-%E4%BF%9D%E6%8C%81%E9%93%BE%E6%8E%A5/","excerpt":"","text":"在学习的过程中看到了grpc-go中的两个概念 保持链接 和 心跳检查，这里先学习一下保持链接 带着问题看世界 保持链接的作用 保持链接的原理 它与TCP的 keepalive 有什么区别 首先来看一下官方的介绍， 保持连接功能可以检测TCP层面的连接故障。在某些情况下，TCP连接会丢失数据包(包括FIN)，需要等待系统TCP超时时间(可能为30分钟)才能检测到故障。使用保持连接功能可以让gRPC更早地检测到这种故障 另外是保持连接活动，比如在L4代理中配置为关闭“空闲连接”的情况下，发送包活消息可以是连接不处于“空闲”状态 这里补充一下TCP的 keepalive 的原理： 当一方发送一个数据包后，如果对方没有回应，那么TCP协议会按照指数退避的算法重新发送数据包，最多会重发12次。如果在这12次内仍然没有收到对方的响应，则会将连接标记为“超时”，并关闭连接。这个过程通常需要30分钟左右 可以手动修改TCP默认的超时时间 sudo sysctl -w net.ipv4.tcp_keepalive_time=300 基本使用 客户端 1234567891011var kacp = keepalive.ClientParameters&#123; Time: 10 * time.Second, // send pings every 10 seconds if there is no activity Timeout: time.Second, // wait 1 second for ping ack before considering the connection dead PermitWithoutStream: true, // send pings even without active streams&#125;func main() &#123; conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithKeepaliveParams(kacp)) //...&#125; 客户端的保持心跳参数一共有三个 Time：在没有活动的情况下，发送 ping 的时间间隔(不能少于10s) Timeout：等待ping确认的超时时间 PermitWithoutStream：及时没有活动的流也要发送ping请求 服务端 1234567891011121314151617var kaep = keepalive.EnforcementPolicy&#123; MinTime: 5 * time.Second, // If a client pings more than once every 5 seconds, terminate the connection PermitWithoutStream: true, // Allow pings even when there are no active streams&#125;var kasp = keepalive.ServerParameters&#123; MaxConnectionIdle: 15 * time.Second, // If a client is idle for 15 seconds, send a GOAWAY MaxConnectionAge: 30 * time.Second, // If any connection is alive for more than 30 seconds, send a GOAWAY MaxConnectionAgeGrace: 5 * time.Second, // Allow 5 seconds for pending RPCs to complete before forcibly closing connections Time: 5 * time.Second, // Ping the client if it is idle for 5 seconds to ensure the connection is still active Timeout: 1 * time.Second, // Wait 1 second for the ping ack before assuming the connection is dead&#125;func main() &#123; s := grpc.NewServer(grpc.KeepaliveEnforcementPolicy(kaep), grpc.KeepaliveParams(kasp)) //...&#125; 这里一共有两个保活配置 EnforcementPolicy 与 ServerParameters EnforcementPolicy 定义了服务端如何执行保活策略 MinTime：如果客户端每隔不到5秒就发送一个ping请求，服务器就终止连接 PermitWithoutStream：表示即使没有活动的流，也允许ping请求 ServerParameters 定义了保活 MaxConnectionIdle：如果客户端闲置超过 15 秒，则发送 GOAWAY MaxConnectionAge： 如果任何连接存在时间超过 30 秒，则发送 GOAWAY MaxConnectionAgeGrace：允许在强制关闭连接之前等待 5 秒钟以完成待处理的 RPC Time：如果客户端闲置超过 5 秒钟，则发送 ping 以确保连接仍处于活动状态 Timeout：等待 1 秒钟以获取 ping 的响应，在此之后假定连接已断开 MaxConnectionAge是指整个连接（connection）的最长存在时间，不是单个流(stream)的最长存在时间。当一个连接的时间超过了 MaxConnectionAge 指定的时间，服务器会发送一个 GOAWAY 帧，表示不再接受来自该连接的新流。任何新的流的创建请求都会被拒绝，并且服务器会等待 MaxConnectionAgeGrace 指定的一段时间，让尚未完成的 RPC 请求完成。在这段时间内，服务器不会发送任何新的数据帧，但仍会响应已有的流。 实现原理 客户端 12345678910111213141516171819202122232425func newHTTP2Client(connectCtx, ctx context.Context, addr resolver.Address, opts ConnectOptions, onPrefaceReceipt func(), onGoAway func(GoAwayReason), onClose func()) (_ *http2Client, err error) &#123; //... kp := opts.KeepaliveParams // Validate keepalive parameters. if kp.Time == 0 &#123; kp.Time = defaultClientKeepaliveTime &#125; if kp.Timeout == 0 &#123; kp.Timeout = defaultClientKeepaliveTimeout &#125; keepaliveEnabled := false if kp.Time != infinity &#123; if err = syscall.SetTCPUserTimeout(conn, kp.Timeout); err != nil &#123; return nil, connectionErrorf(false, err, \"transport: failed to set TCP_USER_TIMEOUT: %v\", err) &#125; keepaliveEnabled = true &#125; //... if t.keepaliveEnabled &#123; //如果开启包活 t.kpDormancyCond = sync.NewCond(&amp;t.mu) go t.keepalive() &#125; //...&#125; 这是在构建链接的时候设置的超时时间 SetTCPUserTimeout，指定发送数据后多久没有收到确认信号就会超时。 这是设置是设置网络层的，并不会影响gRPC应用层，因为gRPC超时会重试 并不是所有的操作系统都支持 syscall.SetTCPUserTimeout(可能老的操作系统不支持，Linux、Windows、MacOS都支持) 如果开启保活，通过开启独立协程发送ping帧确保链接是活的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func (t *http2Client) keepalive() &#123; p := &amp;ping&#123;data: [8]byte&#123;&#125;&#125; outstandingPing := false //是否有等待响应的ping帧 timeoutLeft := time.Duration(0) //记录发送ping之后剩余时间 prevNano := time.Now().UnixNano() //记录活动时间 timer := time.NewTimer(t.kp.Time) //开启定时器 for &#123; select &#123; case &lt;-timer.C: lastRead := atomic.LoadInt64(&amp;t.lastRead) if lastRead &gt; prevNano &#123; //自从上次定时器已经读取过了 outstandingPing = false // Next timer should fire at kp.Time seconds from lastRead time. //重新计算下次触发定时器时间(从上次读取时间开始算) timer.Reset(time.Duration(lastRead) + t.kp.Time - time.Duration(time.Now().UnixNano())) prevNano = lastRead continue &#125; //outstandingPing 有等待响应的ping帧，且已经超时。表示链接超时关闭 if outstandingPing &amp;&amp; timeoutLeft &lt;= 0 &#123; t.Close(connectionErrorf(true, nil, \"keepalive ping failed to receive ACK within timeout\")) return &#125; t.mu.Lock() if t.state == closing &#123; //如果链接的状态是关闭中，那么也退出 t.mu.Unlock() return &#125; //如果没有活动流，且没有活动流不允许发送ping帧 if len(t.activeStreams) &lt; 1 &amp;&amp; !t.kp.PermitWithoutStream &#123; outstandingPing = false t.kpDormant = true t.kpDormancyCond.Wait() //在初始化流或者关闭客户端的时候 会触发 t.kpDormancyCond.Signal() &#125; t.kpDormant = false t.mu.Unlock() //没有等待响应的ping帧，就发送ping帧 if !outstandingPing &#123; //... t.controlBuf.put(p) //发送ping帧 timeoutLeft = t.kp.Timeout outstandingPing = true &#125; sleepDuration := minTime(t.kp.Time, timeoutLeft) timeoutLeft -= sleepDuration timer.Reset(sleepDuration) //计算定时器下一次触发时间 case &lt;-t.ctx.Done(): if !timer.Stop() &#123; &lt;-timer.C &#125; return &#125; &#125;&#125; 其实就是根据 ping帧的状态以及链接是否有活动来计算发送ping帧时间。 有几个注意点 如何判断有ping帧响应的 其实它并没有判断是有有相应帧，而是是否在指定时间内是否读取到消息即可 如果没有活动流又不允许非活动流发送ping，那么它是如何处理的 123456//如果没有活动流，且没有活动流不允许发送ping帧 if len(t.activeStreams) &lt; 1 &amp;&amp; !t.kp.PermitWithoutStream &#123; outstandingPing = false t.kpDormant = true t.kpDormancyCond.Wait() //在初始化流或者关闭客户端的时候 会触发 t.kpDormancyCond.Signal() &#125; 而出发 kpDormancyCond.Signal()的位置是 构建流或者关闭连接 服务端 服务端同样也是使用独立协程来运行保活逻辑 超过 最长闲置时间 MaxConnectionIdle 优雅的关闭连接 超过 最大连接时间 MaxConnectionAge 优雅的关闭连接 等待 MaxConnectionAgeGrace 后强制关闭连接。 以 Time 的频率发送 ping 确保连接存活，并在 Timeout 的额外时间内关闭无响应的连接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879func (t *http2Server) keepalive() &#123; p := &amp;ping&#123;&#125; outstandingPing := false //是否有等待响应帧 kpTimeoutLeft := time.Duration(0) prevNano := time.Now().UnixNano() // Initialize the different timers to their default values. idleTimer := time.NewTimer(t.kp.MaxConnectionIdle) ageTimer := time.NewTimer(t.kp.MaxConnectionAge) kpTimer := time.NewTimer(t.kp.Time) defer func() &#123; idleTimer.Stop() ageTimer.Stop() kpTimer.Stop() &#125;() for &#123; select &#123; case &lt;-idleTimer.C: //闲置定时器 t.mu.Lock() idle := t.idle if idle.IsZero() &#123; //未闲置则重置最大闲置时间 t.mu.Unlock() idleTimer.Reset(t.kp.MaxConnectionIdle) continue &#125; val := t.kp.MaxConnectionIdle - time.Since(idle) t.mu.Unlock() if val &lt;= 0 &#123; t.Drain() //优雅关闭连接 return &#125; idleTimer.Reset(val) //重置闲置连接 case &lt;-ageTimer.C: //最长连接 t.Drain() ageTimer.Reset(t.kp.MaxConnectionAgeGrace) select &#123; case &lt;-ageTimer.C: // Close the connection after grace period. if logger.V(logLevel) &#123; logger.Infof(\"transport: closing server transport due to maximum connection age.\") &#125; t.Close() case &lt;-t.done: &#125; return case &lt;-kpTimer.C: //保活定时器 lastRead := atomic.LoadInt64(&amp;t.lastRead) if lastRead &gt; prevNano &#123; // There has been read activity since the last time we were // here. Setup the timer to fire at kp.Time seconds from // lastRead time and continue. outstandingPing = false kpTimer.Reset(time.Duration(lastRead) + t.kp.Time - time.Duration(time.Now().UnixNano())) prevNano = lastRead continue &#125; if outstandingPing &amp;&amp; kpTimeoutLeft &lt;= 0 &#123; if logger.V(logLevel) &#123; logger.Infof(\"transport: closing server transport due to idleness.\") &#125; t.Close() return &#125; if !outstandingPing &#123; //如果没有等待保活则发送保活帧 if channelz.IsOn() &#123; atomic.AddInt64(&amp;t.czData.kpCount, 1) &#125; t.controlBuf.put(p) kpTimeoutLeft = t.kp.Timeout outstandingPing = true &#125; sleepDuration := minTime(t.kp.Time, kpTimeoutLeft) kpTimeoutLeft -= sleepDuration kpTimer.Reset(sleepDuration) case &lt;-t.done: return &#125; &#125;&#125; 注意点： 连接什么时候闲置的? 123if len(t.activeStreams) == 1 &#123; t.idle = time.Time&#123;&#125;&#125; 在处理头帧的时候(构建了一个活跃流)，如果 len(t.activeStreams) == 1 ，表示刚新建一个流且仅有一个活跃流，那么这个时候连接是闲置的 服务端也会主动发送ping帧 总结 TCP与gRPC的区别 TCP的Keepalive是一种机制，它允许在网络连接空闲时发送探测包（keepalive包）来维护连接的状态。这些探测包不包含有效负载，只是一个空的TCP报文段，主要用于检测连接是否仍然活着。 当TCP连接上没有传输数据时会进入空闲状态，即使连接已经中断或不可用，导致不必要的延迟和资源浪费。Keepalive机制可以在连接空闲时周期性地发送探测包，以检测连接是否仍然活着。如果远程端点没有响应这些探测包，则可以视为连接已经断开，并且可以关闭连接。 在gRPC中，使用Keepalive可以检测底层TCP连接是否失效，并在检测到连接问题时及时重新建立连接，从而提高网络连接的可靠性和性能。 客户端与服务端都会根据配置向对端发送ping帧，只要在指定时间内读取到帧，那么就不会主动发送ping帧 参考链接 https://blog.csdn.net/u011582922/article/details/120279303","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-02-平衡器","slug":"Grpc/Grpc-12-平衡器","date":"2022-10-28T14:58:29.000Z","updated":"2023-05-27T10:55:56.060Z","comments":true,"path":"2022/10/28/Grpc/Grpc-12-平衡器/","link":"","permalink":"http://xboom.github.io/2022/10/28/Grpc/Grpc-12-%E5%B9%B3%E8%A1%A1%E5%99%A8/","excerpt":"","text":"平衡器的核心目的就是用于向服务端发起链接，主要包括 子链接的维护，每个 target 可能对应多个地址(类似一个服务有多个副本)，进行子链接的新增与删除 当平衡器状态发生变化，更新 ClientConn 的状态，自定义选择器，从而让客户端选择具体子链接进行发送 在满足一定条件的情况下，执行解析器操作。如解析器由阻塞状态 --&gt; 非阻塞状态 整体流程 实现原理 代码路径:/balancer/balancer.go，在这个文件中一共定义了五个接口 Builder 接口，主要用来创建平衡器 SubConn 接口，主要用来负责具体的链接(这里的链接指的是一对实际建立链接的客户端与服务器) Picker 接口，主要是从众多 SubConn中按照某个策略选择一个链接进行数据传输，也就是 选择器 Balancer 接口，主要是更新 ClientConn 的状态，更新SubConn状态 ClientConn 接口，主要是负责链路的维护，包括创建、移除、更新一个子链路，更新 ClientConn 状态 所以平衡器也跟解析器一样，由两部分组成平衡器构建器与平衡器，内部在封装到一个结构体实现平衡器功能 平衡器构建器注册 平衡器构建器通过注册的方式将自己平衡器的构建方法存入全局变量中，当需要使用的时候则直接根据名称获取 使用一个全局的 map var m = make(map[string]Builder)进行平衡器构建器 Builder 的存储。k/v 分别对应的是构建 构建器的名称以及构建器的实现方法。 平衡器构建器可以通过 Register 函数存储到这个 map 中 123456789101112//Register 注册平衡器构建器func Register(b Builder) &#123; m[strings.ToLower(b.Name())] = b&#125;//Get 获取平衡器构建器，如果找不到则返回nilfunc Get(name string) Builder &#123; if b, ok := m[strings.ToLower(name)]; ok &#123; return b &#125; return nil&#125; 有了存储与添加，它还增加了一个删除操作 12345678//仅做测试使用，去掉某一个平衡器解析器func unregisterForTesting(name string) &#123; delete(m, name)&#125;func init() &#123; internal.BalancerUnregister = unregisterForTesting&#125; 删除操作则是利用 init 给 BalancerUnregister 复制一个 删除函数 unregisterForTesting 而一个普通的平衡器构建器(已轮询平衡器构建器为例rrPickerBuilder)，则是利用 init 函数直接初始化到这个全局map中，那么就可以在需要使用的时候，直接从全局map 中获取 1234567891011const Name = \"round_robin\"//初始化一个平衡器构建器对洗那个func newBuilder() balancer.Builder &#123; return base.NewBalancerBuilder(Name, &amp;rrPickerBuilder&#123;&#125;, base.Config&#123;HealthCheck: true&#125;)&#125;//初始化将 round_robin平衡器构建器存入全局 map 中func init() &#123; balancer.Register(newBuilder())&#125; 构建平衡器 有了平衡器构建器之后，通过选定的平衡器构建器进行平衡器的构建 设置通过哪个平衡器构建器获取平衡器 123456789101112var serviceConfig = `&#123; \"loadBalancingConfig\": [ &#123; \"grpclb\": &#123; \"childPolicy\": [ &#123;\"round_robin\": \"\"&#125;, ] &#125; &#125; ]&#125;`grpc.WithDefaultServiceConfig(serviceConfig) 如果不设置负载均衡构建器名称，那么grpc-go 会默认使用 round_robin 构建平衡器 指定了构建器之后就会在构建链接过程中调用构建函数 1234567func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) &#123; //... cc.balancerWrapper = newCCBalancerWrapper(cc, balancer.BuildOptions&#123; //... &#125;) //...&#125; 在建立连接的过程中，通过 构建链接 存储链接 实际应用 参考文档 https://blog.csdn.net/u011582922/article/details/117547171","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-03-选择器","slug":"Grpc/Grpc-11-选择器","date":"2022-10-25T14:58:29.000Z","updated":"2023-05-27T10:55:51.743Z","comments":true,"path":"2022/10/25/Grpc/Grpc-11-选择器/","link":"","permalink":"http://xboom.github.io/2022/10/25/Grpc/Grpc-11-%E9%80%89%E6%8B%A9%E5%99%A8/","excerpt":"","text":"什么是Grpc 为什么我们要使用grpc 优点有哪些 从图中可以看出 grpc 客户端与服务端的通信主要有两个阶段 建立链接阶段 帧传输阶段 这其中存在三个主要的功能接口 解析器 resolver 负责根据域名解析地址 平衡器 balancer 负责根据地址构建对应连接 选择器 picker 负责在发送的时候选择合适的连接 在第一阶段建立链接的过程中，主要通过 解析器与平衡器构建链接；在第二阶段发送过程中通过选择器从准备好的连接中选择合适的连接进行帧的传输 根据负载均衡器的位置不同，可以分为 根据作用的位置可以分为客户端负载均衡器、服务端负载均衡器 根据部署的方式可以分为 集中式LB(Proxy Model)、进程内LB(Balancing-aware Client)、独立LB进程(External Load Balancing Service) 独立LB与集中式LB的区别是，独立LB不在进程内部，但是是伴随着进程的，类似SideCar模式。 在 grpc-go 的中的负载均衡器(选择器)，其实是一种 进程内LB，它的核心功能就是：根据规则选择合适的子链接进行消息发送 整体流程 实际应用 代码路径：balancer\\balancer.go 选择器定义了一个 Picker 的接口，按照备注 Pick 函数用于发送RPC消息的连接以及相关信息 Pick 函数不应该被阻塞，如果有任何I/O、阻塞、一定时间开销的操作，那么应该返回 ErrNoSubConnAvailable的内部错误，那么它将重复调用(注意：它会一致重复！)直到Picker更新 ClientConn.UpdateState 如果返回了其他错误 如果返回一个状态错误来源于 status\\status.go，那么 rpc将停止并返回这个错误 如果是其他错误，那么等待的rpc调用将继续调用，不需要等待的rpc调用将立即停止并返回这个错误以及 状态 Code 为 Unavailable 123type Picker interface &#123; Pick(info PickInfo) (PickResult, error)&#125; 选择器执行失败的逻辑处理，代码路径：picker_wrapper.go 12345678910111213141516171819202122for &#123; //... pickResult, err := p.Pick(info) if err != nil &#123; if err == balancer.ErrNoSubConnAvailable &#123; continue &#125; if _, ok := status.FromError(err); ok &#123; // Status error: end the RPC unconditionally with this status. return nil, nil, dropError&#123;error: err&#125; &#125; // For all other errors, wait for ready RPCs should block and other // RPCs should fail with unavailable. if !failfast &#123; lastPickErr = err continue &#125; return nil, nil, status.Error(codes.Unavailable, err.Error()) &#125; //...&#125; 这个 failfast 是通过 serviceconfig.MethodConfig 中的 WaitForReady 控制，可在初始化连接配置 1grpc.WithDefaultServiceConfig(`&#123;\"methodConfig\": [&#123;\"waitForReady\": true&#125;]&#125;`) 实际应用 PickFirst pickFirst 是 grpc-go 内置的选择器，它的作用是从链接中选择第一个已经建立好的链接给流使用 代码路径：pickfirst.go 首先，看下选择器 pickfirstBalancer 结构体 12345type pickfirstBalancer struct &#123; state connectivity.State cc balancer.ClientConn sc balancer.SubConn&#125; connectivity.State 表示选择器的状态 balancer.ClientConn 负责子链接 SubConn 的创建、移除。更新客户端链接 ClientConn 的状态。由 ccBalancerWrapper结构实现 balancer.SubConn 主要负责向 grpc 服务发起链接 其次，pickfirstBalancer 其实实现了平衡器构建接口，也就是说，通过 pickfirstBalancer 可以构建一个平衡器 参考文档 https://zhuanlan.zhihu.com/p/377860784","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-10-连接","slug":"Grpc/Grpc-10-连接","date":"2022-10-22T14:58:29.000Z","updated":"2023-05-27T10:55:43.898Z","comments":true,"path":"2022/10/22/Grpc/Grpc-10-连接/","link":"","permalink":"http://xboom.github.io/2022/10/22/Grpc/Grpc-10-%E8%BF%9E%E6%8E%A5/","excerpt":"","text":"客户端 构建链接阶段设置超时时间 构建链接阶段有单独的超时控制，也就是链接的超时控制与发送请求的超时控制是分开的 12345conn, err := grpc.DialContext(context.Background(), *addr, grpc.WithTransportCredentials(insecure.NewCredentials()))if err != nil &#123; log.Fatalf(\"did not connect: %v\", err)&#125;defer conn.Close() 在构建链接的过程中，通过强制等待 1234567891011121314151617181920212223242526272829303132333435363738394041424344func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) &#123; //... defer func() &#123; //等待最后的结果，要么ctx.Done()结束 select &#123; case &lt;-ctx.Done(): switch &#123; case ctx.Err() == err: //失败错误 conn = nil case err == nil || !cc.dopts.returnLastError: //将服务端的错误与错误信息一起返回给客户端而不是仅仅一个错误码 conn, err = nil, ctx.Err() default: conn, err = nil, fmt.Errorf(\"%v: %v\", ctx.Err(), err) //返回其他错误 &#125; default: &#125; &#125;() //... if cc.dopts.block &#123; //如果是阻塞模式 for &#123; cc.Connect() s := cc.GetState() if s == connectivity.Ready &#123; //连接状态，健康检查的时候也会更新 break &#125; else if cc.dopts.copts.FailOnNonTempDialError &amp;&amp; s == connectivity.TransientFailure &#123; if err = cc.connectionError(); err != nil &#123; terr, ok := err.(interface &#123; Temporary() bool &#125;) if ok &amp;&amp; !terr.Temporary() &#123; return nil, err &#125; &#125; &#125; if !cc.WaitForStateChange(ctx, s) &#123; //为什么是等待状态变化，因为构建链接的时候是每个都使用一个协程进行建立连接 // ctx got timeout or canceled. if err = cc.connectionError(); err != nil &amp;&amp; cc.dopts.returnLastError &#123; return nil, err &#125; return nil, ctx.Err() &#125; &#125; &#125; return cc, nil&#125; 等待连接的状态其实就是等待结果后者超时的过程，通过隧道通知 123456789101112func (cc *ClientConn) WaitForStateChange(ctx context.Context, sourceState connectivity.State) bool &#123; ch := cc.csMgr.getNotifyChan() if cc.csMgr.getState() != sourceState &#123; return true &#125; select &#123; case &lt;-ctx.Done(): return false case &lt;-ch: return true &#125;&#125; Dfasdf","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-05-重试机制","slug":"Grpc/Grpc-09-重试机制","date":"2022-10-19T14:58:29.000Z","updated":"2023-05-27T10:55:34.640Z","comments":true,"path":"2022/10/19/Grpc/Grpc-09-重试机制/","link":"","permalink":"http://xboom.github.io/2022/10/19/Grpc/Grpc-09-%E9%87%8D%E8%AF%95%E6%9C%BA%E5%88%B6/","excerpt":"","text":"重试机制是为了在短暂异常情况下能够正常恢复，也能防止长时间异常而不退出。这里看一下grpc-go 中的重试机制以及使用方法 带着问题看世界： grpc-go 的重试策略的使用 grpc-go 的重试机制是怎样的 grpc-go 的重试机制都应用在哪些地方 基本使用 通过 Dial 可选参数设置自定义重试策略 1234567891011121314151617181920212223242526var ( addr = flag.String(\"addr\", \"localhost:50052\", \"the address to connect to\") // see https://github.com/grpc/grpc/blob/master/doc/service_config.md to know more about service config retryPolicy = `&#123; \"methodConfig\": [&#123; \"name\": [&#123;\"service\": \"grpc.examples.echo.Echo\"&#125;], \"waitForReady\": true, \"retryPolicy\": &#123; \"MaxAttempts\": 4, \"InitialBackoff\": \".01s\", \"MaxBackoff\": \".01s\", \"BackoffMultiplier\": 1.0, \"RetryableStatusCodes\": [ \"UNAVAILABLE\" ] &#125;, \"hedgingPolicy\": &#123; \"MaxAttempts\": \"\", \"HedgingDelay\": \"\", \"NonFatalStatusCodes\": [\"\"] &#125; &#125;]&#125;`)// use grpc.WithDefaultServiceConfig() to set service configfunc retryDial() (*grpc.ClientConn, error) &#123; return grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()), grpc.WithDefaultServiceConfig(retryPolicy))&#125; 首先，这里有两种重试策略 hedgingPolicy (重试) 与 retryPolicy(对冲) 重试中参数的解释 MaxAttempts：最大重试次数 InitialBackoff：默认退避时间 MaxBackoff：最大退避时间 BackoffMultiplier：退避时间增加倍率 RetryableStatusCodes: 服务端返回什么错误码才重试 对冲是指在不等待响应的情况下主动发送单词调用的多个请求。通俗解释就是 如果HedgingDelay时间没有响应，那么直接发送第二次请求，以此类推，直到达到最大次数 MaxAttempts MaxAttempts：最大重试次数 HedgingDelay：等待响应时间 也可以通过可选参数关闭重试策略 12345func WithDisableRetry() DialOption &#123; return newFuncDialOption(func(o *dialOptions) &#123; o.disableRetry = true &#125;)&#125; 重试策略 这两种策略都是通过实现统一的重试接口进行加载 123type isMethodConfig_RetryOrHedgingPolicy interface &#123; isMethodConfig_RetryOrHedgingPolicy()&#125; 通过转换而进行重试测试 1234567891011121314151617181920pushback := 0 //记录重试间隔时间hasPushback := false //记录是否要重试if cs.attempt.s != nil &#123; //存在重试任务 if !cs.attempt.s.TrailersOnly() &#123; //如果流不是只有trailer,则无法直接读取元数据 return false, err &#125; sps := cs.attempt.s.Trailer()[\"grpc-retry-pushback-ms\"] //获取元数据 if len(sps) == 1 &#123; var e error if pushback, e = strconv.Atoi(sps[0]); e != nil || pushback &lt; 0 &#123; cs.retryThrottler.throttle() // 记录重试次数 return false, err &#125; hasPushback = true //重试 &#125; else if len(sps) &gt; 1 &#123; //如果存在多个 sps 表示异常 cs.retryThrottler.throttle() // 记录重试次数(用于重试失败限速) return false, err &#125;&#125; Trailers 是一种消息元数据，用于在相应结束时传递关于次消息的附加信息，通常包含状态码，消息和错误信息的传递。Trailers 通常会在流关闭之前发送。Trailers 发送的情况有几下几种 RPC正常结束，Trailers中携带调用的状态、错误码、响应时间等信息； RPC出错结束，Trailers中携带错误信息，例如错误码、错误消息等； 服务器关闭了stream，Trailers中携带stream关闭的原因； 客户端取消了RPC调用，Trailers中携带调用取消的原因 它的特点： Trailers 并不是每个RPC调用都会发，而是流关闭的时候才会发！！！ 可以包含在最后一个数据帧中附加一个Trailers，所以抓包看不到一个独立的 Trailers。 如果流中不仅仅有Trailers，那么无法直接从流中获取 Trailers，而是需要其他办法，比如拦截器 包含元数据存储在ctx中 123ctx := metadata.NewOutgoingContext(context.Background(), metadata.Pairs( \"grpc-retry-pushback-ms\", \"5000\",)) 这里还多了一个 retryThrottler.throttle，在下面的重试速率中说明 如果有 没有配置 grpc-retry-pushback-ms，那么就使用重试策略， 12345678910111213var dur time.Durationif hasPushback &#123; //设置了重试元数据 dur = time.Millisecond * time.Duration(pushback) cs.numRetriesSincePushback = 0&#125; else &#123; fact := math.Pow(rp.BackoffMultiplier, float64(cs.numRetriesSincePushback)) cur := float64(rp.InitialBackoff) * fact if max := float64(rp.MaxBackoff); cur &gt; max &#123; //最大判断 cur = max &#125; dur = time.Duration(grpcrand.Int63n(int64(cur))) cs.numRetriesSincePushback++&#125; 可以看出，退避时间随着重试次数指数级增长 InitialBackoff * math.Pow(rp.BackoffMultiplier, float64(cs.numRetriesSincePushback)) 根据上述计算的退避时间执行等待逻辑 123456789t := time.NewTimer(dur) //开启一个定时器select &#123;case &lt;-t.C: //定时器触发，重试次数+1 cs.numRetries++ return false, nilcase &lt;-cs.ctx.Done(): //流程结束停止重试 t.Stop() return false, status.FromContextError(cs.ctx.Err()).Err()&#125; 重试速率 重试过程中还能看到 RetryThrottlingPolicy 做了一层重试限制，其实除了配置重试策略，还能控制重试的速率，防止重试的次数太多给服务器造成太大压力 12345678910policy := grpcutil.RetryThrottlingPolicy&#123; MaxTokens: 10, // 最大令牌数 TokenRate: 1, // 每秒产生的令牌数 RetryBudget: 100 * time.Millisecond, // 重试预算&#125;// 创建一个 DialOption，使用上面的 RetryThrottlingPolicydialOption := grpc.WithRetryThrottlingPolicy(policy)// 使用 dialOption 创建 gRPC 客户端 MaxTokens：最大令牌数，任何时间点同时允许的最大令牌数 TokenRate：每秒产生的令牌数，每个请求都会消耗一个令牌 RetryBudget：重试预算，制定重试操作可以使用的总时间，如果一个请求需要重试，但是时间已经超过了 RetryBudget，那么不在进行重试 限速原理 第一步：初始化速率配置 结构体如下： 12345678type retryThrottler struct &#123; max float64 //最大令牌数 thresh float64 //预算 ratio float64 //每秒令牌数 mu sync.Mutex tokens float64 //可用令牌数&#125; 转换规则 1234567891011if cc.sc.retryThrottling != nil &#123; newThrottler := &amp;retryThrottler&#123; tokens: cc.sc.retryThrottling.MaxTokens, max: cc.sc.retryThrottling.MaxTokens, thresh: cc.sc.retryThrottling.MaxTokens / 2, ratio: cc.sc.retryThrottling.TokenRatio, &#125; cc.retryThrottler.Store(newThrottler)&#125; else &#123; cc.retryThrottler.Store((*retryThrottler)(nil))&#125; 第二步：计算是否进行限速 123456789101112func (rt *retryThrottler) throttle() bool &#123; if rt == nil &#123; return false &#125; rt.mu.Lock() defer rt.mu.Unlock() rt.tokens-- if rt.tokens &lt; 0 &#123; rt.tokens = 0 &#125; return rt.tokens &lt;= rt.thresh&#125; 第三步：成功调用则更新可用 tokens 1234567891011func (rt *retryThrottler) successfulRPC() &#123; if rt == nil &#123; return &#125; rt.mu.Lock() defer rt.mu.Unlock() rt.tokens += rt.ratio if rt.tokens &gt; rt.max &#123; rt.tokens = rt.max &#125;&#125; 这里需要注意的是：一个流中只有只有成功调用的时候才会将增加可用 token 数量 业务重试 业务重试一般直接使用重试封装逻辑(已新建流为例) 12345op := func(a *csAttempt) error &#123; return a.newStream() &#125;if err := cs.withRetry(op, func() &#123; cs.bufferForRetryLocked(0, op) &#125;); err != nil &#123; cs.finish(err) return nil, err&#125; 内部逻辑如下 12345678910111213141516171819202122232425262728293031func (cs *clientStream) withRetry(op func(a *csAttempt) error, onSuccess func()) error &#123; cs.mu.Lock() //获取流的写锁，防止在重试过程又出现新的重试 for &#123; //使用for循环不断尝试 if cs.committed &#123; //流不在发送任何东西 cs.mu.Unlock() return toRPCErr(op(cs.attempt)) &#125; a := cs.attempt cs.mu.Unlock() err := op(a) //执行操作 cs.mu.Lock() if a != cs.attempt &#123; //如果重试对象已经变更，则直接跳过继续下一次操作 // We started another attempt already. continue &#125; if err == io.EOF &#123; &lt;-a.s.Done() &#125; //执行成功的后续操作 if err == nil || (err == io.EOF &amp;&amp; a.s.Status().Code() == codes.OK) &#123; onSuccess() cs.mu.Unlock() return err &#125; //执行失败则尝试重试 if err := cs.retryLocked(err); err != nil &#123; cs.mu.Unlock() return err &#125; &#125;&#125; committed：当客户端调用 CloseSend 方法关闭发送流的时候，会标记 ClientStream为已提交，表示客户端不再发送任何消息 有个奇怪的地方如下所示，为什么不发送任何东西还要继续 op 1234if cs.committed &#123; //流不在发送任何东西 cs.mu.Unlock() return toRPCErr(op(cs.attempt))&#125; 原因是如果是正在处理中的请求(尝试重试、取消等操作)，还是会尝试执行 op并返回错误结束尝试 如果在重试过程中，重试任务发生了变化，那么也会跳过直接进行下一次重试 成功 如果执行成功，那么只需要执行后续的 OnSuccess 函数即可，一般有有两种 如果是 commitAttemptLocked，那么表示执行成功更新状态即可 1234567func (cs *clientStream) commitAttemptLocked() &#123; if !cs.committed &amp;&amp; cs.onCommit != nil &#123; cs.onCommit() //配置的重试逻辑 &#125; cs.committed = true //重试逻辑执行成功 cs.buffer = nil //清空这个重试缓存&#125; 如果是 bufferForRetryLocked，那么会缓存当前的重试操作到回放缓冲区(后续发生失败的时候再执行一次) 123456789101112func (cs *clientStream) bufferForRetryLocked(sz int, op func(a *csAttempt) error) &#123; // Note: we still will buffer if retry is disabled (for transparent retries). if cs.committed &#123; //如果重试逻辑执行成功结束 return &#125; cs.bufferSize += sz if cs.bufferSize &gt; cs.callInfo.maxRetryRPCBufferSize &#123; //回放缓冲区太大就不回放了 cs.commitAttemptLocked() return &#125; cs.buffer = append(cs.buffer, op) &#125; 失败 1234567891011121314151617func (cs *clientStream) retryLocked(lastErr error) error &#123; for &#123; cs.attempt.finish(toRPCErr(lastErr)) isTransparent, err := cs.shouldRetry(lastErr) if err != nil &#123; cs.commitAttemptLocked() return err &#125; cs.firstAttempt = false if err := cs.newAttemptLocked(isTransparent); err != nil &#123; //创建一个新attemp return err &#125; if lastErr = cs.replayBufferLocked(); lastErr == nil &#123; //回放缓冲区， return nil &#125; &#125;&#125; 首先，当前的尝试 cs.attempt 将被标记为已完成，其结果会被传递给 cs.finish() 然后，会调用 cs.shouldRetry() 判断当前错误是否应该重试(服务端错误 DoNotTransparentRetry ) 如果返回的错误可以重试，则会通过 cs.newAttemptLocked() 创建新的尝试，即新的流，并在其上发起新的请求 如果没有返回新的错误，则尝试从回放缓冲区 cs.replayBufferLocked() 中获取上一次重试的错误 如果在回放缓冲区中找到了错误，继续进行下一次重试，否则返回 nil 表示重试成功 前面提到了，有一种成功即使执行成功也会加入到 buffer中，在执行失败的之后重新再执行一遍 123456789func (cs *clientStream) replayBufferLocked() error &#123; a := cs.attempt for _, f := range cs.buffer &#123; if err := f(a); err != nil &#123; return err &#125; &#125; return nil&#125; 是在构建流的时候加入的，如果构建流成功，那么会将构建流的函数加入到buffer，如果后续有一次临时性的失败，会将所有成功的构建流全部执行一遍。为什么其中一个创建流失败之后，需要将其他创建成功的逻辑都进行重试？ 答：如果一次失败了，那么执行之前的操作。前提是能够保证和之前成功的那次执行得到相同的结果。这里创建流与发送消息都会将成功的操作放入到缓冲区中，当消息发送完毕，那么这个流就结束了。所以在发送过程中，如果其中一个帧发送失败，那么它会将从流构建到消息发送再次传输一遍(相当于一个新的请求) 总结 重试机制 withRetry 包含的业务 建立流链接 发送头帧 发送消息 接收消息 关闭发送 服务端响应的时候并没有重试机制 重试速率限制是针对整个连接而不是每个流一个 同时执行一个重试任务，并且执行的都是最新的那个 只有重试成功，才会更新令牌tokens数量。那是不是影响其他调用的重试，会但影响不大。第一是因为只要有一个成功，那么token数量都会更新。第二是重试次数过多，单次调用会因为超时停止重试。 如果在发送过程中，某一个帧发送失败，那么它会从创建流开始重新发起调用 参考文档 https://blog.csdn.net/u011582922/article/details/120578941","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-08-超时处理","slug":"Grpc/Grpc-08-超时处理","date":"2022-10-17T14:58:29.000Z","updated":"2023-05-27T10:55:27.452Z","comments":true,"path":"2022/10/17/Grpc/Grpc-08-超时处理/","link":"","permalink":"http://xboom.github.io/2022/10/17/Grpc/Grpc-08-%E8%B6%85%E6%97%B6%E5%A4%84%E7%90%86/","excerpt":"","text":"grpc-go 通信的超时处理有两种方式 业务出现异常或判断处理超时主动取消 grpc-go 存在截止时间，超过截止时间自动停止 带着问题看世界 取消发生在不同时机是如何处理的 链路建立阶段 客户端发送阶段 请求传输阶段 服务端处理阶段 消息响应阶段 截止时间又是如何做到的 服务端是如何在截止时间停止的 主动取消 基本使用 客户端使用context并创建双向流，发送两条之后取消再次发送消息 1234567891011121314151617181920212223ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)stream, err := c.BidirectionalStreamingEcho(ctx)if err != nil &#123; log.Fatalf(\"error creating stream: %v\", err)&#125;// Send some test messages.if err := sendMessage(stream, \"hello\"); err != nil &#123; log.Fatalf(\"error sending on stream: %v\", err)&#125;if err := sendMessage(stream, \"world\"); err != nil &#123; log.Fatalf(\"error sending on stream: %v\", err)&#125;// Ensure the RPC is working.recvMessage(stream, codes.OK)recvMessage(stream, codes.OK)fmt.Println(\"cancelling context\")cancel()//失败sendMessage(stream, \"closed\") 服务端则是响应流的处理 1234567891011121314func (s *server) BidirectionalStreamingEcho(stream pb.Echo_BidirectionalStreamingEchoServer) error &#123; for &#123; in, err := stream.Recv() if err != nil &#123; fmt.Printf(\"server: error receiving from stream: %v\\n\", err) if err == io.EOF &#123; return nil &#125; return err &#125; fmt.Printf(\"echoing message %q\\n\", in.Message) stream.Send(&amp;pb.EchoResponse&#123;Message: in.Message&#125;) &#125;&#125; 实现原理 客户端 从调用方法的内部实现，超时控制大致分为三个阶段 构建流阶段、发送消息阶段、接收消息阶段 12345678910func invoke(ctx context.Context, method string, req, reply interface&#123;&#125;, cc *ClientConn, opts ...CallOption) error &#123; cs, err := newClientStream(ctx, unaryStreamDesc, cc, method, opts...) if err != nil &#123; return err &#125; if err := cs.SendMsg(req); err != nil &#123; return err &#125; return cs.RecvMsg(reply)&#125; 根据实际情况将通过一下几个阶段进行分析 构建流阶段 客户端发送阶段 请求传输阶段 服务端处理阶段 消息响应阶段 构建流 1234567891011121314151617181920212223242526func newClientStreamWithParams(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, mc serviceconfig.MethodConfig, onCommit, doneFunc func(), opts ...CallOption) (_ iresolver.ClientStream, err error) &#123; //... var cancel context.CancelFunc if mc.Timeout != nil &amp;&amp; *mc.Timeout &gt;= 0 &#123; ctx, cancel = context.WithTimeout(ctx, *mc.Timeout) &#125; else &#123; ctx, cancel = context.WithCancel(ctx) &#125; defer func() &#123; if err != nil &#123; cancel() &#125; &#125;() //... //如果一元调用 if desc != unaryStreamDesc &#123; //var unaryStreamDesc = &amp;StreamDesc&#123;ServerStreams: false, ClientStreams: false&#125; go func() &#123; //使用协程来清理结束时候的流 select &#123; case &lt;-cc.ctx.Done(): cs.finish(ErrClientConnClosing) case &lt;-ctx.Done(): cs.finish(toRPCErr(ctx.Err())) &#125; &#125;() &#125;&#125; 如果配置了超时时间则使用超时时间，否则默认ctx，这个超时时间以前是为了针对 WithTimeout，后来被 DialContext替代 接着，将超时时间封装到一个流的头帧的 123456789101112131415161718192021222324252627282930313233343536373839func (t *http2Client) NewStream(ctx context.Context, callHdr *CallHdr) (_ *Stream, err error) &#123; //... //解析成头帧的KV字段 ctx = peer.NewContext(ctx, t.getPeer()) headerFields, err := t.createHeaderFields(ctx, callHdr) if err != nil &#123; return nil, err &#125; s := t.newStream(ctx, callHdr) //... //封装到头帧中 hdr := &amp;headerFrame&#123; hf: headerFields, //... &#125; for &#123; //发送头帧 success, err := t.controlBuf.executeAndPut(func(it interface&#123;&#125;) bool &#123; if !checkForStreamQuota(it) &#123; return false &#125; if !checkForHeaderListSize(it) &#123; return false &#125; return true &#125;, hdr) //... select &#123; case &lt;-ch: //配额没有了 case &lt;-s.ctx.Done(): //流结束 return nil, ContextErr(s.ctx.Err()) case &lt;-t.goAway: //返回关闭流的帧 return nil, errStreamDrain case &lt;-t.ctx.Done(): //连接结束 return nil, ErrConnClosing &#125; &#125;&#125; 构建流的过程中，只发送到队列中，那么流就算发送成功， 所以在构建流的过程中，如果触发 context.Done()，那么 首先会返回 context中的错误 其次更新状态流的状态 服务端 原理分析 客户端向服务端请求的一个完整阶段，需要经历： 链路建立阶段 流建立阶段 数据发送阶段 数据接收阶段 在创建完客户端流clientStream后，取消功能以异步方式启动，开始监听是否有取消指令，有的话，就开始执行取消指令。因此，取消功能可以发生在数据发送阶段和数据接收阶段，且两个阶段的流程是一样的 取消是流级别的而不是链路级别的，也就是可以用于取消单个请求 当客户端主动发起 cancel请求的时候 会构建 cleanupStream的请求关闭流，并发送RST帧，将流ID，取消状态码封装到RST帧中，发送给服务端 服务端收到RST帧之后，会停止读取 stream 中的数据，并移除对应的流 截止时间 截止时间与cancel函数的原理类似，主动取消是通过 参考文档 https://blog.csdn.net/u011582922/article/details/119834758 https://blog.csdn.net/u011582922/article/details/119944259","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-07-滑动窗口","slug":"Grpc/Grpc-07-滑动窗口","date":"2022-10-15T14:58:29.000Z","updated":"2023-05-27T10:55:20.349Z","comments":true,"path":"2022/10/15/Grpc/Grpc-07-滑动窗口/","link":"","permalink":"http://xboom.github.io/2022/10/15/Grpc/Grpc-07-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"","text":"回顾一下 tcp 的滑动窗口 那么这一章需要解决的问题是： tcp 有了滑动窗口为什么 grpc-go又需要滑动窗口，它们两者有什么区分，为什么两者都需要用到滑动窗口 grpc-go 的滑动窗口原理是什么 grpc-go 的滑动窗口是如何实现的 滑动窗口的目的是，计算出下次发送数据帧的最大字节数 客户端的帧发送器在发送数据帧时存在一些发送指标，服务器端在接收数据帧的不同阶段，也存在一些指标 发送指标是如何定义的 TODO? 服务器在接收数据帧时，会根据自己的接收数据帧、存储数据帧、读取数据帧的速度，向客户端发送窗口更新帧或者设置帧； 计算原理是什么 ？ 窗口更新帧与设置帧的区别？ TODO 客户端接收到服务器端发送过来的设置帧或者窗口更新帧后，会更新本地的发送指标 发送指标的更新，会影响到客户端下次发送数据帧的最大字节数 最大字节数的计算，服务器端会通过发送窗口更新帧或者设置帧来动态的影响客户端计算数据帧大小的参数，从而影响下次发送的最大字节数 每个链接、每个流、每个客户端的指标都是如何定义的？ TODO 滑动窗口原理 客户端 构建好数据帧dataFrame 通过发送参数指标，计算本次发送的最大字节数maxSize； 数据帧截取器，从数据帧里截取指定的字节数maxSize，交由帧发送器 帧发送器将截取的字节，转换成http2原生的帧，发送给服务器端 服务端 帧接收器接收到数据帧，交由帧分发器处理 帧分发器根据帧的类型，交由数据处理器handleData处理，数据处理器handleData： 抽样级别流控：对本地的接收参数进行更新(如b.sampelCount,b.sample,b.bwMax,b.bdp)，触发阈值条件后，会向客户端发送窗口更新帧outgoingWindowUpdate，其中设置streamID=0，或者发送设置帧outgoingSettings 链接级别流控：对本地的接收参数进行更新(如f.unacked,f.limit)，触发阈值条件后，会向客户端发送窗口更新帧outgoingWindowUpdate，其中设置streamID=0 流级别流控：对本地的接收参数进行更新(如f.pendingData,f.pendingUpdate,f.limit,f.delta)，触发阈值条件后，会向客户端发送窗口更新帧outgoingWindowUpdate，其中设置streamID=0 将数据帧存储到go语言原生自带的缓存bytes.Buffer里 构建recvMsg结构体，将bytes.Buffer存储到recvMsg里；（recvMsg就是对bytes.Buffer封装） 将recvMsg存储到recvBuffer里：存储逻辑是 i.若recvMsg类型的通道里，没有数据的话，就直接将recvMsg存储到该通道里 ii.若recvMsg类型的通道里，已经有数据了的话，就将recvMsg添加到类型为recvMsg的切片的尾部 接收数据并解压recvAndDecompress： a)读取数据前，先对本地的窗口参数进行调整，如f.delta;满足触发阈值条件的话，就向客户端发送窗口更新帧outgoingWindowUpdate，其中streamID非0； b)recvBufer读取器： i.从recvBuffer的通道里获取数据recvMsg ii.数据获取到后，recvBuffer缓存中，将切片的第一个数据，加载到通道里；(因为通道里刚才已经消费了数据，需要重新添加上) iii.从recvMsg里获取到bytes.Buffer对象 iv.将bytes.Buffer里的数据读取到字节切片里 c)读取完成数据后，更新流级别参数，如f.pendingData,f.pendingUpdate若满足触发阈值条件的话，就向客户端发送窗口更新帧outgoingWindowUpdate，其中streamID非0； d)对切片里的数据进行解压，对解压后的数据，交由handle方法 e)handle方法：就是grpc服务器内部，真正执行客户端请求的方法入口 i.对解压后的数据，进行反序列化，得到请求方法的具体参数值 ii.真正的执行客户端的请求方法，得到执行结果 f)将执行结果封装到数据帧里，存储到controlBuf缓存里 g)帧发送器，从帧缓存里获取到数据帧，发送给客户端 参考文档 https://blog.csdn.net/u011582922/article/details/118887625","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Go-19-Fuzzing","slug":"Go/Go-19-Fuzzing","date":"2022-10-14T15:31:50.000Z","updated":"2023-07-15T05:37:34.497Z","comments":true,"path":"2022/10/14/Go/Go-19-Fuzzing/","link":"","permalink":"http://xboom.github.io/2022/10/14/Go/Go-19-Fuzzing/","excerpt":"","text":"Golang在1.18引入的第三个特性就是 Fuzzing 模糊测试：构造随机数据来找出代码里的漏洞或者可能导致程序崩溃的输入。 单元测试有局限性，每个测试输入必须由开发者指定加到单元测试的测试用例里。fuzzing的优点之一是可以基于开发者代码里指定的测试输入作为基础数据，进一步自动生成新的随机测试数据，用来发现指定测试输入没有覆盖到的边界情况。 通过fuzzing可以找出的漏洞包括SQL注入、缓冲区溢出、拒绝服务(Denial of Service)攻击和XSS(cross-site scripting)攻击等 这里通过编写反转字符串函数通过 fuzz test 来发现并修改问题 第一步：实现基本功能 12345678910111213141516func Reverse(s string) string &#123; b := []byte(s) for i, j := 0, len(b)-1; i &lt; len(b)/2; i, j = i+1, j-1 &#123; b[i], b[j] = b[j], b[i] &#125; return string(b)&#125;func main() &#123; input := \"The quick brown fox jumped over the lazy dog\" rev := Reverse(input) doubleRev := Reverse(rev) fmt.Printf(\"original: %q\\n\", input) fmt.Printf(\"reversed: %q\\n\", rev) fmt.Printf(\"reversed again: %q\\n\", doubleRev)&#125; 输出的结果是 123original: \"The quick brown fox jumped over the lazy dog\"reversed: \"god yzal eht revo depmuj xof nworb kciuq ehT\"reversed again: \"The quick brown fox jumped over the lazy dog\" 第二步：编写单元测试 123456789101112131415func TestReverse(t *testing.T) &#123; testcases :&#x3D; []struct &#123; in, want string &#125;&#123; &#123;&quot;Hello, world&quot;, &quot;dlrow ,olleH&quot;&#125;, &#123;&quot; &quot;, &quot; &quot;&#125;, &#123;&quot;!12345&quot;, &quot;54321!&quot;&#125;, &#125; for _, tc :&#x3D; range testcases &#123; rev :&#x3D; Reverse(tc.in) if rev !&#x3D; tc.want &#123; t.Errorf(&quot;Reverse: %q, want %q&quot;, rev, tc.want) &#125; &#125;&#125; 执行单元测试发现，一切正常 1ok example/fuzz 0.003s 第三步：添加模糊测试 123456789101112131415161718func FuzzReverse(f *testing.F) &#123; testcases := []string&#123;\"Hello, world\", \" \", \"!12345\"&#125; for _, tc := range testcases &#123; f.Add(tc) // Use f.Add to provide a seed corpus &#125; f.Fuzz(func(t *testing.T, orig string) &#123; rev := Reverse(orig) doubleRev := Reverse(rev) //这一行是为了解释后面的错误加的，暂时可以忽略 t.Logf(\"Number of runes: orig=%d, rev=%d, doubleRev=%d\", utf8.RuneCountInString(orig), utf8.RuneCountInString(rev), utf8.RuneCountInString(doubleRev)) if orig != doubleRev &#123; t.Errorf(\"Before: %q, after: %q\", orig, doubleRev) &#125; if utf8.ValidString(orig) &amp;&amp; !utf8.ValidString(rev) &#123; t.Errorf(\"Reverse produced invalid UTF-8 string %q\", rev) &#125; &#125;)&#125; 注意点： Reverse函数如果是一个错误的版本(直接return返回输入的字符串)，虽然可以通过上面的模糊测试，但没法通过第二步的单元测试，所以模糊测试与单元测试是互补的关系 go test 只会使用种子语料库，而不会生成随机测试数据。通过这种方式可以用来验证种子语料库的测试数据是否可以测试通过 如果reverse_test.go文件里有其它单元测试函数或者模糊测试函数，但只想运行FuzzReverse模糊测试函数，我们可以执行go test -run=FuzzReverse命令 如果要基于种子语料库生成随机测试数据用于模糊测试，需要给go test命令增加-fuzz参数 123456789101112131415root@13ce5bc74ac3:/code/fuzz# go test -fuzz .fuzz: elapsed: 0s, gathering baseline coverage: 0/7 completedfuzz: elapsed: 0s, gathering baseline coverage: 7/7 completed, now fuzzing with 2 workersfuzz: elapsed: 0s, execs: 648 (17784/sec), new interesting: 1 (total: 8)--- FAIL: FuzzReverse (0.04s) --- FAIL: FuzzReverse (0.00s) hello_test.go:32: Number of runes: orig=1, rev=2, doubleRev=1 hello_test.go:36: Reverse produced invalid UTF-8 string \"\\x9e\\xdb\" Failing input written to testdata/fuzz/FuzzReverse/d6a654c77ca8db001e0bbe2cbb5493efcd20e17777911523bf59bd30bd33e199 To re-run: go test -run=FuzzReverse/d6a654c77ca8db001e0bbe2cbb5493efcd20e17777911523bf59bd30bd33e199FAILexit status 1FAIL example/fuzz 0.048s 运行之后会生成testdata的文件夹，那么下次即使没有带上 -fuzz 参数，也会使用该数据进行模糊测试 路径：./testdata/fuzz/FuzzReverse/d6a654c77ca8db001e0bbe2cbb5493efcd20e17777911523bf59bd30bd33e199，内容是： 12go test fuzz v1 //语料库文件里的第1行标识的是编码版本string(\"۞\") 从第2行开始，每一行数据对应的是语料库的每条测试数据(corpus entry)的其中一个参数，按照参数先后顺序排列，因为fuzz target函数func(t *testing.T, orig string)只有orig这1个参数作为真正的测试输入，也就是每条测试数据其实就1个输入，因此在上面示例的testdata/fuzz/FuzzReverse目录下的文件里只有string(“۞”)这一行 第四步：修复Bug 模糊测试中得出的错误为 Reverse produced invalid UTF-8 string &quot;\\x9e\\xdb&quot; Reverse函数是按照字节(byte)为维度进行字符串反转，这就是问题所在。比如字符string(&quot;۞&quot;) 如果按照字节反转，反转后得到的就是一个无效的字符串了。因此为了保证字符串反转后得到的仍然是一个有效的UTF-8编码的字符串，需要按照rune进行字符串反转。 1234567func Reverse(s string) string &#123; r := []rune(s) for i, j := 0, len(r)-1; i &lt; len(r)/2; i, j = i+1, j-1 &#123; r[i], r[j] = r[j], r[i] &#125; return string(r)&#125; 运行 go test 命令单元测试通过，表示老的数据能够正常的处理，但是如果再次执行 go test -fuzz 会出现新的错误 1234567891011121314root@13ce5bc74ac3:/code/fuzz# go test -fuzz .fuzz: elapsed: 0s, gathering baseline coverage: 0/9 completedfuzz: minimizing 38-byte failing input filefuzz: elapsed: 0s, gathering baseline coverage: 4/9 completed--- FAIL: FuzzReverse (0.01s) --- FAIL: FuzzReverse (0.00s) hello_test.go:34: Before: \"\\xe5\", after: \"�\" Failing input written to testdata/fuzz/FuzzReverse/91862839dc552bd95b4e42be6576a6c198f0d4c8fc2884c953030d898573b014 To re-run: go test -run=FuzzReverse/91862839dc552bd95b4e42be6576a6c198f0d4c8fc2884c953030d898573b014FAILexit status 1FAIL example/fuzz 0.011s 结构就是对一个字符串做了2次反转后得到的和原字符串不一样，这次测试输入本身是非法的unicode 12345678910func Reverse(s string) (string, error) &#123; if !utf8.ValidString(s) &#123; //判断是否是合法的 utf-8编码 return s, errors.New(\"input is not valid UTF-8\") &#125; r := []rune(s) for i, j := 0, len(r)-1; i &lt; len(r)/2; i, j = i+1, j-1 &#123; r[i], r[j] = r[j], r[i] &#125; return string(r), nil&#125; 修改对应的引用和单元测试后，通过测试 语法 Go模糊测试和单元测试在语法上有如下差异： Go模糊测试函数以FuzzXxx开头，单元测试函数以TestXxx开头 Go模糊测试函数以 *testing.F作为入参，单元测试函数以*testing.T作为入参 Go模糊测试会调用f.Add函数和f.Fuzz函数。 f.Add函数把指定输入作为模糊测试的种子语料库(seed corpus)，fuzzing基于种子语料库生成随机输入。 f.Fuzz函数接收一个fuzz target函数作为入参。fuzz target函数有多个参数，第一个参数是*testing.T，其它参数是被模糊的类型(注意：被模糊的类型目前只支持部分内置类型, string, []byte int, int8, int16, int32/rune, int64 uint, uint8/byte, uint16, uint32, uint64 float32, float64 bool 参考链接 https://go.dev/doc/tutorial/fuzz https://segmentfault.com/a/1190000041650681 https://segmentfault.com/a/1190000041467510 https://go.googlesource.com/proposal/+/master/design/draft-fuzzing.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Grpc-06-多路复用","slug":"Grpc/Grpc-06-多路复用","date":"2022-10-13T14:58:29.000Z","updated":"2023-05-27T10:55:14.892Z","comments":true,"path":"2022/10/13/Grpc/Grpc-06-多路复用/","link":"","permalink":"http://xboom.github.io/2022/10/13/Grpc/Grpc-06-%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/","excerpt":"","text":"多路复用是 HTTP/2中的重要特性，允许同一个TCP连接上同时传输多个HTTP请求和响应 带着问题看世界： 通过《帧发送器》可知帧是一个一个进行消息发送，多个流的帧如何发送 服务端是如何区分不同的流的帧 帧太大一定会进行分包，最大是多少，服务端如何存储分包的帧，保证它的顺序 服务端如何从分包的数据帧的恢复数据 发送端 多路复用从 《帧接收器》说起，一个请求入上图所示，由头帧跟多个数据帧组成，现在直接看数据帧的发送逻辑 12345678910it, err := l.cbuf.get(true)if err != nil &#123; return err&#125;if err = l.handle(it); err != nil &#123; return err&#125;if _, err = l.processData(); err != nil &#123; return err&#125; 上面是从发送缓冲区获取数据进行发送，一共分为三步 从缓冲区获取帧(这里的帧是应用帧中的数据帧 dataFrame) 将帧进行流的处理 1234567891011121314func (l *loopyWriter) preprocessData(df *dataFrame) error &#123; str, ok := l.estdStreams[df.streamID] if !ok &#123; return nil &#125; // If we got data for a stream it means that // stream was originated and the headers were sent out. str.itl.enqueue(df) if str.state == empty &#123; str.state = active l.activeStreams.enqueue(str) &#125; return nil&#125; 意思就是： 根据帧ID streamID 从 已经建立连接的流中 estdStreams 获取到流 estdStreams 表示所有已建立但未被清除的流(stream) 在客户端，表示所有已发送头部信息的流 在服务端，表示所有已接收头部信息的流 activeStreams 就表示的是已经发送或接收了头帧以及部分数据帧的流 将帧存入到流的单向链表中 如果链表为空(表示仅仅是发送了头帧，还没有数据帧)，那么将流加入到 activeStreams 数据流中 进行消息发送 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func (l *loopyWriter) processData() (bool, error) &#123; //... //1. 从数据流中拿第一个流的数据帧 str := l.activeStreams.dequeue() // Remove the first stream. if str == nil &#123; return true, nil &#125; dataItem := str.itl.peek().(*dataFrame) //dataItem 是业务数据帧，在这里讲真正分解为发送的多个数据帧 dataFrame if len(dataItem.h) == 0 &amp;&amp; len(dataItem.d) == 0 &#123; // 如果是空的 //发送结束帧 if err := l.framer.fr.WriteData(dataItem.streamID, dataItem.endStream, nil); err != nil &#123; return false, err &#125; //...根据流的帧后续是否仍然有数据进行处理 return false, nil &#125; var ( buf []byte ) maxSize := http2MaxFrameLen //16KB //计算一个帧最大值，默认16KB hSize := min(maxSize, len(dataItem.h)) //头部的最大长度 dSize := min(maxSize-hSize, len(dataItem.d)) //负载的最大长度 //... 将dataItem 部分数据写入到buf中 size := hSize + dSize //... var endStream bool //是否是流的最后一帧 if dataItem.endStream &amp;&amp; len(dataItem.h)+len(dataItem.d) &lt;= size &#123; endStream = true &#125; //... if err := l.framer.fr.WriteData(dataItem.streamID, endStream, buf[:size]); err != nil &#123; return false, err &#125; str.bytesOutStanding += size l.sendQuota -= uint32(size) dataItem.h = dataItem.h[hSize:] //剩余头部长度 dataItem.d = dataItem.d[dSize:] //剩余负载长度 //如果数据为空，则从链表中删除这个数据帧 if len(dataItem.h) == 0 &amp;&amp; len(dataItem.d) == 0 &#123; // All the data from that message was written out. str.itl.dequeue() &#125; //如果流中的数据帧为空，那么标记流为 empty //如果流中下一个为头帧，那么发送头帧并清理流 //如果还有一部分数据未发送完毕，那么将剩下的帧存入 activeStreams 的链表中下次再发 //...发送处理 return false, nil&#125; 对应代码具体步骤如下： 获取第一个活动流activeStream(有Data) 拿到流中间的数据帧 dataFrame (应用帧) 如果需要发送的数据是空的，那么发送数据帧并表示结束 endStream == true 如果有一部分没有发送完毕，那么将剩下的部分作为数据帧存入 activeStreams中下次再计算发送 总结： 在发送端，由于同一个链接中多个流公用一个帧缓冲区，所以虽然是多路复用，但其实客户端所有的帧还是一个一个发送的。只是可能存在一个流的帧没有发完就会发送另一个流的帧 一个帧最大为 16KB，剩下部分会作为一个帧重新加入到帧缓冲区链表中，下一次重新计算发送 接收端 接收端可以分为上述三个问题 如何接收数据帧分帧的 如何存储数据帧分帧的 帧的读取 123456789101112131415161718192021222324252627282930313233343536func (fr *Framer) ReadFrame() (Frame, error) &#123; fr.errDetail = nil if fr.lastFrame != nil &#123; fr.lastFrame.invalidate() &#125; //1. 利用第三方依赖，直接读取帧的头部 fh, err := readFrameHeader(fr.headerBuf[:], fr.r) if err != nil &#123; return nil, err &#125; if fh.Length &gt; fr.maxReadSize &#123; return nil, ErrFrameTooLarge &#125; //2. 获取负载长度并读取 payload := fr.getReadBuf(fh.Length) if _, err := io.ReadFull(fr.r, payload); err != nil &#123; return nil, err &#125; //根据帧的类型，将真转换为程序数据结构 f, err := typeFrameParser(fh.Type)(fr.frameCache, fh, payload) if err != nil &#123; if ce, ok := err.(connError); ok &#123; return nil, fr.connError(ce.Code, ce.Reason) &#125; return nil, err &#125; //检查帧的顺序 if err := fr.checkFrameOrder(f); err != nil &#123; return nil, err &#125; //如果是头帧，还需要将其中的元数据再解析一次，最终变为 MetaHeadersFrame 帧 if fh.Type == FrameHeaders &amp;&amp; fr.ReadMetaHeaders != nil &#123; return fr.readMetaFrame(f.(*HeadersFrame)) &#125; return f, nil&#125; 帧转换器 12345678910111213141516171819var frameParsers = map[FrameType]frameParser&#123; FrameData: parseDataFrame, FrameHeaders: parseHeadersFrame, FramePriority: parsePriorityFrame, FrameRSTStream: parseRSTStreamFrame, FrameSettings: parseSettingsFrame, FramePushPromise: parsePushPromise, FramePing: parsePingFrame, FrameGoAway: parseGoAwayFrame, FrameWindowUpdate: parseWindowUpdateFrame, FrameContinuation: parseContinuationFrame,&#125;func typeFrameParser(t FrameType) frameParser &#123; if f := frameParsers[t]; f != nil &#123; return f &#125; return parseUnknownFrame&#125; 数据帧处理 1234567891011121314151617181920212223func (t *http2Server) handleData(f *http2.DataFrame) &#123; size := f.Header().Length //...流控 // Select the right stream to dispatch. s, ok := t.getStream(f) //... if size &gt; 0 &#123; //负载大于0 //... 流控 if len(f.Data()) &gt; 0 &#123; buffer := t.bufferPool.get() buffer.Reset() buffer.Write(f.Data()) s.write(recvMsg&#123;buffer: buffer&#125;) &#125; &#125; //如果流结束了， if f.StreamEnded() &#123; // Received the end of stream from the client. s.compareAndSwapState(streamActive, streamReadDone) s.write(recvMsg&#123;err: io.EOF&#125;) &#125;&#125; 流程： 如果是数据帧则获取流的大小以及负载 将负载封装成recvMsg写入到流中 如果对方告知流结束了，那么将流的状态从 streamActive 改为 streamReadDone 封装帧结束消息 recvMsg{err: io.EOF} 核心就是讲数据写入流中的 s.write(recvMsg{buffer: buffer}) 帧缓冲区 与发送端所有流的帧都放入到一个帧缓冲区不一样，这里的缓冲区是一个缓冲区切片，用于接收同一个流中的分帧数据 123456type recvBuffer struct &#123; c chan recvMsg //缓冲区为1 的 channel mu sync.Mutex //原子锁 backlog []recvMsg //缓冲切片 err error //错误信息&#125; 帧的存储 如果 切片 里面没有没有数据，就直接存入隧道，说明切面的消息已经都消费了 如果 切片 里面有数据，那么就存入到切片的后面，保证消息的顺序消费 123456789101112131415161718func (b *recvBuffer) put(r recvMsg) &#123; b.mu.Lock() if b.err != nil &#123; b.mu.Unlock() return &#125; b.err = r.err if len(b.backlog) == 0 &#123; //切片为空，则直接存入隧道 select &#123; case b.c &lt;- r: b.mu.Unlock() return default: &#125; &#125; b.backlog = append(b.backlog, r) //切片不为空则直接存入切片尾部 b.mu.Unlock()&#125; 帧的消费 如果切片长度大于0，说明切片内部有消息，则直接将切片第一条数据传入隧道，这样每次读取隧道中的数据即可，也能缓冲一部分数据 123456789101112func (b *recvBuffer) load() &#123; b.mu.Lock() if len(b.backlog) &gt; 0 &#123; select &#123; case b.c &lt;- b.backlog[0]: b.backlog[0] = recvMsg&#123;&#125; b.backlog = b.backlog[1:] default: &#125; &#125; b.mu.Unlock()&#125; 虽然帧被放入到了同一个流的 recvBuffer中，但还是没有说明：如果一个帧的分帧是前后发送的，但是接收的顺序是乱序的时候，即使顺序消费也无法保证数据帧能够正常的解析？ 答：这里可以这样想，由于TCP是有序的，那么所有的TCP包都会按照发送顺序在接收端组装完成。也就是说只要发送顺序一定，那么接收端的顺序与发送端的是一样的。又因为同一个连接共用一个帧缓冲器，也就是说同一个流中的帧都是顺序发送的，所以接收端收到的帧的顺序也是不会乱序的。 总结 多路复用可以在同一个TCP连接上同时传输多个HTTP请求和响应，避免建立和关闭连接的开销 和长连接的区别是: 多路复用 可以避免 队头阻塞(Head-of-Line Blocking) 问题。如果某个请求在传输过程过程中出现阻塞，那么后续的请求也会被阻塞 和分包的区别是:HTTP分包是指将一个HTTP消息分为多个TCP数据包(Packet)进行传输，而多路复用是在同一个TCP连接上同时传输多个HTTP请求和响应 HTTP2 帧的最大传输字节是 16KB，TCP最大传输MSS是 1460B，IP层最大传输单元 MTU 1500B HTTP2 的帧有一个固定9B的头部，用于描述帧的类型，长度，标志等信息。其中保存林该帧的有效载荷的长度，最大长度 2^24 - 1 大约为 16KB 多路复使用相同的 StreamID 来标识属于同一个流，使用recvBuffer进行帧的缓存(多个数据帧)，使用独立协程或者生产者消费者模式进行帧处理 参考文档 https://blog.csdn.net/u011582922/article/details/120426690","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-03-帧","slug":"Grpc/Grpc-05-帧","date":"2022-10-11T14:58:29.000Z","updated":"2023-05-27T10:55:06.561Z","comments":true,"path":"2022/10/11/Grpc/Grpc-05-帧/","link":"","permalink":"http://xboom.github.io/2022/10/11/Grpc/Grpc-05-%E5%B8%A7/","excerpt":"","text":"从前面可知，流单独负责一次调用，并且在服务端使用多路复用实现消息的处理。流其实是通过 ID 标志的虚拟概念，真正传输的其实是帧，帧通过发送器与帧接收器进行发送处理。 带着问题看世界： 帧的类型有哪些，具体结构是怎样的 当消息体过大的时候分帧是如何处理的 当帧的缓冲区是否也会满，什么时候清理 在 grpc-go 中，通过对 HTTP/2 中协议的封装，增加了grpc-go自己的业务类型。看源码的时候注意不要弄混了。这里说的协议帧 是 HTTP/2 中定义实际传输过程中的帧 是由 net 库实现的；而应用帧则是 grpc-go 根据自己的实际需要由转换了一层 协议帧 每个协议帧都是由头部与负载两个部分组成 头部 协议帧指的就是HTTP2中帧的结构，每个帧也分为头部与负载 123+--------+--------+--------+--------+--------+--------+--------+--------+| Length(24) |Type(8) |Flags(8)|R| StreamID(31) |+--------+--------+--------+--------+--------+--------+--------+--------+ 虽然Length 是24位，但这不意味着就能处理 2^24 16M大小的帧，一般是默认只支持2^16 16k以下的帧，而2^16 - 2^24 16M 的帧 需要接收端公布自己可以处理这么大的帧，需要在 SETTINGS_MAX_FRAME_SIZE 帧中告知 将二进制buffer转换为帧头部结构 1234567type FrameHeader struct &#123; valid bool // 是否有效，供内部使用 Type FrameType // 帧的类型 Flags Flags // 标志位 Length uint32 // 帧的负载长度(即不包括头部) StreamID uint32 // 流ID，有的帧没有流，那么StreamID == 0&#125; 所以头部的整体解析流程就变成了 12345678910111213func readFrameHeader(buf []byte, r io.Reader) (FrameHeader, error) &#123; _, err := io.ReadFull(r, buf[:frameHeaderLen]) //读取9B到buf中 if err != nil &#123; return FrameHeader&#123;&#125;, err &#125; return FrameHeader&#123; //解析成头部接头 Length: (uint32(buf[0])&lt;&lt;16 | uint32(buf[1])&lt;&lt;8 | uint32(buf[2])), //头部长度 Type: FrameType(buf[3]), //类型 Flags: Flags(buf[4]), //标志位 StreamID: binary.BigEndian.Uint32(buf[5:]) &amp; (1&lt;&lt;31 - 1), valid: true, //是否有效 &#125;, nil&#125; 值得注意的是： binary.BigEndian.Uint32 这个作用是将 4个字节的二进制数据转换为 uint32类型的数据 1&lt;&lt;31 - 1 用于将高位设置为0，确保能被正确的解析为uint32位的值 最后数据帧的格式如下 帧的类型 其中 FrameType 代表的帧类型，一共有10中 帧类型 说明 值 字符 FrameData 表示数据帧，用于传输 HTTP 报文的实际数据部分 0x0 “DATA” FrameHeaders 携带请求或响应头部，也可以分为多个帧进行传输 0x1 “HEADERS” FramePriority 表示某个流的优先级，用于流量控制 0x2 “PRIORITY” FrameRSTStream 重置某个流，表示这个流将不再使用 0x3 “RST_STREAM” FrameSettings 表示设置帧，用于在连接和流级别上传输参数设置 0x4 “SETTINGS” FramePushPromise 允许服务端在客户端还没有请求的情况下发送响应头部，用于服务端推送(server push)功能 0x5 “PUSH_PROMISE” FramePing 表示 Ping 帧，用于检测连接是否存活 0x6 “PING” FrameGoAway 表示断开帧，用于通知对端关闭连接 0x7 “GOAWAY” FrameWindowUpdate 用于流量控制，通知对端窗口大小的变化 0x8 “WINDOW_UPDATE” FrameContinuation 表示继续帧，将头部帧或推送帧拆分为多个帧进行传输时，用于指示后续帧属于同一个头部块 0x9 “CONTINUATION” 帧的标志位 另外一个标志位Flags则是帧 帧类型 Flags类型 说明 值 FrameData FlagDataEndStream 标志位用于 DATA 帧，表示这是最后一个 DATA 帧，即流已经结束，接收方不应该再等待更多的 DATA 帧 0x1 FlagDataPadded 数据帧的填充(Padded)标志，表示数据帧后面跟随一个填充字段 0x8 FrameHeaders FlagHeadersEndStream 首部帧(Headers Frame)的结束流标志，表示发送方已经发送完整个消息 0x1 FlagHeadersEndHeaders 首部帧的结束头(End Headers)标志，表示这个首部块是消息的最后一个首部块 0x4 FlagHeadersPadded 首部帧的填充标志，表示首部帧后面跟随一个填充字段 0x8 FlagHeadersPriority 首部帧的优先级(Priority)标志，表示这个首部块包含了一个优先级信息 0x20 FrameSettings FlagSettingsAck 设置帧(Settings Frame)的确认(Ack)标志，表示这是一个确认帧 0x1 FramePing FlagPingAck Ping帧的确认标志，表示这是一个确认帧 0x1 FrameContinuation FlagContinuationEndHeaders 连续帧(Continuation Frame)的结束头标志，表示这个连续块是消息的最后一个连续块 0x4 FramePushPromise FlagPushPromiseEndHeaders 推送帧(Push Promise Frame)的结束头标志，表示这个推送帧包含了一个完整的首部块 0x4 FlagPushPromisePadded 推送帧的填充标志，表示推送帧后面跟随一个填充字段 0x8 有了上述的帧的关键字段说明，接下来来看看各个数据帧的解析 负载 头部定义之后，负载通过头部长度 Length 进行解析，负载解析如下 1234567891011payload := fr.getReadBuf(fh.Length) //获取负载长度if _, err := io.ReadFull(fr.r, payload); err != nil &#123; //读取负载长度 return nil, err&#125;f, err := typeFrameParser(fh.Type)(fr.frameCache, fh, payload) //将负载根据类型解析为对应的帧 fh是上一步读取的帧头if err != nil &#123; if ce, ok := err.(connError); ok &#123; return nil, fr.connError(ce.Code, ce.Reason) &#125; return nil, err&#125; 数据帧的解析是通过类型参数进行解析 1f, err := typeFrameParser(fh.Type)(fr.frameCache, fh, payload) 其中 fh.Type 是从头部解析出来的帧的类型，用于找到指定类型的数据帧解析器 fr.frameCache 流都具有自己的帧内存缓存池。在需要发送帧时，可以从自己的缓存池中获取内存，并在完成后将其返回以供重用 fh 之前解析出的帧的头部，用来与解析完的负载合并成一个完整的帧 payload 则是帧的负载，用于解析帧的数据部分 需要关注的是 是如何转换成其他类型的 如果有存在分帧，又是如何进行解析的 如果有帧的丢失怎么办 FrameData 数据帧的解析对应的是 parseDataFrame 123456789101112131415161718192021func parseDataFrame(fc *frameCache, fh FrameHeader, payload []byte) (Frame, error) &#123; if fh.StreamID == 0 &#123; //数据帧StreamID必不为0 return nil, connError&#123;ErrCodeProtocol, \"DATA frame with stream ID 0\"&#125; &#125; f := fc.getDataFrame() //从内存池获取一个数据帧结构 f.FrameHeader = fh //帧头部 var padSize byte if fh.Flags.Has(FlagDataPadded) &#123; //如果有追加 var err error payload, padSize, err = readByte(payload) if err != nil &#123; return nil, err &#125; &#125; if int(padSize) &gt; len(payload) &#123; //当追加的长度大于负载的长度 return nil, connError&#123;ErrCodeProtocol, \"pad size larger than data payload\"&#125; &#125; f.data = payload[:len(payload)-int(padSize)] //帧的负载就是 负载 - 追加 return f, nil&#125; 当存在追加帧 FlagDataPadded 的时候 使用 readByte进行解析出去除追加长度的负载 123456func readByte(p []byte) (remain []byte, b byte, err error) &#123; if len(p) == 0 &#123; return nil, 0, io.ErrUnexpectedEOF &#125; return p[1:], p[0], nil&#125; 所以，正常情况只有负载，但是如果数据帧存在追加 FlagDataPadded，数据帧的负载还有一个长度 1234567+--------+--------+--------+--------+| padSize(32) |+--------+--------+--------+--------+| data ... +--------+--------+--------+--------+| padding ...+--------+--------+--------+--------+ 追加的内容在解析成数据帧的过程中，丢掉了！！！！！ 这是因为填充字段仅仅是为了将数据帧填充到规定的长度 最终组成数据帧 1234type DataFrame struct &#123; FrameHeader data []byte&#125; 那么另外一个标志位 FlagDataEndStream 是在应用gRPC使用 服务端收到数据帧结束流之后，设置流的状态并结束继续读取流数据 客户端收到数据帧结束流之后，关闭流(结束读取并清理流) 1234567891011//客户端if f.StreamEnded() &#123; t.closeStream(s, io.EOF, false, http2.ErrCodeNo, status.New(codes.Internal, \"server closed the stream without sending trailers\"), nil, true) &#125;//服务端if f.StreamEnded() &#123; // Received the end of stream from the client. s.compareAndSwapState(streamActive, streamReadDone) s.write(recvMsg&#123;err: io.EOF&#125;)&#125; 其他注意事项 如果数据帧中 StreamID == 0 ，那么这个数据帧异常，也就是说数据帧必须绑定一个流 FrameHeaders 头帧也是由头部与负载组成，通过 parseHeadersFrame 解析 1234567891011121314151617181920212223242526272829303132func parseHeadersFrame(_ *frameCache, fh FrameHeader, p []byte) (_ Frame, err error) &#123; hf := &amp;HeadersFrame&#123; FrameHeader: fh, &#125; if fh.StreamID == 0 &#123; //头帧流ID不能为0 return nil, connError&#123;ErrCodeProtocol, \"HEADERS frame with stream ID 0\"&#125; &#125; var padLength uint8 if fh.Flags.Has(FlagHeadersPadded) &#123; //如果有头部追加，同理解析出负载与追加长度 if p, padLength, err = readByte(p); err != nil &#123; return &#125; &#125; if fh.Flags.Has(FlagHeadersPriority) &#123; //如果有优先级 var v uint32 p, v, err = readUint32(p) if err != nil &#123; return nil, err &#125; hf.Priority.StreamDep = v &amp; 0x7fffffff hf.Priority.Exclusive = (v != hf.Priority.StreamDep) // high bit was set p, hf.Priority.Weight, err = readByte(p) if err != nil &#123; return nil, err &#125; &#125; if len(p)-int(padLength) &lt;= 0 &#123; return nil, streamError(fh.StreamID, ErrCodeProtocol) &#125; hf.headerFragBuf = p[:len(p)-int(padLength)] //去掉追加部分 return hf, nil&#125; 整体来看，头帧结构如下所示 1234567891011+--------+--------+--------+--------+| padSize(32) |+--------+--------+--------+--------+| priority(32) |+--------+--------+--------+--------+|weight(8)|+--------+--------+--------+--------+| payload |+--------+--------+--------+--------+| padding |+--------+--------+--------+--------+ 这里比数据帧多的第一个标志位就是 优先级 FlagHeadersPriority，它的作用是表示流的优先级 12345type PriorityParam struct &#123; StreamDep uint32 Exclusive bool Weight uint8&#125; StreamDep：一个31位的流标识符，表示此流依赖的流。如果为0，表示没有依赖关系。 Exclusive：表示此流是否具有互斥性。如果此字段为true，表示此流是在其依赖的流和同级的兄弟流之间互斥的。 Weight：此流的权重值，是一个0-255的值。根据规范，应该将此字段与StreamDep一起设置，或者两者都不设置。为了获得1到256之间的权重，请将此值加1 为什么流有依赖关系或者互斥关系??? 流之间的依赖关系或者互斥关系可以帮助控制流之间的优先级和竞争关系，从而更有效地利用网络带宽和资源。 例如1：如果一个网页需要加载多个资源，比如图片、脚本和样式表，那么这些资源就可以分别被分配到不同的流中。为了更快地呈现网页，图片这类占用带宽较大的资源可以被赋予更高的优先级，使其在竞争网络资源时更优先被传输，从而减少用户等待时间。 例如2：如果一个客户端同时向服务器发起了多个请求，这些请求可能会在服务器端形成竞争关系，造成某些请求的延迟和等待时间过长。通过为请求之间建立依赖关系和互斥关系，可以更好地控制请求的执行顺序和资源利用，提高服务质量和用户体验 最后组成结构体 12345type HeadersFrame struct &#123; FrameHeader Priority PriorityParam headerFragBuf []byte // not owned&#125; 另外两个标志位 FlagHeadersEndStream 和 FlagHeadersEndHeaders 同理，是在应用层使用 12345678switch fh.Type &#123;case FrameHeaders, FrameContinuation: if fh.Flags.Has(FlagHeadersEndHeaders) &#123; fr.lastHeaderStream = 0 &#125; else &#123; fr.lastHeaderStream = fh.StreamID &#125;&#125; FramePriority 虽然头帧中可以设置流的优先级，但是如果有大量的流需要设置优先级，将会导致头帧变得非常庞大，传输的效率也会受到影响。优先级帧可以独立于其他帧来设置流的优先级。同时，PRIORITY 帧也可以用于修改流的依赖关系，因此它具有比头帧更强的功能 123456789101112131415161718func parsePriorityFrame(_ *frameCache, fh FrameHeader, payload []byte) (Frame, error) &#123; if fh.StreamID == 0 &#123; return nil, connError&#123;ErrCodeProtocol, \"PRIORITY frame with stream ID 0\"&#125; &#125; if len(payload) != 5 &#123; //负载必须是5 return nil, connError&#123;ErrCodeFrameSize, fmt.Sprintf(\"PRIORITY frame payload size was %d; want 5\", len(payload))&#125; &#125; v := binary.BigEndian.Uint32(payload[:4]) streamID := v &amp; 0x7fffffff // mask off high bit return &amp;PriorityFrame&#123; FrameHeader: fh, PriorityParam: PriorityParam&#123; Weight: payload[4], //256 StreamDep: streamID, Exclusive: streamID != v, //是否是独立 &#125;, &#125;, nil&#125; 组成的优先级帧如下 12345+--------+--------+--------+--------+| StreamDep(32) |+--------+--------+--------+--------+|Weight(8)|+--------+ FrameRSTStream 重置帧则只需要负载的前4个字节用于标志重置原因 123456789func parseRSTStreamFrame(_ *frameCache, fh FrameHeader, p []byte) (Frame, error) &#123; if len(p) !&#x3D; 4 &#123; return nil, ConnectionError(ErrCodeFrameSize) &#125; if fh.StreamID &#x3D;&#x3D; 0 &#123; return nil, ConnectionError(ErrCodeProtocol) &#125; return &amp;RSTStreamFrame&#123;fh, ErrCode(binary.BigEndian.Uint32(p[:4]))&#125;, nil&#125; 解析成如下结构 123+--------+--------+--------+--------+| ErrCode(32) |+--------+--------+--------+--------+ 组成的数据结构 1234type RSTStreamFrame struct &#123; FrameHeader ErrCode ErrCode&#125; FrameSettings 设置帧是通过 parseSettingsFrame 解析 12345678910111213141516func parseSettingsFrame(_ *frameCache, fh FrameHeader, p []byte) (Frame, error) &#123; if fh.Flags.Has(FlagSettingsAck) &amp;&amp; fh.Length &gt; 0 &#123; //如果负载长度大于0，则肯定不是ACK帧 return nil, ConnectionError(ErrCodeFrameSize) &#125; if fh.StreamID != 0 &#123; return nil, ConnectionError(ErrCodeProtocol) &#125; if len(p)%6 != 0 &#123; //设置帧固定是 6的倍数 return nil, ConnectionError(ErrCodeFrameSize) &#125; f := &amp;SettingsFrame&#123;FrameHeader: fh, p: p&#125; //直接将负载存入设置帧中 if v, ok := f.Value(SettingInitialWindowSize); ok &amp;&amp; v &gt; (1&lt;&lt;31)-1 &#123; return nil, ConnectionError(ErrCodeFlowControl) &#125; return f, nil&#125; 它因为直接使用了KV结构，所以固定每一对的长度是6，那么就能统计出每一对的内容 1234567891011121314151617func (f *SettingsFrame) Value(id SettingID) (v uint32, ok bool) &#123; f.checkValid() for i := 0; i &lt; f.NumSettings(); i++ &#123; if s := f.Setting(i); s.ID == id &#123; return s.Val, true &#125; &#125; return 0, false&#125;func (f *SettingsFrame) Setting(i int) Setting &#123; buf := f.p return Setting&#123; ID: SettingID(binary.BigEndian.Uint16(buf[i*6 : i*6+2])), Val: binary.BigEndian.Uint32(buf[i*6+2 : i*6+6]), &#125;&#125; 一共存在6对 标志 说明 值 SettingHeaderTableSize 客户端和服务器通信时，指定用于HPACK头表大小的值 0x1 SettingEnablePush 服务器指示客户端是否可以推送资源 0x2 SettingMaxConcurrentStreams 指定客户端可以同时使用的最大流数 0x3 SettingInitialWindowSize 指定流控制窗口的初始大小 0x4 SettingMaxFrameSize 指定帧大小的最大值 0x5 SettingMaxHeaderListSize 指定头部列表大小的最大值 0x6 所以对应的结构就是 1234type SettingsFrame struct &#123; FrameHeader p []byte&#125; 初始化窗口大小不能大于 v &gt; (1&lt;&lt;31)-1 FramePushPromise FramePushPromise 通过 parsePushPromise 进行数据解析 123456789101112131415161718192021222324252627282930func parsePushPromise(_ *frameCache, fh FrameHeader, countError func(string), p []byte) (_ Frame, err error) &#123; pp := &amp;PushPromiseFrame&#123; FrameHeader: fh, &#125; if pp.StreamID == 0 &#123; countError(\"frame_pushpromise_zero_stream\") return nil, ConnectionError(ErrCodeProtocol) &#125; //如果有追加，那么就读取追加长度 var padLength uint8 if fh.Flags.Has(FlagPushPromisePadded) &#123; if p, padLength, err = readByte(p); err != nil &#123; countError(\"frame_pushpromise_pad_short\") return &#125; &#125; p, pp.PromiseID, err = readUint32(p) //读取PromiseID if err != nil &#123; countError(\"frame_pushpromise_promiseid_short\") return &#125; pp.PromiseID = pp.PromiseID &amp; (1&lt;&lt;31 - 1) if int(padLength) &gt; len(p) &#123; return nil, ConnectionError(ErrCodeProtocol) &#125; pp.headerFragBuf = p[:len(p)-int(padLength)] return pp, nil&#125; 所以二进制可以看出 123456789+--------+--------+--------+--------+| padLength(32) |+--------+--------+--------+--------+| PromiseID(32) |+--------+--------+--------+--------+| payload |+--------+--------+--------+--------+| padding |+--------+--------+--------+--------+ 所以最后组成的结构体 12345type PushPromiseFrame struct &#123; FrameHeader PromiseID uint32 headerFragBuf []byte // not owned&#125; FramePing FramePing 通过 parsePingFrame 进行数据解析 12345678910111213func parsePingFrame(_ *frameCache, fh FrameHeader, countError func(string), payload []byte) (Frame, error) &#123; if len(payload) != 8 &#123; countError(\"frame_ping_length\") return nil, ConnectionError(ErrCodeFrameSize) &#125; if fh.StreamID != 0 &#123; countError(\"frame_ping_has_stream\") return nil, ConnectionError(ErrCodeProtocol) &#125; f := &amp;PingFrame&#123;FrameHeader: fh&#125; copy(f.Data[:], payload) return f, nil&#125; 组成的结构体 1234type PingFrame struct &#123; FrameHeader Data [8]byte&#125; ping帧中很特别的是负载是指定长度的8，通常是时间戳 FrameGoAway FrameGoAway 通过 parseGoAwayFrame 进行数据解析 12345678910111213141516func parseGoAwayFrame(_ *frameCache, fh FrameHeader, countError func(string), p []byte) (Frame, error) &#123; if fh.StreamID != 0 &#123; countError(\"frame_goaway_has_stream\") return nil, ConnectionError(ErrCodeProtocol) &#125; if len(p) &lt; 8 &#123; countError(\"frame_goaway_short\") return nil, ConnectionError(ErrCodeFrameSize) &#125; return &amp;GoAwayFrame&#123; FrameHeader: fh, LastStreamID: binary.BigEndian.Uint32(p[:4]) &amp; (1&lt;&lt;31 - 1), ErrCode: ErrCode(binary.BigEndian.Uint32(p[4:8])), debugData: p[8:], &#125;, nil&#125; GoAway帧用于通知对端，当前的连接即将关闭，以及关闭的原因。LastStreamID 表示这个流之后的所有流都将被关闭 组成的结构体 123456type GoAwayFrame struct &#123; FrameHeader LastStreamID uint32 ErrCode ErrCode debugData []byte&#125; FrameWindowUpdate FrameWindowUpdate 通过 parseWindowUpdateFrame 进行数据解析 12345678910111213141516171819func parseWindowUpdateFrame(_ *frameCache, fh FrameHeader, countError func(string), p []byte) (Frame, error) &#123; if len(p) != 4 &#123; countError(\"frame_windowupdate_bad_len\") return nil, ConnectionError(ErrCodeFrameSize) &#125; inc := binary.BigEndian.Uint32(p[:4]) &amp; 0x7fffffff // mask off high reserved bit if inc == 0 &#123; if fh.StreamID == 0 &#123; countError(\"frame_windowupdate_zero_inc_conn\") return nil, ConnectionError(ErrCodeProtocol) &#125; countError(\"frame_windowupdate_zero_inc_stream\") return nil, streamError(fh.StreamID, ErrCodeProtocol) &#125; return &amp;WindowUpdateFrame&#123; FrameHeader: fh, Increment: inc, &#125;, nil&#125; 组成的结构体 1234type WindowUpdateFrame struct &#123; FrameHeader Increment uint32 // never read with high bit set&#125; FrameContinuation FrameContinuation 通过 parseContinuationFrame 进行数据解析 1234567func parseContinuationFrame(_ *frameCache, fh FrameHeader, countError func(string), p []byte) (Frame, error) &#123; if fh.StreamID == 0 &#123; countError(\"frame_continuation_zero_stream\") return nil, connError&#123;ErrCodeProtocol, \"CONTINUATION frame with stream ID 0\"&#125; &#125; return &amp;ContinuationFrame&#123;fh, p&#125;, nil&#125; 组成的结构体 1234type ContinuationFrame struct &#123; FrameHeader headerFragBuf []byte&#125; 这个将用在帧过大的时候进行数据分帧，这个逻辑将在下面的分帧逻辑进行进一步分析 应用帧 上面其实看的是HTTP2协议中的协议帧，而实际在grpc-go中又做了进一步的区分 帧类型 说明 incomingWindowUpdate 通知发送方更新发送窗口的大小 outgoingWindowUpdate 通知接收方更新接收窗口的大小 incomingSettings 设置帧 outgoingSettings 设置帧 headerFrame 帧的头部信息 registerStream 服务器专用 cleanupStream 针对RST帧 earlyAbortStream incomingGoAway 为客户端服务，客户端一旦接受此帧，帧发送器状态为draining dataFrame 数据帧 ping Ping帧 goAway goAway帧 outFlowControlSizeRequest 这里特殊说明一个数据帧，因为在后续的分帧会说到 1234567type dataFrame struct &#123; streamID uint32 //帧所属的流 endStream bool //该帧是否为该流的最后一帧 h []byte //帧的头部，包含了一些控制信息，例如帧长度、类型、标志等 d []byte //帧的数据负载 onEachWrite func() //在每次写出帧的一部分数据时调用的回调函数&#125; 特殊注意的就是d 字段，表示的数据负载在传输时可能被切割成多个 dataFrame 进行传输。 分帧 当数据过大时，就需要分批发送 TCP 分段(segment):将应用层数据分成多个 TCP 段进行传输，每个 TCP 段都有自己的头部信息 应用层 分包(packet)：分包是指将一个应用层数据包分成多个 IP 数据报进行传输 IP数据包 分片(fragmentation)：分片是指将一个 IP 数据报分成多个较小的数据报进行传输 将发送的请求Body超过限制16KB就能看到分帧了 抓包可以看到其实单个帧的大小是 16384，而他们两个区别是 在第二个中间带上的是0x01 EndStream，表示这个流结束 因为负载不同，所以他们头部Length也不一样，但是最大就是16384 然后直接看看分帧分别是如何发送与接收的 客户端 接着上述 dataFrame的结构说起，它并不是直接切分成多个，而是一部分一部分的切割发送 1234567891011121314151617181920func (t *http2Client) Write(s *Stream, hdr []byte, data []byte, opts *Options) error &#123; //... df := &amp;dataFrame&#123; streamID: s.id, endStream: opts.Last, &#125; if hdr != nil || data != nil &#123; // If it's not an empty data frame. // Add some data to grpc message header so that we can equally // distribute bytes across frames. emptyLen := http2MaxFrameLen - len(hdr) if emptyLen &gt; len(data) &#123; emptyLen = len(data) &#125; hdr = append(hdr, data[:emptyLen]...) data = data[emptyLen:] df.h, df.d = hdr, data //... &#125; return t.controlBuf.put(df)&#125; 分析如下： hdr 是应用层grpc帧的头 长度为 5B(1B是否压缩 + 4B负载长度) data 是应用层grpc帧的负载数据 构建步骤如下： 首先构建一个数据帧(应用层的)，指定流ID以及是否是最后一个流(!cs.desc.ClientStreams) 然后判断数据是否超过http2MaxFrameLen(16384) 接着 **从data中截取一部分放入到 数据帧的h，然后将剩余部分放入到d中！！！！！**所以这里的h与d并不是指的头与负载 最后将整个帧放入到帧缓冲器中 经过获取帧并将帧放入到链表之后到数据帧的处理中 123456789101112131415161718192021222324252627282930313233343536373839404142func (l *loopyWriter) processData() (bool, error) &#123; //... dataItem := str.itl.peek().(*dataFrame) //头部数据帧的指针，并不是出栈 //如果数据帧是空的，那么直接发送结束帧并关闭流 if len(dataItem.h) == 0 &amp;&amp; len(dataItem.d) == 0 &#123; // Empty data frame //... return false, nil &#125; var ( idx int buf []byte ) if len(dataItem.h) != 0 &#123; // data header has not been written out yet. buf = dataItem.h &#125; else &#123; idx = 1 buf = dataItem.d &#125; size := http2MaxFrameLen if len(buf) &lt; size &#123; size = len(buf) &#125; //... 流级别与连接级别流控 //... //发送最大帧以及流控控制下的size if err := l.framer.fr.WriteData(dataItem.streamID, endStream, buf[:size]); err != nil &#123; return false, err &#125; buf = buf[size:] //更新剩余部分 if idx == 0 &#123; dataItem.h = buf &#125; else &#123; dataItem.d = buf &#125; //如果消息发完那么流头部消息发送完毕，则退出该消息 if len(dataItem.h) == 0 &amp;&amp; len(dataItem.d) == 0 &#123; // All the data from that message was written out. str.itl.dequeue() &#125; //... return false, nil&#125; 相当于整个数据一次性放入到流的消息链表中，然后每次从数据中截取一部分给HTTP2进行发送 服务端 服务端则是从结束数据开始看 1234567891011121314151617func (p *parser) recvMsg(maxReceiveMessageSize int) (pf payloadFormat, msg []byte, err error) &#123; if _, err := p.r.Read(p.header[:]); err != nil &#123; return 0, nil, err &#125; pf = payloadFormat(p.header[0]) length := binary.BigEndian.Uint32(p.header[1:]) //...负载长度判断 if _, err := p.r.Read(msg); err != nil &#123; if err == io.EOF &#123; err = io.ErrUnexpectedEOF &#125; return 0, nil, err &#125; return pf, msg, nil&#125; 首先直接读取头部长度 接着校验负载长度是否异常(不能超过 接收最大长度) 接着读取指定长度 到这里其实消息体已经是完整的应用层数据帧了，接着我们看下这个应用层数据帧是如何缓存的以及怎样触发读取的 分帧得从 由于流使用的是多路复用，所以每个流需要使用缓冲保存分帧数据，当判断已经获取到完整的帧的时候，再从缓冲中获取数据即可。这里用到的结构是 recvBuffer 123456type recvBuffer struct &#123; c chan recvMsg //make(chan recvMsg, 1) mu sync.Mutex backlog []recvMsg err error&#125; 获取到数据帧之后，会将帧存入到 recvBuffer 中 123func (s *Stream) write(m recvMsg) &#123; s.buf.put(m)&#125; 然后这里做了一个特殊的设计 1234567891011121314151617181920func (b *recvBuffer) put(r recvMsg) &#123; b.mu.Lock() if b.err != nil &#123; b.mu.Unlock() // An error had occurred earlier, don't accept more // data or errors. return &#125; b.err = r.err if len(b.backlog) == 0 &#123; select &#123; case b.c &lt;- r: b.mu.Unlock() return default: &#125; &#125; b.backlog = append(b.backlog, r) b.mu.Unlock()&#125; 这里有两个操作 当 backlog 为空，则直接写入到隧道中 当 backlog 不为空，则追加到backlog后面 总结 帧分为帧头与负载，帧头长度为9 帧类型为10种，并结合flags进行使用 参考文档 https://blog.csdn.net/u011582922/article/details/120578941 https://www.jianshu.com/p/e22fef60a7f0","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-04-帧接收器","slug":"Grpc/Grpc-04-帧接收器","date":"2022-10-09T14:58:29.000Z","updated":"2023-05-27T10:54:58.119Z","comments":true,"path":"2022/10/09/Grpc/Grpc-04-帧接收器/","link":"","permalink":"http://xboom.github.io/2022/10/09/Grpc/Grpc-04-%E5%B8%A7%E6%8E%A5%E6%94%B6%E5%99%A8/","excerpt":"","text":"看完帧的发送器，看看帧是如何接收的。 带着问题看世界： 上一节中，一个连接对应一个帧发送器，那么一个连接有几个帧接收器。 服务端又是如何存储帧的 实现原理 跟帧发送器一样，帧接收器收到帧之后，也会根据不同类型进行分发，不同的是它并没有缓存的逻辑。 开启读取 在构建 httpClient 客户端连接的时候会同时使用独立协程进行帧的读取 1go t.reader() 每个连接(具体连接)会创建一个 Framer , 这个 Framer 就是实际上负责发送和接收 HTTP2 frame 的接口. 每一个 client 都会对应一个 Framer 来处理来自该 client 的所有 frame, 不管这些 frame 是不是属于一个 stream 1234567891011121314type framer struct &#123; writer *bufWriter //buf输入 fr *http2.Framer //http帧处理器&#125;type bufWriter struct &#123; buf []byte offset int batchSize int conn net.Conn err error onFlush func()&#125; 读取帧 1234567891011121314151617181920212223242526272829303132func (t *http2Client) reader() &#123; defer close(t.readerDone) frame, err := t.framer.fr.ReadFrame() //检查帧的读取是否正常 //... 读取失败则关闭连接，触发重建连接 t.conn.SetReadDeadline(time.Time&#123;&#125;) //设置超时时间为0 //... 更新最近读取帧的时间 //... 这个帧必须是设置帧并更新设置 //循环读取帧 for &#123; t.controlBuf.throttle() frame, err := t.framer.fr.ReadFrame() if t.keepaliveEnabled &#123; atomic.StoreInt64(&amp;t.lastRead, time.Now().UnixNano()) &#125; if err != nil &#123; //如果返回流错误则关闭流，并解析错误信息。仅仅当服务器返回错误信息才会返回 if se, ok := err.(http2.StreamError); ok &#123; //... &#125; continue &#125; else &#123; //否则关闭连接 // Transport error. t.Close(connectionErrorf(true, err, \"error reading from server: %v\", err)) return &#125; &#125; //... 根据帧的类型进行处理 &#125; &#125;&#125; 这里有一个设置超时时间的逻辑需要注意 1t.conn.SetReadDeadline(time.Time&#123;&#125;) //设置成 读取没有超时时间 为什么这样设置就不会有超时时间可以看 SetReadDeadline 源码，当设置 零值的时候读取将不超时 123456789101112131415161718192021// SetReadDeadline sets the deadline for future Read calls// and any currently-blocked Read call.// A zero value for t means Read will not time out.SetReadDeadline(t time.Time) errorfunc (c *conn) SetReadDeadline(t time.Time) error &#123; p := c.Reader.(*pipe) p.mu.Lock() defer p.mu.Unlock() p.rtimer.Stop() p.rtimedout = false if !t.IsZero() &#123; p.rtimer = time.AfterFunc(time.Until(t), func() &#123; p.mu.Lock() defer p.mu.Unlock() p.rtimedout = true p.rwait.Broadcast() &#125;) &#125; return nil&#125; 那么为什么 time.Time{} 被认为是 0，因为 time.Time{} 是 0001-01-01 00:00:00 +0000 UTC，而 IsZero的判断标准就是从起始时间开始的秒与毫秒 123func (t Time) IsZero() bool &#123; return t.sec() == 0 &amp;&amp; t.nsec() == 0&#125; 帧处理 读取到的帧会根据不同类型直接处理 1234567891011121314151617181920switch frame := frame.(type) &#123;case *http2.MetaHeadersFrame: t.operateHeaders(frame)case *http2.DataFrame: t.handleData(frame)case *http2.RSTStreamFrame: t.handleRSTStream(frame)case *http2.SettingsFrame: t.handleSettings(frame, false)case *http2.PingFrame: t.handlePing(frame)case *http2.GoAwayFrame: t.handleGoAway(frame)case *http2.WindowUpdateFrame: t.handleWindowUpdate(frame)default: if logger.V(logLevel) &#123; logger.Errorf(\"transport: http2Client.reader got unhandled frame type %v.\", frame) &#125;&#125; 读取帧 1234567891011121314151617181920212223242526272829303132333435func (fr *Framer) ReadFrame() (Frame, error) &#123; fr.errDetail = nil if fr.lastFrame != nil &#123; fr.lastFrame.invalidate() &#125; fh, err := readFrameHeader(fr.headerBuf[:], fr.r) if err != nil &#123; return nil, err &#125; //超过最大读取格式，那么将报错 if fh.Length &gt; fr.maxReadSize &#123; return nil, ErrFrameTooLarge &#125; payload := fr.getReadBuf(fh.Length) if _, err := io.ReadFull(fr.r, payload); err != nil &#123; return nil, err &#125; f, err := typeFrameParser(fh.Type)(fr.frameCache, fh, fr.countError, payload) if err != nil &#123; if ce, ok := err.(connError); ok &#123; return nil, fr.connError(ce.Code, ce.Reason) &#125; return nil, err &#125; if err := fr.checkFrameOrder(f); err != nil &#123; return nil, err &#125; if fr.logReads &#123; fr.debugReadLoggerf(\"http2: Framer %p: read %v\", fr, summarizeFrame(f)) &#125; if fh.Type == FrameHeaders &amp;&amp; fr.ReadMetaHeaders != nil &#123; return fr.readMetaFrame(f.(*HeadersFrame)) &#125; return f, nil&#125; 参考文档 https://blog.csdn.net/u011582922/article/details/117979181 https://juejin.cn/post/7092975446257565726","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-03-帧发送器","slug":"Grpc/Grpc-03-帧发送器","date":"2022-10-07T14:58:29.000Z","updated":"2023-05-27T10:54:52.069Z","comments":true,"path":"2022/10/07/Grpc/Grpc-03-帧发送器/","link":"","permalink":"http://xboom.github.io/2022/10/07/Grpc/Grpc-03-%E5%B8%A7%E5%8F%91%E9%80%81%E5%99%A8/","excerpt":"","text":"在客户端跟服务器交互过程中，流具体是通过帧来进行数据的发送。这里学习如何发送帧的，整体如下 主要流程： 各种类型的帧通过帧存储器接口(put, executeAndPut)将帧存储到帧缓存器controlBuf里，一个连接里的所有帧共享一个帧缓存器controlBuf 帧发送器开始发送 帧加载接口获取帧的策略，从帧缓存器controlBuf里加载帧，将帧传递给帧分发器 帧分发器根据帧的类型，分发给不同类型的帧处理器 不同类型的帧处理器接收到帧后，根据设置好的规则进行处理 带着问题看世界 帧缓冲区可以无限存储帧吗，满了怎么办，什么时候删，大小可以设置吗 多个流公用一个帧缓冲区吗(显而易见的，因为帧属于不同的流，它们都存入的帧缓冲区中) 帧缓冲区是如何进行消息发送的 实现原理 在之前的流当中，有帧发送器的逻辑，即帧会在发送之前将帧都存储到帧缓冲器中 帧缓冲器 帧缓存器结构 123456789101112131415161718192021type controlBuffer struct &#123; ch chan struct&#123;&#125; done &lt;-chan struct&#123;&#125; mu sync.Mutex consumerWaiting bool list *itemList err error transportResponseFrames int trfChan atomic.Value // chan struct&#123;&#125;&#125;type itemNode struct &#123; it interface&#123;&#125; next *itemNode&#125;type itemList struct &#123; head *itemNode tail *itemNode&#125; 使用 itemList 单向链表存储帧信息 帧进队列 12345678910//数据加入到链表尾部func (il *itemList) enqueue(i interface&#123;&#125;) &#123; n := &amp;itemNode&#123;it: i&#125; if il.tail == nil &#123; //如果没有尾巴，说明链表为空，新节点为链表 il.head, il.tail = n, n return &#125; il.tail.next = n //将新节点加入到链表尾部 il.tail = n&#125; 其实就是将帧添加到链表中，这里并没有限制链表长度，超过负载则是单独处理的 throttle() 帧出队列 12345678910111213141516171819202122232425//弹出链表头部数据func (il *itemList) dequeue() interface&#123;&#125; &#123; if il.head == nil &#123; //如果head为空，说明链表为空 return nil &#125; i := il.head.it //更新头部与尾部指针 il.head = il.head.next if il.head == nil &#123; //注意尾部指针 il.tail = nil &#125; return i&#125;//返回链表头部数据func (il *itemList) peek() interface&#123;&#125; &#123; return il.head.it&#125;//返回链表，缓存器清空func (il *itemList) dequeueAll() *itemNode &#123; h := il.head il.head, il.tail = nil, nil return h&#125; 帧缓存器限制 itemList仅仅单纯用于消息存储，并没有数量限制。但是如果不限制消息数量，可能导致OOM或者消息大面积丢失，其实它单独做了一个限制 当特殊帧达到上限 maxQueuedTransportResponseFrames = 56 之后，会存储一个流控隧道 123456789c.list.enqueue(it)if it.isTransportResponseFrame() &#123; c.transportResponseFrames++ if c.transportResponseFrames == maxQueuedTransportResponseFrames &#123; // We are adding the frame that puts us over the threshold; create // a throttling channel. c.trfChan.Store(make(chan struct&#123;&#125;)) &#125;&#125; 记录条件的帧数量，而不是发送的所有帧都记录进去，符合条件的只有两种帧 incomingSettings 和 ping ，存储了一个隧道，直接通过隧道控制流量 123456789101112func (c *controlBuffer) throttle() &#123; ch, _ := c.trfChan.Load().(chan struct&#123;&#125;) if ch != nil &#123; select &#123; case &lt;-ch: //要么隧道关闭 case &lt;-c.done: //要么超时 &#125; &#125;&#125;//一共只有两个地方用到//http_client.go 读取帧的之后 reader//http_server.go 处理流中帧逻辑 HandleStreams 注意这里用的是等于 c.transportResponseFrames == maxQueuedTransportResponseFrames，每个客户端http_client都有一个 帧缓存器 帧存储与获取 生产生通过一个隧道通知消费者消息的处理，否则消费者阻塞等待 帧获取 12345678910111213141516171819func (c *controlBuffer) get(block bool) (interface&#123;&#125;, error) &#123; for &#123; //... if !c.list.isEmpty() &#123; //... &#125; if !block &#123; //如果不阻塞，直接退出 c.mu.Unlock() return nil, nil &#125; c.consumerWaiting = true //否则等待消息 c.mu.Unlock() select &#123; case &lt;-c.ch: case &lt;-c.done: return nil, ErrConnClosing &#125; &#125;&#125; 帧存储 1234567891011121314151617func (c *controlBuffer) executeAndPut(f func(it interface&#123;&#125;) bool, it cbItem) (bool, error) &#123; var wakeUp bool //... if c.consumerWaiting &#123; //如果消费端还在等待 wakeUp = true c.consumerWaiting = false &#125; c.list.enqueue(it) //... if wakeUp &#123; select &#123; case c.ch &lt;- struct&#123;&#125;&#123;&#125;: //通知消费者有消息来了 default: &#125; &#125; return true, nil&#125; 帧分发器 将帧从帧缓冲器获取出来进行分发，就需要用到帧分发器，执行逻辑帧逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func (l *loopyWriter) run() (err error) &#123; //defer 打印链接 ErrConnClosing for &#123; it, err := l.cbuf.get(true) //阻塞式获取 if err != nil &#123; return err &#125; if err = l.handle(it); err != nil &#123; return err &#125; if _, err = l.processData(); err != nil &#123; return err &#125; gosched := true hasdata: for &#123; it, err := l.cbuf.get(false) //非阻塞式获取 if err != nil &#123; return err &#125; if it != nil &#123; //拿到了消息 if err = l.handle(it); err != nil &#123; return err &#125; if _, err = l.processData(); err != nil &#123; return err &#125; continue hasdata &#125; isEmpty, err := l.processData() if err != nil &#123; return err &#125; if !isEmpty &#123; continue hasdata &#125; if gosched &#123; gosched = false if l.framer.writer.offset &lt; minBatchSize &#123; //如果 runtime.Gosched() //让出CPU时间片 continue hasdata &#125; &#125; l.framer.writer.Flush() //向连接中写入数据 break hasdata &#125; &#125;&#125; 注意： 一开始阻塞式获取， 后来非阻塞式获取 一开始阻塞式获取，因为初始化的时候一般没有消息，那么这里就一直等到有消息了 processData 的作用是将数据写入到连接中，并且如果还有数据则继续写入 有一段让出CPU的操作，意思是：在数据量不足最小批处理大小时，让出CPU时间片，以等待更多数据到达，以尽可能地将数据一次性写入连接中，从而提高性能。 具体来说，当 l.framer.writer.offset（写入缓冲区中的数据量）小于 minBatchSize（最小批处理大小）时，表示当前缓冲区中的数据量不足以一次性写入连接中，这时候我们不应该立即将数据写入连接中，而是应该等待更多的数据到达，以尽可能地将数据一次性写入连接中，从而减少系统调用的次数，提高性能。因此，我们使用 runtime.Gosched() 方法让出CPU时间片，等待更多的数据到达，然后继续循环，直到数据量达到最小批处理大小时，再将数据一次性写入连接中，以提高性能 1234567if gosched &#123; gosched = false if l.framer.writer.offset &lt; minBatchSize &#123; //如果 runtime.Gosched() //让出CPU时间片 continue hasdata &#125;&#125; 总结 首先回答一开始的问题 问题1：帧缓冲区可以无限存储帧吗，满了怎么办，什么时候删，大小可以设置吗 答：理论上可以无限存储(链表)，通过ping与窗口更新帧来控制来控制负载 问题2：多个流公用一个帧缓冲区吗 答：因为帧属于不同的流，它们都公用同一个帧缓冲区中 问题3：帧缓冲区是如何进行消息发送的 答：利用生产者消费者模式进行消息存储与发送 参考文档 https://blog.csdn.net/u011582922/article/details/117979181 https://juejin.cn/post/7092975446257565726","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-02-流","slug":"Grpc/Grpc-02-流","date":"2022-10-05T14:58:29.000Z","updated":"2023-05-27T10:54:46.383Z","comments":true,"path":"2022/10/05/Grpc/Grpc-02-流/","link":"","permalink":"http://xboom.github.io/2022/10/05/Grpc/Grpc-02-%E6%B5%81/","excerpt":"","text":"上一节中，不管是单次调用还是流式调用都会有一个流的概念，这里分析一下流作用与实现原理 回顾协议说明，在 HTTP/2 中，二进制分帧之后，HTTP /2 不再依赖 TCP 链接去实现多流并行, 同域名下所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流。 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。 二进制分帧与grpc分片的区别： 实现方式：TCP 分包是在传输层进行的，即将一个大的数据包分割成多个小的 TCP 包进行传输。而 gRPC 的二进制分帧是在应用层进行的，即将一个大的数据流分割成多个小的 gRPC 帧进行传输。 作用不同：TCP 分包的主要作用是将一个大的数据包分割成多个小的 TCP 包进行传输，以避免网络传输时发生丢包或拥塞等问题。而 gRPC 的二进制分帧的主要作用是将一个大的数据流分割成多个小的 gRPC 帧进行传输，以避免在传输大量数据时需要等待全部数据传输完成的问题。此外，gRPC 的二进制分帧还可以实现多路复用和流控等功能，从而提高数据传输的效率和可靠性。 数据格式不同：TCP 分包只是将一个大的数据包分割成多个小的 TCP 包进行传输，数据格式不变。而 gRPC 的二进制分帧将一个大的数据流分割成多个小的 gRPC 帧进行传输，每个 gRPC 帧都是由消息头和消息体组成的二进制数据，其中消息头包含了帧的元数据信息，如帧的长度、类型、标识符等。 尽管 gRPC 的二进制分帧和 TCP 的分包是不同的概念，但是它们都是为了将一个大的数据流分割成多个小块进行传输，从而提高数据传输的效率和可靠性。 实现原理 代码路径：stream.go 构建一个流 123456789101112131415161718func newClientStream(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, opts ...CallOption) (_ ClientStream, err error) &#123; //...数据搜集 //第一次调用的时候等待需要等待地址解析完成 if err := cc.waitForResolvedAddrs(ctx); err != nil &#123; return nil, err &#125; var mc serviceconfig.MethodConfig var onCommit func() var newStream = func(ctx context.Context, done func()) (iresolver.ClientStream, error) &#123; return newClientStreamWithParams(ctx, desc, cc, method, mc, onCommit, done, opts...) &#125; //... 如果有拦截器，则构建拦截器的流 return newStream(ctx, func() &#123;&#125;)&#125; 拦截器后续再看，这里需要进一步查看创建流的过程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func newClientStreamWithParams(ctx context.Context, desc *StreamDesc, cc *ClientConn, method string, mc serviceconfig.MethodConfig, onCommit, doneFunc func(), opts ...CallOption) (_ iresolver.ClientStream, err error) &#123; c := defaultCallInfo() if mc.WaitForReady != nil &#123; c.failFast = !*mc.WaitForReady &#125; //... callHdr := &amp;transport.CallHdr&#123; Host: cc.authority, Method: method, ContentSubtype: c.contentSubtype, DoneFunc: doneFunc, &#125; //... cs := &amp;clientStream&#123; //... &#125; //... if err := cs.newAttemptLocked(false /* isTransparent */); err != nil &#123; cs.finish(err) return nil, err &#125; op := func(a *csAttempt) error &#123; return a.newStream() &#125; if err := cs.withRetry(op, func() &#123; cs.bufferForRetryLocked(0, op) &#125;); err != nil &#123; cs.finish(err) return nil, err &#125; //... if desc != unaryStreamDesc &#123; //非一元调用 go func() &#123; select &#123; case &lt;-cc.ctx.Done(): cs.finish(ErrClientConnClosing) case &lt;-ctx.Done(): cs.finish(toRPCErr(ctx.Err())) &#125; &#125;() &#125; return cs, nil&#125; 核心有两点 构建流的过程 使用的是 http2_client库，这个时候才会去发送 头帧 1234567891011121314151617181920212223242526272829303132type HeaderField struct &#123; Name, Value string // Sensitive means that this header field should never be // indexed. Sensitive bool&#125;//头帧hdr := &amp;headerFrame&#123; hf: headerFields, //切片 键值对 endStream: false, initStream: func(id uint32) error &#123; t.mu.Lock() if state := t.state; state != reachable &#123; t.mu.Unlock() // Do a quick cleanup. err := error(errStreamDrain) if state == closing &#123; err = ErrConnClosing &#125; cleanup(err) return err &#125; t.activeStreams[id] = s //加入到 t 的activeStreams中 //... t.mu.Unlock() return nil &#125;, onOrphaned: cleanup, wq: s.wq,&#125; 如果非一元调用，那么就需要使用一个独立协程等待流结束，关闭流 发送消息 流构建成功之后，直接调用流的发送消息逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041func (cs *clientStream) SendMsg(m interface&#123;&#125;) (err error) &#123; defer func() &#123; if err != nil &amp;&amp; err != io.EOF &#123; // Call finish on the client stream for errors generated by this SendMsg // call, as these indicate problems created by this client. (Transport // errors are converted to an io.EOF error in csAttempt.sendMsg; the real // error will be returned from RecvMsg eventually in that case, or be // retried.) cs.finish(err) &#125; &#125;() if cs.sentLast &#123; return status.Errorf(codes.Internal, \"SendMsg called after CloseSend\") &#125; //如果是客户端流 if !cs.desc.ClientStreams &#123; cs.sentLast = true &#125; //将数据准备为三部分 头 + 负载 + data hdr, payload, data, err := prepareMsg(m, cs.codec, cs.cp, cs.comp) if err != nil &#123; return err &#125; //... 发送大小限制判断 msgBytes := data // Store the pointer before setting to nil. For binary logging. op := func(a *csAttempt) error &#123; err := a.sendMsg(m, hdr, payload, data) // nil out the message and uncomp when replaying; they are only needed for // stats which is disabled for subsequent attempts. m, data = nil, nil return err &#125; //...使用重试机制进行消息发送 err = cs.withRetry(op, func() &#123; cs.bufferForRetryLocked(len(hdr)+len(payload), op) &#125;) return&#125; 消息准备 将消息体转换为发送前的两部分 123456789101112func prepareMsg(m interface&#123;&#125;, codec baseCodec, cp Compressor, comp encoding.Compressor) (hdr, payload, data []byte, err error) &#123; //消息重试中可以直接使用 if preparedMsg, ok := m.(*PreparedMsg); ok &#123; return preparedMsg.hdr, preparedMsg.payload, preparedMsg.encodedData, nil &#125; //1. 消息序列化 //2. 消息压缩 //3. 消息头部与负载 hdr, payload = msgHeader(data, compData) return hdr, payload, data, nil&#125; 消息发送前的准备最重要的就是 msgHeader 1234567891011121314func msgHeader(data, compData []byte) (hdr []byte, payload []byte) &#123; hdr = make([]byte, headerLen) // headerLen 5字节 //是否压缩 if compData != nil &#123; hdr[0] = byte(compressionMade) data = compData &#125; else &#123; hdr[0] = byte(compressionNone) &#125; //四个字节用来记录数据长度，并进行大小端准换 // Write length of payload into buf binary.BigEndian.PutUint32(hdr[payloadLen:], uint32(len(data))) return hdr, data&#125; 在大小端中，字节的存储顺序是按照其地址的高低位来决定的。 **大端模式（Big Endian）**是指将高位字节存储在低地址处，低位字节存储在高地址处 **小端模式（Little Endian）**则是将低位字节存储在低地址处，高位字节存储在高地址处 举个例子，对于一个 4 字节的整数 0x12345678，它在大端模式下的存储顺序为 0x12 0x34 0x56 0x78，而在小端模式下的存储顺序为 0x78 0x56 0x34 0x12 消息发送 123456789101112131415161718192021222324// Write formats the data into HTTP2 data frame(s) and sends it out. The caller// should proceed only if Write returns nil.func (t *http2Client) Write(s *Stream, hdr []byte, data []byte, opts *Options) error &#123; df := &amp;dataFrame&#123; streamID: s.id, endStream: opts.Last, &#125; if hdr != nil || data != nil &#123; // If it's not an empty data frame. // Add some data to grpc message header so that we can equally // distribute bytes across frames. emptyLen := http2MaxFrameLen - len(hdr) if emptyLen &gt; len(data) &#123; emptyLen = len(data) &#125; hdr = append(hdr, data[:emptyLen]...) data = data[emptyLen:] df.h, df.d = hdr, data // TODO(mmukhi): The above logic in this if can be moved to loopyWriter's data handler. if err := s.wq.get(int32(len(hdr) + len(data))); err != nil &#123; return err &#125; &#125; return t.controlBuf.put(df)&#125; 将hdr + body 组建成数据帧，然后放入帧发送缓存器中 消息的响应 123456789101112131415161718func (cs *clientStream) RecvMsg(m interface&#123;&#125;) error &#123; //... var recvInfo *payloadInfo if cs.binlog != nil &#123; recvInfo = &amp;payloadInfo&#123;&#125; &#125; err := cs.withRetry(func(a *csAttempt) error &#123; return a.recvMsg(m, recvInfo) &#125;, cs.commitAttemptLocked) //... if err != nil || !cs.desc.ServerStreams &#123; // err != nil or non-server-streaming indicates end of stream. cs.finish(err) //... &#125; return err&#125; 实际的读取 123456789101112131415161718192021222324252627func (p *parser) recvMsg(maxReceiveMessageSize int) (pf payloadFormat, msg []byte, err error) &#123; if _, err := p.r.Read(p.header[:]); err != nil &#123; //先读取头 return 0, nil, err &#125; pf = payloadFormat(p.header[0]) length := binary.BigEndian.Uint32(p.header[1:]) //大小端转换 if length == 0 &#123; return pf, nil, nil &#125; if int64(length) &gt; int64(maxInt) &#123; return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max length allowed on current machine (%d vs. %d)\", length, maxInt) &#125; if int(length) &gt; maxReceiveMessageSize &#123; //长度限制 return 0, nil, status.Errorf(codes.ResourceExhausted, \"grpc: received message larger than max (%d vs. %d)\", length, maxReceiveMessageSize) &#125; msg = make([]byte, int(length)) //body读取 if _, err := p.r.Read(msg); err != nil &#123; if err == io.EOF &#123; err = io.ErrUnexpectedEOF &#125; return 0, nil, err &#125; return pf, msg, nil&#125; 与发送类似，发送分为四步 读取帧的头信息(不是头帧，而是每个帧都有头信息) 大小端转换 长度限制 根据帧头的长度阅读消息 然后通过解压缩(非必要)与反序列化转化为对应的消息，那么通信就完成了 接收消息 流的处理 上面是流处理消息的流程，服务端的流是怎么建立的呢？从前面可以知道，客户端建流其实是发送一个头帧 headerFrame 过去，那么，服务端就需要根据头帧建立不同的流 1234567func (s *Server) initServerWorkers() &#123; s.serverWorkerChannels = make([]chan *serverWorkerData, s.opts.numServerWorkers) for i := uint32(0); i &lt; s.opts.numServerWorkers; i++ &#123; s.serverWorkerChannels[i] = make(chan *serverWorkerData) go s.serverWorker(s.serverWorkerChannels[i]) &#125;&#125; 它是通过 serverWorkerChannels 的隧道，来进行流的处理的，而在协程的处理中，首先由一个专门的函数将数据传输到 serverWorkerChannels 隧道中 1234567891011121314151617181920st.HandleStreams(func(stream *transport.Stream) &#123; wg.Add(1) if s.opts.numServerWorkers &gt; 0 &#123; data := &amp;serverWorkerData&#123;st: st, wg: &amp;wg, stream: stream&#125; select &#123; case s.serverWorkerChannels[atomic.AddUint32(&amp;roundRobinCounter, 1)%s.opts.numServerWorkers] &lt;- data: default: // 如果隧道是满的，那么再起一个协程进行流的处理 go func() &#123; s.handleStream(st, stream, s.traceInfo(st, stream)) wg.Done() &#125;() &#125; &#125; else &#123; //单消费者模式 go func() &#123; defer wg.Done() s.handleStream(st, stream, s.traceInfo(st, stream)) &#125;() &#125;&#125;, //...) 那么数据又是如何转换为流的呢 12345678910111213141516171819202122232425262728293031323334353637383940414243func (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) &#123; defer close(t.readerDone) for &#123; t.controlBuf.throttle() frame, err := t.framer.fr.ReadFrame() //循环读取帧 atomic.StoreInt64(&amp;t.lastRead, time.Now().UnixNano()) //记录最近一次读取时间 if err != nil &#123; if se, ok := err.(http2.StreamError); ok &#123; //如果发生流错误 if logger.V(logLevel) &#123; logger.Warningf(\"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se) &#125; t.mu.Lock() s := t.activeStreams[se.StreamID] //拿出来关掉 t.mu.Unlock() if s != nil &#123; t.closeStream(s, true, se.Code, false) &#125; else &#123; t.controlBuf.put(&amp;cleanupStream&#123; streamID: se.StreamID, rst: true, rstCode: se.Code, onWrite: func() &#123;&#125;, &#125;) &#125; continue &#125; //其他错误 t.Close() return &#125; //不同类型帧的处理 switch frame := frame.(type) &#123; case *http2.MetaHeadersFrame: if t.operateHeaders(frame, handle, traceCtx) &#123; t.Close() break &#125; case *http2.DataFrame: t.handleData(frame) //... 不同帧的处理 &#125; &#125;&#125; 注意事项： 流错误只会影响一条流，关闭流之后 发送 cleanupStream的帧，StreamError 的目的就是记录流错误 1234567// StreamError is an error that only affects one stream within an// HTTP/2 connection.type StreamError struct &#123; StreamID uint32 Code ErrCode Cause error // optional additional detail&#125; 来看一下头帧(构建流的时候作用)的操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func (t *http2Server) operateHeaders(frame *http2.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) &#123; // Acquire max stream ID lock for entire duration t.maxStreamMu.Lock() defer t.maxStreamMu.Unlock() streamID := frame.Header().StreamID //流id // 当头部大小超过限制，那么服务端会直接清理掉这条流 if frame.Truncated &#123; t.controlBuf.put(&amp;cleanupStream&#123; streamID: streamID, rst: true, rstCode: http2.ErrCodeFrameSize, onWrite: func() &#123;&#125;, &#125;) return false &#125; //流ID必定是单数，且大于当前最大流ID if streamID%2 != 1 || streamID &lt;= t.maxStreamID &#123; return true &#125; t.maxStreamID = streamID buf := newRecvBuffer() s := &amp;Stream&#123; id: streamID, st: t, buf: buf, fc: &amp;inFlow&#123;limit: uint32(t.initialWindowSize)&#125;, &#125; //... for _, hf := range frame.Fields &#123; //解析 fields &#125; //通过头部field帧 grpc-timeout 设置超时时间 if timeoutSet &#123; s.ctx, s.cancel = context.WithTimeout(t.ctx, timeout) &#125; else &#123; s.ctx, s.cancel = context.WithCancel(t.ctx) &#125; // Attach the received metadata to the context. //跟踪 //... t.activeStreams[streamID] = s //流加入到 活动流 中 //... // Register the stream with loopy. //注册流的帧 t.controlBuf.put(&amp;registerStream&#123; streamID: s.id, wq: s.wq, &#125;) handle(s) //处理流 return false&#125; 问题1：streamID%2 != 1 为什么要限制为单数， 答：防止两端流ID冲突，客户端发起的流具有奇数ID，服务器端发起的流具有偶数ID 可以看到，收到头枕之后会加入到帧缓冲器中，其实也是在服务端会转换为一个 registerStream 的帧放入到帧缓冲器中 消息的处理 首先，消息是通过服务端work对应的隧道进行生产者-消费者处理的 1234567891011121314151617181920func (s *Server) handleStream(t transport.ServerTransport, stream *transport.Stream, trInfo *traceInfo) &#123; sm := stream.Method() //解析 服务与方法 service := sm[:pos] method := sm[pos+1:] srv, knownService := s.services[service] //查找proto定义的犯法进行处理 if knownService &#123; if md, ok := srv.methods[method]; ok &#123; s.processUnaryRPC(t, stream, srv, md, trInfo) return &#125; if sd, ok := srv.streams[method]; ok &#123; s.processStreamingRPC(t, stream, srv, sd, trInfo) return &#125; &#125; //未知的service 或者 method 异常情况处理 //信息记录&#125; 通过 processUnaryRPC 进入到定义中，只需要关心 reply, appErr := md.Handler(info.serviceImpl, ctx, df, s.opts.unaryInt) 执行业务逻辑 总结 每个RPC调用都可以看做一个stream，客户端发送请求会创建一个新的流，服务器也会创建流进行消息响应 流的状态流转 1234567const ( stateIdle streamState = iota stateOpen stateHalfClosedLocal stateHalfClosedRemote stateClosed) 参考文档 https://blog.csdn.net/u011582922/article/details/120578941","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Grpc-01-概述","slug":"Grpc/Grpc-01-概述","date":"2022-10-03T14:58:29.000Z","updated":"2023-05-27T10:54:41.259Z","comments":true,"path":"2022/10/03/Grpc/Grpc-01-概述/","link":"","permalink":"http://xboom.github.io/2022/10/03/Grpc/Grpc-01-%E6%A6%82%E8%BF%B0/","excerpt":"","text":"从协议-http2 知道了 grpc 是基于 http2，优化了通信效率，整体流程如下图所示，细节后续在慢慢增加 带着问题看世界： 基本的使用是怎么样的 使用的基本原理是什么 基本使用 可参考 grpc-go 源码例子 examples/helloworld/helloworld/ proto 123456789101112131415161718192021package helloworld;// The greeting service definition.// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125; rpc SayHello1 (HelloRequest) returns (stream HelloReply) &#123;&#125; rpc SayHello2 (stream HelloRequest) returns (HelloReply) &#123;&#125; rpc SayHello3 (stream HelloRequest) returns (stream HelloReply) &#123;&#125;&#125;// The request message containing the user's name.message HelloRequest &#123; string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123; string message = 1;&#125; 消息除了rpc 是否存在其他的定义方法 单一请求 客户端流式请求 服务端流式请求 双向流失请求 服务端: 1234567891011121314//1. 监听网络端口lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))if err != nil &#123; log.Fatalf(\"failed to listen: %v\", err)&#125;//2. 初始化Grpc服务端s := grpc.NewServer()//3. 注册服务端接口pb.RegisterGreeterServer(s, &amp;server&#123;&#125;)log.Printf(\"server listening at %v\", lis.Addr())//4. 监听请求if err := s.Serve(lis); err != nil &#123; log.Fatalf(\"failed to serve: %v\", err)&#125; 可以看出一共经历了四步 监听TCP连接 初始化grpc服务端 注册服务端业务 grpc处理TCP请求 客户端 12345678910111213141516//1. 建立TCP连接conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials()))if err != nil &#123; log.Fatalf(\"did not connect: %v\", err)&#125;defer conn.Close()//2. 初始化客户端c := pb.NewGreeterClient(conn)// 3. 发送消息并接受响应ctx, cancel := context.WithTimeout(context.Background(), time.Second)defer cancel()r, err := c.SayHello(ctx, &amp;pb.HelloRequest&#123;Name: *name&#125;)if err != nil &#123; log.Fatalf(\"could not greet: %v\", err)&#125; 可以看出一共经历了三步 监听TCP连接 初始化grpc客户端 发送并接受响应 从 proto 生成的文件可以发现 helloworld_grpc.pb.go 描述信息 123456789101112var Greeter_ServiceDesc = grpc.ServiceDesc&#123; ServiceName: \"helloworld.Greeter\", HandlerType: (*GreeterServer)(nil), Methods: []grpc.MethodDesc&#123; &#123; MethodName: \"SayHello\", Handler: _Greeter_SayHello_Handler, &#125;, &#125;, Streams: []grpc.StreamDesc&#123;&#125;, Metadata: \"examples/helloworld/helloworld/helloworld.proto\",&#125; 实现架构 运用上述的代码执行一次通信，可以看到通信的效果是 客户端发送 代码路径(带备注，版本 1.45.0-dev)：read-grpc-go 发送端的使用流程只有三个步骤 完成grpc连接 初始化grpc客户端 开始发送消息 建立grpc连接 从时序图可以看出，完成grpc一共会分为两个步骤，建立TCP连接，设置帧 通过解析器解析地址完成 与解析器的地址建立TCP连接 设置 Setting 帧 构建grpc client 这里不需要初始化什么，直接将 grpc 连接对象封装到 proto 客户端对象中 123func NewGreeterClient(cc grpc.ClientConnInterface) GreeterClient &#123; return &amp;greeterClient&#123;cc&#125;&#125; 方法发送 通过proto生成的 一元调用方法类似 12345678func (c *greeterClient) SayHello(ctx context.Context, in *HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) &#123; out := new(HelloReply) //回复对象 err := c.cc.Invoke(ctx, \"/helloworld.Greeter/SayHello\", in, out, opts...) if err != nil &#123; return nil, err &#125; return out, nil&#125; 核心就是这个 Invoke 函数进行请求调用 12345678910func invoke(ctx context.Context, method string, req, reply interface&#123;&#125;, cc *ClientConn, opts ...CallOption) error &#123; cs, err := newClientStream(ctx, unaryStreamDesc, cc, method, opts...) if err != nil &#123; return err &#125; if err := cs.SendMsg(req); err != nil &#123; return err &#125; return cs.RecvMsg(reply)&#125; 这里就分为了三步： 构建一个新的流 流进行消息发送 流接收消息 也就是说每次进行单一请求的时候都会使用单独的流进行构建，而 unaryStreamDesc 表示的表示这个流是一个单次调用的流 1var unaryStreamDesc = &amp;StreamDesc&#123;ServerStreams: false, ClientStreams: false&#125; 服务端接受 代码路径(带备注，版本 1.45.0-dev)：read-grpc-go 服务端的使用可以分为四个步骤 构建网络监听句柄 构建新grpc server 将第二步的服务指针注册到proto声明的服务注册函数中 开始监听服务 构建grpc server 12345678910111213141516171819202122func NewServer(opt ...ServerOption) *Server &#123; opts := defaultServerOptions for _, o := range opt &#123; o.apply(&amp;opts) &#125; s := &amp;Server&#123; lis: make(map[net.Listener]bool), opts: opts, conns: make(map[string]map[transport.ServerTransport]bool), services: make(map[string]*serviceInfo), quit: grpcsync.NewEvent(), done: grpcsync.NewEvent(), czData: new(channelzData), &#125; //... if s.opts.numServerWorkers &gt; 0 &#123; s.initServerWorkers() &#125; //... return s&#125; 这里有二个地方需要注意的 可选参数 1234//可选参数接口type ServerOption interface &#123; apply(*serverOptions)&#125; 对全局变量进行可选参数应用，这样能够通过在默认的基础上支持外部的自定义应用 12345//应用可选参数opts := defaultServerOptionsfor _, o := range opt &#123; o.apply(&amp;opts)&#125; 当中的 serverOptions 或者说 defaultServerOptions 就是内部默认参数 服务工人 这里会设置服务端的消息消费数量 1234567func (s *Server) initServerWorkers() &#123; s.serverWorkerChannels = make([]chan *serverWorkerData, s.opts.numServerWorkers) for i := uint32(0); i &lt; s.opts.numServerWorkers; i++ &#123; s.serverWorkerChannels[i] = make(chan *serverWorkerData) //阻塞隧道 go s.serverWorker(s.serverWorkerChannels[i]) //对应服务端数量与隧道 &#125;&#125; 每个 serverWorker 都会从ch中读取数据，并再次启动一个 serverWorker 12345678910111213func (s *Server) serverWorker(ch chan *serverWorkerData) &#123; //确保不会所有的 work 不会同时 reset，增加一个随机数 threshold := serverWorkerResetThreshold + grpcrand.Intn(serverWorkerResetThreshold) for completed := 0; completed &lt; threshold; completed++ &#123; data, ok := &lt;-ch if !ok &#123; return &#125; s.handleStream(data.st, data.stream, s.traceInfo(data.st, data.stream)) data.wg.Done() //记住这个 wg.Done() &#125; go s.serverWorker(ch)&#125; 这个地方与固定的 worker有什么区别吗，为什么要在执行一段时间后，再次拉起一个协程work? 参考： https://github.com/golang/go/issues/18138 https://github.com/grpc/grpc-go/pull/3204 https://www.zenlife.tk/goroutine-pool.md?hmsr=toutiao.io&amp;utm_campaign=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io 这样干的原因是:Goroutine 的默认堆栈大小是 2KB。处理GRPC的时候，至少需要4KB的堆栈。导致每次RPCruntime.morestack：重新分配堆栈信息，并复制更改指针指向。为了避免栈中大量信息每次都需要复制，于是go在运行一段时间后就构建一个新的协程 问题是： 栈中的信息不是会被垃圾回收吗，为什么会随着栈的增长而都要复制？ 垃圾回收可能并不及时，导致执行 runtime.morestack 的时候还是需要将历史堆栈信息复制过去 如何避免这种情况 使用指针传递参数，减少递归调用 使用协程池 服务注册 服务注册其实是将 proto 服务对象与 grpc server 对象关联到一起 1234567891011121314151617181920212223242526func (s *Server) register(sd *ServiceDesc, ss interface&#123;&#125;) &#123; s.mu.Lock() //写锁 defer s.mu.Unlock() s.printf(\"RegisterService(%q)\", sd.ServiceName) if s.serve &#123; logger.Fatalf(\"grpc: Server.RegisterService after Server.Serve for %q\", sd.ServiceName) &#125; if _, ok := s.services[sd.ServiceName]; ok &#123; //重复注册没有关系 logger.Fatalf(\"grpc: Server.RegisterService found duplicate service registration for %q\", sd.ServiceName) &#125; info := &amp;serviceInfo&#123; serviceImpl: ss, //具体实现服务(proto 服务实现 对象) methods: make(map[string]*MethodDesc), //指的值 rpc service 的方法 streams: make(map[string]*StreamDesc), //指的是 流 service 的方式 mdata: sd.Metadata, &#125; for i := range sd.Methods &#123; d := &amp;sd.Methods[i] info.methods[d.MethodName] = d //存储 rpc方法 &#125; for i := range sd.Streams &#123; d := &amp;sd.Streams[i] info.streams[d.StreamName] = d //存储 流 方法 &#125; s.services[sd.ServiceName] = info //存储方法&#125; 开启服务 开启服务之后，表示已经能接受消息 12345678910111213141516171819func (s *Server) Serve(lis net.Listener) error &#123; //... var tempDelay time.Duration // how long to sleep on accept failure for &#123; rawConn, err := lis.Accept() //可以监听多个链接 if err != nil &#123; //退避逻辑 一开始 5ms,后续的等待时间 增加2倍，最大值是1s &#125; tempDelay = 0 //单独起协程监听 s.serveWG.Add(1) go func() &#123; s.handleRawConn(lis.Addr().String(), rawConn) s.serveWG.Done() &#125;() &#125;&#125; 注意点： 当监听失败，那么会使用退避逻辑 一开始 5ms，然后每次延时时间 增加2倍,最大值是1s 每个连接都使用单独的协程进行处理 然后在连接成功的链接中，再次使用独立的写成处理每个流 12345678910111213141516171819func (s *Server) handleRawConn(lisAddr string, rawConn net.Conn) &#123; //... rawConn.SetDeadline(time.Now().Add(s.opts.connectionTimeout)) //设置连接超时时间 // Finish handshaking (HTTP2) st := s.newHTTP2Transport(rawConn) rawConn.SetDeadline(time.Time&#123;&#125;) //完成连接，那么连接永远不超时了 if st == nil &#123; return &#125; if !s.addConn(lisAddr, st) &#123; return &#125; go func() &#123; s.serveStreams(st) //使用独立协程处理流(阻塞) s.removeConn(lisAddr, st) &#125;()&#125; 到此就达到了完成连接，开始后续的处理流逻辑 参考文档 https://blog.csdn.net/u011582922/article/details/119834758 https://blog.csdn.net/u011582922/article/details/119944259 https://github.com/grpc/grpc-go/pull/3204 https://www.zenlife.tk/goroutine-pool.md?hmsr=toutiao.io&amp;utm_campaign=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io","categories":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"}],"tags":[{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"}]},{"title":"Go-14-泛型","slug":"Go/Go-14-泛型","date":"2022-09-17T07:45:35.000Z","updated":"2023-07-15T05:37:03.692Z","comments":true,"path":"2022/09/17/Go/Go-14-泛型/","link":"","permalink":"http://xboom.github.io/2022/09/17/Go/Go-14-%E6%B3%9B%E5%9E%8B/","excerpt":"","text":"问题背景 Golang 在1.18 推出了新特性 泛型 Generics 为什么要有泛型 什么是泛型 如何使用泛型 使用的golang版本 go version go1.18.5 linux/amd6 为什么要有泛型从一个经典的问题开始说起，实现计算两数之和的函数 123func Add(a int, b int) int &#123; return a + b&#125; 很快就写完了，那么要计算浮点型或者字符串怎么办？ 1234567func AddFloat32(a float32, b float32) float32 &#123; return a + b&#125;func AddString(a string, b string) string &#123; return a + b&#125; 重复代码太多，有没有办法只写一个函数实现它们 12345678910111213141516171819202122232425func AddInterface(a, b interface&#123;&#125;) interface&#123;&#125; &#123; at := reflect.TypeOf(a) bt := reflect.TypeOf(b) if at != bt &#123; return nil &#125; switch a.(type) &#123; case int: return a.(int) + b.(int) case string: return a.(string) + b.(string) case float32: return a.(float32) + b.(float32) &#125; return nil&#125;func AddInt(a, b int) int &#123; return a + b&#125;fmt.Printf(\"no Generic1 %v \\n\", AddInterface(1, 2)) //3fmt.Printf(\"no Generic2 %v \\n\", AddInterface(\"1\", \"2\")) //\"12\"fmt.Printf(\"no Generic3 %v \\n\", AddInterface(float32(0.1), float32(0.2))) //0.3 上述函数有两个要求： 两者的类型要一样，否则无法相加 需要是函数支持的类型，否则无法相加 同时，也存在两个问题 需要利用反射进行类型判断写起来很繁琐 添加反射操作必然导致性能的降低 1234567891011func BenchmarkAddInterface(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInterface(30, 30) // run AddInterface(30, 30) b.N times &#125;&#125;func BenchmarkAddInt(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInt(30, 30) // AddInt(30, 30) b.N times &#125;&#125; 执行 go test -bench &quot;Add*&quot; 得到如下图所示，性能大约降低20倍 泛型就能很好的解决上述问题 1234567func AddT[T int | string | float32](a, b T) T &#123; return a + b&#125;fmt.Printf(\"Generic1 %v \\n\", AddT(1, 2)) //3fmt.Printf(\"Generic2 %v \\n\", AddT(\"1\", \"2\")) //\"12\"fmt.Printf(\"Generic3 %v \\n\", AddT(float32(0.1), float32(0.2))) //0.3 那么是否泛型的性能又是怎么样 1234567891011121314151617func BenchmarkAddInterface(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInterface(30, 30) // run AddInterface(30, 30) b.N times &#125;&#125;func BenchmarkAddInt(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddInt(30, 30) // AddInt(30, 30) b.N times &#125;&#125;func BenchmarkAddT(b *testing.B)&#123; for n := 0; n &lt; b.N; n++ &#123; AddT(30, 30) // AddT(30, 30) b.N times &#125;&#125; 得到的结果如下图，与固定类型的增加基本没有差别 泛型 泛型的基本概念 函数存在 形参(parameter) 和 实参(argument) 这一基本概念 123456func Add(a int, b int) int &#123; // 变量a,b是函数的形参 \"a int, b int\" 这一串被称为形参列表 return a + b&#125;Add(100,200) // 调用函数时，传入的100和200是实参 为了替代固定的 int 类型，Go 引入了 类型形参 与 类型实参，让一个函数获取了处理多种不同类型数据的能力，这种编程方式就叫做 泛型编程 1234// 假设 T 是类型形参，在定义函数时它的类型是不确定的，类似占位符func AddT[T int | string | float32](a, b T) T &#123; return a + b&#125; 类型形参(Type parameter)：T 表示代表的具体类型并不确定，类似一个占位符(可以使用其他符号代替) 类型约束(Type constraint): 表示类型形参 T 只可以接收 int 或 float32 或 string 类型的实参 类型形参列表(type parameter list): T int | string | float32这一整串定义了所有的类型形参 泛型类型(Generic type)：类型定义中带 类型形参 的类型 类型实参(Type argument)：泛型类型不能直接拿来使用，必须传入类型实参(Type argument) 将其确定为具体的类型之后才可使用。 实例化(Instantiations) ：传入类型实参确定具体类型的操作被称为 实例化(Instantiations) 类型参数的声明紧随： 函数名之后 类型名之后。类型参数通过类型集进行约束。 类型集本质上就是接口，类型集可以作为类型参数的约束，一个接口也可以具有类型参数。 类型集的规则是：同行并集，不同行交集 使用方法 基本类型 123456789101112131415161718type Slice[T int|float32|float64 ] []T // 这里传入了类型实参int，泛型类型Slice[T]被实例化为具体的类型 Slice[int]var a Slice[int] = []int&#123;1, 2, 3&#125; fmt.Printf(\"Type Name: %T\",a) //输出：Type Name: Slice[int]// 传入类型实参float32, 将泛型类型Slice[T]实例化为具体的类型 Slice[string]var b Slice[float32] = []float32&#123;1.0, 2.0, 3.0&#125; fmt.Printf(\"Type Name: %T\",b) //输出：Type Name: Slice[float32]// ✗ 错误。因为变量a的类型为Slice[int]，b的类型为Slice[float32]，两者类型不同a = b // ✗ 错误。string不在类型约束 int|float32|float64 中，不能用来实例化泛型类型var c Slice[string] = []string&#123;\"Hello\", \"World\"&#125; // ✗ 错误。Slice[T]是泛型类型，不可直接使用必须实例化为具体的类型var x Slice[T] = []int&#123;1, 2, 3&#125; map 的使用 123456789// MyMap类型定义了两个类型形参 KEY 和 VALUE。分别为两个形参指定了不同的类型约束// 这个泛型类型的名字叫： MyMap[KEY, VALUE]type MyMap[KEY int | string, VALUE float32 | float64] map[KEY]VALUE // 用类型实参 string 和 flaot64 替换了类型形参 KEY 、 VALUE，泛型类型被实例化为具体的类型：MyMap[string, float64]var a MyMap[string, float64] = map[string]float64&#123; \"jack_score\": 9.6, \"bob_score\": 8.4,&#125; KEY和 VALUE 是类型形参 int|string 是KEY的类型约束， float32|float64 是VALUE的类型约束 KEY int|string, VALUE float32|float64 整个一串文本因为定义了所有形参所以被称为类型形参列表 Map[KEY, VALUE] 是泛型类型，类型的名字就叫 Map[KEY, VALUE] var a MyMap[string, float64] = xx 中的string和float64是类型实参，用于分别替换KEY和VALUE，实例化出了具体的类型 MyMap[string, float64] 其他类型 struct 的使用 123456789101112131415161718192021222324252627// 一个泛型类型的结构体。可用 int 或 sring 类型实例化type MyStruct[T int | string] struct &#123; Name string Data T&#125;//实例话的时候需要指定T的类型var mystruct1 = []MyStruct[int]&#123; &#123; \"1\", 1, &#125;, &#123; \"2\", 2, &#125;,&#125;//✗ 错误。var mystruct2 = []MyStruct[int]&#123; &#123; \"1\", 1, &#125;, &#123; \"2\", \"2\", //cannot use \"2\" (untyped string constant) as int value in struct literal &#125;,&#125; 接口的泛型使用 1234567891011// 一个泛型接口(关于泛型接口在后半部分会详细讲解）type IPrintData[T int | float32 | string] interface &#123; Print(data T)&#125;//实现类型func (m *MyStruct[string]) Print(data string) &#123;&#125;var _ IPrintData[string] = (*MyStruct[string])(nil) chan 的泛型使用 1234// 一个泛型通道，可用类型实参 int 或 string 实例化type MyChan[T int | string] chan Tvar myChan = make(MyChan[int], 1) 类型的互相嵌套 123456789101112131415161718type WowStruct[T int | float32, S []T] struct &#123; Data S MaxValue T MinValue T&#125;var wowStruct1 = &amp;WowStruct[int, []int]&#123; Data: []int&#123;&#125;, MaxValue: 1, MinValue: 1,&#125;//✗ 错误。[]float32 dose not implement []intvar wowStruct2 = &amp;WowStruct[int, []float32]&#123; Data: []float32&#123;&#125;, MaxValue: 1, MinValue: 1,&#125; 其他错误的使用 定义泛型类型的时候，基础类型不能只有类型形参，如下： 12// ✗ 错误，类型形参不能单独使用 cannot use a type parameter as RHS in type declarationtype CommonType[T int|string|float32] T 当类型约束的一些写法会被编译器误认为是表达式时会报错。如下 12345678910//✗ 错误。T *int会被编译器误认为是表达式 T乘以int，而不是int指针type NewType[T *int] []T// 上面代码再编译器眼中：它认为你要定义一个存放切片的数组，数组长度由 T 乘以 int 计算得到type NewType [T * int][]T //✗ 错误。和上面一样，这里不光*被会认为是乘号，| 还会被认为是按位或操作type NewType2[T *int|*float64] []T //✗ 错误。 undeclare name: Ttype NewType2 [T (int)] []T 为了避免这种误解，解决办法就是给类型约束包上 interface{} 或加上逗号消除歧义 12345678type NewType[T interface&#123;*int&#125;] []Ttype NewType2[T interface&#123;*int|*float64&#125;] []T // 如果类型约束中只有一个类型，可以添加个逗号消除歧义type NewType3[T *int,] []T //没错，这样可以而 type NewType3[T *int] []T 不行//✗ 错误。如果类型约束不止一个类型，加逗号是不行的, unexpected comma; expecting ]type NewType4[T *int|*float32,] []T 特殊的泛型类型 12345type Wow[T int | string] intvar a Wow[int] = 123 // 编译正确var b Wow[string] = 123 // 编译正确var c Wow[string] = \"hello\" // 编译错误，因为\"hello\"不能赋值给底层类型int 这里虽然使用了类型形参，但因为类型定义是 type Wow[T int|string] int ，所以无论传入什么类型实参，实例化后的新类型的底层类型都是 int 。所以int类型的数字123可以赋值给变量a和b，但string类型的字符串 “hello” 不能赋值给c，没有什么具体意义，但可以让我们理解泛型类型的实例化的机制 泛型类型的套娃 1234567891011121314151617181920// 先定义个泛型类型 Slice[T]type Slice[T int|string|float32|float64] []T// ✗ 错误。泛型类型Slice[T]的类型约束中不包含uint, uint8type UintSlice[T uint|uint8] Slice[T] // ✓ 正确。基于泛型类型Slice[T]定义了新的泛型类型 FloatSlice[T] 。FloatSlice[T]只接受float32和float64两种类型type FloatSlice[T float32|float64] Slice[T] // ✓ 正确。基于泛型类型Slice[T]定义的新泛型类型 IntAndStringSlice[T]type IntAndStringSlice[T int|string] Slice[T] // ✓ 正确 基于IntAndStringSlice[T]套娃定义出的新泛型类型type IntSlice[T int] IntAndStringSlice[T] // 在map中套一个泛型类型Slice[T]type WowMap[T int|string] map[string]Slice[T]// 在map中套Slice[T]的另一种写法type WowMap2[T Slice[int] | Slice[string]] map[string]T //!!! T 直接代替了 Slice[T] 匿名结构体不支持泛型 123456789testCase := struct &#123; caseName string got int want int&#125;&#123; caseName: \"test OK\", got: 100, want: 100,&#125; 那么匿名结构体能不能使用泛型呢？答案是不能，下面的用法是错误的： 12345678910//✗ 错误。 expected expressiontestCase := struct[T int|string] &#123; caseName string got T want T&#125;[int]&#123; caseName: \"test OK\", got: 100, want: 100,&#125; 在使用泛型的时候我们只能放弃使用匿名结构体，对于很多场景来说这会造成麻烦(最主要麻烦集中在单元测试的时候，为泛型做单元测试会非常麻烦) 泛型receiver 为泛型类型 MySlice[T] 添加了一个计算成员总和的方法 Sum() 。注意观察这个方法的定义 123456789type MySlice[T int | float32] []Tfunc (s MySlice[T]) Sum() T &#123; var sum T for _, value := range s &#123; sum += value &#125; return sum&#125; 首先看receiver (s MySlice[T]) ，直接把类型名称 MySlice[T] 写入了receiver中 然后方法的返回参数我们使用了类型形参 T (方法的接收参数也可以实用类型形参) 在方法的定义中，我们也可以使用类型形参 T (在这个例子里，我们通过 var sum T 定义了一个新的变量 sum ) 泛型类型无论如何都需要先用类型实参实例化!!! 12345var s MySlice[int] = []int&#123;1, 2, 3, 4&#125;fmt.Println(s.Sum()) // 输出：10var s2 MySlice[float32] = []float32&#123;1.0, 2.0, 3.0, 4.0&#125;fmt.Println(s2.Sum()) // 输出：10.0 用类型实参 int 实例化了泛型类型 MySlice[T]，所以泛型类型定义中的所有 T 都被替换为 int，最终我们可以把代码看作下面这样 12345678910type MySlice[int] []int // 实例化后的类型名叫 MyIntSlice[int]// 方法中所有类型形参 T 都被替换为类型实参 intfunc (s MySlice[int]) Sum() int &#123; var sum int for _, value := range s &#123; sum += value &#125; return sum&#125; 基于泛型的队列 1234567891011121314151617181920212223242526// 这里类型约束使用了空接口，代表的意思是所有类型都可以用来实例化泛型类型 Queue[T] (关于接口在后半部分会详细介绍）type Queue[T interface&#123;&#125;] struct &#123; elements []T&#125;// 将数据放入队列尾部func (q *Queue[T]) Put(value T) &#123; q.elements = append(q.elements, value)&#125;// 从队列头部取出并从头部删除对应数据func (q *Queue[T]) Pop() (T, bool) &#123; var value T if len(q.elements) == 0 &#123; return value, true &#125; value = q.elements[0] q.elements = q.elements[1:] return value, len(q.elements) == 0&#125;// 队列大小func (q Queue[T]) Size() int &#123; return len(q.elements)&#125; Queue[T] 因为是泛型类型，所以要使用的话必须实例化，实例化与使用方法如下所示： 123456789101112131415161718192021var q1 Queue[int] // 可存放int类型数据的队列q1.Put(1)q1.Put(2)q1.Put(3)q1.Pop() // 1q1.Pop() // 2q1.Pop() // 3var q2 Queue[string] // 可存放string类型数据的队列q2.Put(\"A\")q2.Put(\"B\")q2.Put(\"C\")q2.Pop() // \"A\"q2.Pop() // \"B\"q2.Pop() // \"C\"var q3 Queue[struct&#123;Name string&#125;] var q4 Queue[[]int] // 可存放[]int切片的队列var q5 Queue[chan int] // 可存放int通道的队列var q6 Queue[io.Reader] // 可存放接口的队列// ...... 动态判断变量的类型 使用接口的时候经常会用到类型断言或 type swith 来确定接口具体的类型，然后对不同类型做出不同的处理 12345678910111213var i interface&#123;&#125; = 123i.(int) // 类型断言// type switchswitch i.(type) &#123; case int: // do something case string: // do something default: // do something &#125;&#125; 那么对于 valut T 这样通过类型形参定义的变量，能不能判断具体类型然后对不同类型做出不同处理呢？答案是不允许的!!! 1234567891011121314151617// invalid operation: cannot use type assertion on type parameter value value (variable of type T constrained by interface&#123;&#125;)func (q *Queue[T]) Put(value T) &#123; value.(int) // 错误。泛型类型定义的变量不能使用类型断言 // 错误。不允许使用type switch 来判断 value 的具体类型 // cannot use type switch on type parameter value value (variable of type T constrained by interface&#123;&#125;) switch value.(type) &#123; case int: // do something case string: // do something default: // do something &#125; // ...&#125; 虽然type switch和类型断言不能用，但我们可通过反射机制达到目的： 12345678910111213141516func (receiver Queue[T]) Put(value T) &#123; // Printf() 可输出变量value的类型(底层就是通过反射实现的) fmt.Printf(\"%T\", value) // 通过反射可以动态获得变量value的类型从而分情况处理 v := reflect.ValueOf(value) switch v.Kind() &#123; case reflect.Int: // do something case reflect.String: // do something &#125; // ...&#125; 为了避免使用反射而选择了泛型，结果到头来又为了一些功能在在泛型中使用反射，当出现这种情况的时候你可能需要重新思考一下，自己的需求是不是真的需要用泛型(毕竟泛型机制本身就很复杂了，再加上反射的复杂度，增加的复杂度并不一定值得) 泛型函数 带类型形参的函数被称为泛型函数，匿名函数不支持泛型 12345fn := func(a, b int) int &#123; return a + b &#125; // 定义了一个匿名函数并赋值给 fn fmt.Println(fn(1, 2)) // 输出: 3 那么Go支不支持匿名泛型函数呢？答案是不能——匿名函数不能自己定义类型形参 1234567// 错误，匿名函数不能自己定义类型实参// function type must have no type parametersfnGeneric := func[T int | float32](a, b T) T &#123; return a + b&#125; fmt.Println(fnGeneric(1, 2)) 但是匿名函数可以使用别处定义好的类型实参，如 123456789func MyFunc[T int | float32 | float64](a, b T) &#123; // 匿名函数可使用已经定义好的类型形参 fn2 := func(i T, j T) T &#123; return i*2 - j*2 &#125; fn2(a, b)&#125; 泛型方法 Go的方法并不支持泛型 12345678type A struct &#123;&#125;// 不支持泛型方法// method must have no type parametersfunc (receiver A) Add[T int | float32 | float64](a T, b T) T &#123; return a + b&#125; 但是因为receiver支持泛型， 所以如果想在方法中使用泛型的话，目前唯一的办法就是曲线救国，迂回地通过receiver使用类型形参 1234567891011121314type A[T int | float32 | float64] struct &#123;&#125;// 方法可以使用类型定义中的形参 T func (receiver A[T]) Add(a T, b T) T &#123; return a + b&#125;// 用法：var a A[int]a.Add(1, 2)var aa A[float32]aa.Add(1.0, 2.0) 泛型使用进阶 复杂的接口 有时候使用泛型编程时，会书写长长的类型约束，如下 12// 一个可以容纳所有int,uint以及浮点类型的泛型切片type Slice[T int | int8 | int16 | int32 | int64 | uint | uint8 | uint16 | uint32 | uint64 | float32 | float64] []T 这种写法是无法忍受也难以维护的，而Go支持将类型约束单独拿出来定义到接口中，从而让代码更容易维护 12345type IntUintFloat interface &#123; int | int8 | int16 | int32 | int64 | uint | uint8 | uint16 | uint32 | uint64 | float32 | float64&#125;type Slice[T IntUintFloat] []T 不过这样的代码依旧不好维护，而接口和接口、接口和普通类型之间也是可以通过 | 进行组合 12345678910111213type Int interface &#123; int | int8 | int16 | int32 | int64&#125;type Uint interface &#123; uint | uint8 | uint16 | uint32&#125;type Float interface &#123; float32 | float64&#125;type Slice[T Int | Uint | Float] []T // 使用 '|' 将多个接口类型组合 在接口里也能直接组合其他接口 12345type SliceElement interface &#123; Int | Uint | Float | string // 组合了三个接口类型并额外增加了一个 string 类型&#125;type Slice[T SliceElement] []T ~ 符号 上面定义的 Slie[T] 虽然可以达到目的，但是有一个缺点： 12345var s1 Slice[int] // 正确 type MyInt int// MyInt does not implement int|string|float32|float64 (possibly missing ~ for int in constraint int|string|float32|float64)var s2 Slice[MyInt] // ✗ 错误。MyInt类型底层类型是int但并不是int类型，不符合 Slice[T] 的类型约束 错误原因：泛型类型 Slice[T] 允许的是 int 作为类型实参，而不是 MyInt （虽然 MyInt 类型底层类型是 int ，但它依旧不是 int 类型）。 为了从根本上解决这个问题，Go新增了一个符号 ~ ，在类型约束中使用类似 ~int 这种写法的话，就代表着不光是 int ，所有以 int 为底层类型的类型也都可用于实例化 1234567891011121314151617181920212223type Int interface &#123; ~int | ~int8 | ~int16 | ~int32 | ~int64&#125;type Uint interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32&#125;type Float interface &#123; ~float32 | ~float64&#125;type Slice[T Int | Uint | Float] []T var s Slice[int] // 正确type MyInt intvar s2 Slice[MyInt] // MyInt底层类型是int，所以可以用于实例化type MyMyInt MyIntvar s3 Slice[MyMyInt] // 正确。MyMyInt 虽然基于 MyInt ，但底层类型也是int，所以也能用于实例化type MyFloat32 float32 // 正确var s4 Slice[MyFloat32] 限制：使用 ~ 时有一定的限制： ~后面的类型不能为接口 ~后面的类型必须为基本类型 1234567type MyInt inttype _ interface &#123; ~[]byte // 正确 ~MyInt // 错误，~后的类型必须为基本类型 invalid use of ~ (underlying type of MyInt is int) ~error // 错误，~后的类型不能为接口 invalid use of ~ (error is an interface)&#125; 接口的变化 从方法集(Method set)到类型集(Type set) 在Go1.18之前，Go官方对 接口(interface) 的定义是：接口是一个方法集(method set) ReadWriter 接口定义了一个接口(方法集)，这个集合中包含了 Read() 和 Write() 这两个方法。所有同时定义了这两种方法的类型被视为实现了这一接口 An interface type specifies a method set called its interface 1234type ReadWriter interface &#123; Read(p []byte) (n int, err error) Write(p []byte) (n int, err error)&#125; 换个角度重新理解 把 ReaderWriter 接口看成代表了一个 类型的集合，所有实现了 Read() Writer() 这两个方法的类型都在接口代表的类型集合当中 接口的定义就从 方法集(method set) 变为了 类型集(type set)。而Go1.18开始就是依据这一点将接口的定义正式更改为了 类型集(Type set) An interface type defines a *type set* (一个接口类型定义了一个类型集) 12345type Float interface &#123; ~float32 | ~float64&#125;type Slice[T Float] []T 接口类型 Float 代表了一个 类型集合， 所有以 float32 或 float64 为底层类型的类型，都在这一类型集之中 而 type Slice[T Float] []T 中， 类型约束 的真正意思是：指定了类型形参可接受的类型集合，只有属于这个集合中的类型才能替换形参用于实例化 123var s Slice[int] // int 属于类型集 Float ，所以int可以作为类型实参// chan int does not implement int|string|float32|float64var s Slice[chan int] // chan int 类型不在类型集 Float 中，所以错误 接口实现(implement)定义的变化 当满足以下条件时，可以说 类型 T 实现了接口 I ( type T implements interface I)： T 不是接口时：类型 T 是接口 I 代表的类型集中的一个成员 (T is an element of the type set of I) T 是接口时： T 接口代表的类型集是 I 代表的类型集的子集(Type set of T is a subset of the type set of I) 类型的并集 一直使用的 | 符号就是求类型的并集( union ) 123type Uint interface &#123; // 类型集 Uint 是 ~uint 和 ~uint8 等类型的并集 ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125; 类型的交集 接口可以不止书写一行，如果一个接口有多行类型定义，那么取它们之间的 交集 12345678910111213141516171819202122type AllInt interface &#123; ~int | ~int8 | ~int16 | ~int32 | ~int64 | ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint32&#125;type Uint interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125;type A interface &#123; // 接口A代表的类型集是 AllInt 和 Uint 的交集 AllInt Uint&#125;type B interface &#123; // 接口B代表的类型集是 AllInt 和 ~int 的交集 AllInt ~int&#125;type C interface &#123; // 接口C代表的类型集是 int 和 ~int 的交集 int ~int&#125; 接口 A 代表的是 AllInt 与 Uint 的 交集，即 ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 接口 B 代表的则是 AllInt 和 ~int 的交集，即 ~int 很显然，~int 和 int 的交集只有int一种类型，所以接口C代表的类型集中只有int一种类型 空集 当多个类型的交集如下面 Bad 这样为空的时候， Bad 这个接口代表的类型集为一个空集： 1234type Bad interface &#123; int float32 &#125; // 类型 int 和 float32 没有相交的类型，所以接口 Bad 代表的类型集为空 没有任何一种类型属于空集。虽然 Bad 这样的写法是可以编译的，但实际上并没有什么意义 空接口和 any 接下来说一个特殊的类型集——空接口 interface{} 。因为，Go1.18开始接口的定义发生了改变，所以 interface{} 的定义也发生了一些变更： 空接口代表了所有类型的集合 所以，对于Go1.18之后的空接口应该这样理解： 虽然空接口内没有写入任何的类型，但它代表的是所有类型的集合，而非一个 空集 类型约束中指定 空接口 的意思是指定了一个包含所有类型的类型集，并不是类型约束限定了只能使用 空接口 来做类型形参 1234567// 空接口代表所有类型的集合。写入类型约束意味着所有类型都可拿来做类型实参type Slice[T interface&#123;&#125;] []Tvar s1 Slice[int] // 正确var s2 Slice[map[string]string] // 正确var s3 Slice[chan int] // 正确var s4 Slice[interface&#123;&#125;] // 正确 因为空接口是一个包含了所有类型的类型集，所以我们经常会用到它。于是，Go1.18开始提供了一个和空接口 interface{} 等价的新关键词 any ，用来使代码更简单： 1type Slice[T any] []T // 代码等价于 type Slice[T interface&#123;&#125;] []T 实际上 any 的定义就位于Go语言的 builtin.go 文件中（参考如下）， any 实际上就是 interaface{}的别名(alias)，两者完全等价 所以从 Go 1.18 开始，所有可以用到空接口的地方其实都可以直接替换为any，如： 123456var s []any // 等价于 var s []interface&#123;&#125;var m map[string]any // 等价于 var m map[string]interface&#123;&#125;func MyPrint(value any)&#123; fmt.Println(value)&#125; comparable(可比较) 和 可排序(ordered) golang中类型的比较情况 Boolean(布尔值)、Integer(整型)、Floating-point(浮点数)、Complex(复数)、String(字符)这些类型毫无疑问可以比较。 Poniter (指针) 可以比较：如果两个指针指向同一个变量，或者两个指针类型相同且值都为 nil，则它们相等。注意，指向不同的零大小变量的指针可能相等，也可能不相等。 Channel (通道)具有可比性：如果两个通道值是由同一个 make 调用创建的，则它们相等 123456789c1 := make(chan int, 2) c2 := make(chan int, 2) c3 := c1 fmt.Println(c3 == c1) // true fmt.Println(c2 == c1) // false Interface (接口值)具有可比性：如果两个接口值具有相同的动态类型和相等的动态值，则它们相等。 当类型 X 的值具有可比性且 X 实现 T 时，非接口类型 X 的值 x 和接口类型 T 的值 t 具有可比性。如果 t 的动态类型与 X 相同且 t 的动态值等于 x，则它们相等。 如果所有字段都具有可比性，则 struct (结构体值)具有可比性：如果它们对应的非空字段相等，则两个结构体值相等。 如果 array(数组)元素类型的值是可比较的，则数组值是可比较的：如果它们对应的元素相等，则两个数组值相等 slice、map、function 这些是不可以比较的，但是也有特殊情况，那就是当他们值是 nil 时，可以与 nil 进行比较 对于一些数据类型，需要在类型约束中限制只接受能 != 和 == 对比的类型，如map： 123// 错误。因为 map 中键的类型必须是可进行 != 和 == 比较的类型// incomparable map key type KEY (missing comparable constraint)type MyMap[KEY any, VALUE any] map[KEY]VALUE 所以Go直接内置了一个叫 comparable 的接口，它代表了所有可用 != 以及 == 对比的类型： 1type MyMap[KEY comparable, VALUE any] map[KEY]VALUE // 正确 comparable 比较容易引起误解的一点是很多人容易把他与可排序搞混淆。可比较指的是 可以执行 != == 操作的类型，并没确保这个类型可以执行大小比较（ &gt;,&lt;,&lt;=,&gt;= ）。如下 1234567891011121314151617type OhMyStruct struct &#123; a int&#125;var a, b OhMyStructa == b // 正确。结构体可使用 == 进行比较a != b // 正确a &gt; b // 错误。结构体不可比大小func Index[E comparable](s []E, v E) int &#123; for i, vs := range s &#123; if v == vs &#123; return i &#125; &#125; return -1&#125; 而可进行大小比较的类型被称为 Orderd 。目前Go语言并没有像 comparable 这样直接内置对应的关键词，所以想要的话需要自己来定义相关接口，比如我们可以参考Go官方包golang.org/x/exp/constraints 如何定义： 123456789101112131415161718192021222324252627// Ordered 代表所有可比大小排序的类型type Ordered interface &#123; Integer | Float | ~string&#125;type Integer interface &#123; Signed | Unsigned&#125;type Signed interface &#123; ~int | ~int8 | ~int16 | ~int32 | ~int64&#125;type Unsigned interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64 | ~uintptr&#125;type Float interface &#123; ~float32 | ~float64&#125;func IsSorted[E constraints.Ordered](x []E) bool &#123; for i := len(x) - 1; i &gt; 0; i-- &#123; if x[i] &lt; x[i-1] &#123; return false &#125; &#125; return true&#125; 这里虽然可以直接使用官方包 golang.org/x/exp/constraints ，但因为这个包属于实验性质的 x 包，今后可能会发生非常大变动，所以并不推荐直接使用 接口两种类型 123456type ReadWriter interface &#123; ~string | ~[]rune Read(p []byte) (n int, err error) Write(p []byte) (n int, err error)&#125; 接口类型 ReadWriter 代表了一个类型集合，所有以 string 或 []rune 为底层类型，并且实现了 Read() Write() 这两个方法的类型都在 ReadWriter 代表的类型集当中，例如： StringReadWriter 存在于接口 ReadWriter 代表的类型集中，而 BytesReadWriter 因为底层类型是 []byte（既不是string也是不[]rune） ，所以它不属于 ReadWriter 代表的类型集 123456789101112131415161718192021// 类型 StringReadWriter 实现了接口 Readwritertype StringReadWriter string func (s StringReadWriter) Read(p []byte) (n int, err error) &#123; // ...&#125;func (s StringReadWriter) Write(p []byte) (n int, err error) &#123; // ...&#125;// 类型BytesReadWriter 没有实现接口 Readwritertype BytesReadWriter []byte func (s BytesReadWriter) Read(p []byte) (n int, err error) &#123; ...&#125;func (s BytesReadWriter) Write(p []byte) (n int, err error) &#123; ...&#125; 定义一个 ReadWriter 类型的接口变量，然后接口变量赋值的时候不光要考虑到方法的实现，还必须考虑到具体底层类型？心智负担也太大了吧。是的，为了解决这个问题也为了保持Go语言的兼容性，Go1.18开始将接口分为了两种类型 基本接口(Basic interface) 一般接口(General interface) 基本接口 接口定义中如果只有方法的话，那么这种接口被称为基本接口(Basic interface)。这种接口就是Go1.18之前的接口，用法也基本和Go1.18之前保持一致。基本接口大致可以用于如下几个地方 最常用的，定义接口变量并赋值 123456type MyError interface &#123; // 接口中只有方法，所以是基本接口 Error() string&#125;// 用法和 Go1.18之前保持一致var err MyError = fmt.Errorf(\"hello world\") 基本接口因为也代表了一个类型集，所以也可用在类型约束中 12// io.Reader 和 io.Writer 都是基本接口，也可以用在类型约束中type MySlice[T io.Reader | io.Writer] []Slice 一般接口(General interface) 如果接口内不光只有方法，还有类型的话，这种接口被称为 一般接口(General interface) 12345678910type Uint interface &#123; // 接口 Uint 中有类型，所以是一般接口 ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125;type ReadWriter interface &#123; // ReadWriter 接口既有方法也有类型，所以是一般接口 ~string | ~[]rune Read(p []byte) (n int, err error) Write(p []byte) (n int, err error)&#125; 一般接口类型不能用来定义变量，只能用于泛型的类型约束中。所以以下的用法是错误的： 12345type Uint interface &#123; ~uint | ~uint8 | ~uint16 | ~uint32 | ~uint64&#125;var uintInf Uint // 错误。Uint是一般接口，只能用于类型约束，不得用于变量定义 这一限制保证了一般接口的使用被限定在了泛型之中，不会影响到Go1.18之前的代码，同时也极大减少了书写代码时的心智负担 泛型接口 所有类型的定义中都可以使用类型形参，所以接口定义自然也可以使用类型形参，观察下面这两个例子 1234567891011type DataProcessor[T any] interface &#123; Process(oriData T) (newData T) Save(data T) error&#125;type DataProcessor2[T any] interface &#123; int | ~struct&#123; Data interface&#123;&#125; &#125; Process(data T) (newData T) Save(data T) error&#125; 因为引入了类型形参，所以这两个接口是泛型类型。而泛型类型要使用的话必须传入类型实参实例化才有意义。所以尝试实例化一下这两个接口。因为 T 的类型约束是 any，所以可以随便挑一个类型来当实参(比如string) 1234567DataProcessor[string]// 实例化之后的接口定义相当于如下所示：type DataProcessor[string] interface &#123; Process(oriData string) (newData string) Save(data string) error&#125; 经过实例化之后就好理解了， DataProcessor[string] 因为只有方法，所以它实际上就是个 基本接口(Basic interface)，这个接口包含两个能处理string类型的方法。像下面这样实现了这两个能处理string类型的方法就算实现了这个接口 12345678910111213141516171819type CSVProcessor struct &#123;&#125;// 注意，方法中 oriData 等的类型是 stringfunc (c CSVProcessor) Process(oriData string) (newData string) &#123; ....&#125;func (c CSVProcessor) Save(oriData string) error &#123; ...&#125;// CSVProcessor实现了接口 DataProcessor[string] ，所以可赋值var processor DataProcessor[string] = CSVProcessor&#123;&#125; processor.Process(\"name,age\\nbob,12\\njack,30\")processor.Save(\"name,age\\nbob,13\\njack,31\")// 错误。CSVProcessor没有实现接口 DataProcessor[int]var processor2 DataProcessor[int] = CSVProcessor&#123;&#125; 再用同样的方法实例化 DataProcessor2[T] 123456789DataProcessor2[string]// 实例化后的接口定义可视为type DataProcessor2[T string] interface &#123; int | ~struct&#123; Data interface&#123;&#125; &#125; Process(data string) (newData string) Save(data string) error&#125; DataProcessor2[string] 因为带有类型并集所以它是 一般接口(General interface)，所以实例化之后的这个接口代表的意思是： 只有实现了 Process(string) string 和 Save(string) error 这两个方法，并且以 int 或 struct{ Data interface{} } 为底层类型的类型才算实现了这个接口 一般接口(General interface) 不能用于变量定义只能用于类型约束，所以接口 DataProcessor2[string] 只是定义了一个用于类型约束的类型集 12345678910111213141516171819202122232425262728293031323334353637383940414243// XMLProcessor 虽然实现了接口 DataProcessor2[string] 的两个方法，但是因为它的底层类型是 []byte，所以依旧是未实现 DataProcessor2[string]type XMLProcessor []bytefunc (c XMLProcessor) Process(oriData string) (newData string) &#123;&#125;func (c XMLProcessor) Save(oriData string) error &#123;&#125;// JsonProcessor 实现了接口 DataProcessor2[string] 的两个方法，同时底层类型是 struct&#123; Data interface&#123;&#125; &#125;。所以实现了接口 DataProcessor2[string]type JsonProcessor struct &#123; Data interface&#123;&#125;&#125;func (c JsonProcessor) Process(oriData string) (newData string) &#123;&#125;func (c JsonProcessor) Save(oriData string) error &#123;&#125;// 错误。DataProcessor2[string]是一般接口不能用于创建变量var processor DataProcessor2[string]// 正确，实例化之后的 DataProcessor2[string] 可用于泛型的类型约束type ProcessorList[T DataProcessor2[string]] []T// 正确，接口可以并入其他接口type StringProcessor interface &#123; DataProcessor2[string] PrintString()&#125;// 错误，带方法的一般接口不能作为类型并集的成员type StringProcessor interface &#123; DataProcessor2[string] | DataProcessor2[[]byte] PrintString()&#125; 接口定义的种种限制规则 Go1.18从开始，在定义类型集(接口)的时候增加了非常多十分琐碎的限制规则，其中很多规则都在之前的内容中介绍过了，但剩下还有一些规则因为找不到好的地方介绍，所以在这里统一介绍下 用 | 连接多个类型的时候，类型之间不能有相交的部分(即必须是不交集): 123456type MyInt int// 错误，MyInt的底层类型是int,和 ~int 有相交的部分type _ interface &#123; ~int | MyInt&#125; 但是相交的类型中是接口的话，则不受这一限制： 12345678910111213type MyInt inttype _ interface &#123; ~int | interface&#123; MyInt &#125; // 正确&#125;type _ interface &#123; interface&#123; ~int &#125; | MyInt // 也正确&#125;type _ interface &#123; interface&#123; ~int &#125; | interface&#123; MyInt &#125; // 也正确&#125; 类型的并集中不能有类型形参 1234567type MyInf[T ~int | ~string] interface &#123; ~float32 | T // 错误。T是类型形参&#125;type MyInf2[T ~int | ~string] interface &#123; T // 错误&#125; 接口不能直接或间接地并入自己 1234567891011121314type Bad interface &#123; Bad // 错误，接口不能直接并入自己&#125;type Bad2 interface &#123; Bad1&#125;type Bad1 interface &#123; Bad2 // 错误，接口Bad1通过Bad2间接并入了自己&#125;type Bad3 interface &#123; ~int | ~string | Bad3 // 错误，通过类型的并集并入了自己&#125; 接口的并集成员个数大于一的时候不能直接或间接并入 comparable 接口 1234567891011121314151617type OK interface &#123; comparable // 正确。只有一个类型的时候可以使用 comparable&#125;type Bad1 interface &#123; []int | comparable // 错误，类型并集不能直接并入 comparable 接口&#125;type CmpInf interface &#123; comparable&#125;type Bad2 interface &#123; chan int | CmpInf // 错误，类型并集通过 CmpInf 间接并入了comparable&#125;type Bad3 interface &#123; chan int | interface&#123;comparable&#125; // 理所当然，这样也是不行的&#125; 带方法的接口(无论是基本接口还是一般接口)，都不能写入接口的并集中： 12345678910111213141516171819type _ interface &#123; ~int | ~string | error // 错误，error是带方法的接口(一般接口) 不能写入并集中&#125;type DataProcessor[T any] interface &#123; ~string | ~[]byte Process(data T) (newData T) Save(data T) error&#125;// 错误，实例化之后的 DataProcessor[string] 是带方法的一般接口，不能写入类型并集type _ interface &#123; ~int | ~string | DataProcessor[string] &#125;type Bad[T any] interface &#123; ~int | ~string | DataProcessor[T] // 也不行&#125; 泛型并不取代Go1.18之前用接口+反射实现的动态类型，在下面情景的时候非常适合使用泛型：当你需要针对不同类型书写同样的逻辑，使用泛型来简化代码是最好的 (比如你想写个队列，写个链表、栈、堆之类的数据结构） 参考链接 https://talkgo.org/t/topic/3582 https://go.dev/doc/tutorial/generics https://segmentfault.com/a/1190000041634906 https://go.dev/blog/go1.18 https://go.googlesource.com/proposal/+/refs/heads/master/design/43651-type-parameters.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-18-init","slug":"Go/Go-18-init","date":"2022-09-12T10:22:10.000Z","updated":"2023-07-15T05:37:28.802Z","comments":true,"path":"2022/09/12/Go/Go-18-init/","link":"","permalink":"http://xboom.github.io/2022/09/12/Go/Go-18-init/","excerpt":"","text":"见下面的代码输出 1234567891011121314151617var WhatIsThe = AnswerToLife()func AnswerToLife() int &#123; // 1 return 42&#125;func init() &#123; // 2 WhatIsThe = 0&#125;func main() &#123; // 3 if WhatIsThe == 0 &#123; fmt.Println(\"It's all a lie.\") &#125;&#125;//It's all a lie. Golang 中 init 的执行顺序 如果一个包导入了其他包，则首先初始化导入的包。 然后初始化当前包的常量。 接下来初始化当前包的变量。 最后，调用当前包的 init() 函数 参考链接 https://stackoverflow.com/questions/24790175/when-is-the-init-function-run https://learnku.com/go/t/47135","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-17-工作区","slug":"Go/Go-17-工作区","date":"2022-09-12T09:45:46.000Z","updated":"2023-07-15T05:37:25.267Z","comments":true,"path":"2022/09/12/Go/Go-17-工作区/","link":"","permalink":"http://xboom.github.io/2022/09/12/Go/Go-17-%E5%B7%A5%E4%BD%9C%E5%8C%BA/","excerpt":"","text":"问题背景 Golang 在1.18 推出了新特性 workspace 工作区模式 为什么要有 workspace 什么是 workspace 怎么使用 workspace 在工作过程中发现，模块的go.mod中添加了很多 replace，其实是为了解决依赖的问题。例如有2个module处于开发阶段，一个是example.com/web，一个是example.com/util。example.com/web依赖example.com/util的函数，当两个模块同时开发过程中，为了 web 模块能够时候用到最新的 util 模块代码，有两种方案 方案1：就是将模块及时提交代码到代码仓库 如果 util 有修改，那么就需要将代码提交到代码仓库，然后打上 tag 然后使用 go get -u 更新 main 中依赖的 util 的版本号(tag) 缺点：每次需要提交代码并更新到最新 方案2：使用go.mod中使用replace指令，见代码 目录结构: 1234567module|-- web| |-- go.mod |-- main.go|-- util |-- go.mod |-- util.go 文件web/web.go : 123456789101112package mainimport ( \"fmt\" \"example.com/util\")func main() &#123; result := util.Add(1, 2) fmt.Println(result)&#125; 文件web/go.mod: 1234567module example.com/webgo 1.18require \"example.com/util\" v0.0.0replace \"example.com/util\" =&gt; ../util 文件util/util.go: 12345package utilfunc Add(a int, b int) int &#123; return a + b&#125; 文件util/go.mod 123module example.com/utilgo 1.18 通过replace指令，使用go命令编译代码的时候，会找到本地的util目录，这样example.com/web就可以使用到本地最新的example.com/util代码 缺点：提交example.com/web这个module的代码到代码仓库时，需要删除最后的replace指令，否则其他开发者下载后会编译报错，因为他们本地可能没有util目录，或者util目录的路径和你的不一样 工作区 为了解决方案2的痛点，Go1.18 新增了工作区模式(workspace mode) 去掉 web/go.mod 中的 replace命令 12345module example.com/webgo 1.18require \"example.com/util\" v0.0.0 在 web 模块中执行 go run hello.go，提示找不到依赖模块 12hello.go:6:2: missing go.sum entry for module providing package example.com/util; to add: go mod download example.com/util 使用 go work init web util指定工作模块 web 与 util，会生成 go.work 文件 123456go 1.18use ( ./util ./web) 其中 use 用于表示需要指定的模块，再次执行 go run hello.go，则运行成功 目录结构 12345678module|-- go.work|-- web| |-- go.mod |-- main.go|-- util |-- go.mod |-- util.go 命令 1234567891011root@13ce5bc74ac3:/code/module-workspace# go help workUsage: go work &lt;command&gt; [arguments]The commands are: edit edit go.work from tools or scripts init initialize workspace file sync sync workspace build list to modules use add modules to workspace fileUse \"go help work &lt;command&gt;\" for more information about a command. 通常情况下，建议不要提交 go.work 文件到 git上，因为它主要用于本地代码开发 推荐在 $GOPATH 路径下执行，生成 go.work 文件 go work init 初始化工作区文件，用于生成go.work工作区文件 go work use 添加新的模块到工作区间 123go work use ./example 添加一个模块到工作区go work use ./example ./example1 添加多个模块到工作区go work use -r ./example 递归 ./example 目录到当前工作区 go work edit 用于编辑 go.work文件 123456//可以使用 edit 命令编辑和手动编辑 go.work 文件效果是相同的 示例:go work edit -fmt go.work 重新格式化 go.work 文件go work edit -replace=github.com/link1st/example=./example go.work 替换代码模块go work edit -dropreplace=github.com/link1st/example 删除替换代码模块go work edit -use=./example go.work 添加新的模块到工作区go work edit -dropuse=./example go.work 从工作区中删除模块 go work sync 将工作区的构建列表同步到工作区的模块 go env GOWORK 查看环境变量，查看当前工作区文件路径 可以排查工作区文件是否设置正确，go.work 路径找不到可以使用 GOWORK 指定，生成多个go.work则指向最后一次生成的路径 replace 指令的语法与 go.mod 中的 replace 指令相同，并优先于 go.mod 文件中的替换。这是主要用于覆盖不同工作区中的冲突替换模块 参考链接 https://go.dev/blog/go1.18 https://go.dev/doc/tutorial/workspaces https://tonybai.com/2021/11/12/go-workspace-mode-in-go-1-18/ https://go.dev/ref/mod#workspaces","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-16-Sync","slug":"Go/Go-16-Sync","date":"2022-09-10T15:06:20.000Z","updated":"2023-07-15T05:37:18.023Z","comments":true,"path":"2022/09/10/Go/Go-16-Sync/","link":"","permalink":"http://xboom.github.io/2022/09/10/Go/Go-16-Sync/","excerpt":"","text":"Golang 源码 sync 包提供了同步操作 互斥锁 sync.Mutext 读写锁 sync.RWMutex 等待组 sync.WaitGroup 单次操作 sync.Once 内存池 sync.Pool 安全map sync.Map 同步条件 sync.Cond Mutex Mutex是一把公平锁（Mutex fairness） Mutex 有两种模式：正常模式 和 饥饿模式 正常模式：在正常模式，请求锁的 goroutines 是按照先进先出的顺序进行排队行程 waiters 的。 调用 Unlock() 方法释放锁资源时，如果发现有等待唤起的 waiter 时，则会将队头的 waiter 唤起。被唤起后调用CAS方法去尝试 修改锁的状态，如果修改成功则表示占有锁的资源成功。 锁一共有三个状态：锁住状态、唤起状态、饥饿状态 饥饿模式：当调用 Unlock()方法释放锁资源的时候，唤起的 waiter G1 需要通过 CAS 操作获取锁资源，如果此时有新请求锁资源的 goroutine G2，那么它们会一起通过 CAS 方法竞争获取锁资源。当不断有G2来进行锁资源争夺，就有可能导致 G1 一直无法获取到 锁资源而饿死，所以Go采用饥饿模式 当 G1 在超过一定时间获取不到资源之后，会在 Unlock 释放锁资源时，直接将锁的资源交给 G1，并且将当前状态改为 饥饿模式 当 G1 获取到锁的所有权时，发现自己是队列中最后一个waiter或者自己等待时间小于1ms，那么锁将切换回正常模式 正常模式拥有非常好的性能表现，因为即使存在阻塞的 waiter，一个goroutine也能够多次获取锁。 饥饿模式对于预防极端的长尾时延(tail latency) 实现原理 代码路径：/go/src/sync/mutex.go 1234567891011type Locker interface &#123; Lock() Unlock()&#125;// Mutex 是互斥锁。// 互斥锁的零值是未锁定的互斥锁。type Mutex struct &#123; state int32 //锁的状态 sema uint32 //信号&#125; Mutex 结构体有两个字段： state: 表示当前互斥锁的状态 sema: 是个信号量变量，用来控制等待 goroutine 的阻塞休眠和唤醒 注意，首次使用后不要进行Mutex的值拷贝，否则 Mutex 会失效 state 状态字段代表多个意思，mutexWaiterShift = 3低三位记录三种状态，剩下的位置，用来表示可以有1&lt;&lt;(32-3)个 Goroutine 等待互斥锁的释放 1234567const ( mutexLocked = 1 &lt;&lt; iota // 表示当前对象锁的状态 0-未锁住，1-已锁住 mutexWoken //表示当前对象是否被唤醒 0-唤醒，1-未唤醒 mutexStarving //表示当前对象是否为饥饿模式 0-正常模式，1为饥饿模式 mutexWaiterShift = iota //从倒数第四位往前的bit位表示在排队等待的goroutine starvationThresholdNs = 1e6 //1ms) 自旋 互斥锁中提到了比较重要的自旋操作 runtime_canSpin：比较保守的自旋，golang中自旋锁并不会一直自旋下去，在runtime包中runtime_canSpin方法做了一些限制 1234567891011121314//判断能否自旋func sync_runtime_canSpin(i int) bool &#123; // 1. i 大于 active_spin = 4 的时候不能自旋 // 2. 机器是单核 的时候不能自旋 // 3. gomaxprocs 小于 (闲置的p + 自旋的m + 1) 的时候不能自旋 // 4. 本地队列不为空 的时候不能自旋 if i &gt;= active_spin || ncpu &lt;= 1 || gomaxprocs &lt;= int32(sched.npidle+sched.nmspinning)+1 &#123; return false &#125; if p := getg().m.p.ptr(); !runqempty(p) &#123; //本地可运行G队列不为空 return false &#125; return true&#125; **runtime_doSpin：**会调用procyield函数，该函数也是汇编语言实现。函数内部循环调用PAUSE指令。PAUSE指令什么都不做，但是会消耗CPU时间，在执行PAUSE指令时，CPU不会对它做不必要的优化 1234567891011func sync_runtime_doSpin() &#123; procyield(active_spin_cnt) //自旋操作&#125;TEXT runtime·procyield(SB),NOSPLIT,$0-0 MOVL cycles+0(FP), AXagain: PAUSE //空命令 SUBL $1, AX JNZ again RET Lock 123456789func (m *Mutex) Lock() &#123; // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) &#123; //...竞争检测 return &#125; // Slow path (outlined so that the fast path can be inlined) m.lockSlow()&#125; 加锁首先会使用 CompareAndSwapInt32 看能不能拿到锁，否则进入到 lockSlow 流程 12//如果*addr中的值 与 old 相等，则将 *addr与 new进行交换，并返回true,否则返回falsefunc CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) lockSlow 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121func (m *Mutex) lockSlow() &#123; var waitStartTime int64 //当前groutine的等待时间 starving := false //当前goroutine是否是饥饿标记 awoke := false //当前goroutine是否是唤醒标记 iter := 0 //当前goroutine自旋次数自旋次数 old := m.state //copy锁的状态为历史状态 for &#123; // 在饥饿模式不进行自旋，锁的所有权会直接移交给waiters。 // 当锁是locked状态并且当前goroutine可以自旋时，开始自旋。 // 当锁是starving状态，就直接false，不自旋 if old&amp;(mutexLocked|mutexStarving) == mutexLocked &amp;&amp; runtime_canSpin(iter) &#123; // 触发自旋是有意义的。 // 尝试设置woken标志来通知unlock，以便不唤起其他阻塞的goroutines if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 &amp;&amp; atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) &#123; // 如果当前goroutine是未唤醒状态，互斥锁也是未唤醒状态，并且互斥锁的waiter数量不等于0， // 就比较锁的最新状态（m.state）和历史状态（old），如果未发生改变，将锁的状态更新为woken。 // 并且设置当前goroutine为awoke状态。 awoke = true &#125; //自旋 runtime_doSpin() //自旋次数增加 iter++ // copy锁的状态为历史状态，自旋期间其他goroutine可能修改了state，所以要更新 old = m.state // 继续尝试自旋 continue &#125; //此时，到这里有两个可能： //1. 锁的状态发生了变化(唤起状态、饥饿状态) //2. 无法继续进行自旋 new := old // copy锁的历史状态为new状态(为什么不是直接获取m.state) // 如果锁的历史状态（old）不是starving状态，将锁的新状态（new）更新为locked状态 if old&amp;mutexStarving == 0 &#123; new |= mutexLocked &#125; // 如果锁的历史状态（old）是locked状态或者是starving状态，将锁的waiter数量加1 if old&amp;(mutexLocked|mutexStarving) != 0 &#123; new += 1 &lt;&lt; mutexWaiterShift //注意 waiter数量怎样记录数量的 &#125; // 如果当前goroutine是starving状态且锁的历史状态（old）是locked状态，将锁的新状态(new）更新为starving状态 if starving &amp;&amp; old&amp;mutexLocked != 0 &#123; new |= mutexStarving &#125; // 如果当前goroutine是awoke状态 if awoke &#123; // 锁的状态是非唤醒状态则直接报错 if new&amp;mutexWoken == 0 &#123; throw(\"sync: inconsistent mutex state\") &#125; // &amp;^ 是 bit clear (AND NOT) // 取消锁的新状态（new）的woken状态标志。 new &amp;^= mutexWoken &#125; // 比较锁的最新状态（m.state）和历史状态（old），如果未发生改变，那么更新为new。 if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; //如果cas更新成功，并且锁的历史状态（old）即不是locked也不是starving，那么结束循环，通过CAS加锁成功。 if old&amp;(mutexLocked|mutexStarving) == 0 &#123; break // locked the mutex with CAS &#125; // 如果之前已经等待，将排在队列前面。 // 当前goroutine是否等待过。 queueLifo := waitStartTime != 0 // 如果开始等待时间为0，更新为当前时间为开始等待时间。 if waitStartTime == 0 &#123; waitStartTime = runtime_nanotime() &#125; // 通过信号量获取锁 // runtime实现代码：https://github.com/golang/go/blob/go1.15.5/src/runtime/sema.go#L69-L72 // runtime信号量获取：https://github.com/golang/go/blob/go1.15.5/src/runtime/sema.go#L98-L153 runtime_SemacquireMutex(&amp;m.sema, queueLifo, 1) // 如果当前goroutine是starving状态或者等待时间大于1ms，更新当前goroutine为starving状态。 starving = starving || runtime_nanotime()-waitStartTime &gt; starvationThresholdNs // 更新锁的历史状态（old） old = m.state //锁是饥饿状态 if old&amp;mutexStarving != 0 &#123; // 如果当前goroutine是唤醒状态并且锁在饥饿模式， // 锁的所有权转移给当前goroutine，但是锁处于不一致的状态中：mutexLocked没有设置 // 并且我们将任然被认为是waiter。这个状态需要被修复。 // 如果锁的历史状态（old）是locked或者woken的，或者waiters的数量不为0，触发锁状态异常。 if old&amp;(mutexLocked|mutexWoken) != 0 || old&gt;&gt;mutexWaiterShift == 0 &#123; throw(\"sync: inconsistent mutex state\") &#125; // 当前goroutine获取锁，waiter数量-1 delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift) // 如果当前goroutine不是starving状态或者锁的历史状态（old）的waiter数量是1，delta减去3。 if !starving || old&gt;&gt;mutexWaiterShift == 1 &#123; // 退出饥饿模式 // 在这里这么做至关重要，还要考虑等待时间。 // 饥饿模式是非常低效率的，一旦两个goroutine将互斥锁切换为饥饿模式，它们便可以无限锁。 delta -= mutexStarving &#125; // 更新锁的状态 atomic.AddInt32(&amp;m.state, delta) break //退出自旋 &#125; // 当前goroutine更新为awoke状态 awoke = true // 当前goroutine自旋次数清零 iter = 0 &#125; else &#123; // 更新锁的历史状态（old） //当在处理 new 状态的时候 锁的状态发生了变化，那么重新复制 old 再进行逻辑判断 old = m.state &#125; &#125;//for end //...竞争判断&#125; Unlock 解锁的前提是加锁 123456789func (m *Mutex) Unlock() &#123; //竞争.. // 如果waiter数量为0，三个标志位去除locked后也为0，那么可以直接解锁了。 new := atomic.AddInt32(&amp;m.state, -mutexLocked) if new != 0 &#123; m.unlockSlow(new) &#125;&#125; unlockSlow 123456789101112131415161718192021222324252627282930func (m *Mutex) unlockSlow(new int32) &#123; if (new+mutexLocked)&amp;mutexLocked == 0 &#123; // 当new不是锁住状态 throw(\"sync: unlock of unlocked mutex\") &#125; //如果不是饥饿模式 if new&amp;mutexStarving == 0 &#123; old := new for &#123; // 如果waiter数量为0，锁的三个标志位任一非0，直接返回 if old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken|mutexStarving) != 0 &#123; return &#125; // 尝试将锁更新为woken状态，如果成功了，就通过信号量去唤醒goroutine new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(&amp;m.state, old, new) &#123; runtime_Semrelease(&amp;m.sema, false, 1) return &#125; old = m.state &#125; &#125; else &#123; // Starving mode: handoff mutex ownership to the next waiter, and yield // our time slice so that the next waiter can start to run immediately. // Note: mutexLocked is not set, the waiter will set it after wakeup. // But mutex is still considered locked if mutexStarving is set, // so new coming goroutines won't acquire it. // 饥饿模式直接手把手交接锁的控制权 runtime_Semrelease(&amp;m.sema, true, 1) &#125;&#125; 总结 互斥锁只有在普通模式才能进入自旋 能够自旋的条件：a. 需要运行在多 CPU 的机器上；b. 当前的Goroutine 为了获取该锁进入自旋的次数小于四次；c. 当前机器上至少存在一个正在运行的处理器 P 并且处理的运行队列为空 一旦当前 Goroutine 能够进入自旋就会调用runtime.sync_runtime_doSpin 和 runtime.procyield 并执行 30 次的 PAUSE 指令，该指令只会占用 CPU 并消耗 CPU 时间 RWMutex 读写互斥锁 sync.RWMutex 是细粒度的互斥锁，它不限制资源的并发读，但是读写、写写操作无法并行执行 实现原理 代码路径：/go/src/sync/rwmutex.go 123456789type RWMutex struct &#123; w Mutex // 复制原子锁的能力 writerSem uint32 // 写等待读完成信号量 readerSem uint32 // 读等待写完成信号量 readerCount int32 // 当前正在执行的读操作的数量 readerWait int32 // 当写操作被阻塞时等待的读操作个数&#125;const rwmutexMaxReaders = 1 &lt;&lt; 30 同理，首次使用后不要进行值拷贝，否则会失效 写锁 12345678910111213func (rw *RWMutex) Lock() &#123; //竞争检查 // 首先调用锁的能力，阻塞后续的写 rw.w.Lock() // Announce to readers there is a pending writer. r := atomic.AddInt32(&amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders // Wait for active readers. if r != 0 &amp;&amp; atomic.AddInt32(&amp;rw.readerWait, r) != 0 &#123; runtime_SemacquireMutex(&amp;rw.writerSem, false, 0) &#125; //竞争检查&#125; 获取写锁时会先阻塞写锁的获取，后阻塞读锁的获取，这种策略能够保证读操作不会被连续的写操作饿死 1234567891011121314151617func (rw *RWMutex) Unlock() &#123; //竞争检查... // Announce to readers there is no active writer. r := atomic.AddInt32(&amp;rw.readerCount, rwmutexMaxReaders) if r &gt;= rwmutexMaxReaders &#123; race.Enable() throw(\"sync: Unlock of unlocked RWMutex\") &#125; // Unblock blocked readers, if any. for i := 0; i &lt; int(r); i++ &#123; runtime_Semrelease(&amp;rw.readerSem, false, 0) &#125; // Allow other writers to proceed. rw.w.Unlock() //竞争检查&#125; 读锁 12345678func (rw *RWMutex) RLock() &#123; //竞争检查... if atomic.AddInt32(&amp;rw.readerCount, 1) &lt; 0 &#123; // A writer is pending, wait for it. runtime_SemacquireMutex(&amp;rw.readerSem, false, 0) &#125; //竞争检查...&#125; 1234567891011121314151617181920func (rw *RWMutex) RUnlock() &#123; //竞争检查... if r := atomic.AddInt32(&amp;rw.readerCount, -1); r &lt; 0 &#123; // Outlined slow-path to allow the fast-path to be inlined rw.rUnlockSlow(r) &#125; //竞争检查...&#125;func (rw *RWMutex) rUnlockSlow(r int32) &#123; if r+1 == 0 || r+1 == -rwmutexMaxReaders &#123; race.Enable() throw(\"sync: RUnlock of unlocked RWMutex\") &#125; // A writer is pending. if atomic.AddInt32(&amp;rw.readerWait, -1) == 0 &#123; // The last reader unblocks the writer. runtime_Semrelease(&amp;rw.writerSem, false, 1) &#125;&#125; WaitGroup WaitGroup 等待一组 goroutine 完成，主 goroutine 调用 Add 设置数量要等待的 goroutine。然后每个 goroutine 完成后运行并调用 Done。同时，等待可用于阻塞，直到所有 goroutine 完成。 代码路径：go/src/sync/waitgroup.go 实现原理 123456789type WaitGroup struct &#123; noCopy noCopy // 64 位值：高 32 位是计数器，低 32 位是等待者的数量。 // 64位原子操作需要64位对齐，但是32位编译器只保证 64 位字段是 32 位对齐的。 // 出于这个原因，在 32 位架构上，需要检查 state()，判断 state1 是否对齐并自动“交换”字段顺序 state1 uint64 state2 uint32&#125; 同理，首次使用后不要进行值拷贝，否则会失效 state 1234567891011func (wg *WaitGroup) state() (statep *uint64, semap *uint32) &#123; if unsafe.Alignof(wg.state1) == 8 || uintptr(unsafe.Pointer(&amp;wg.state1))%8 == 0 &#123; // state1 is 64-bit aligned: nothing to do. return &amp;wg.state1, &amp;wg.state2 &#125; else &#123; // state1 is 32-bit aligned but not 64-bit aligned: this means that // (&amp;state1)+4 is 64-bit aligned. state := (*[3]uint32)(unsafe.Pointer(&amp;wg.state1)) return (*uint64)(unsafe.Pointer(&amp;state[1])), &amp;state[0] &#125;&#125; Add/Done Add(n) 表示添加一个等待 123456789101112131415161718192021222324252627282930func (wg *WaitGroup) Add(delta int) &#123; statep, semap := wg.state() //竞争检查... state := atomic.AddUint64(statep, uint64(delta)&lt;&lt;32) v := int32(state &gt;&gt; 32) w := uint32(state) //竞争检查... if v &lt; 0 &#123; panic(\"sync: negative WaitGroup counter\") &#125; if w != 0 &amp;&amp; delta &gt; 0 &amp;&amp; v == int32(delta) &#123; panic(\"sync: WaitGroup misuse: Add called concurrently with Wait\") &#125; if v &gt; 0 || w == 0 &#123; return &#125; // This goroutine has set counter to 0 when waiters &gt; 0. // Now there can't be concurrent mutations of state: // - Adds must not happen concurrently with Wait, // - Wait does not increment waiters if it sees counter == 0. // Still do a cheap sanity check to detect WaitGroup misuse. if *statep != state &#123; panic(\"sync: WaitGroup misuse: Add called concurrently with Wait\") &#125; // Reset waiters count to 0. *statep = 0 for ; w != 0; w-- &#123; runtime_Semrelease(semap, false, 0) &#125;&#125; Done() 操作其实是 减1 123func (wg *WaitGroup) Done() &#123; wg.Add(-1)&#125; Wait 1234567891011121314151617181920212223242526func (wg *WaitGroup) Wait() &#123; statep, semap := wg.state() //竞争检查... for &#123; state := atomic.LoadUint64(statep) v := int32(state &gt;&gt; 32) w := uint32(state) if v == 0 &#123; // Counter is 0, no need to wait. //竞争检查... return &#125; // Increment waiters count. if atomic.CompareAndSwapUint64(statep, state, state+1) &#123; //竞争检查... runtime_Semacquire(semap) if *statep != 0 &#123; panic(\"sync: WaitGroup is reused before previous Wait has returned\") &#125; //竞争检查... return &#125; &#125;&#125; Once Once 是一个仅执行一次操作的对象 使用例子 12345678var d sync.Oncefor i := 0; i &lt; 5; i++ &#123; d.Do(func() &#123; fmt.Println(\"hello\") &#125;)&#125;//hello 循环执行打印输出操作，不同的是使用 sync.Once 对象执行打印方法，仅执行一次。并发执行效果一致 实现原理 代码路径：/go/src/sync/once.go 1234type Once struct &#123; done uint32 //done 表示动作是否已经执行 m Mutex&#125; 执行函数逻辑 123456789101112131415func (o *Once) Do(f func()) &#123; if atomic.LoadUint32(&amp;o.done) == 0 &#123; //如果不等于0直接退出 // Outlined slow-path to allow inlining of the fast-path. o.doSlow(f) &#125;&#125;func (o *Once) doSlow(f func()) &#123; o.m.Lock() defer o.m.Unlock() //通过获取锁来进行done状态修改 if o.done == 0 &#123; //这里直接使用o.done defer atomic.StoreUint32(&amp;o.done, 1) f() &#125;&#125; 直接使用 o.done == 0 判断是否执行过的原因是上面获取到了写锁 将 atomic.StoreUint32(&amp;o.done, 1)放置在 f() 的后面从而保证当 done 变化，f() 已经执行完毕 在注释中，它提到了另外一种错误的实现方式 12345func (o *Once) Do(f func()) &#123; if atomic.CompareAndSwapUint32(&amp;o.done, 0, 1) &#123; f() &#125;&#125; Do() 保证当它返回时，f 已经完成。此实现不会实现该保证：给定两个同时调用，cas 的获胜者将调用 f，第二个会立即返回，没有等待第一个对 f 的调用完成。 Cond sync.Cond 条件变量用来协调想要访问共享资源的那些 goroutine，当共享资源的状态发生变化的时候，它可以用来通知被互斥锁阻塞的 goroutine 它和互斥锁的区别是：互斥锁 sync.Mutex 通常用来保护临界区和共享资源，条件变量 sync.Cond 用来协调想要访问共享资源的 goroutine 注意点：每个Cond都会关联一个 Lock(*sync.Mutex or *sync.RWMutex)，当修改条件或者调用Wait方法时，必须加锁保护condition 使用案例 123456789101112131415161718192021222324252627282930313233var done = falsefunc read(name string, c *sync.Cond) &#123; c.L.Lock() for !done &#123; log.Println(name, \"starts wait\") c.Wait() log.Println(name, \"end wait\") &#125; c.L.Unlock() log.Println(name, \"end reading\")&#125;func write(name string, c *sync.Cond) &#123; log.Println(name, \"starts writing\") time.Sleep(time.Second) c.L.Lock() done = true c.L.Unlock() log.Println(name, \"wakes all\") c.Broadcast()&#125;func main() &#123; cond := sync.NewCond(&amp;sync.Mutex&#123;&#125;) go read(\"reader1\", cond) go read(\"reader2\", cond) go read(\"reader3\", cond) write(\"writer\", cond) time.Sleep(time.Second * 3)&#125; done 即互斥锁需要保护的条件变量。 read() 调用 Wait() 等待通知，直到 done 为 true。 write() 接收数据，接收完成后，将 done 置为 true，调用 Broadcast() 通知所有等待的协程。 write() 中的暂停了 1s，一方面是模拟耗时，另一方面是确保前面的 3 个 read 协程都执行到 Wait()，处于等待状态。main 函数最后暂停了 3s，确保所有操作执行完毕 运行结果如下； 12345678910112022/09/10 22:40:00 writer starts writing2022/09/10 22:40:00 reader1 starts wait2022/09/10 22:40:00 reader2 starts wait2022/09/10 22:40:00 reader3 starts wait2022/09/10 22:40:01 writer wakes all2022/09/10 22:40:01 reader3 end wait2022/09/10 22:40:01 reader3 end reading2022/09/10 22:40:01 reader1 end wait2022/09/10 22:40:01 reader1 end reading2022/09/10 22:40:01 reader2 end wait2022/09/10 22:40:01 reader2 end reading 注意点： 每次都是 end reading 下一个才会开始执行，原因其实是 c.L.Unlock()，只有释放锁然后其他Wait()中才能获取写锁继续执行 因为 done = false 所以所有的 reader 都进入 wait，但是在构建条件 sync.Cond的时候传入的是锁，那么它又是怎么跟 done 以及 Broadcast 联系在一起的呢？其实没有关系，done 只是用于判断是否重复进入 Wait() 当去掉 done 的变换 123456789func write(name string, c *sync.Cond) &#123; log.Println(name, \"starts writing\") time.Sleep(time.Second) // c.L.Lock() // done = true // c.L.Unlock() log.Println(name, \"wakes all\") c.Broadcast()&#125; 结果变成了 12345678910112022/09/10 22:46:14 writer starts writing2022/09/10 22:46:14 reader3 starts wait2022/09/10 22:46:14 reader1 starts wait2022/09/10 22:46:14 reader2 starts wait2022/09/10 22:46:15 writer wakes all2022/09/10 22:46:15 reader2 end wait2022/09/10 22:46:15 reader2 starts wait2022/09/10 22:46:15 reader3 end wait2022/09/10 22:46:15 reader3 starts wait2022/09/10 22:46:15 reader1 end wait2022/09/10 22:46:15 reader1 starts wait 可以看出来，当 Broadcast的时候 wait 才会返回，由于 done 没有变化，所以reader重新进入了Wait状态 实现原理 1234567891011121314151617181920212223242526272829303132type Cond struct &#123; noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker&#125;type notifyList struct &#123; wait uint32 notify uint32 lock uintptr // key field of the mutex head unsafe.Pointer tail unsafe.Pointer&#125;type copyChecker uintptr//判断是否复制过func (c *copyChecker) check() &#123; if uintptr(*c) != uintptr(unsafe.Pointer(c)) &amp;&amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) &amp;&amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) &#123; panic(\"sync.Cond is copied\") &#125;&#125;func NewCond(l Locker) *Cond &#123; return &amp;Cond&#123;L: l&#125;&#125; 注意： noCopy 只在 go vet 语法检测的时候有效。即使发生拷贝，编译与运行都能正常的运行 Wait Wait()会自动释放c.L，并挂起调用者的goroutine。之后恢复执行，Wait()会在返回时对c.L加锁。除非被Signal或者Broadcast唤醒，否则Wait()不会返回。由于Wait()第一次恢复时，C.L并没有加锁，所以当Wait返回时，调用者通常并不能假设条件为真。取而代之的是, 调用者应该在循环中调用Wait。(简单来说，只要想使用condition，就必须加锁) 1234567func (c *Cond) Wait() &#123; c.checker.check() t := runtime_notifyListAdd(&amp;c.notify) //添加通知 c.L.Unlock() //释放锁 runtime_notifyListWait(&amp;c.notify, t) //等待通知 c.L.Lock() //加锁&#125; 对条件的检查，使用了 for !condition() 而非 if，是因为当前协程被唤醒时，条件不一定符合要求，需要再次 Wait 等待下次被唤醒。为了保险起见，使用 for 能够确保条件符合要求后，再执行后续的代码 123456c.L.Lock()for !condition() &#123; c.Wait()&#125;... make use of condition ...c.L.Unlock() Singal Signal只唤醒1个等待c的goroutine。 调用Signal的时候，可以加锁，也可以不加锁。 1234func (c *Cond) Signal() &#123; c.checker.check() runtime_notifyListNotifyOne(&amp;c.notify)&#125; Boardcase Broadcast会唤醒所有等待c 的 goroutine 1234func (c *Cond) Broadcast() &#123; c.checker.check() runtime_notifyListNotifyAll(&amp;c.notify)&#125; 总结 这里一直出现的 race 到底是干嘛的后续文章说明 这里一直出现的 runtime_Semacquire 是干嘛的后续文章说明 参考链接 https://www.modb.pro/db/170131 https://vearne.cc/archives/680","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-13-style","slug":"Go/Go-13-style","date":"2022-09-10T15:05:52.000Z","updated":"2023-07-15T05:36:54.976Z","comments":true,"path":"2022/09/10/Go/Go-13-style/","link":"","permalink":"http://xboom.github.io/2022/09/10/Go/Go-13-style/","excerpt":"","text":"所有代码都应该通过golint和go vet的检查并无错误。建议将编辑器设置为： 保存时运行 goimports 运行 golint 和 go vet 检查错误 指导原则 指向 interface 的指针 应该将接口作为值进行传递，在这样的传递过程中，实质上传递的底层数据仍然可以是指针。接口实质上在底层用两个字段表示： 一个指向某些特定类型信息的指针。可以将其视为&quot;type&quot;。 数据指针。如果存储的数据是指针，则直接存储。如果存储的数据是一个值，则存储指向该值的指针。 如果希望接口方法修改基础数据，则必须使用指针传递 (将对象指针赋值给接口变量)。 12345678910111213141516type F interface &#123; f()&#125;type S1 struct&#123;&#125;func (s S1) f() &#123;&#125;type S2 struct&#123;&#125;func (s *S2) f() &#123;&#125;// f1.f() 无法修改底层数据// f2.f() 可以修改底层数据，给接口变量 f2 赋值时使用的是对象指针var f1 F = S1&#123;&#125;var f2 F = &amp;S2&#123;&#125; Interface 合理性验证 在编译时验证接口的符合性。这包括： 将实现特定接口的导出类型作为接口 API 的一部分进行检查 实现同一接口的 (导出和非导出) 类型属于实现类型的集合 任何违反接口合理性检查的场景，都会终止编译，并通知给用户 上面 3 条是编译器对接口的检查机制，错误使用接口会在编译期报错。所以可以利用这个机制让部分问题在编译期暴露&gt; BadGood 12345678910// 如果 Handler 没有实现 http.Handler，会在运行时报错type Handler struct &#123; // ...&#125;func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request,) &#123; ...&#125; 123456789101112type Handler struct &#123; // ...&#125;// 用于触发编译期的接口的合理性检查机制// 如果 Handler 没有实现 http.Handler，会在编译期报错var _ http.Handler = (*Handler)(nil)func (h *Handler) ServeHTTP( w http.ResponseWriter, r *http.Request,) &#123; // ...&#125; 如果 *Handler 与 http.Handler 的接口不匹配，那么语句 var _ http.Handler = (*Handler)(nil) 将无法编译通过。 赋值的右边应该是断言类型的零值。对于指针类型（如 *Handler）、切片和映射，这是 nil；对于结构类型，这是空结构。 1234567891011type LogHandler struct &#123; h http.Handler log *zap.Logger&#125;var _ http.Handler = LogHandler&#123;&#125;func (h LogHandler) ServeHTTP( w http.ResponseWriter, r *http.Request,) &#123; // ...&#125; 接收器 (receiver) 与接口 使用值接收器的方法既可以通过值调用，也可以通过指针调用。带指针接收器的方法只能通过指针或 addressable values 调用。 例如， 12345678910111213141516171819202122232425type S struct &#123; data string&#125;func (s S) Read() string &#123; return s.data&#125;func (s *S) Write(str string) &#123; s.data = str&#125;sVals := map[int]S&#123;1: &#123;\"A\"&#125;&#125;// 通过值只能调用 ReadsVals[1].Read()// 这不能编译通过：Cannot call pointer method on 'sVals[1]'// sVals[1].Write(\"test\")sPtrs := map[int]*S&#123;1: &#123;\"A\"&#125;&#125;// 通过指针既可以调用 Read，也可以调用 Write 方法sPtrs[1].Read()sPtrs[1].Write(\"test\") 注意：map[int]*S{1: {&quot;A&quot;}} 与 map[int]s{1: {&quot;A&quot;}} 都是用 {&quot;A&quot;} 进行初始化 类似的，即使方法有了值接收器，也同样可以用指针接收器来满足接口。 123456789101112131415161718192021222324type F interface &#123; f()&#125;type S1 struct&#123;&#125;func (s S1) f() &#123;&#125;type S2 struct&#123;&#125;func (s *S2) f() &#123;&#125;s1Val := S1&#123;&#125;s1Ptr := &amp;S1&#123;&#125;s2Val := S2&#123;&#125;s2Ptr := &amp;S2&#123;&#125;var i Fi = s1Vali = s1Ptri = s2Ptr// 下面代码无法通过编译。因为 s2Val 是一个值，而 S2 的 f 方法中没有使用值接收器// i = s2Val Effective Go 中有一段关于 pointers vs. values 的精彩讲解。 补充： 一个类型可以有值接收器方法集和指针接收器方法集 值接收器方法集是指针接收器方法集的子集，反之不是 规则 值对象只可以使用值接收器方法集 指针对象可以使用 值接收器方法集 + 指针接收器方法集 接口的匹配 (或者叫实现) 类型实现了接口的所有方法，叫匹配 具体的讲，要么是类型的值方法集匹配接口，要么是指针方法集匹配接口 具体的匹配分两种： 值方法集和接口匹配 给接口变量赋值的不管是值还是指针对象，都 ok，因为都包含值方法集 指针方法集和接口匹配 只能将指针对象赋值给接口变量，因为只有指针方法集和接口匹配 如果将值对象赋值给接口变量，会在编译期报错 (会触发接口合理性检查机制) 为啥 i = s2Val 会报错，因为值方法集和接口不匹配。 零值 Mutex 是有效的 零值 sync.Mutex 和 sync.RWMutex 是有效的。所以指向 mutex 的指针基本是不必要的。 BadGood 12mu := new(sync.Mutex)mu.Lock() 12var mu sync.Mutexmu.Lock() 如果使用结构体指针，mutex 应该作为结构体的非指针字段。即使该结构体不被导出，也不要直接把 mutex 嵌入到结构体中。 BadGood 123456789101112131415161718type SMap struct &#123; sync.Mutex data map[string]string&#125;func NewSMap() *SMap &#123; return &amp;SMap&#123; data: make(map[string]string), &#125;&#125;func (m *SMap) Get(k string) string &#123; m.Lock() defer m.Unlock() return m.data[k]&#125; 123456789101112131415161718type SMap struct &#123; mu sync.Mutex data map[string]string&#125;func NewSMap() *SMap &#123; return &amp;SMap&#123; data: make(map[string]string), &#125;&#125;func (m *SMap) Get(k string) string &#123; m.mu.Lock() defer m.mu.Unlock() return m.data[k]&#125; Mutex 字段， Lock 和 Unlock 方法是 SMap 导出的 API 中不刻意说明的一部分。 mutex 及其方法是 SMap 的实现细节，对其调用者不可见。 在边界处拷贝 Slices 和 Maps slices 和 maps 包含了指向底层数据的指针，因此在需要复制它们时要特别注意。 接收 Slices 和 Maps 请记住，当 map 或 slice 作为函数参数传入时，如果存储了对它们的引用，则用户可以对其进行修改。 Bad Good 123456789func (d *Driver) SetTrips(trips []Trip) &#123; d.trips = trips&#125;trips := ...d1.SetTrips(trips)// 要修改 d1.trips 吗？trips[0] = ... 12345678910func (d *Driver) SetTrips(trips []Trip) &#123; d.trips = make([]Trip, len(trips)) copy(d.trips, trips)&#125;trips := ...d1.SetTrips(trips)// 这里修改 trips[0]，但不会影响到 d1.tripstrips[0] = ... 返回 slices 或 maps 同样，请注意用户对暴露内部状态的 map 或 slice 的修改。 BadGood 123456789101112131415161718type Stats struct &#123; mu sync.Mutex counters map[string]int&#125;// Snapshot 返回当前状态。func (s *Stats) Snapshot() map[string]int &#123; s.mu.Lock() defer s.mu.Unlock() return s.counters&#125;// snapshot 不再受互斥锁保护// 因此对 snapshot 的任何访问都将受到数据竞争的影响// 影响 stats.counterssnapshot := stats.Snapshot() 12345678910111213141516171819type Stats struct &#123; mu sync.Mutex counters map[string]int&#125;func (s *Stats) Snapshot() map[string]int &#123; s.mu.Lock() defer s.mu.Unlock() result := make(map[string]int, len(s.counters)) for k, v := range s.counters &#123; result[k] = v &#125; return result&#125;// snapshot 现在是一个拷贝snapshot := stats.Snapshot() 使用 defer 释放资源 使用 defer 释放资源，诸如文件和锁。 BadGood 12345678910111213p.Lock()if p.count &lt; 10 &#123; p.Unlock() return p.count&#125;p.count++newCount := p.countp.Unlock()return newCount// 当有多个 return 分支时，很容易遗忘 unlock 1234567891011p.Lock()defer p.Unlock()if p.count &lt; 10 &#123; return p.count&#125;p.count++return p.count// 更可读 Defer 的开销非常小，只有可以证明函数执行时间处于纳秒级的程度时，才应避免这样做。使用 defer 提升可读性是值得的，因为使用它们的成本微不足道。尤其适用于那些不仅仅是简单内存访问的较大的方法，在这些方法中其他计算的资源消耗远超过 defer。 Channel 的 size 要么是 1，要么是无缓冲的 channel 通常 size 应为 1 或是无缓冲的。默认情况下，channel 是无缓冲的，其 size 为零。任何其他尺寸都必须经过严格的审查。需要考虑如何确定大小，考虑是什么阻止了 channel 在高负载下和阻塞写时的写入，以及当这种情况发生时系统逻辑有哪些变化。(翻译解释：按照原文意思是需要界定通道边界，竞态条件，以及逻辑上下文梳理) BadGood 12// 应该足以满足任何情况！c := make(chan int, 64) 1234// 大小：1c := make(chan int, 1) // 或者// 无缓冲 channel，大小为 0c := make(chan int) 枚举从 1 开始 在 Go 中引入枚举的标准方法是声明一个自定义类型和一个使用了 iota 的 const 组。由于变量的默认值为 0，因此通常应以非零值开头枚举。 BadGood 123456789type Operation intconst ( Add Operation = iota Subtract Multiply)// Add=0, Subtract=1, Multiply=2 123456789type Operation intconst ( Add Operation = iota + 1 Subtract Multiply)// Add=1, Subtract=2, Multiply=3 在某些情况下，使用零值是有意义的（枚举从零开始），例如，当零值是理想的默认行为时。 123456789type LogOutput intconst ( LogToStdout LogOutput = iota LogToFile LogToRemote)// LogToStdout=0, LogToFile=1, LogToRemote=2 使用 time 处理时间 时间处理很复杂。关于时间的错误假设通常包括以下几点。 一天有 24 小时 一小时有 60 分钟 一周有七天 一年 365 天 还有更多 例如，1 表示在一个时间点上加上 24 小时并不总是产生一个新的日历日。 因此，在处理时间时始终使用 &quot;time&quot; 包，因为它有助于以更安全、更准确的方式处理这些不正确的假设。 使用 time.Time 表达瞬时时间 在处理时间的瞬间时使用 time.Time，在比较、添加或减去时间时使用 time.Time 中的方法。 BadGood 123func isActive(now, start, stop int) bool &#123; return start &lt;= now &amp;&amp; now &lt; stop&#125; 123func isActive(now, start, stop time.Time) bool &#123; return (start.Before(now) || start.Equal(now)) &amp;&amp; now.Before(stop)&#125; 使用 time.Duration 表达时间段 在处理时间段时使用 time.Duration . BadGood 1234567func poll(delay int) &#123; for &#123; // ... time.Sleep(time.Duration(delay) * time.Millisecond) &#125;&#125;poll(10) // 是几秒钟还是几毫秒？ 1234567func poll(delay time.Duration) &#123; for &#123; // ... time.Sleep(delay) &#125;&#125;poll(10*time.Second) 回到第一个例子，在一个时间瞬间加上 24 小时，用于添加时间的方法取决于意图。如果想要下一个日历日 (当前天的下一天) 的同一个时间点，应该使用 Time.AddDate。但是，如果想保证某一时刻比前一时刻晚 24 小时，应该使用 Time.Add。 12newDay := t.AddDate(0 /* years */, 0 /* months */, 1 /* days */)maybeNewDay := t.Add(24 * time.Hour) 对外部系统使用 time.Time 和 time.Duration 尽可能在与外部系统的交互中使用 time.Duration 和 time.Time 例如 : Command-line 标志: flag 通过 time.ParseDuration 支持 time.Duration JSON: encoding/json 通过其 UnmarshalJSON method 方法支持将 time.Time 编码为 RFC 3339 字符串 SQL: database/sql 支持将 DATETIME 或 TIMESTAMP 列转换为 time.Time，如果底层驱动程序支持则返回 YAML: gopkg.in/yaml.v2 支持将 time.Time 作为 RFC 3339 字符串，并通过 time.ParseDuration 支持 time.Duration。 当不能在这些交互中使用 time.Duration 时，请使用 int 或 float64，并在字段名称中包含单位。 例如，由于 encoding/json 不支持 time.Duration，因此该单位包含在字段的名称中。 BadGood 1234// &#123;\"interval\": 2&#125;type Config struct &#123; Interval int `json:\"interval\"`&#125; 1234// &#123;\"intervalMillis\": 2000&#125;type Config struct &#123; IntervalMillis int `json:\"intervalMillis\"`&#125; 当在这些交互中不能使用 time.Time 时，除非达成一致，否则使用 string 和 RFC 3339 中定义的格式时间戳。默认情况下，Time.UnmarshalText 使用此格式，并可通过 time.RFC3339 在 Time.Format 和 time.Parse 中使用。 尽管这在实践中并不成问题，但请记住，&quot;time&quot; 包不支持解析闰秒时间戳（8728），也不在计算中考虑闰秒（15190）。如果比较两个时间瞬间，则差异将不包括这两个瞬间之间可能发生的闰秒。 Errors 错误类型 声明错误的选项很少。在选择最适合的用例的选项之前，请考虑以下事项。 调用者是否需要匹配错误以便他们可以处理它？ 如果是，必须通过声明顶级错误变量或自定义类型来支持 errors.Is 或 errors.As 函数。 错误消息是否为静态字符串，还是需要上下文信息的动态字符串？ 如果是静态字符串，可以使用 errors.New，但对于后者，必须使用 fmt.Errorf 或自定义错误类型。 是否正在传递由下游函数返回的新错误？ 如果是这样，请参阅错误包装部分。 错误匹配？ 错误消息 指导 No static errors.New No dynamic fmt.Errorf Yes static top-level var with errors.New Yes dynamic custom error type 例如，使用 errors.New 表示带有静态字符串的错误。 如果调用者需要匹配并处理此错误，则将此错误导出为变量以支持将其与 errors.Is 匹配。 无错误匹配错误匹配 123456789101112// package foofunc Open() error &#123; return errors.New(\"could not open\")&#125;// package barif err := foo.Open(); err != nil &#123; // Can't handle the error. panic(\"unknown error\")&#125; 1234567891011121314151617// package foovar ErrCouldNotOpen = errors.New(\"could not open\")func Open() error &#123; return ErrCouldNotOpen&#125;// package barif err := foo.Open(); err != nil &#123; if errors.Is(err, foo.ErrCouldNotOpen) &#123; // handle the error &#125; else &#123; panic(\"unknown error\") &#125;&#125; 对于动态字符串的错误， 如果调用者不需要匹配它，则使用 fmt.Errorf， 如果调用者确实需要匹配它，则自定义 error。 无错误匹配错误匹配 123456789101112// package foofunc Open(file string) error &#123; return fmt.Errorf(\"file %q not found\", file)&#125;// package barif err := foo.Open(\"testfile.txt\"); err != nil &#123; // Can't handle the error. panic(\"unknown error\")&#125; 12345678910111213141516171819202122232425// package footype NotFoundError struct &#123; File string&#125;func (e *NotFoundError) Error() string &#123; return fmt.Sprintf(\"file %q not found\", e.File)&#125;func Open(file string) error &#123; return &amp;NotFoundError&#123;File: file&#125;&#125;// package barif err := foo.Open(\"testfile.txt\"); err != nil &#123; var notFound *NotFoundError if errors.As(err, &amp;notFound) &#123; // handle the error &#125; else &#123; panic(\"unknown error\") &#125;&#125; 请注意，如果从包中导出错误变量或类型， 它们将成为包的公共 API 的一部分。 错误包装 如果调用其他方法时出现错误, 通常有三种处理方式可以选择： 将原始错误原样返回 使用 fmt.Errorf 搭配 %w 将错误添加进上下文后返回 使用 fmt.Errorf 搭配 %v 将错误添加进上下文后返回 如果没有要添加的其他上下文，则按原样返回原始错误。 这将保留原始错误类型和消息。 这非常适合底层错误消息有足够的信息来追踪它来自哪里的错误。 否则，尽可能在错误消息中添加上下文 这样就不会出现诸如“连接被拒绝”之类的模糊错误， 会收到更多有用的错误，例如“调用服务 foo：连接被拒绝”。 使用 fmt.Errorf 为错误添加上下文，根据调用者是否应该能够匹配和提取根本原因，在 %w 或 %v 动词之间进行选择。 如果调用者应该可以访问底层错误，请使用 %w。 对于大多数包装错误，这是一个很好的默认值， 但请注意，调用者可能会开始依赖此行为。因此，对于包装错误是已知var或类型的情况，请将其作为函数契约的一部分进行记录和测试。 使用 %v 来混淆底层错误。 调用者将无法匹配它，但如果需要，可以在将来切换到 %w。 在为返回的错误添加上下文时，通过避免使用&quot;failed to&quot;之类的短语来保持上下文简洁，当错误通过堆栈向上渗透时，它会一层一层被堆积起来： BadGood 12345s, err := store.New()if err != nil &#123; return fmt.Errorf( \"failed to create new store: %w\", err)&#125; 12345s, err := store.New()if err != nil &#123; return fmt.Errorf( \"new store: %w\", err)&#125; 1failed to x: failed to y: failed to create new store: the error 1x: y: new store: the error 然而，一旦错误被发送到另一个系统，应该清楚消息是一个错误（例如err 标签或日志中的&quot;Failed&quot;前缀）。 另见 不要只检查错误，优雅地处理它们。 错误命名 对于存储为全局变量的错误值， 根据是否导出，使用前缀 Err 或 err。 请看指南 对于未导出的顶层常量和变量，使用_作为前缀。 12345678910var ( // 导出以下两个错误，以便此包的用户可以将它们与 errors.Is 进行匹配。 ErrBrokenLink = errors.New(\"link is broken\") ErrCouldNotOpen = errors.New(\"could not open\") // 这个错误没有被导出，因为不想让它成为公共 API 的一部分。 可能仍然在带有错误的包内使用它。 errNotFound = errors.New(\"not found\")) 对于自定义错误类型，请改用后缀 Error。 123456789101112131415161718// 同样，这个错误被导出，以便这个包的用户可以将它与 errors.As 匹配。type NotFoundError struct &#123; File string&#125;func (e *NotFoundError) Error() string &#123; return fmt.Sprintf(\"file %q not found\", e.File)&#125;// 并且这个错误没有被导出，因为不想让它成为公共 API 的一部分。 仍然可以在带有 errors.As 的包中使用它。type resolveError struct &#123; Path string&#125;func (e *resolveError) Error() string &#123; return fmt.Sprintf(\"resolve %q\", e.Path)&#125; 处理断言失败 类型断言 将会在检测到不正确的类型时，以单一返回值形式返回 panic。 因此，请始终使用“逗号 ok”习语。 BadGood 1t := i.(string) 1234t, ok := i.(string)if !ok &#123; // 优雅地处理错误&#125; 不要使用 panic 在生产环境中运行的代码必须避免出现 panic。panic 是 级联失败 的主要根源 。如果发生错误，该函数必须返回错误，并允许调用方决定如何处理它。 BadGood 12345678910func run(args []string) &#123; if len(args) == 0 &#123; panic(\"an argument is required\") &#125; // ...&#125;func main() &#123; run(os.Args[1:])&#125; 1234567891011121314func run(args []string) error &#123; if len(args) == 0 &#123; return errors.New(\"an argument is required\") &#125; // ... return nil&#125;func main() &#123; if err := run(os.Args[1:]); err != nil &#123; fmt.Fprintln(os.Stderr, err) os.Exit(1) &#125;&#125; panic/recover 不是错误处理策略。仅当发生不可恢复的事情（例如：nil 引用）时，程序才必须 panic。程序初始化是一个例外：程序启动时应使程序中止的不良情况可能会引起 panic。 1var _statusTemplate = template.Must(template.New(\"name\").Parse(\"_statusHTML\")) 即使在测试代码中，也优先使用t.Fatal或者t.FailNow而不是 panic 来确保失败被标记。 BadGood 12345// func TestFoo(t *testing.T)f, err := ioutil.TempFile(\"\", \"test\")if err != nil &#123; panic(\"failed to set up test\")&#125; 12345// func TestFoo(t *testing.T)f, err := ioutil.TempFile(\"\", \"test\")if err != nil &#123; t.Fatal(\"failed to set up test\")&#125; 使用 go.uber.org/atomic 使用 [sync/atomic] 包的原子操作对原始类型 (int32, int64等）进行操作，因为很容易忘记使用原子操作来读取或修改变量。 [go.uber.org/atomic] 通过隐藏基础类型为这些操作增加了类型安全性。此外，它包括一个方便的atomic.Bool类型。 BadGood 123456789101112131415type foo struct &#123; running int32 // atomic&#125;func (f* foo) start() &#123; if atomic.SwapInt32(&amp;f.running, 1) == 1 &#123; // already running… return &#125; // start the Foo&#125;func (f *foo) isRunning() bool &#123; return f.running == 1 // race!&#125; 123456789101112131415type foo struct &#123; running atomic.Bool&#125;func (f *foo) start() &#123; if f.running.Swap(true) &#123; // already running… return &#125; // start the Foo&#125;func (f *foo) isRunning() bool &#123; return f.running.Load()&#125; 避免可变全局变量 使用选择依赖注入方式避免改变全局变量。既适用于函数指针又适用于其他值类型 BadGood 123456// sign.govar _timeNow = time.Nowfunc sign(msg string) string &#123; now := _timeNow() return signWithTime(msg, now)&#125; 12345678910111213// sign.gotype signer struct &#123; now func() time.Time&#125;func newSigner() *signer &#123; return &amp;signer&#123; now: time.Now, &#125;&#125;func (s *signer) Sign(msg string) string &#123; now := s.now() return signWithTime(msg, now)&#125; 123456789// sign_test.gofunc TestSign(t *testing.T) &#123; oldTimeNow := _timeNow _timeNow = func() time.Time &#123; return someFixedTime &#125; defer func() &#123; _timeNow = oldTimeNow &#125;() assert.Equal(t, want, sign(give))&#125; 12345678// sign_test.gofunc TestSigner(t *testing.T) &#123; s := newSigner() s.now = func() time.Time &#123; return someFixedTime &#125; assert.Equal(t, want, s.Sign(give))&#125; 避免在公共结构中嵌入类型 这些嵌入的类型泄漏实现细节、禁止类型演化和模糊的文档。 假设使用共享的 AbstractList 实现了多种列表类型，请避免在具体的列表实现中嵌入 AbstractList。 相反，只需手动将方法写入具体的列表，该列表将委托给抽象列表。 123456789type AbstractList struct &#123;&#125;// 添加将实体添加到列表中。func (l *AbstractList) Add(e Entity) &#123; // ...&#125;// 移除从列表中移除实体。func (l *AbstractList) Remove(e Entity) &#123; // ...&#125; BadGood 1234// ConcreteList 是一个实体列表。type ConcreteList struct &#123; *AbstractList&#125; 123456789101112// ConcreteList 是一个实体列表。type ConcreteList struct &#123; list *AbstractList&#125;// 添加将实体添加到列表中。func (l *ConcreteList) Add(e Entity) &#123; l.list.Add(e)&#125;// 移除从列表中移除实体。func (l *ConcreteList) Remove(e Entity) &#123; l.list.Remove(e)&#125; Go 允许 类型嵌入 作为继承和组合之间的折衷。外部类型获取嵌入类型的方法的隐式副本。默认情况下，这些方法委托给嵌入实例的同一方法。 结构还获得与类型同名的字段。所以，如果嵌入的类型是 public，那么字段是 public。为了保持向后兼容性，外部类型的每个未来版本都必须保留嵌入类型。 很少需要嵌入类型。 这是一种方便，可以帮助避免编写冗长的委托方法。 即使嵌入兼容的抽象列表 interface，而不是结构体，这将为开发人员提供更大的灵活性来改变未来，但仍然泄露了具体列表使用抽象实现的细节。 BadGood 123456789// AbstractList 是各种实体列表的通用实现。type AbstractList interface &#123; Add(Entity) Remove(Entity)&#125;// ConcreteList 是一个实体列表。type ConcreteList struct &#123; AbstractList&#125; 123456789101112// ConcreteList 是一个实体列表。type ConcreteList struct &#123; list AbstractList&#125;// 添加将实体添加到列表中。func (l *ConcreteList) Add(e Entity) &#123; l.list.Add(e)&#125;// 移除从列表中移除实体。func (l *ConcreteList) Remove(e Entity) &#123; l.list.Remove(e)&#125; 无论是使用嵌入结构还是嵌入接口，都会限制类型的演化。 向嵌入接口添加方法是一个破坏性的改变。 从嵌入结构体删除方法是一个破坏性改变。 删除嵌入类型是一个破坏性的改变。 即使使用满足相同接口的类型替换嵌入类型，也是一个破坏性的改变。 尽管编写这些委托方法是乏味的，但是额外的工作隐藏了实现细节，留下了更多的更改机会，还消除了在文档中发现完整列表接口的间接性操作。 避免使用内置名称 Go 语言规范 概述了几个内置的，不应在 Go 项目中使用的 预先声明的标识符。 根据上下文的不同，将这些标识符作为名称重复使用，将在当前作用域（或任何嵌套作用域）中隐藏原始标识符，或者混淆代码。 在最好的情况下，编译器会报错；在最坏的情况下，这样的代码可能会引入潜在的、难以恢复的错误。 BadGood 12345678var error string// `error` 作用域隐式覆盖// orfunc handleErrorMessage(error string) &#123; // `error` 作用域隐式覆盖&#125; 12345678var errorMessage string// `error` 指向内置的非覆盖// orfunc handleErrorMessage(msg string) &#123; // `error` 指向内置的非覆盖&#125; 123456789101112131415type Foo struct &#123; // 虽然这些字段在技术上不构成阴影，但`error`或`string`字符串的重映射现在是不明确的。 error error string string&#125;func (f Foo) Error() error &#123; // `error` 和 `f.error` 在视觉上是相似的 return f.error&#125;func (f Foo) String() string &#123; // `string` and `f.string` 在视觉上是相似的 return f.string&#125; 12345678910111213type Foo struct &#123; // `error` and `string` 现在是明确的。 err error str string&#125;func (f Foo) Error() error &#123; return f.err&#125;func (f Foo) String() string &#123; return f.str&#125; 注意，编译器在使用预先分隔的标识符时不会生成错误， 但是诸如go vet之类的工具会正确地指出这些和其他情况下的隐式问题。 避免使用 init() 尽可能避免使用init()。当init()是不可避免或可取的，代码应先尝试： 无论程序环境或调用如何，都要完全确定。 避免依赖于其他init()函数的顺序或副作用。虽然init()顺序是明确的，但代码可以更改， 因此init()函数之间的关系可能会使代码变得脆弱和容易出错。 避免访问或操作全局或环境状态，如机器信息、环境变量、工作目录、程序参数/输入等。 避免I/O，包括文件系统、网络和系统调用。 init 调用顺序 在同一个 package 中，可以多个文件中定义 init 方法，按照文件名先后执行各个文件中的 init 方法 在同一个 go 文件中，可以重复定义 init 方法，按照在代码中编写顺序依次执行不同的 init 方法 对于不同的 package， 如果不相互依赖的话，按照 main 包中 import 的顺序调用其包中的 init() 函数 如果 package 存在依赖，调用顺序为最后被依赖的最先被初始化，例如：导入顺序 main –&gt; A –&gt; B –&gt; C，则初始化顺序为 C –&gt; B –&gt; A –&gt; main，一次执行对应的 init 方法 不能满足这些要求的代码可能属于要作为main()调用的一部分(或程序生命周期中的其他地方)，或者作为main()本身的一部分写入。特别是，打算由其他程序使用的库应该特别注意完全确定性，而不是执行“init magic” BadGood 123456789type Foo struct &#123; // ...&#125;var _defaultFoo Foofunc init() &#123; _defaultFoo = Foo&#123; // ... &#125;&#125; 12345678910var _defaultFoo = Foo&#123; // ...&#125;// or，为了更好的可测试性：var _defaultFoo = defaultFoo()func defaultFoo() Foo &#123; return Foo&#123; // ... &#125;&#125; 12345678910111213type Config struct &#123; // ...&#125;var _config Configfunc init() &#123; // Bad: 基于当前目录 cwd, _ := os.Getwd() // Bad: I/O raw, _ := ioutil.ReadFile( path.Join(cwd, \"config\", \"config.yaml\"), ) yaml.Unmarshal(raw, &amp;_config)&#125; 1234567891011121314type Config struct &#123; // ...&#125;func loadConfig() Config &#123; cwd, err := os.Getwd() // handle err raw, err := ioutil.ReadFile( path.Join(cwd, \"config\", \"config.yaml\"), ) // handle err var config Config yaml.Unmarshal(raw, &amp;config) return config&#125; 考虑到上述情况，在某些情况下，init()可能更可取或是必要的，可能包括： 不能表示为单个赋值的复杂表达式。 可插入的钩子，如database/sql、编码类型注册表等。 对 Google Cloud Functions 和其他形式的确定性预计算的优化。 追加时优先指定切片容量 追加时优先指定切片容量，在尽可能的情况下，在初始化要追加的切片时为make()提供一个容量值。 BadGood 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0, size) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 1BenchmarkBad-4 100000000 2.48s 1BenchmarkGood-4 100000000 0.21s 主函数退出方式 (Exit) Go 程序使用 os.Exit 或者 log.Fatal* 立即退出 (使用panic不是退出程序的好方法，请 不要使用 panic。) 仅在main() 中调用其中一个 os.Exit 或者 log.Fatal*。所有其他函数应将错误返回到信号失败中。 BadGood 123456789101112131415func main() &#123; body := readFile(path) fmt.Println(body)&#125;func readFile(path string) string &#123; f, err := os.Open(path) if err != nil &#123; log.Fatal(err) &#125; b, err := ioutil.ReadAll(f) if err != nil &#123; log.Fatal(err) &#125; return string(b)&#125; 123456789101112131415161718func main() &#123; body, err := readFile(path) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(body)&#125;func readFile(path string) (string, error) &#123; f, err := os.Open(path) if err != nil &#123; return \"\", err &#125; b, err := ioutil.ReadAll(f) if err != nil &#123; return \"\", err &#125; return string(b), nil&#125; 原则上：退出的具有多种功能的程序存在一些问题： 不明显的控制流：任何函数都可以退出程序，因此很难对控制流进行推理。 难以测试：退出程序的函数也将退出调用它的测试。这使得函数很难测试，并引入了跳过 go test 尚未运行的其他测试的风险。 跳过清理：当函数退出程序时，会跳过已经进入defer队列里的函数调用。这增加了跳过重要清理任务的风险。 一次性退出 如果可能的话，main（）函数中 最多一次 调用 os.Exit或者log.Fatal。如果有多个错误场景停止程序执行，请将该逻辑放在单独的函数下并从中返回错误。 这会缩短 main() 函数，并将所有关键业务逻辑放入一个单独的、可测试的函数中。 BadGood 1234567891011121314151617181920package mainfunc main() &#123; args := os.Args[1:] if len(args) != 1 &#123; log.Fatal(\"missing file\") &#125; name := args[0] f, err := os.Open(name) if err != nil &#123; log.Fatal(err) &#125; defer f.Close() // 如果调用 log.Fatal 在这条线之后 // f.Close 将会被执行。 b, err := ioutil.ReadAll(f) if err != nil &#123; log.Fatal(err) &#125; // ...&#125; 1234567891011121314151617181920212223package mainfunc main() &#123; if err := run(); err != nil &#123; log.Fatal(err) &#125;&#125;func run() error &#123; args := os.Args[1:] if len(args) != 1 &#123; return errors.New(\"missing file\") &#125; name := args[0] f, err := os.Open(name) if err != nil &#123; return err &#125; defer f.Close() b, err := ioutil.ReadAll(f) if err != nil &#123; return err &#125; // ...&#125; 在序列化结构中使用字段标记 任何序列化到JSON、YAML，或其他支持基于标记的字段命名的格式应使用相关标记进行注释。 BadGood 12345678type Stock struct &#123; Price int Name string&#125;bytes, err := json.Marshal(Stock&#123; Price: 137, Name: \"UBER\",&#125;) 123456789type Stock struct &#123; Price int `json:\"price\"` Name string `json:\"name\"` // Safe to rename Name to Symbol.&#125;bytes, err := json.Marshal(Stock&#123; Price: 137, Name: \"UBER\",&#125;) 理论上： 结构的序列化形式是不同系统之间的契约。 对序列化表单结构（包括字段名）的更改会破坏此约定。在标记中指定字段名使约定明确， 它还可以通过重构或重命名字段来防止意外违反约定。 性能 性能方面的特定准则只适用于高频场景。 优先使用 strconv 而不是 fmt 将原语转换为字符串或从字符串转换时，strconv速度比fmt快。 BadGood 123for i := 0; i &lt; b.N; i++ &#123; s := fmt.Sprint(rand.Int())&#125; 123for i := 0; i &lt; b.N; i++ &#123; s := strconv.Itoa(rand.Int())&#125; 1BenchmarkFmtSprint-4 143 ns&#x2F;op 2 allocs&#x2F;op 1BenchmarkStrconv-4 64.2 ns&#x2F;op 1 allocs&#x2F;op 避免字符串到字节的转换 不要反复从固定字符串创建字节 slice。相反，请执行一次转换并捕获结果。 BadGood 123for i := 0; i &lt; b.N; i++ &#123; w.Write([]byte(\"Hello world\"))&#125; 1234data := []byte(\"Hello world\")for i := 0; i &lt; b.N; i++ &#123; w.Write(data)&#125; 1BenchmarkBad-4 50000000 22.2 ns&#x2F;op 1BenchmarkGood-4 500000000 3.25 ns&#x2F;op 即使是内容相同的字符串，它们都会单独在分配一块内存 另外在测试的时候发现：字符串常量无法获取地址！！！，因为取值符号&amp; 仅适用于堆栈内存或堆内存中的值，即可以实际写入的值。表达式和常量(它们本身是程序本身)实际上存储在内存中，但无法写入该内存。因此，能够引入该内存就没有意义 指定容器容量 尽可能指定容器容量，以便为容器预先分配内存。这将在添加元素时最小化后续分配（通过复制和调整容器大小）。 指定 Map 容量提示 在尽可能的情况下，在使用 make() 初始化的时候提供容量信息 1make(map[T1]T2, hint) 向make()提供容量提示会在初始化时尝试调整 map 的大小，这将减少在将元素添加到 map 时为 map 重新分配内存。 注意，与 slices 不同。map capacity 提示并不保证完全的抢占式分配，而是用于估计所需的 hashmap bucket 的数量。因此，在将元素添加到 map 时，甚至在指定 map 容量时，仍可能发生分配。 BadGood 123456m := make(map[string]os.FileInfo)files, _ := ioutil.ReadDir(\"./files\")for _, f := range files &#123; m[f.Name()] = f&#125; 123456files, _ := ioutil.ReadDir(\"./files\")m := make(map[string]os.FileInfo, len(files))for _, f := range files &#123; m[f.Name()] = f&#125; m 是在没有大小提示的情况下创建的； 在运行时可能会有更多分配。 m 是有大小提示创建的；在运行时可能会有更少的分配。 指定切片容量 在尽可能的情况下，在使用make()初始化切片时提供容量信息，特别是在追加切片时。 1make([]T, length, capacity) 与 maps 不同，slice capacity 不是一个提示：编译器将为提供给make()的 slice 的容量分配足够的内存， 这意味着后续的 append()`操作将导致零分配（直到 slice 的长度与容量匹配，在此之后，任何 append 都可能调整大小以容纳其他元素）。 BadGood 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 123456for n := 0; n &lt; b.N; n++ &#123; data := make([]int, 0, size) for k := 0; k &lt; size; k++&#123; data = append(data, k) &#125;&#125; 1BenchmarkBad-4 100000000 2.48s 1BenchmarkGood-4 100000000 0.21s 规范 避免过长的行 避免使用需要读者水平滚动或过度转动头部的代码行。建议将行长度限制为 99 characters (99 个字符)。但这不是硬性限制。允许代码超过此限制。 一致性 本文中概述的一些标准都是客观性的评估，是根据场景、上下文、或者主观性的判断； 但是最重要的是，保持一致. 一致性的代码更容易维护、是更合理的、需要更少的学习成本、并且随着新的约定出现或者出现错误后更容易迁移、更新、修复 bug 相反，在一个代码库中包含多个完全不同或冲突的代码风格会导致维护成本开销、不确定性和认知偏差。所有这些都会直接导致速度降低、代码审查痛苦、而且增加 bug 数量。 将这些标准应用于代码库时，建议在 package（或更大）级别进行更改，子包级别的应用程序通过将多个样式引入到同一代码中，违反了上述关注点。 相似的声明放在一组 Go 语言支持将相似的声明放在一个组内。 BadGood 12import \"a\"import \"b\" 1234import ( \"a\" \"b\") 这同样适用于常量、变量和类型声明： BadGood 12345678const a = 1const b = 2var a = 1var b = 2type Area float64type Volume float64 1234567891011121314const ( a = 1 b = 2)var ( a = 1 b = 2)type ( Area float64 Volume float64) 仅将相关的声明放在一组。不要将不相关的声明放在一组。 BadGood 12345678type Operation intconst ( Add Operation = iota + 1 Subtract Multiply EnvVar = \"MY_ENV\") 123456789type Operation intconst ( Add Operation = iota + 1 Subtract Multiply)const EnvVar = \"MY_ENV\" 分组使用的位置没有限制，例如：可以在函数内部使用它们： BadGood 1234567func f() string &#123; red := color.New(0xff0000) green := color.New(0x00ff00) blue := color.New(0x0000ff) ...&#125; 123456789func f() string &#123; var ( red = color.New(0xff0000) green = color.New(0x00ff00) blue = color.New(0x0000ff) ) ...&#125; 例外：如果变量声明与其他变量相邻，则应将变量声明（尤其是函数内部的声明）分组在一起。对一起声明的变量执行此操作，即使它们不相关。 BadGood 1234567func (c *client) request() &#123; caller := c.name format := \"json\" timeout := 5*time.Second var err error // ...&#125; 123456789func (c *client) request() &#123; var ( caller = c.name format = \"json\" timeout = 5*time.Second err error ) // ...&#125; import 分组 导入应该分为两组： 标准库 其他库 默认情况下，这是 goimports 应用的分组。 BadGood 123456import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\") 1234567import ( \"fmt\" \"os\" \"go.uber.org/atomic\" \"golang.org/x/sync/errgroup\") 包名 当命名包时，请按下面规则选择一个名称： 全部小写。没有大写或下划线。 大多数使用命名导入的情况下，不需要重命名。 简短而简洁。请记住，在每个使用的地方都完整标识了该名称。 不用复数。例如net/url，而不是net/urls。 不要用“common”，“util”，“shared”或“lib”。这些是不好的，信息量不足的名称。 另请参阅 Go 包命名规则 和 Go 包样式指南. 函数名 遵循 Go 社区关于使用 MixedCaps 作为函数名 的约定。有一个例外，为了对相关的测试用例进行分组，函数名可能包含下划线，如：TestMyFunction_WhatIsBeingTested. 导入别名 如果程序包名称与导入路径的最后一个元素不匹配，则必须使用导入别名。 123456import ( \"net/http\" client \"example.com/client-go\" trace \"example.com/trace/v2\") 在所有其他情况下，除非导入之间有直接冲突，否则应避免导入别名。 BadGood 123456import ( \"fmt\" \"os\" nettrace \"golang.net/x/trace\") 1234567import ( \"fmt\" \"os\" \"runtime/trace\" nettrace \"golang.net/x/trace\") 函数分组与顺序 函数应按粗略的调用顺序排序。 同一文件中的函数应按接收者分组。 因此，导出的函数应先出现在文件中，放在struct, const, var定义的后面。 在定义类型之后，但在接收者的其余方法之前，可能会出现一个 newXYZ()/NewXYZ() 由于函数是按接收者分组的，因此普通工具函数应在文件末尾出现。 BadGood 12345678910111213func (s *something) Cost() &#123; return calcCost(s.weights)&#125;type something struct&#123; ... &#125;func calcCost(n []int) int &#123;...&#125;func (s *something) Stop() &#123;...&#125;func newSomething() *something &#123; return &amp;something&#123;&#125;&#125; 12345678910111213type something struct&#123; ... &#125;func newSomething() *something &#123; return &amp;something&#123;&#125;&#125;func (s *something) Cost() &#123; return calcCost(s.weights)&#125;func (s *something) Stop() &#123;...&#125;func calcCost(n []int) int &#123;...&#125; 减少嵌套 代码应通过尽可能先处理错误情况/特殊情况并尽早返回或继续循环来减少嵌套。减少嵌套多个级别的代码的代码量。 BadGood 123456789101112for _, v := range data &#123; if v.F1 == 1 &#123; v = process(v) if err := v.Call(); err == nil &#123; v.Send() &#125; else &#123; return err &#125; &#125; else &#123; log.Printf(\"Invalid v: %v\", v) &#125;&#125; 123456789101112for _, v := range data &#123; if v.F1 != 1 &#123; log.Printf(\"Invalid v: %v\", v) continue &#125; v = process(v) if err := v.Call(); err != nil &#123; return err &#125; v.Send()&#125; 不必要的 else 如果在 if 的两个分支中都设置了变量，则可以将其替换为单个 if。 BadGood 123456var a intif b &#123; a = 100&#125; else &#123; a = 10&#125; 1234a := 10if b &#123; a = 100&#125; 顶层变量声明 在顶层，使用标准var关键字。请勿指定类型，除非它与表达式的类型不同。 BadGood 123var _s string = F()func F() string &#123; return \"A\" &#125; 12345var _s = F()// 由于 F 已经明确了返回一个字符串类型，因此没有必要显式指定_s 的类型// 还是那种类型func F() string &#123; return \"A\" &#125; 如果表达式的类型与所需的类型不完全匹配，请指定类型。 12345678type myError struct&#123;&#125;func (myError) Error() string &#123; return \"error\" &#125;func F() myError &#123; return myError&#123;&#125; &#125;var _e error = F()// F 返回一个 myError 类型的实例，但是要 error 类型 对于未导出的顶层常量和变量，使用_作为前缀 在未导出的顶级vars和consts， 前面加上前缀_，以使它们在使用时明确表示它们是全局符号。 基本依据：顶级变量和常量具有包范围作用域。使用通用名称可能很容易在其他文件中意外使用错误的值。 BadGood 1234567891011121314151617// foo.goconst ( defaultPort = 8080 defaultUser = \"user\")// bar.gofunc Bar() &#123; defaultPort := 9090 ... fmt.Println(\"Default port\", defaultPort) // We will not see a compile error if the first line of // Bar() is deleted.&#125; 123456// foo.goconst ( _defaultPort = 8080 _defaultUser = \"user\") 例外：未导出的错误值可以使用不带下划线的前缀 err。 参见错误命名。 结构体中的嵌入 嵌入式类型（例如 mutex）应位于结构体内的字段列表的顶部，并且必须有一个空行将嵌入式字段与常规字段分隔开。 BadGood 1234type Client struct &#123; version int http.Client&#125; 12345type Client struct &#123; http.Client version int&#125; 内嵌应该提供切实的好处，比如以语义上合适的方式添加或增强功能。它应该在对用户没有任何不利影响的情况下使用。（另请参见：避免在公共结构中嵌入类型）。例外：即使在未导出类型中，Mutex 也不应该作为内嵌字段。另请参见：零值 Mutex 是有效的。 嵌入 不应该: 纯粹是为了美观或方便。 使外部类型更难构造或使用。 影响外部类型的零值。如果外部类型有一个有用的零值，则在嵌入内部类型之后应该仍然有一个有用的零值。 作为嵌入内部类型的副作用，从外部类型公开不相关的函数或字段。 公开未导出的类型。 影响外部类型的复制形式。 更改外部类型的 API 或类型语义。 嵌入内部类型的非规范形式。 公开外部类型的实现详细信息。 允许用户观察或控制类型内部。 通过包装的方式改变内部函数的一般行为，这种包装方式会给用户带来一些意料之外情况。 简单地说，有意识地和有目的地嵌入。一种很好的测试体验是， “是否所有这些导出的内部方法/字段都将直接添加到外部类型” 如果答案是some或no，不要嵌入内部类型 - 而是使用字段。 BadGood 12345type A struct &#123; // Bad: A.Lock() and A.Unlock() 现在可用 // 不提供任何功能性好处，并允许用户控制有关 A 的内部细节。 sync.Mutex&#125; 12345678910type countingWriteCloser struct &#123; // Good: Write() 在外层提供用于特定目的， // 并且委托工作到内部类型的 Write() 中。 io.WriteCloser count int&#125;func (w *countingWriteCloser) Write(bs []byte) (int, error) &#123; w.count += len(bs) return w.WriteCloser.Write(bs)&#125; 12345678910type Book struct &#123; // Bad: 指针更改零值的有用性 io.ReadWriter // other fields&#125;// latervar b Bookb.Read(...) // panic: nil pointerb.String() // panic: nil pointerb.Write(...) // panic: nil pointer 12345678910type Book struct &#123; // Good: 有用的零值 bytes.Buffer // other fields&#125;// latervar b Bookb.Read(...) // okb.String() // okb.Write(...) // ok 123456type Client struct &#123; sync.Mutex sync.WaitGroup bytes.Buffer url.URL&#125; 123456type Client struct &#123; mtx sync.Mutex wg sync.WaitGroup buf bytes.Buffer url url.URL&#125; 本地变量声明 如果将变量明确设置为某个值，则应使用短变量声明形式 (:=)。 BadGood 1var s = \"foo\" 1s := \"foo\" 但是，在某些情况下，var 使用关键字时默认值会更清晰。例如，声明空切片。 BadGood 12345678func f(list []int) &#123; filtered := []int&#123;&#125; for _, v := range list &#123; if v &gt; 10 &#123; filtered = append(filtered, v) &#125; &#125;&#125; 12345678func f(list []int) &#123; var filtered []int for _, v := range list &#123; if v &gt; 10 &#123; filtered = append(filtered, v) &#125; &#125;&#125; nil 是一个有效的 slice nil 是一个有效的长度为 0 的 slice，这意味着， 不应明确返回长度为零的切片。应该返回nil 来代替。 BadGood 123if x == \"\" &#123; return []int&#123;&#125;&#125; 123if x == \"\" &#123; return nil&#125; 要检查切片是否为空，请始终使用len(s) == 0。而非 nil。 BadGood 123func isEmpty(s []string) bool &#123; return s == nil&#125; 123func isEmpty(s []string) bool &#123; return len(s) == 0&#125; 零值切片（用var声明的切片）可立即使用，无需调用make()创建。 BadGood 12345678910nums := []int&#123;&#125;// or, nums := make([]int)if add1 &#123; nums = append(nums, 1)&#125;if add2 &#123; nums = append(nums, 2)&#125; 123456789var nums []intif add1 &#123; nums = append(nums, 1)&#125;if add2 &#123; nums = append(nums, 2)&#125; 记住，虽然 nil 切片是有效的切片，但它不等于长度为 0 的切片（一个为 nil，另一个不是），并且在不同的情况下（例如序列化），这两个切片的处理方式可能不同。 缩小变量作用域 如果有可能，尽量缩小变量作用范围。除非它与 减少嵌套的规则冲突。 BadGood 1234err := ioutil.WriteFile(name, data, 0644)if err != nil &#123; return err&#125; 123if err := ioutil.WriteFile(name, data, 0644); err != nil &#123; return err&#125; 如果需要在 if 之外使用函数调用的结果，则不应尝试缩小范围。 BadGood 1234567891011if data, err := ioutil.ReadFile(name); err == nil &#123; err = cfg.Decode(data) if err != nil &#123; return err &#125; fmt.Println(cfg) return nil&#125; else &#123; return err //注意这个err的范围&#125; 1234567891011data, err := ioutil.ReadFile(name)if err != nil &#123; return err&#125;if err := cfg.Decode(data); err != nil &#123; return err&#125;fmt.Println(cfg)return nil 注意 if 中变量的范围 12345678910111213func f1() (int, error) &#123; return 0, errors.New(\"err test\")&#125;func main() &#123; if v, err := f1(); err == nil &#123; //为了在 else中打印 err 这里使用 == a := 1 // &#125; else &#123; fmt.Printf(\"%v %v\", err, v) fmt.Printf(\"%v\", a) //编译错误，undefined: a &#125; &#125; 避免参数语义不明确 (Avoid Naked Parameters) 函数调用中的意义不明确的参数可能会损害可读性。当参数名称的含义不明显时，请为参数添加 C 样式注释 (/* ... */) BadGood 123// func printInfo(name string, isLocal, done bool)printInfo(\"foo\", true, true) 123// func printInfo(name string, isLocal, done bool)printInfo(\"foo\", true /* isLocal */, true /* done */) 对于上面的示例代码，还有一种更好的处理方式是将上面的 bool 类型换成自定义类型。将来，该参数可以支持不仅仅局限于两个状态（true/false）。 12345678910111213141516type Region intconst ( UnknownRegion Region = iota Local)type Status intconst ( StatusReady Status= iota + 1 StatusDone // Maybe we will have a StatusInProgress in the future.)func printInfo(name string, region Region, status Status) 使用原始字符串字面值，避免转义 Go 支持使用 原始字符串字面值，也就是 &quot; ` &quot; 来表示原生字符串，在需要转义的场景下，应该尽量使用这种方案来替换。 可以跨越多行并包含引号。使用这些字符串可以避免更难阅读的手工转义的字符串。 BadGood 1wantError := \"unknown name:\\\"test\\\"\" 1wantError := `unknown error:\"test\"` 初始化结构体 使用字段名初始化结构 初始化结构时，几乎应该始终指定字段名。目前由 go vet 强制执行。 BadGood 1k := User&#123;\"John\", \"Doe\", true&#125; 12345k := User&#123; FirstName: \"John\", LastName: \"Doe\", Admin: true,&#125; 例外：当有 3 个或更少的字段时，测试表中的字段名may可以省略。 1234567tests := []struct&#123; op Operation want string&#125;&#123; &#123;Add, \"add\"&#125;, &#123;Subtract, \"subtract\"&#125;,&#125; 省略结构中的零值字段 初始化具有字段名的结构时，除非提供有意义的上下文，否则忽略值为零的字段。也就是，让自动将这些设置为零值 BadGood 123456user := User&#123; FirstName: \"John\", LastName: \"Doe\", MiddleName: \"\", Admin: false,&#125; 1234user := User&#123; FirstName: \"John\", LastName: \"Doe\",&#125; 这有助于通过省略该上下文中的默认值来减少阅读的障碍。只指定有意义的值。 在字段名提供有意义上下文的地方包含零值。例如，表驱动测试 中的测试用例可以受益于字段的名称，即使它们是零值的。 1234567tests := []struct&#123; give string want int&#125;&#123; &#123;give: \"0\", want: 0&#125;, // ...&#125; 对零值结构使用 var 如果在声明中省略了结构的所有字段，请使用 var 声明结构。 BadGood 1user := User&#123;&#125; 1var user User 这将零值结构与那些具有类似于为 初始化 Maps 创建的，区别于非零值字段的结构区分开来， 并与更喜欢的 声明空切片 方式相匹配。 初始化 Struct 引用 在初始化结构引用时，请使用&amp;T{}代替new(T)，以使其与结构体初始化一致。 BadGood 12345sval := T&#123;Name: \"foo\"&#125;// inconsistentsptr := new(T)sptr.Name = \"bar\" 123sval := T&#123;Name: \"foo\"&#125;sptr := &amp;T&#123;Name: \"bar\"&#125; 初始化 Maps 对于空 map 请使用 make(..) 初始化， 并且 map 是通过编程方式填充的。 这使得 map 初始化在表现上不同于声明，并且它还可以方便地在 make 后添加大小提示。 BadGood 123456var ( // m1 读写安全; // m2 在写入时会 panic m1 = map[T1]T2&#123;&#125; m2 map[T1]T2) 123456var ( // m1 读写安全; // m2 在写入时会 panic m1 = make(map[T1]T2) m2 map[T1]T2) 声明和初始化看起来非常相似的。 声明和初始化看起来差别非常大。 在尽可能的情况下，请在初始化时提供 map 容量大小，详细请看 指定 Map 容量提示。另外，如果 map 包含固定的元素列表，则使用 map literals(map 初始化列表) 初始化映射。 BadGood 1234m := make(map[T1]T2, 3)m[k1] = v1m[k2] = v2m[k3] = v3 12345m := map[T1]T2&#123; k1: v1, k2: v2, k3: v3,&#125; 基本准则是：在初始化时使用 map 初始化列表 来添加一组固定的元素。否则使用 make (如果可以，请尽量指定 map 容量)。 字符串 string format 如果在函数外声明Printf-style 函数的格式字符串，请将其设置为const常量。这有助于go vet对格式字符串执行静态分析。 BadGood 12msg := \"unexpected values %v, %v\\n\"fmt.Printf(msg, 1, 2) 12const msg = \"unexpected values %v, %v\\n\"fmt.Printf(msg, 1, 2) 命名 Printf 样式的函数 声明Printf-style 函数时，请确保go vet可以检测到它并检查格式字符串。 这意味着应尽可能使用预定义的Printf-style 函数名称。go vet将默认检查这些。有关更多信息，请参见 Printf 系列。 如果不能使用预定义的名称，请以 f 结束选择的名称：Wrapf，而不是Wrap。go vet可以要求检查特定的 Printf 样式名称，但名称必须以f结尾。 1$ go vet -printfuncs=wrapf,statusf 另请参阅 go vet: Printf family check. 编程模式 表驱动测试 当测试逻辑是重复的时候，通过 subtests 使用 table 驱动的方式编写 case 代码看上去会更简洁。 BadGood 123456789101112131415161718192021// func TestSplitHostPort(t *testing.T)host, port, err := net.SplitHostPort(\"192.0.2.0:8000\")require.NoError(t, err)assert.Equal(t, \"192.0.2.0\", host)assert.Equal(t, \"8000\", port)host, port, err = net.SplitHostPort(\"192.0.2.0:http\")require.NoError(t, err)assert.Equal(t, \"192.0.2.0\", host)assert.Equal(t, \"http\", port)host, port, err = net.SplitHostPort(\":8000\")require.NoError(t, err)assert.Equal(t, \"\", host)assert.Equal(t, \"8000\", port)host, port, err = net.SplitHostPort(\"1:8\")require.NoError(t, err)assert.Equal(t, \"1\", host)assert.Equal(t, \"8\", port) 12345678910111213141516171819202122232425262728293031323334353637// func TestSplitHostPort(t *testing.T)tests := []struct&#123; give string wantHost string wantPort string&#125;&#123; &#123; give: \"192.0.2.0:8000\", wantHost: \"192.0.2.0\", wantPort: \"8000\", &#125;, &#123; give: \"192.0.2.0:http\", wantHost: \"192.0.2.0\", wantPort: \"http\", &#125;, &#123; give: \":8000\", wantHost: \"\", wantPort: \"8000\", &#125;, &#123; give: \"1:8\", wantHost: \"1\", wantPort: \"8\", &#125;,&#125;for _, tt := range tests &#123; t.Run(tt.give, func(t *testing.T) &#123; host, port, err := net.SplitHostPort(tt.give) require.NoError(t, err) assert.Equal(t, tt.wantHost, host) assert.Equal(t, tt.wantPort, port) &#125;)&#125; 很明显，使用 test table 的方式在代码逻辑扩展的时候，比如新增 test case，都会显得更加的清晰。 遵循这样的约定：将结构体切片称为tests。 每个测试用例称为tt。此外，鼓励使用give和want前缀说明每个测试用例的输入和输出值。 1234567891011tests := []struct&#123; give string wantHost string wantPort string&#125;&#123; // ...&#125;for _, tt := range tests &#123; // ...&#125; 并行测试，比如一些专门的循环（例如，生成goroutine或捕获引用作为循环体的一部分的那些循环） 必须注意在循环的范围内显式地分配循环变量，以确保它们保持预期的值。 12345678910111213tests := []struct&#123; give string // ...&#125;&#123; // ...&#125;for _, tt := range tests &#123; tt := tt // for t.Parallel t.Run(tt.give, func(t *testing.T) &#123; t.Parallel() // ... &#125;)&#125; 在上面的例子中，由于下面使用了t.Parallel()，必须声明一个作用域为循环迭代的tt变量。 如果不这样做，大多数或所有测试都会收到一个意外的tt值，或者一个在运行时发生变化的值。 功能选项 功能选项是一种模式，可以在其中声明一个不透明 Option 类型，该类型在某些内部结构中记录信息。接受这些选项的可变编号，并根据内部结构上的选项记录的全部信息采取行动。 将此模式用于需要扩展的构造函数和其他公共 API 中的可选参数，尤其是在这些功能上已经具有三个或更多参数的情况下。 BadGood 123456789// package dbfunc Open( addr string, cache bool, logger *zap.Logger) (*Connection, error) &#123; // ...&#125; 123456789101112131415161718192021// package dbtype Option interface &#123; // ...&#125;func WithCache(c bool) Option &#123; // ...&#125;func WithLogger(log *zap.Logger) Option &#123; // ...&#125;// Open creates a connection.func Open( addr string, opts ...Option,) (*Connection, error) &#123; // ...&#125; 必须始终提供缓存和记录器参数，即使用户希望使用默认值。 1234db.Open(addr, db.DefaultCache, zap.NewNop())db.Open(addr, db.DefaultCache, log)db.Open(addr, false /* cache */, zap.NewNop())db.Open(addr, false /* cache */, log) 只有在需要时才提供选项。 12345678db.Open(addr)db.Open(addr, db.WithLogger(log))db.Open(addr, db.WithCache(false))db.Open( addr, db.WithCache(false), db.WithLogger(log),) 我建议实现此模式的方法是使用一个 Option 接口，该接口保存一个未导出的方法，在一个未导出的 options 结构上记录选项。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647type options struct &#123; cache bool logger *zap.Logger&#125;type Option interface &#123; apply(*options)&#125;type cacheOption boolfunc (c cacheOption) apply(opts *options) &#123; opts.cache = bool(c)&#125;func WithCache(c bool) Option &#123; return cacheOption(c)&#125;type loggerOption struct &#123; Log *zap.Logger&#125;func (l loggerOption) apply(opts *options) &#123; opts.logger = l.Log&#125;func WithLogger(log *zap.Logger) Option &#123; return loggerOption&#123;Log: log&#125;&#125;// Open creates a connection.func Open( addr string, opts ...Option,) (*Connection, error) &#123; options := options&#123; cache: defaultCache, logger: zap.NewNop(), &#125; for _, o := range opts &#123; o.apply(&amp;options) &#125; // ...&#125; 注意：还有一种使用闭包实现这个模式的方法，但是相信上面的模式为作者提供了更多的灵活性，并且更容易对用户进行调试和测试。特别是，在不可能进行比较的情况下它允许在测试和模拟中对选项进行比较。此外，它还允许选项实现其他接口，包括 fmt.Stringer，允许用户读取选项的字符串表示形式。 还可以参考下面资料： Self-referential functions and the design of options Functional options for friendly APIs Linting 比任何 “blessed” linter 集更重要的是，lint 在一个代码库中始终保持一致。 建议至少使用以下 linters，因为我认为它们有助于发现最常见的问题，并在不需要规定的情况下为代码质量建立一个高标准： errcheck 以确保错误得到处理 goimports 格式化代码和管理 imports golint 指出常见的文体错误 govet 分析代码中的常见错误 staticcheck 各种静态分析检查 Lint Runners 推荐 golangci-lint 作为 go-to lint 的运行程序，这主要是因为它在较大的代码库中的性能以及能够同时配置和使用许多规范。这个 repo 有一个示例配置文件 .golangci.yml 和推荐的 linter 设置。 golangci-lint 有 various-linters 可供使用。建议将上述 linters 作为基本 set，鼓励团队添加对他们的项目有意义的任何附加 linters。 Stargazers over time 参考链接 https://github.com/xxjwxc/uber_go_guide_cn https://golang.org/doc/effective_go.html https://github.com/golang/go/wiki/CommonMistakes https://github.com/golang/go/wiki/CodeReviewComments https://go.dev/doc/effective_go#pointers_vs_values https://go.dev/ref/spec#Method_values","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"设计模式-25-中介者模式","slug":"Design Patterns/设计模式-25-中介者模式","date":"2022-09-07T14:58:29.000Z","updated":"2023-07-23T07:32:28.519Z","comments":true,"path":"2022/09/07/Design Patterns/设计模式-25-中介者模式/","link":"","permalink":"http://xboom.github.io/2022/09/07/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-25-%E4%B8%AD%E4%BB%8B%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"**中介模式(Mediator Pattern)**通过创建一个中介者对象，用于处理各个对象之间的交互。中介者对象负责解耦其他对象之间的通信，从而减少对象之间的直接依赖关系 逻辑结构 在中介者模式中，有以下几个关键角色： 中介者（Mediator）：中介者定义了对象之间通信的接口，它负责协调对象之间的交互，并封装了对象之间的具体通信细节。 具体中介者（Concrete Mediator）：具体中介者实现了中介者接口，它具体实现了对象之间的通信逻辑，包括接收和发送消息等。 同伴类（Colleague）：同伴类是指参与中介者模式的对象，它们之间通过中介者进行通信。每个同伴类都需要持有一个中介者对象的引用。 具体同伴类（Concrete Colleague）：具体同伴类实现了同伴类接口，它包含自身的一些业务逻辑，同时在需要与其他同伴类通信时，通过中介者进行消息的发送和接收 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/24_Mediator 首先，定义同伴接口，表示同伴相互收发消息 12345// Colleague 同伴类接口type Colleague interface &#123; SendMessage(message string) ReceiveMessage(message string)&#125; 其次，定义中介者接口，表示同伴之间发送消息都需要通过中介 1234// Mediator 中介者接口type Mediator interface &#123; SendMessage(message string, colleague Colleague)&#125; 接着，实现具体的中介逻辑，中介实现同伴的通信 12345678910111213// ConcreteMediator 具体中介者type ConcreteMediator struct &#123; colleague1 Colleague colleague2 Colleague&#125;func (c *ConcreteMediator) SendMessage(message string, colleague Colleague) &#123; if colleague == c.colleague1 &#123; c.colleague2.ReceiveMessage(message) &#125; else &#123; c.colleague1.ReceiveMessage(message) &#125;&#125; 最后，定义同伴 1234567891011121314// ConcreteColleague 具体同伴类type ConcreteColleague struct &#123; name string mediator Mediator&#125;//发送消息func (c *ConcreteColleague) SendMessage(message string) &#123; c.mediator.SendMessage(message, c)&#125;func (c *ConcreteColleague) ReceiveMessage(message string) &#123; fmt.Printf(\"%s 收到消息：%s\\n\", c.name, message)&#125; 运行 12345678910//1. 中介对象包含具体对象mediator := &amp;ConcreteMediator&#123;&#125;colleague1 := &amp;ConcreteColleague&#123;name: \"Colleague1\", mediator: mediator&#125;colleague2 := &amp;ConcreteColleague&#123;name: \"Colleague2\", mediator: mediator&#125;//2. 中介双方mediator.colleague1 = colleague1mediator.colleague2 = colleague2//3. 发送消息colleague1.SendMessage(\"Hello, colleague2!\")colleague2.SendMessage(\"Hi, colleague1!\") 结果 12Colleague2 收到消息：Hello, colleague2!Colleague1 收到消息：Hi, colleague1! 中介者模式与观察者模式的区别？ 在观察者模式中，尽管一个参与者既可以是观察者，同时也可以是被观察者，但大部分情况下，交互关系往往都是单向的，一个参与者要么是观察者，要么是被观察者，不会兼具两种身份。 而中介模式正好相反。只有当参与者之间的交互关系错综复杂，维护成本很高的时候，才考虑使用中介模式。而且，如果一个参与者状态的改变，其他参与者执行的操作有一定先后顺序的要求，这个时候，中介模式就可以利用中介类，通过先后调用不同参与者的方法，来实现顺序的控制，而观察者模式是无法实现这样的顺序要求的 适用场景 当一些对象和其他对象紧密耦合以致难以对其进行修改时 当组件因过于依赖其他组件而无法在不同应用中复用时，可使用中介者模式 如果为了能在不同情景下复用一些基本行为，导致需要被迫创建大量组件子类时 总结 单一职责原则。 可以将多个组件间的交流抽取到同一位置， 使其更易于理解和维护。 开闭原则。 无需修改实际组件就能增加新的中介者 可以减轻应用中多个组件间的耦合情况 可以更方便地复用各个组件 中介模式会产生大而复杂的上帝类 参考链接 https://refactoringguru.cn/design-patterns/mediator https://lailin.xyz/post/mediator.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-24-解释器模式","slug":"Design Patterns/设计模式-24-解释器模式","date":"2022-09-06T14:58:29.000Z","updated":"2023-07-23T07:31:57.783Z","comments":true,"path":"2022/09/06/Design Patterns/设计模式-24-解释器模式/","link":"","permalink":"http://xboom.github.io/2022/09/06/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-24-%E8%A7%A3%E9%87%8A%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"解释器模式定义一套语言文法，并设计该语言解释器，使用户能使用特定文法控制解释器行为。解释器模式的意义在于，它分离多种复杂功能的实现，每个功能只需关注自身的解释。对于调用者不用关心内部的解释器的工作，只需要用简单的方式组合命令就可以 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/23_Expression 首先，定义了解释器接口 1234// Expression 表达式接口type Expression interface &#123; Interpret() bool&#125; 接着，实现终结符表达式 12345678910111213141516// TerminalExpression 终结符表达式type TerminalExpression struct &#123; data string&#125;func NewTerminalExpression(data string) *TerminalExpression &#123; return &amp;TerminalExpression&#123; data: data, &#125;&#125;func (t *TerminalExpression) Interpret() bool &#123; // 在实际应用中，此处可以进行语言文法解释的具体逻辑判断 // 这里简化，当 data 为 \"true\" 时返回 true，否则返回 false return t.data == \"true\"&#125; 然后，实现非终结符表达式 123456789101112131415161718192021222324252627// NonTerminalExpression 非终结符表达式type NonTerminalExpression struct &#123; exp1 Expression exp2 Expression operator string&#125;func NewNonTerminalExpression(exp1, exp2 Expression, operator string) *NonTerminalExpression &#123; return &amp;NonTerminalExpression&#123; exp1: exp1, exp2: exp2, operator: operator, &#125;&#125;func (n *NonTerminalExpression) Interpret() bool &#123; // 在实际应用中，此处可以进行语言文法解释的具体逻辑判断 // 这里简化，只实现简单的逻辑运算，支持 \"and\" 和 \"or\" 操作符 switch n.operator &#123; case \"and\": return n.exp1.Interpret() &amp;&amp; n.exp2.Interpret() case \"or\": return n.exp1.Interpret() || n.exp2.Interpret() default: return false &#125;&#125; 运行 1234567891011121314// 构建解释器规则：(true and false) or (true or true)exp1 := NewTerminalExpression(\"true\")exp2 := NewTerminalExpression(\"false\")andExpr := NewNonTerminalExpression(exp1, exp2, \"and\")exp3 := NewTerminalExpression(\"true\")exp4 := NewTerminalExpression(\"true\")orExpr := NewNonTerminalExpression(exp3, exp4, \"or\")rootExpr := NewNonTerminalExpression(andExpr, orExpr, \"or\")// 解释并计算表达式结果result := rootExpr.Interpret()fmt.Println(\"表达式计算结果：\", result) // Output: 表达式计算结果： true 结果 1表达式计算结果： true 应用场景 自定义实现一个自定义接口告警规则功能 适用于需要解释和执行某种语言文法的场景，例如编程语言的解释器、正则表达式引擎等。它可以灵活地处理复杂的文法规则，并将其转化为具体的操作和结果 参考链接 https://lailin.xyz/post/interpreter.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-23-命令模式","slug":"Design Patterns/设计模式-23-命令模式","date":"2022-09-05T14:58:29.000Z","updated":"2023-07-23T07:31:46.474Z","comments":true,"path":"2022/09/05/Design Patterns/设计模式-23-命令模式/","link":"","permalink":"http://xboom.github.io/2022/09/05/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-23-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"命令模式(Command Pattern)将请求或操作封装为一个对象，从而允许使用不同的请求、队列或日志来参数化其他对象。在这种模式下，命令的调用者与接收者之间解耦，使得调用者不需要知道接收者的具体实现 逻辑结构 在命令模式中，涉及以下几个核心角色： 命令(Command)：命令对象封装了执行特定操作的方法，并在需要时将该操作的接收者(执行者)绑定到命令对象上。 接收者(Receiver)：接收者是命令的实际执行者，负责执行命令所代表的操作。 调用者(Invoker)：调用者负责调用命令对象并触发执行相应的操作。它并不直接知道命令的具体细节，只需知道如何触发命令的执行。 客户端(Client)：客户端创建具体的命令对象，并将命令对象与相应的接收者进行绑定。客户端负责组装命令对象和接收者，并决定命令的执行时间和顺序。 实现代码 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/22_Command 首先，定义命令接口，封装执行特定操作的方法 1234// Command 接口定义了命令对象的执行方法type Command interface &#123; Execute()&#125; 然后，定义接受接口，命令的实际执行者 1234// Receiver 接收者接口type Receiver interface &#123; Action()&#125; 接着，定义具体的接受者 123456// ConcreteReceiver 具体接收者type ConcreteReceiver struct&#123;&#125;func (cr *ConcreteReceiver) Action() &#123; fmt.Println(\"接收者执行具体操作\")&#125; 然后，定义具体命令，将接受者绑定到命令，并进行执行 1234567891011121314// ConcreteCommand 具体命令type ConcreteCommand struct &#123; receiver Receiver&#125;func NewConcreteCommand(receiver Receiver) *ConcreteCommand &#123; return &amp;ConcreteCommand&#123; receiver: receiver, &#125;&#125;func (cc *ConcreteCommand) Execute() &#123; cc.receiver.Action()&#125; 最后，定义调用者负责命令执行 123456789101112131415161718// Invoker 调用者type Invoker struct &#123; command Command&#125;func NewInvoker(command Command) *Invoker &#123; return &amp;Invoker&#123; command: command, &#125;&#125;func (i *Invoker) SetCommand(command Command) &#123; i.command = command&#125;func (i *Invoker) ExecuteCommand() &#123; i.command.Execute()&#125; 运行 1234567//1. 命令接受者receiver := &amp;ConcreteReceiver&#123;&#125;//2. 命令command := NewConcreteCommand(receiver)//3. 命令调用invoker := NewInvoker(command)invoker.ExecuteCommand() 结果 1接收者执行具体操作 适用场景 菜单和按钮操作：在图形用户界面中，菜单和按钮通常需要与不同的操作关联。使用命令模式，可以将每个操作封装成一个命令对象，然后在菜单和按钮上设置相应的命令，使得菜单和按钮可以触发执行不同的操作。 撤销和重做：命令模式非常适合实现撤销和重做功能。通过将每个操作封装成一个命令对象，并将命令对象的历史记录保存下来，可以轻松地实现撤销和重做操作，只需按照命令历史记录依次执行即可。 任务调度和队列处理：命令模式可以用于实现任务调度和队列处理。将每个任务封装成一个命令对象，并将命令对象放入任务队列中，调度器可以按照队列的顺序依次执行命令，从而实现任务的调度和顺序处理。 日志记录：命令模式可以用于实现日志记录功能。将每个操作封装成一个命令对象，并在执行命令时记录相关的日志信息，可以方便地实现对操作的日志记录和分析。 事务处理：命令模式可以用于实现事务处理功能。将一系列操作封装成多个命令对象，并在执行命令时进行事务的提交或回滚，从而实现对事务的管理和控制 总的来说，命令模式适用于需要将请求发送者和请求接收者解耦的场景，以及需要支持撤销、重做、任务调度、日志记录和事务处理等功能的场景。通过将操作封装成命令对象，可以使得系统更灵活、可扩展，并提供更好的代码结构和可维护性 总结 单一职责原则。 可以解耦触发和执行操作的类 开闭原则。 可以在不修改已有客户端代码的情况下在程序中创建新的命令 可以实现撤销和恢复功能 可以实现操作的延迟执行 可以将一组简单命令组合成一个复杂命令 参考链接 https://refactoringguru.cn/design-patterns/command https://lailin.xyz/post/command.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-22-备忘录模式","slug":"Design Patterns/设计模式-22-备忘录模式","date":"2022-09-04T14:58:29.000Z","updated":"2023-07-23T07:31:34.977Z","comments":true,"path":"2022/09/04/Design Patterns/设计模式-22-备忘录模式/","link":"","permalink":"http://xboom.github.io/2022/09/04/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-22-%E5%A4%87%E5%BF%98%E5%BD%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"备忘录模式是一种行为设计模式，允许在不暴露对象实现细节的情况下保存和恢复对象之前的状态 逻辑结构 备忘录模式涉及三个核心角色： Originator(原发器): 需要被保存和恢复状态的对象。创建一个备忘录对象，用于保存当前状态，也可以通过备忘录对象恢复到之前的状态。 Memento(备忘录): 用于存储原发器对象状态的对象。备忘录对象可以记录原发器的内部状态，也可以提供访问原发器状态的接口。 Caretaker(负责人): 负责保存备忘录对象，但是不能对备忘录对象进行操作或检查其内容。它只负责将备忘录对象传递给原发器对象 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/21_Memento 首先，定义原发器结构体 Originator，该结构体具有保存和恢复状态的方法 12345678// Memento 备忘录对象type Memento struct &#123; state string&#125;func (m *Memento) GetState() string &#123; return m.state&#125; 然后，定义备忘录结构体 Memento，用于存储原发器的状态： 12345678910111213141516// Originator 发起者对象type Originator struct &#123; state string&#125;func (o *Originator) SetState(state string) &#123; o.state = state&#125;func (o *Originator) CreateMemento() *Memento &#123; return &amp;Memento&#123;state: o.state&#125;&#125;func (o *Originator) RestoreMemento(m *Memento) &#123; o.state = m.GetState()&#125; 最后，定义负责人结构体 Caretaker，负责保存和获取备忘录对象： 123456789101112// Caretaker 备忘录管理器type Caretaker struct &#123; memento *Memento&#125;func (c *Caretaker) GetMemento() *Memento &#123; return c.memento&#125;func (c *Caretaker) SetMemento(m *Memento) &#123; c.memento = m&#125; 执行 12345678910111213141516//1. 原始对象设置状态originator := &amp;Originator&#123;&#125;originator.SetState(\"State 1\")fmt.Println(\"当前状态:\", originator.state)//2. 备忘发起者caretaker := &amp;Caretaker&#123;&#125;// 创建备忘录并保存状态caretaker.SetMemento(originator.CreateMemento())originator.SetState(\"State 2\")fmt.Println(\"更新后状态:\", originator.state)//3. 恢复状态originator.RestoreMemento(caretaker.GetMemento())fmt.Println(\"恢复后状态:\", originator.state) 结果 123当前状态: State 1更新后状态: State 2恢复后状态: State 1 为什么不直接保存原发器而要将对象中的内容保存到另外一个对象中？ 答：当直接保存的时候，或多或少原发器中的内容就已经暴露出来了。而通过单独封装一个备忘录与负责人，客户端是无感知的，仅仅是恢复内容 适用场景 当需要保存和恢复对象的状态，并且希望在不破坏对象封装性的情况下进行操作时，可以使用备忘录模式。 当需要提供撤销操作的功能，或者需要实现 “撤销-重做” 的功能时，备忘录模式可以很好地支持这种需求。 当需要保存对象状态的历史记录，以便进行回溯或者比较时，备忘录模式也是一个不错的选择。 总结 可以在不破坏对象封装情况的前提下创建对象状态快照 可以通过让负责人维护原发器状态历史记录来简化原发器代码 参考链接 https://refactoringguru.cn/design-patterns/memento https://lailin.xyz/post/memento.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-21-访问者模式","slug":"Design Patterns/设计模式-21-访问者模式","date":"2022-09-03T14:58:29.000Z","updated":"2023-07-23T07:31:23.901Z","comments":true,"path":"2022/09/03/Design Patterns/设计模式-21-访问者模式/","link":"","permalink":"http://xboom.github.io/2022/09/03/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-21-%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"访问者模式是一种行为设计模式，它允许在不改变数据结构的情况下，定义新的操作（访问者）并应用于数据结构中的元素。在这种模式下，我们将数据结构和操作之间的耦合分离，使得新增操作时不需要修改元素的类 访问者模式的核心思想是将操作封装在访问者对象中，而不是分散在被访问的对象中。被访问的对象通过接受访问者的访问，将自身作为参数传递给访问者，从而实现对自身操作的解耦和扩展 它会导致代码的可读性、可维护性变差，所以，访问者模式在实际的软件开发中很 少被用到，在没有特别必要的情况下，建议不要使用访问者模式。 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/20_Visitor 首先，定义一个访问者接口 Visitor，表示访问不同的元素 12345678910111213141516// Visitor 访问者接口type Visitor interface &#123; VisitConcreteElementA(element *ConcreteElementA) VisitConcreteElementB(element *ConcreteElementB)&#125;// ConcreteVisitor 具体访问者type ConcreteVisitor struct&#123;&#125;func (v *ConcreteVisitor) VisitConcreteElementA(element *ConcreteElementA) &#123; fmt.Println(\"访问者正在访问具体元素 A\")&#125;func (v *ConcreteVisitor) VisitConcreteElementB(element *ConcreteElementB) &#123; fmt.Println(\"访问者正在访问具体元素 B\")&#125; 然后，定义多个具体元素结构体，它们实现了接受访问者访问的方法： 123456789101112131415161718// Element 元素接口type Element interface &#123; Accept(visitor Visitor)&#125;// ConcreteElementA 具体元素 Atype ConcreteElementA struct&#123;&#125;func (e *ConcreteElementA) Accept(visitor Visitor) &#123; visitor.VisitConcreteElementA(e)&#125;// ConcreteElementB 具体元素 Btype ConcreteElementB struct&#123;&#125;func (e *ConcreteElementB) Accept(visitor Visitor) &#123; visitor.VisitConcreteElementB(e)&#125; 接着，定义一个具体的访问者结构体，它实现了访问者接口中定义的方法 1234567891011121314// ObjectStructure 对象结构type ObjectStructure struct &#123; elements []Element&#125;func (o *ObjectStructure) Attach(element Element) &#123; o.elements = append(o.elements, element)&#125;func (o *ObjectStructure) Accept(visitor Visitor) &#123; for _, element := range o.elements &#123; element.Accept(visitor) &#125;&#125; 运行 123456objectStructure := &amp;ObjectStructure&#123;&#125;objectStructure.Attach(&amp;ConcreteElementA&#123;&#125;)objectStructure.Attach(&amp;ConcreteElementB&#123;&#125;)visitor := &amp;ConcreteVisitor&#123;&#125;objectStructure.Accept(visitor) 结果 12访问者正在访问具体元素 A访问者正在访问具体元素 B 适用场景 当一个对象的结构较为稳定，但对该对象的操作却经常变化时，使用访问者模式可以将操作的变化封装在访问者对象中，而不影响元素对象的稳定性。 当需要对一个对象结构中的多个元素进行操作，并且这些操作具有一定的关联性时，可以使用访问者模式将这些操作封装在一个访问者中，提高代码的复用性和可维护性。 当对象结构中的元素类数量较少且固定时，使用访问者模式可以简化代码结构，将元素的操作集中在访问者中，减少代码的分散性。 总结 开闭原则。 可以引入在不同类对象上执行的新行为， 且无需对这些类做出修改。 单一职责原则。 可将同一行为的不同版本移到同一个类中。 访问者对象可以在与各种对象交互时收集一些有用的信息。 当你想要遍历一些复杂的对象结构 （例如对象树）， 并在结构中的每个对象上应用访问者时， 这些信息可能会有所帮助 参考链接 https://refactoringguru.cn/design-patterns/visitor https://lailin.xyz/post/visitor.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-20-迭代器模式","slug":"Design Patterns/设计模式-20-迭代器模式","date":"2022-09-02T14:58:29.000Z","updated":"2023-07-23T07:31:12.374Z","comments":true,"path":"2022/09/02/Design Patterns/设计模式-20-迭代器模式/","link":"","permalink":"http://xboom.github.io/2022/09/02/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-20-%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"迭代器模式是一种行为设计模式， 让你能在不暴露集合底层表现形式(列表、 栈和树等) 的情况下遍历集合中所有的元素 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/19_Iterator 定义一个迭代器接口 Iterator，该接口包含了访问聚合对象元素的方法 12345// Iterator 迭代器接口type Iterator interface &#123; HasNext() bool Next() interface&#123;&#125;&#125; 定义一个聚合对象接口 Aggregate，该接口包含了创建迭代器对象的方法 1234// Aggregate 聚合对象接口type Aggregate interface &#123; CreateIterator() Iterator&#125; 接着，实现具体的迭代器结构体 ConcreteIterator，它实现了 Iterator 接口： 123456789101112131415// ConcreteIterator 具体迭代器type ConcreteIterator struct &#123; aggregate *ConcreteAggregate index int&#125;func (i *ConcreteIterator) HasNext() bool &#123; return i.index &lt; len(i.aggregate.items)&#125;func (i *ConcreteIterator) Next() interface&#123;&#125; &#123; item := i.aggregate.items[i.index] i.index++ return item&#125; 再实现具体的聚合对象结构体 ConcreteAggregate，它实现了 Aggregate 接口，并在 CreateIterator 方法中返回具体的迭代器对象 123456789101112131415// ConcreteAggregate 具体聚合对象type ConcreteAggregate struct &#123; items []interface&#123;&#125;&#125;func (a *ConcreteAggregate) CreateIterator() Iterator &#123; return &amp;ConcreteIterator&#123; aggregate: a, index: 0, &#125;&#125;func (a *ConcreteAggregate) AddItem(item interface&#123;&#125;) &#123; a.items = append(a.items, item)&#125; 运行 12345678910aggregate := &amp;ConcreteAggregate&#123;&#125;aggregate.AddItem(\"Item 1\")aggregate.AddItem(\"Item 2\")aggregate.AddItem(\"Item 3\")iterator := aggregate.CreateIterator()for iterator.HasNext() &#123; item := iterator.Next() fmt.Println(item)&#125; 结果 123Item 1Item 2Item 3 适用场景 集合类容器的遍历：使用迭代器模式可以方便地遍历集合类容器（如数组、链表、哈希表等）中的元素，而无需关心容器内部的具体实现方式。 文件系统遍历：对于文件系统中的目录和文件，可以使用迭代器模式进行遍历操作，递归地访问每个目录并获取其中的文件信息。 数据库结果集遍历：在数据库操作中，可以使用迭代器模式遍历查询结果集，逐行获取数据并进行相应的处理。 总结 单一职责原则。 通过将体积庞大的遍历算法代码抽取为独立的类， 对客户端代码和集合进行整理。 开闭原则。 可实现新型的集合和迭代器并将其传递给现有代码， 无需修改现有代码。 可以并行遍历同一集合， 因为每个迭代器对象都包含其自身的遍历状态。 相似的， 可以暂停遍历并在需要时继续。 参考链接 https://refactoringguru.cn/design-patterns/iterator https://lailin.xyz/post/iterator.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-19-状态模式","slug":"Design Patterns/设计模式-19-状态模式","date":"2022-09-01T14:58:29.000Z","updated":"2023-07-23T07:31:01.158Z","comments":true,"path":"2022/09/01/Design Patterns/设计模式-19-状态模式/","link":"","permalink":"http://xboom.github.io/2022/09/01/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-19-%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"状态模式是一种行为设计模式， 能在一个对象的内部状态变化时改变其行为， 使其看上去就像改变了自身所属的类一样。 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/18_State 定义一个状态接口 State，该接口包含了对象在不同状态下可能执行的方法 1234// State 状态接口type State interface &#123; HandleState()&#125; 然后，定义一个上下文对象 Context，持有当前状态并且可以在不同状态下执行相应的方法 123456789101112// Context 环境类type Context struct &#123; state State&#125;func (c *Context) SetState(state State) &#123; c.state = state&#125;func (c *Context) Request() &#123; c.state.HandleState()&#125; 接着，实现具体的状态结构体，每个状态结构体都实现了 State 接口中定义的方法 12345678910111213// ConcreteStateA 具体状态 Atype ConcreteStateA struct&#123;&#125;func (s *ConcreteStateA) HandleState() &#123; fmt.Println(\"处理具体状态 A\")&#125;// ConcreteStateB 具体状态 Btype ConcreteStateB struct&#123;&#125;func (s *ConcreteStateB) HandleState() &#123; fmt.Println(\"处理具体状态 B\")&#125; 运行 123456789context := &amp;Context&#123;&#125;stateA := &amp;ConcreteStateA&#123;&#125;context.SetState(stateA)context.Request() // Output: 处理具体状态 AstateB := &amp;ConcreteStateB&#123;&#125;context.SetState(stateB)context.Request() // Output: 处理具体状态 B 结果 12处理具体状态 A处理具体状态 B 应用场景 订单状态管理：订单可以处于不同的状态（如待支付、已支付、已发货、已完成等），每个状态下订单的行为和处理方式不同，可以使用状态模式来管理订单的状态转换和行为。 交通信号灯控制：交通信号灯可以处于红灯、黄灯和绿灯等不同的状态，每个状态下车辆的行为和规则也不同，可以使用状态模式来控制交通信号灯的状态转换和车辆的行为。 游戏角色状态：游戏角色可以处于不同的状态（如正常、受伤、死亡等），每个状态下角色的行为和属性也不同，可以使用状态模式来管理游戏角色的状态和行为 总结 单一职责原则。 将与特定状态相关的代码放在单独的类中。 开闭原则。 无需修改已有状态类和上下文就能引入新状态。 通过消除臃肿的状态机条件语句简化上下文代码。 参考链接 https://refactoringguru.cn/design-patterns/state https://lailin.xyz/post/state.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-18-职责链模式","slug":"Design Patterns/设计模式-18-职责链模式","date":"2022-08-31T14:58:29.000Z","updated":"2023-07-23T07:30:49.831Z","comments":true,"path":"2022/08/31/Design Patterns/设计模式-18-职责链模式/","link":"","permalink":"http://xboom.github.io/2022/08/31/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-18-%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"责任链模式是一种行为设计模式， 允许你将请求沿着处理者链进行发送。 收到请求后， 每个处理者均可对请求进行处理， 或将其传递给链上的下个处理者 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/17_Chain 首先，定义一个请求接口 12345// Request 请求接口type Request interface &#123; GetLevel() int GetContent() string&#125; 定义一个处理者接口 Handler，该接口定义了处理请求的方法 12345// Handler 处理者接口type Handler interface &#123; HandleRequest(request Request) SetNext(handler Handler)&#125; 具体处理类 12345678910111213141516171819202122232425// ConcreteHandler 具体处理者类type ConcreteHandler struct &#123; level int next Handler&#125;func NewConcreteHandler(level int) *ConcreteHandler &#123; return &amp;ConcreteHandler&#123; level: level, &#125;&#125;func (h *ConcreteHandler) HandleRequest(request Request) &#123; if request.GetLevel() == h.level &#123; fmt.Printf(\"Handler%d 处理请求：%s\\n\", h.level, request.GetContent()) &#125; else if h.next != nil &#123; h.next.HandleRequest(request) &#125; else &#123; fmt.Println(\"没有合适的处理者处理该请求\") &#125;&#125;func (h *ConcreteHandler) SetNext(handler Handler) &#123; h.next = handler&#125; 最后，定义具体的处理者结构体，实现 Handler 接口 1234567891011121314151617181920// ConcreteRequest 具体请求类type ConcreteRequest struct &#123; level int content string&#125;func NewConcreteRequest(level int, content string) *ConcreteRequest &#123; return &amp;ConcreteRequest&#123; level: level, content: content, &#125;&#125;func (r *ConcreteRequest) GetLevel() int &#123; return r.level&#125;func (r *ConcreteRequest) GetContent() string &#123; return r.content&#125; 运行 1234567891011121314handler1 := NewConcreteHandler(1)handler2 := NewConcreteHandler(2)handler3 := NewConcreteHandler(3)handler1.SetNext(handler2)handler2.SetNext(handler3)request1 := NewConcreteRequest(2, \"请求 1\")request2 := NewConcreteRequest(3, \"请求 2\")request3 := NewConcreteRequest(1, \"请求 3\")handler1.HandleRequest(request1)handler1.HandleRequest(request2)handler1.HandleRequest(request3) 结果 123Handler2 处理请求：请求 1Handler3 处理请求：请求 2Handler1 处理请求：请求 3 错误处理 错误处理也是一种类似职责链的逻辑 12345678910111213141516171819202122232425262728293031type check struct &#123; err error&#125;func (c check) checkName(name string) check &#123; if c.err != nil &#123; if len(name) &lt; 0 &#123; c.err = errors.New(\"name length wrong\") return c &#125; &#125; return check&#123;&#125;&#125;func (c check) checkAge(age uint8) check &#123; if c.err != nil &#123; if age &gt; 200 &#123; c.err = errors.New(\"age wrong\") return c &#125; &#125; return check&#123;&#125;&#125;func main() &#123; c := check&#123;&#125; c := c.checkName(\"hello\").checkAge(18) if c.err != nil &#123; //.... &#125;&#125; Functiton Option Function Option 也算是一种职责链模式 123456789101112131415161718192021222324252627282930313233type Person struct &#123; name string age int gender string&#125;type Option func(*Person)func WithName(name string) Option &#123; return func(p *Person) &#123; p.name = name &#125;&#125;func WithAge(age int) Option &#123; return func(p *Person) &#123; p.age = age &#125;&#125;func WithGender(gender string) Option &#123; return func(p *Person) &#123; p.gender = gender &#125;&#125;func NewPerson(opts ...Option) *Person &#123; p := &amp;Person&#123;&#125; for _, opt := range opts &#123; opt(p) &#125; return p&#125; 函数选项模式可以实现一些类似职责链的行为，但它并不是职责链模式的直接实现。职责链模式更加注重请求的传递和处理过程，通常涉及多个处理者的协作，而函数选项模式主要用于对象构造和定制 总结 可以控制请求处理的顺序， 单一职责原则。 可对发起操作和执行操作的类进行解耦 开闭原则。 可以在不更改现有代码的情况下在程序中新增处理者 当职责链过长，可能需要遍历整个链才能被处理 适用于异常处理链、权限验证等 参考链接 https://refactoringguru.cn/design-patterns/chain-of-responsibility https://lailin.xyz/post/chain.html https://time.geekbang.org/column/article/330212 https://time.geekbang.org/column/article/330207","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-17-策略模式","slug":"Design Patterns/设计模式-17-策略模式","date":"2022-08-30T14:58:29.000Z","updated":"2023-07-23T07:30:35.636Z","comments":true,"path":"2022/08/30/Design Patterns/设计模式-17-策略模式/","link":"","permalink":"http://xboom.github.io/2022/08/30/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-17-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"策略模式是一种行为设计模式， 它能让你定义一系列算法， 并将每种算法分别放入独立的类中， 以使算法的对象能够相互替换 业务逻辑 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/16_Strategy 定义一个策略接口，其中包含一个方法 Execute()，代表具体的算法 1234// Strategy 策略接口type Strategy interface &#123; Execute(num1, num2 int) int&#125; 创建多个实现了策略接口的具体策略类，分别实现不同的算法 12345678910111213// ConcreteStrategyAdd 具体策略类 - 加法type ConcreteStrategyAdd struct&#123;&#125;func (s *ConcreteStrategyAdd) Execute(num1, num2 int) int &#123; return num1 + num2&#125;// ConcreteStrategySubtract 具体策略类 - 减法type ConcreteStrategySubtract struct&#123;&#125;func (s *ConcreteStrategySubtract) Execute(num1, num2 int) int &#123; return num1 - num2&#125; 创建一个上下文结构体，用于调用具体的算法 123456789101112// Context 上下文类type Context struct &#123; strategy Strategy&#125;func (c *Context) SetStrategy(strategy Strategy) &#123; c.strategy = strategy&#125;func (c *Context) ExecuteStrategy(num1, num2 int) int &#123; return c.strategy.Execute(num1, num2)&#125; 运行 1234567891011context := &amp;Context&#123;&#125;// 使用加法策略context.SetStrategy(&amp;ConcreteStrategyAdd&#123;&#125;)result := context.ExecuteStrategy(10, 5)fmt.Println(\"加法策略结果:\", result) // Output: 加法策略结果: 15// 使用减法策略context.SetStrategy(&amp;ConcreteStrategySubtract&#123;&#125;)result = context.ExecuteStrategy(10, 5)fmt.Println(\"减法策略结果:\", result) // Output: 减法策略结果: 5 结果 12加法策略结果: 15减法策略结果: 5 总结 当想使用对象中各种不同的算法变体，并希望能在运行时切换算法时 可以将算法的实现和使用算法的代码隔离开来 开闭原则。 无需对上下文进行修改就能够引入新的策略 参考链接 https://refactoringguru.cn/design-patterns/strategy https://lailin.xyz/post/strategy.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-16-模板方法模式","slug":"Design Patterns/设计模式-16-模板方法模式","date":"2022-08-29T14:58:29.000Z","updated":"2023-07-23T07:30:16.134Z","comments":true,"path":"2022/08/29/Design Patterns/设计模式-16-模板方法模式/","link":"","permalink":"http://xboom.github.io/2022/08/29/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-16-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"模板方法模式是一种行为设计模式， 它在超类中定义了一个算法的框架， 允许子类在不修改结构的情况下重写算法的特定步骤 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/15_Template 定义一个模板方法，其中包含算法的骨架 123456789101112// Template 模板流程type Template interface &#123; Step1() Step2() Step3()&#125;func (t *ConcreteTemplate) TemplateMethod() &#123; t.step1() t.step2() t.step3()&#125; 定义一个模板的实现(很重要，因为 Golang 不能直接继承) 12345678910111213type ConcreteTemplate struct&#123;&#125;func (t *ConcreteTemplate) step1() &#123; fmt.Println(\"ConcreteTemplate:step1\")&#125;func (t *ConcreteTemplate) step2() &#123; fmt.Println(\"ConcreteTemplate:step2\")&#125;func (t *ConcreteTemplate) step3() &#123; fmt.Println(\"ConcreteTemplate:step3\")&#125; 接着，有一个模板的操作方法 12345func GetResult(t Template) &#123; t.Step1() t.Step2() t.Step3()&#125; 构建另外两个模板方法，嵌套了 ConcreteTemplate，并重写了部分步骤 123456789101112131415type ConcreteTemplateA struct &#123; ConcreteTemplate&#125;func (t *ConcreteTemplateA) Step1() &#123; fmt.Println(\"ConcreteTemplateA:step1\")&#125;type ConcreteTemplateB struct &#123; ConcreteTemplate&#125;func (t *ConcreteTemplateB) Step3() &#123; fmt.Println(\"ConcreteTemplateB:step3\")&#125; 运行 12GetResult(&amp;ConcreteTemplateA&#123;&#125;)GetResult(&amp;ConcreteTemplateB&#123;&#125;) 结果 123456ConcreteTemplateA:step1ConcreteTemplate:step2ConcreteTemplate:step3ConcreteTemplate:step1ConcreteTemplate:step2ConcreteTemplateB:step3 类似于重写林父类方法 总结 模板方法模式只适用于具有相似算法结构的场景，如果算法结构非常不同，则不太适合使用该模式 可将重复代码提取到一个超类中 当一个算法的具体实现需要发生改变时，可以通过在子类中重写抽象方法来灵活地扩展算法 当一个算法需要在多个子类中共享时，可以将其骨架放在父类中实现","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-15-观察者模式","slug":"Design Patterns/设计模式-15-观察者模式","date":"2022-08-28T14:58:29.000Z","updated":"2023-07-23T07:29:43.473Z","comments":true,"path":"2022/08/28/Design Patterns/设计模式-15-观察者模式/","link":"","permalink":"http://xboom.github.io/2022/08/28/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-15-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"观察者模式是一种行为设计模式，允许你定义一种订阅机制，可在对象事件发生时通知多个“观察” 该对象的其他对象 逻辑结构 在观察者模式中，有两个角色： Subject(目标): 定义了对象之间的一对多依赖关系。当其状态发生改变时，通知所有注册的观察者对象。 Observer(观察者): 定义了接收到目标通知时所执行的操作 ConcreteSubject 类与 Observer 接口之间有关联关系。意味着 ConcreteSubject 类通过 RegisterObserver 和 RemoveObserver 方法来管理多个 Observer 观察者对象 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/14_Observer 首先，定义观察者接口，表示收到目标通知触发更新的接口 1234// Observer 观察者接口type Observer interface &#123; Update(message string)&#125; 其次，定义目标接口，每个目标都可以添加、删除观察者，并可以通知观察者 123456// Subject 被观察者接口type Subject interface &#123; RegisterObserver(observer Observer) RemoveObserver(observer Observer) NotifyObservers()&#125; 接着，实现观察者，也就是收到通知之后触发的业务 12345678// ConcreteObserver 具体观察者type ConcreteObserver struct &#123; name string&#125;func (co *ConcreteObserver) Update(message string) &#123; fmt.Printf(\"[%s] 收到消息：%s\\n\", co.name, message)&#125; 最后，实现目标，目标通知注册的每个观察者。所以目标中有存储观察者们 1234567891011121314151617181920212223// ConcreteSubject 具体被观察者type ConcreteSubject struct &#123; observers []Observer //观察者们&#125;// RegisterObserver 注册观察者func (cs *ConcreteSubject) RegisterObserver(observer Observer) &#123; cs.observers = append(cs.observers, observer)&#125;// RemoveObserver 删除观察者func (cs *ConcreteSubject) RemoveObserver(observer Observer) &#123; index := -1 for i, obs := range cs.observers &#123; if obs == observer &#123; index = i break &#125; &#125; if index &gt;= 0 &#123; cs.observers = append(cs.observers[:index], cs.observers[index+1:]...) &#125;&#125; 运行逻辑 123456789101112131415161718//定义观察者与被观察者subject := &amp;ConcreteSubject&#123;&#125;observer1 := &amp;ConcreteObserver&#123;name: \"Observer1\"&#125;observer2 := &amp;ConcreteObserver&#123;name: \"Observer2\"&#125;//对象注册观察者subject.RegisterObserver(observer1)subject.RegisterObserver(observer2)//通知观察者subject.NotifyObservers(\"Hello, observers!\")//删除一个观察者subject.RemoveObserver(observer1)//再次通知观察者subject.NotifyObservers(\"Hi, observers!\") 运行结果 123[Observer1] 收到消息：Hello, observers![Observer2] 收到消息：Hello, observers![Observer2] 收到消息：Hi, observers! 总结 当一个对象状态的改变需要改变其他对象，或实际对象是事先未知的或动态变化的时 当应用中的一些对象必须观察其他对象时，但仅能在有限时间内或特定情况下使用 开闭原则。 无需修改发布者代码就能引入新的订阅者类 (如果是发布者接口则可轻松引入发布者类)，channel 也存在类似功能。 由于主题和观察者之间的关系是通过接口而不是具体实现建立的，因此它们可以在不同的上下文中重复使用 参考链接 https://refactoringguru.cn/design-patterns/observer https://lailin.xyz/post/observer.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-14-享元模式","slug":"Design Patterns/设计模式-14-享元模式","date":"2022-08-27T14:58:29.000Z","updated":"2023-07-23T07:29:27.317Z","comments":true,"path":"2022/08/27/Design Patterns/设计模式-14-享元模式/","link":"","permalink":"http://xboom.github.io/2022/08/27/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-14-%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"享元模式是一种结构型设计模式， 它摒弃了在每个对象中保存所有数据的方式， 通过共享多个对象所共有的相同状态， 能在有限的内存容量中载入更多对象 逻辑结构 在享元模式中，有四个角色： Flyweight(抽象享元)：声明公共方法，这些方法可以向外界提供对象的内部状态。 ConcreteFlyweight(具体享元)：实现抽象享元接口，保存对象内部状态，并且可以被共享。 FlyweightFactory(享元工厂)：维护一个享元池(Flyweight Pool)，用于存储已经创建的共享对象。客户端可以向工厂请求一个享元，如果工厂中不存在该享元，则创建新的享元并将其加入到享元池中；否则直接从享元池中返回已有的享元 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/13_Component 首先，定义公共方法，需要被享元的内容 1234// Flyweight 是享元接口type Flyweight interface &#123; Operation(extrinsicState string)&#125; 然后定义具体的享元操作 1234567891011121314// ConcreteFlyweight 是具体享元类type ConcreteFlyweight struct &#123; intrinsicState string&#125;func NewConcreteFlyweight(intrinsicState string) *ConcreteFlyweight &#123; return &amp;ConcreteFlyweight&#123; intrinsicState: intrinsicState, &#125;&#125;func (f *ConcreteFlyweight) Operation(extrinsicState string) &#123; fmt.Printf(\"具体享元对象：内部状态为 %s，外部状态为 %s\\n\", f.intrinsicState, extrinsicState)&#125; 第三步就是一个享元工厂 1234567891011121314151617181920// FlyweightFactory 是享元工厂类type FlyweightFactory struct &#123; flyweights map[string]Flyweight&#125;func NewFlyweightFactory() *FlyweightFactory &#123; return &amp;FlyweightFactory&#123; flyweights: make(map[string]Flyweight), &#125;&#125;func (ff *FlyweightFactory) GetFlyweight(key string) Flyweight &#123; if flyweight, ok := ff.flyweights[key]; ok &#123; return flyweight &#125; flyweight := NewConcreteFlyweight(key) ff.flyweights[key] = flyweight return flyweight&#125; 执行 12345678910factory := NewFlyweightFactory()flyweight1 := factory.GetFlyweight(\"A\")flyweight1.Operation(\"1\")flyweight2 := factory.GetFlyweight(\"B\")flyweight2.Operation(\"2\")flyweight3 := factory.GetFlyweight(\"A\")flyweight3.Operation(\"3\") 结果 123具体享元对象：内部状态为 A，外部状态为 1具体享元对象：内部状态为 B，外部状态为 2具体享元对象：内部状态为 A，外部状态为 3 总结 仅在程序必须支持大量对象且没有足够的内存容量时使用享元模式(类似与内存池，但是这个重复利用的不是分配的内存而是对象) 享元中共享的是对象，如果对象多处使用修改对象内容需要注意(类似指针)，其次需要考虑并发处理 好处就是减少了内存使用，但这种适用的是常驻对象，临时对象需要考虑内存上限与过期删除问题 有一个类似的 共享调用 共享的是请求也可以看看 参考链接 https://refactoringguru.cn/design-patterns/flyweight https://lailin.xyz/post/flyweight.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-13-组合模式","slug":"Design Patterns/设计模式-13-组合模式","date":"2022-08-26T14:58:29.000Z","updated":"2023-07-23T07:29:07.218Z","comments":true,"path":"2022/08/26/Design Patterns/设计模式-13-组合模式/","link":"","permalink":"http://xboom.github.io/2022/08/26/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-13-%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"组合模式是一种结构型设计模式， 使用它将对象组合成树状结构， 并且能像使用独立对象一样使用它们 逻辑结构 在组合模式中，有三个角色： Component(抽象构件): 定义了叶子节点和组合节点的公共接口。 Leaf(叶子节点): 表示树形结构中的叶子节点对象，它没有子节点。 Composite(组合节点): 表示树形结构中的组合节点对象，它可以包含子节点 实现代码 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/12_Component 定义抽象组件 123456// 抽象构件type Component interface &#123; Add(component Component) Remove(component Component) Display(depth int)&#125; 叶子节点实现抽象组件 123456789101112// 叶子节点type Leaf struct &#123; name string&#125;func (l *Leaf) Add(component Component) &#123;&#125;func (l *Leaf) Remove(component Component) &#123;&#125;func (l *Leaf) Display(depth int) &#123; fmt.Println(strings.Repeat(\"-\", depth) + l.name)&#125; 实现组合 12345678910111213141516171819202122232425262728// 组合节点type Composite struct &#123; name string components []Component&#125;func NewComposite(name string) *Composite &#123; return &amp;Composite&#123;name, make([]Component, 0)&#125;&#125;func (c *Composite) Add(component Component) &#123; c.components = append(c.components, component)&#125;func (c *Composite) Remove(component Component) &#123; for i, v := range c.components &#123; if v == component &#123; c.components = append(c.components[:i], c.components[i+1:]...) &#125; &#125;&#125;func (c *Composite) Display(depth int) &#123; fmt.Println(strings.Repeat(\"-\", depth) + c.name) for _, component := range c.components &#123; component.Display(depth + 2) &#125;&#125; 运行 1234567891011root := NewComposite(\"root\")root.Add(&amp;Leaf&#123;\"Leaf A\"&#125;)root.Add(&amp;Leaf&#123;\"Leaf B\"&#125;)comp := NewComposite(\"Composite X\")comp.Add(&amp;Leaf&#123;\"Leaf XA\"&#125;)comp.Add(&amp;Leaf&#123;\"Leaf XB\"&#125;)root.Add(comp)root.Display(1) 结果 123456-root---Leaf A---Leaf B---Composite X-----Leaf XA-----Leaf XB 输出一个树形结构 总结 可以利用多态和递归机制更方便地使用复杂树结构。 开闭原则，无需更改现有代码， 就可以在应用中添加新元素， 使其成为对象树的一部分 参考链接 https://refactoringguru.cn/design-patterns/composite https://lailin.xyz/post/composite.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-12-门面模式","slug":"Design Patterns/设计模式-12-门面模式","date":"2022-08-25T14:58:29.000Z","updated":"2023-07-23T07:28:50.082Z","comments":true,"path":"2022/08/25/Design Patterns/设计模式-12-门面模式/","link":"","permalink":"http://xboom.github.io/2022/08/25/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-12-%E9%97%A8%E9%9D%A2%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"**门面模式(外观模式)**是一种结构型设计模式， 能为程序库、 框架或其他复杂类提供一个简单的接口。 问题描述：假设必须在代码中使用某个复杂的库或框架中的众多对象。 正常情况下，需要负责所有对象的初始化工作、 管理其依赖关系并按正确的顺序执行方法等。最终， 程序中类的业务逻辑将与第三方类的实现细节紧密耦合， 使得理解和维护代码的工作很难进行。 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/11_Facade 比如有三个组件 123456789101112131415161718// ComponentA 子系统中的组件type ComponentA struct&#123;&#125;func (c *ComponentA) MethodA() &#123; fmt.Println(\"ComponentA: MethodA\")&#125;type ComponentB struct&#123;&#125;func (c *ComponentB) MethodB() &#123; fmt.Println(\"ComponentB: MethodB\")&#125;type ComponentC struct&#123;&#125;func (c *ComponentC) MethodC() &#123; fmt.Println(\"ComponentC: MethodC\")&#125; 使用门面模式统一处理 12345678910111213141516171819202122// Facade 门面type Facade struct &#123; componentA *ComponentA componentB *ComponentB componentC *ComponentC&#125;func NewFacade() *Facade &#123; return &amp;Facade&#123; componentA: &amp;ComponentA&#123;&#125;, componentB: &amp;ComponentB&#123;&#125;, componentC: &amp;ComponentC&#123;&#125;, &#125;&#125;// Operation 提供简单统一的接口给客户端使用func (f *Facade) Operation() &#123; fmt.Println(\"Facade: Operation\") f.componentA.MethodA() f.componentB.MethodB() f.componentC.MethodC()&#125; 运行 12facade := NewFacade()facade.Operation() 结果 1234Facade: OperationComponentA: MethodAComponentB: MethodBComponentC: MethodC 适用场景 如果需要一个指向复杂子系统的直接接口，且该接口的功能有限，则可以使用门面模式 如果需要将子系统组织为多层结构，可以使用外观 门面模式适用于一个系统的外部和内部之间有着很复杂的交互关系，并且希望通过一个简单的接口对其进行封装的场景，如果门面设计不当，可能会导致门面过于庞大和复杂 参考链接 https://refactoringguru.cn/design-patterns/facade https://lailin.xyz/post/facade.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-11-适配器模式","slug":"Design Patterns/设计模式-11-适配器模式","date":"2022-08-24T14:58:29.000Z","updated":"2023-07-23T07:28:32.923Z","comments":true,"path":"2022/08/24/Design Patterns/设计模式-11-适配器模式/","link":"","permalink":"http://xboom.github.io/2022/08/24/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-11-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"适配器模式是一种结构型设计模式， 它能使接口不兼容的对象能够相互合作。 适配器模式通过封装对象将复杂的转换过程隐藏于幕后， 被封装的对象甚至察觉不到适配器的存在。 其还有助于采用不同接口的对象之间的合作。 在适配器模式中，有三个角色： Target(目标接口): 定义客户端使用的与特定领域相关的接口。 Adapter(适配器): 把源接口转换成目标接口。它通常是一个包装器，它会封装一个已有的类，并将这个类的接口转换为目标接口。 Adaptee(源接口): 定义了需要被适配的接口 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/10_Adapter 1234567891011121314151617181920212223242526// 目标接口type Target interface &#123; Request() string&#125;// 源接口type Adaptee interface &#123; SpecificRequest() string&#125;// 源接口的具体实现type ConcreteAdaptee struct&#123;&#125;func (a *ConcreteAdaptee) SpecificRequest() string &#123; return \"Specific request.\"&#125;// 适配器type Adapter struct &#123; adaptee Adaptee //包含源接口&#125;//实现目标接口func (a *Adapter) Request() string &#123; return \"Adapter: \" + a.adaptee.SpecificRequest()&#125; 运行 12345adaptee := &amp;ConcreteAdaptee&#123;&#125;adapter := &amp;Adapter&#123;adaptee&#125;result := adapter.Request()fmt.Println(result) 结果 1Adapter: Specific request. 其实就是适配器包含原接口，实现目标接口，在实现中调用原接口逻辑 适用场景 当你希望使用某个类，但是其接口与其他代码不兼容时，可以使用适配器类 如果您需要复用这样一些类，他们处于同一个继承体系，并且他们又有了额外的一些共同的方法，但是这些共同的方法不是所有在这一继承体系中的子类所具有的共性 总结 单一职责原则你可以将接口或数据转换代码从程序主要业务逻辑中分离。 开闭原则。 只要客户端代码通过客户端接口与适配器进行交互， 你就能在不修改现有客户端代码的情况下在程序中添加新类型的适配器 参考链接 https://lailin.xyz/post/adapter.html https://refactoringguru.cn/design-patterns/adapter","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-10-装饰模式","slug":"Design Patterns/设计模式-10-装饰模式","date":"2022-08-23T14:58:29.000Z","updated":"2023-07-23T07:28:09.778Z","comments":true,"path":"2022/08/23/Design Patterns/设计模式-10-装饰模式/","link":"","permalink":"http://xboom.github.io/2022/08/23/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-10-%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"装饰模式是一种结构型设计模式， 允许在运行时动态地给对象添加新的行为。 更改一个对象的行为时，不能忽视继承可能引发的几个严重问题 继承是静态的，无法在运行时更改已有对象的行为， 只能使用由不同子类创建的对象替代当前的整个对象。 子类只能有一个父类。 大部分编程语言不允许一个类同时继承多个类的行为 业务逻辑 装饰模式通过将对象放入包装器中来实现这一点，每个包装器都提供了额外的功能。在装饰模式中，有四个角色： Component（抽象构件）：定义了一个对象接口，可以给这些对象动态地添加职责。 ConcreteComponent（具体构件）：定义了一个具体的对象，也可以给这个对象添加一些职责。 Decorator（装饰者）：持有一个抽象构件的引用，并定义了一个与抽象构件接口一致的接口。 ConcreteDecorator（具体装饰者）：负责给具体构件添加额外的职责 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/09_Decorator 首先有一个具体的组件ConcreteComponent构建(这里为具体电池组件 凝聚态电池) 12345type BatteryComponent struct&#123;&#125;func (c *BatteryComponent) Name() string &#123; return \"Condensed Battery\"&#125; Component装饰构建类似封装一层，在ConcreteComponent的基础上进一步拓展，那么就声明一个抽象组件(这里指电池) 1234// Battery 抽象构件，给这个对象动态地添加职责type Battery interface &#123; Name() string&#125; 装饰者Decorator则是在前面组件ConcreteComponent的基础上拓展(这里举例汽车) 1234// Car 装饰者type Car interface &#123; Run() string&#125; 这里构建2 个具体装饰者 1234567891011121314151617// BYD 具体装饰者 BYDtype BYD struct &#123; battery Battery&#125;func (d *BYD) Run() string &#123; return \"BYD use \" + d.battery.Name() + \" run\"&#125;// Tesla 具体装饰者 Teslatype Tesla struct &#123; battery Battery&#125;func (t *Tesla) Run() string &#123; return \"Tesla use \" + t.battery.Name() + \" run\"&#125; 单元测试 1234567891011121314151617181920212223242526272829303132333435var ExpectBattery = \"Condensed Battery\"var ExpectBYD = \"BYD use Condensed Battery run\"var ExpectTesla = \"Tesla use Condensed Battery run\"func TestBYD_Run(t *testing.T) &#123; //1. 构建电池组件 battery := new(BatteryComponent) res := battery.Name() if strings.Compare(res, ExpectBattery) != 0 &#123; t.Fatalf(\"Eoncrete fail expect %s acture %s\", ExpectBattery, res) &#125; //2. 汽车装饰电池 byd := &amp;BYD&#123;battery&#125; res = byd.Run() if strings.Compare(res, ExpectBYD) != 0 &#123; t.Fatalf(\"Eoncrete fail expect %s acture %s\", ExpectBYD, res) &#125;&#125;func TestTesla_Run(t *testing.T) &#123; //1. 构建电池组件 battery := new(BatteryComponent) res := battery.Name() if strings.Compare(res, ExpectBattery) != 0 &#123; t.Fatalf(\"Eoncrete fail expect %s acture %s\", ExpectBattery, res) &#125; //2. 汽车装饰电池 tesla := &amp;Tesla&#123;battery&#125; res = tesla.Run() if strings.Compare(res, ExpectTesla) != 0 &#123; t.Fatalf(\"Eoncrete fail expect %s acture %s\", ExpectTesla, res) &#125;&#125; 适用场景 如果希望在无需修改代码的情况下即可使用对象，且希望在运行时为对象新增额外的行为，可以使用装饰模式。 如果用继承来扩展对象行为的方案难以实现或者根本不可行，你可以使用该模式 总结 无需创建新子类即可扩展对象的行为。 可以在运行时添加或删除对象的功能。 可以用多个装饰封装对象来组合几种行为。 单一职责原则。 可以将实现了许多不同行为的一个大类拆分为多个较小的类 参考链接 https://refactoringguru.cn/design-patterns/decorator https://lailin.xyz/post/decorator.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-09-桥接模式","slug":"Design Patterns/设计模式-09-桥接模式","date":"2022-08-22T14:58:29.000Z","updated":"2023-07-23T07:27:50.526Z","comments":true,"path":"2022/08/22/Design Patterns/设计模式-09-桥接模式/","link":"","permalink":"http://xboom.github.io/2022/08/22/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-09-%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"桥接模式是一种结构型设计模式，将抽象和实现解耦，让它们可以独立变化 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/08_Bridge 定义实现类的接口 123type Implementor interface &#123; operationImpl() string&#125; 实现具体的实现类 1234567891011type ConcreteImplementorA struct&#123;&#125;func (c *ConcreteImplementorA) operationImpl() string &#123; return &quot;ConcreteImplementorA&quot;&#125;type ConcreteImplementorB struct&#123;&#125;func (c *ConcreteImplementorB) operationImpl() string &#123; return &quot;ConcreteImplementorB&quot;&#125; 定义抽象类及其方法 1234567type Abstraction struct &#123; implementor Implementor&#125;func (a *Abstraction) Operation() string &#123; return a.implementor.operationImpl()&#125; 实现具体的抽象类 1234567type RefinedAbstraction struct &#123; Abstraction&#125;func NewRefinedAbstraction(implementor Implementor) *RefinedAbstraction &#123; return &amp;RefinedAbstraction&#123;Abstraction&#123;implementor&#125;&#125;&#125; 通过以上步骤，就可以使用桥接模式了。例如使用 ConcreteImplementorA 类来完成某个操作 1234567implementorA := &amp;ConcreteImplementorA&#123;&#125;abstractionA := NewRefinedAbstraction(implementorA)resultA := abstractionA.Operation() // result = \"ConcreteImplementorA\"implementorB := &amp;ConcreteImplementorB&#123;&#125;abstractionB := NewRefinedAbstraction(implementorB)resultB := abstractionB.Operation() // result = \"ConcreteImplementorB\" 适用场景 想要拆分或重组一个具有多重功能的庞杂类(例如能与多个数据库服务器进行交互的类)，可以使用桥接模式 希望在几个独立维度上扩展一个类 需要在运行时切换不同实现方法 总结 开闭原则。 你可以新增抽象部分和实现部分， 且它们之间不会相互影响。 单一职责原则。 抽象部分专注于处理高层逻辑， 实现部分处理平台细节 桥接模式需要额外的工作量来设计抽象和实现部分的接口，并且增加了系统的理解难度。 如果某个类在多个维度上都有变化，那么桥接模式可能会导致类的数量急剧增加，从而增加系统的复杂度 参考链接 https://refactoringguru.cn/design-patterns/bridge https://lailin.xyz/post/bridge.html","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-08-代理模式","slug":"Design Patterns/设计模式-08-代理模式","date":"2022-08-21T14:58:29.000Z","updated":"2023-07-23T07:27:32.063Z","comments":true,"path":"2022/08/21/Design Patterns/设计模式-08-代理模式/","link":"","permalink":"http://xboom.github.io/2022/08/21/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-08-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"代理模式 在不改变原始类(或叫被代理类)代码的情况下，通过引入代理类来给原始类附加功能，常用在业务系统中开发一些非功能性需求，比如：监控、统计、鉴权、限流、事务、幂等、 日志 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/07_Proxy 需要被代理的行为是卖票 1234// Seller 售票type Seller interface &#123; Sell(name string)&#125; 正常情况是火车站卖票 12345678910111213// Station 火车站type Station struct &#123; stock int //库存&#125;func (station *Station) Sell(name string) &#123; if station.stock &gt; 0 &#123; station.stock-- fmt.Printf(\"%s purchased 1 ticket, %d remaining\\n\", name, station.stock) &#125; else &#123; fmt.Println(\"tickets are sold out\") &#125;&#125; 而使用代理点卖票 123456789101112131415// StationProxy 火车代理点type StationProxy struct &#123; station *Station // 持有一个火车站对象&#125;func (proxy *StationProxy) Sell(name string) &#123; if proxy.station.stock &gt; 0 &#123; proxy.station.stock-- fmt.Printf(\"%s purchased 1 ticket, %d remaining\\n\", name, proxy.station.stock) &#125; else &#123; fmt.Println(\"tickets are sold out\") &#125;&#125;var _ Seller = (*StationProxy)(nil) 单元测试 12345678func TestStationProxy_sell(t *testing.T) &#123; station :&#x3D; &amp;Station&#123;3&#125; proxy :&#x3D; &amp;StationProxy&#123;station&#125; station.Sell(&quot;A&quot;) proxy.Sell(&quot;B&quot;) proxy.Sell(&quot;C&quot;) proxy.Sell(&quot;D&quot;)&#125; 运行的结果是 1234A purchased 1 ticket, 2 remainingB purchased 1 ticket, 1 remainingC purchased 1 ticket, 0 remainingtickets are sold out 总结 延迟初始化， 如果有一个偶尔使用的重量级服务对象，一直保持该对象运行会消耗系统资源时使用代理模式(类似单例模式，但单例模式强调的是单实例) 访问控制(保护代理)，如果你只希望特定客户端使用服务对象，这里的对象可以是操作系统中非常重要的部分，而客户端则是各种已启动的程序(包括恶意程序)，此时可使用代理模式 参考链接 https://lailin.xyz/post/factory.html https://refactoringguru.cn/design-patterns/proxy https://learnku.com/articles/33707","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-07-原型模式","slug":"Design Patterns/设计模式-07-原型模式","date":"2022-08-20T14:58:29.000Z","updated":"2023-07-23T07:27:19.799Z","comments":true,"path":"2022/08/20/Design Patterns/设计模式-07-原型模式/","link":"","permalink":"http://xboom.github.io/2022/08/20/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-07-%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"原型模式是一种创建型设计模式， 使你能够复制已有对象， 而又无需使代码依赖它们所属的类(亦称： 克隆、Clone、Prototype) 如果你有一个对象， 并希望生成与其完全相同的一个复制品， 你该如何实现呢？ 首先， 你必须新建一个属于相同类的对象。 然后， 你必须遍历原始对象的所有成员变量， 并将成员变量值复制到新对象中。 可能存在的问题 并非所有对象都能通过这种方式进行复制， 因为有些对象可能拥有私有成员变量， 它们在对象本身以外是不可见的 有时只知道对象所实现的接口， 而不知道其所属的具体类， 比如可向方法的某个参数传入实现了某个接口的任何对象 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/06_Prototype 具体需要被复制的对象 12345// ConcretePrototype 是具体原型结构体type ConcretePrototype struct &#123; part1 string part2 int&#125; 定义复制接口 1234// Prototype 是原型接口type Prototype interface &#123; Clone() Prototype&#125; 实现复制 12345// Clone 是克隆方法(深拷贝)func (p *ConcretePrototype) Clone() Prototype &#123; clone := *p //值拷贝 return &amp;clone&#125; 这是因为 clone := *p 是值拷贝，这个地方容易忽略！！！，值拷贝，那么改成下面也可以 123func (p ConcretePrototype) Clone() Prototype &#123; return &amp;p&#125; 单元测试 12345678910111213func TestConcretePrototype_Clone(t *testing.T) &#123; prototype :&#x3D; &amp;ConcretePrototype&#123; part1: &quot;hello&quot;, part2: 0, &#125; clone1 :&#x3D; prototype.Clone() prototype.part1 &#x3D; &quot;world&quot; prototype.part2 &#x3D; 100 clone2 :&#x3D; prototype.Clone() fmt.Printf(&quot;p1 %p %+v \\n&quot;, &amp;prototype, prototype) fmt.Printf(&quot;p2 %p %+v \\n&quot;, &amp;clone1, clone1) fmt.Printf(&quot;p3 %p %+v \\n&quot;, &amp;clone2, clone2)&#125; 输出结果为 123p1 0xc0000a6048 &amp;&#123;part1:world part2:100&#125; p2 0xc00008e260 &amp;&#123;part1:hello part2:0&#125; p3 0xc00008e270 &amp;&#123;part1:world part2:100&#125; 总结 另外还可以通过序列号与反序列化实现深拷贝，不过需要考虑到序列化性能 考虑使用内存池进行对象创建 适用场景 创建对象的过程比较复杂，需要进行多项初始化配置才能完成。 需要创建大量相似或相同的对象，并且每个对象都有不同的状态和属性。 由于某些原因，无法或不方便使用其他创建模式（如工厂方法模式、建造者模式等）来创建对象。 希望在运行时动态地生成新对象，并且能够通过克隆来获得更好的性能 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-06-建造者模式","slug":"Design Patterns/设计模式-06-建造者模式","date":"2022-08-19T14:58:29.000Z","updated":"2023-07-23T07:25:04.221Z","comments":true,"path":"2022/08/19/Design Patterns/设计模式-06-建造者模式/","link":"","permalink":"http://xboom.github.io/2022/08/19/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-06-%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"建造者模式 该模式允许你使用相同的创建代码生成不同类型和形式的对象。将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 代码路径： 逻辑结构 建造者模式又可以称为生成器模式，主要分为四个部分 Builder：抽象建造者 (例子中类似 汽车图纸) ConcreteBuilder：具体建造者（例子中是BYD/Tesla的造车工厂） Director：指挥者 (例子中 类似一个高级工程师，根据不同的图纸对应流程生产不同的汽车) Product：产品角色（例子中是车） 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/05_Builder 创建一个产品类，并定义其中所需的属性。 123456// Car 定义一个产品 汽车type Car struct &#123; battery string //电池 electricMotor string //电机 electronicControl string //电控&#125; 创建一个抽象建造者接口，并定义其中需要实现的方法 1234567// Builder 抽象即按照这接口，定义其需要实现的方法type Builder interface &#123; BuildBattery() BuildElectricMotor() BuildElectronicControl() GetCar() *Car&#125; 创建具体的建造者结构体，并实现抽象建造者接口中的方法，这里构建了两个类似的对象 1234567891011121314151617181920212223// BYD 具体建造者type BYD struct &#123; car Car&#125;func (b *BYD) BuildBattery() &#123; b.car.battery = \"BYD Battery\"&#125;func (b *BYD) BuildElectricMotor() &#123; b.car.electricMotor = \"BYD Electric Motor\"&#125;func (b *BYD) BuildElectronicControl() &#123; b.car.electronicControl = \"BYD Electronic Control\"&#125;func (b *BYD) GetCar() *Car &#123; return &amp;b.car&#125;// 具体构建者实现抽象接口var _ Builder = (*BYD)(nil) 另外一个建造者 12345678910111213141516171819202122// Tesla 具体建造者type Tesla struct &#123; car Car&#125;func (t *Tesla) BuildBattery() &#123; t.car.electronicControl = \"Tesla Battery\"&#125;func (t *Tesla) BuildElectricMotor() &#123; t.car.electronicControl = \"Tesla Electronic Motor\"&#125;func (t *Tesla) BuildElectronicControl() &#123; t.car.electronicControl = \"Tesla Electronic Control\"&#125;func (t *Tesla) GetCar() *Car &#123; return &amp;t.car&#125;var _ Builder = (*Tesla)(nil) 创建指挥者结构体，并定义其中需要使用的建造者对象和建造过程 123456789101112131415// Director 构建指挥者type Director struct &#123; builder Builder&#125;// NewDirector 传入具体构建者，创建一个构建指挥者func NewDirector(builder Builder) *Director &#123; return &amp;Director&#123;builder: builder&#125;&#125;func (d *Director) Construct() &#123; d.builder.BuildBattery() d.builder.BuildElectricMotor() d.builder.BuildElectronicControl()&#125; 单元测试 123456789101112131415161718192021222324252627282930var ExpectBydCar = Car&#123;\"BYD Battery\", \"BYD Electric Motor\", \"BYD Electronic Control\"&#125;var ExpectTeslaCar = Car&#123;\"Tesla Battery\", \"Tesla Electronic Motor\", \"Tesla Electronic Control\"&#125;func TestBuilderBYD(t *testing.T) &#123; //1. BYD图纸 byd := new(BYD) //2. BYD工厂 director := NewDirector(byd) //3. 生产BYD汽车 director.Construct() //4. 客户拿到BYD汽车 res := byd.GetCar() if reflect.DeepEqual(res, ExpectBydCar) &#123; t.Fatalf(\"Builder1 fail expect 123 acture %s\", res) &#125;&#125;func TestBuilderTesla(t *testing.T) &#123; //1. Tesla图纸 byd := new(Tesla) //2. Tesla工厂 director := NewDirector(byd) //3. 生产Tesla汽车 director.Construct() //4. 客户拿到Tesla汽车 res := byd.GetCar() if reflect.DeepEqual(res, ExpectTeslaCar) &#123; t.Fatalf(\"Builder1 fail expect 123 acture %s\", res) &#125;&#125; 可选参数 类似于可选参数的应用，可能更好理解，Option 是一种函数式编程技巧，这里也需要构建一个产品，并定义其中的属性 12345678910111213141516171819202122232425262728293031323334353637383940// Product 是要创建的产品结构体type Product struct &#123; part1 string part2 string part3 string&#125;// Option 是可选参数的类型type Option func(*Product)// Part1Option 是 part1 字段的设置函数func Part1Option(value string) Option &#123; return func(p *Product) &#123; p.part1 = value &#125;&#125;// Part2Option 是 part2 字段的设置函数func Part2Option(value string) Option &#123; return func(p *Product) &#123; p.part2 = value &#125;&#125;// Part3Option 是 part3 字段的设置函数func Part3Option(value string) Option &#123; return func(p *Product) &#123; p.part3 = value &#125;&#125;// NewProduct 是创建 Product 的函数，通过传入可选参数并返回 Product 的指针func NewProduct(opts ...Option) *Product &#123; product := &amp;Product&#123;&#125; // 遍历所有可选参数，并逐一将其设置给 Product 结构体的字段 for _, opt := range opts &#123; opt(product) &#125; return product&#125; 在使用的时候传入需要的可选参数即可 12product1 := NewProduct(Part1Option(\"part1\"))product2 := NewProduct(Part2Option(\"part2\"), Part3Option(\"part3\")) 总结 建造者模式适用于对象创建成本比较大需要经过复杂计算的情况 参考文档 https://lailin.xyz/post/builder.html https://refactoringguru.cn/design-patterns/catalog https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-05-抽象工厂模式","slug":"Design Patterns/设计模式-05-抽象工厂模式","date":"2022-08-18T14:58:29.000Z","updated":"2023-07-23T07:26:22.829Z","comments":true,"path":"2022/08/18/Design Patterns/设计模式-05-抽象工厂模式/","link":"","permalink":"http://xboom.github.io/2022/08/18/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-05-%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"工厂模式分为三种更加细分的类型 简单工厂(Simple Factory) 工厂方法(Factory Method) 抽象工厂(Abstract Factory) 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/04_AbstractFactory 有一个简单的产品接口(这里指汽车) 123type Car interface &#123; Name() string&#125; 有两个具体的实现类 12345678type BYD struct &#123;&#125;func (B BYD) Name() string &#123; return \"BYD\"&#125;var _ Car = (*BYD)(nil) 另外一个具体类 12345678type Tesla struct &#123;&#125;func (t Tesla) Name() string &#123; return \"Tesla\"&#125;var _ Car = (*Tesla)(nil) 工厂方法有一个抽象接口，对应具体的产品 1234type Factory interface &#123; CreateBYD() Car CreateTesla() Car&#125; 实现工厂的具体对象 123456789101112type WuhanFactory struct &#123;&#125;func (w WuhanFactory) CreateBYD() Car &#123; return &amp;BYD&#123;&#125;&#125;func (w WuhanFactory) CreateTesla() Car &#123; return &amp;Tesla&#123;&#125;&#125;var _ Factory = (*WuhanFactory)(nil) 另外一个实现对象 123456789101112type ShangHaiFactory struct &#123;&#125;func (s ShangHaiFactory) CreateBYD() Car &#123; return &amp;BYD&#123;&#125;&#125;func (s ShangHaiFactory) CreateTesla() Car &#123; return &amp;Tesla&#123;&#125;&#125;var _ Factory = (*ShangHaiFactory)(nil) 提供一个创建一系列相关或相互依赖对象的接口 Factory，而无需指定它们具体的类。即通过一个工厂接口来创建一组相关联的对象，由具体的工厂 WuhanFactory 、ShangHaiFactory 实现这个接口并创建一系列相关的对象。这样做的好处是能够保证一系列对象被一起创建，并且这些对象之间相互协调 抽象工厂：每个工厂都能产生全系列产品，这个全系列在抽象工厂接口中定义 单元测试 123456789101112131415161718192021222324252627var expectBYD = \"BYD\"var expectTesla = \"Tesla\"func TestShangHaiFactory(t *testing.T) &#123; factory := new(ShangHaiFactory) byd := factory.CreateBYD() if !reflect.DeepEqual(byd.Name(), expectBYD) &#123; t.Fatalf(\"expect %s acture %s\", expectBYD, byd.Name()) &#125; tesla := factory.CreateTesla() if !reflect.DeepEqual(tesla.Name(), expectTesla) &#123; t.Fatalf(\"expect %s acture %s\", expectTesla, tesla.Name()) &#125;&#125;func TestWuhanFactory(t *testing.T) &#123; factory := new(WuhanFactory) byd := factory.CreateBYD() if !reflect.DeepEqual(byd.Name(), expectBYD) &#123; t.Fatalf(\"expect %s acture %s\", expectBYD, byd.Name()) &#125; tesla := factory.CreateTesla() if !reflect.DeepEqual(tesla.Name(), expectTesla) &#123; t.Fatalf(\"expect %s acture %s\", expectTesla, tesla.Name()) &#125;&#125; 总结 工厂方法与抽象工厂的区别： 工厂方法模式（Factory Method Pattern）：定义一个用于创建对象的接口，让子类决定将哪一个类实例化。即通过一个公共的接口来创建具体的对象，由子类来实现这个接口并创建具体的对象。这样做的好处是将具体类的实例化过程封装到子类中，符合开闭原则，有利于系统的扩展。 抽象工厂模式（Abstract Factory Pattern）：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。即通过一个工厂接口来创建一组相关联的对象，由具体的工厂实现这个接口并创建一系列相关的对象。这样做的好处是能够保证一系列对象被一起创建，并且这些对象之间相互协调 抽象工厂模式并没有违背开闭原则，当需要新增一族产品时，只需要扩展抽象工厂接口，并创建对应的具体工厂类来实现该接口。保证在不影响现有功能的情况下，增加新的功能(在抽象接口中新增类型不算修改…) 在实际应用中，如果需要创建一系列相关或依赖的对象，可以选择抽象工厂模式；如果需要创建单个对象，并且需要扩展性和灵活性，可以选择工厂方法模式。所以抽象工厂并不是工厂方式的优化，两者应用场景不一样 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-04-工厂方法模式","slug":"Design Patterns/设计模式-04-工厂方法模式","date":"2022-08-17T14:58:29.000Z","updated":"2023-07-23T07:23:15.038Z","comments":true,"path":"2022/08/17/Design Patterns/设计模式-04-工厂方法模式/","link":"","permalink":"http://xboom.github.io/2022/08/17/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-04-%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"简单工厂模式违背的开闭原则，拓展新的产品的时候都需要修改旧的代码 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/03_FactoryMethod 有一个简单的产品接口(这里指汽车) 123type Car interface &#123; Name() string&#125; 有两个具体的实现类 12345678type BYD struct &#123;&#125;func (B BYD) Name() string &#123; return \"BYD\"&#125;var _ Car = (*BYD)(nil) 另外一个具体类 12345678type Tesla struct &#123;&#125;func (t Tesla) Name() string &#123; return \"Tesla\"&#125;var _ Car = (*Tesla)(nil) 工厂方法有一个抽象接口，对应具体的产品 123type Factory interface &#123; CreateCar() Car&#125; 实现具体产品的工厂 12345678type BYDFactory struct &#123;&#125;func (B BYDFactory) CreateCar() Car &#123; return &amp;BYD&#123;&#125;&#125;var _ Factory = (*BYDFactory)(nil) 另外一个对应具体产品的工厂 12345678type TeslaFactory struct &#123;&#125;func (t TeslaFactory) CreateCar() Car &#123; return &amp;Tesla&#123;&#125;&#125;var _ Factory = (*TeslaFactory)(nil) 单元测试 123456789101112131415161718var expectBYD = \"BYD\"var expectTesla = \"Tesla\"func TestBYDFactory_CreateCar(t *testing.T) &#123; factory := new(BYDFactory) car := factory.CreateCar() if !reflect.DeepEqual(car.Name(), expectBYD) &#123; t.Fatalf(\"expect %s acture %s\", expectBYD, car.Name()) &#125;&#125;func TestTeslaFactory_CreateCar(t *testing.T) &#123; factory := new(TeslaFactory) car := factory.CreateCar() if !reflect.DeepEqual(car.Name(), expectTesla) &#123; t.Fatalf(\"expect %s acture %s\", expectTesla, car.Name()) &#125;&#125; 跟简单工厂模式相比，当需要增加新的产品的时候，完全不会影响旧的逻辑 总结 工厂方法相当于简单工厂模式，实际是通过将对象的创建延迟到具体的工厂类中来解决简单工厂模式的缺点 工厂方法模式完全符合开闭原则 工厂方法模式适用于类型创建比较复杂不是简单的New，将复杂的创建逻辑拆分到多个工厂类中 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern https://chat.openai.com/c/574c8562-2f17-4d1c-8a02-9d1c65f1f6c3","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-03-简单工厂模式","slug":"Design Patterns/设计模式-03-简单工厂模式","date":"2022-08-16T14:58:29.000Z","updated":"2023-07-23T07:23:23.226Z","comments":true,"path":"2022/08/16/Design Patterns/设计模式-03-简单工厂模式/","link":"","permalink":"http://xboom.github.io/2022/08/16/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-03-%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"工厂模式分为三种更加细分的类型 简单工厂(Simple Factory) 工厂方法(Factory Method) 抽象工厂(Abstract Factory) 由于 Go 本身是没有构造函数的，一般采用 NewName 的方式创建对象/接口，当它返回的是接口的时候，其实就是简单工厂模式 逻辑结构 代码实现 代码路径：https://github.com/XBoom/DesignPatterns/tree/main/02_SimpleFactory 有一个简单的产品接口(这里指汽车) 123type Car interface &#123; Name() string&#125; 有两个具体的实现类 12345678type BYD struct &#123;&#125;func (B BYD) Name() string &#123; return \"BYD\"&#125;var _ Car = (*BYD)(nil) 另外一个具体类 12345678type Tesla struct &#123;&#125;func (t Tesla) Name() string &#123; return \"Tesla\"&#125;var _ Car = (*Tesla)(nil) 实现一个简单工厂 12345678910111213type Factory struct &#123;&#125;func (f *Factory) CreateCar(typ string) Car &#123; switch typ &#123; case \"byd\": return &amp;BYD&#123;&#125; case \"tesla\": return &amp;Tesla&#123;&#125; default: panic(\"unknown type\") &#125;&#125; 单元测试 123456789101112131415var expectByd = \"BYD\"var expectTesla = \"Tesla\"func TestFactory_CreateCar(t *testing.T) &#123; factory := new(Factory) car1 := factory.CreateCar(\"byd\") if !reflect.DeepEqual(car1.Name(), expectByd) &#123; t.Fatalf(\"expect %s acture %s\", expectByd, car1.Name()) &#125; car2 := factory.CreateCar(\"tesla\") if !reflect.DeepEqual(car2.Name(), expectTesla) &#123; t.Fatalf(\"expect %s acture %s\", expectTesla, car2.Name()) &#125;&#125; 根据参数的参数不同构建不同的产品对象，当增加一种产品的时候，这里就需要修改 CreateCar 方法，并在使用的时候追加CreateCar 新的类型与业务，违反了开闭原则 总结 增加新的产品类型时需要修改工厂类的代码，违反了开闭原则 简单工厂模式适用于对象较少、对象类型固定的场景，可以提高对象的创建效率和统一管理对象的创建过程 参考文档 https://lailin.xyz/post/factory.html https://github.com/senghoo/golang-design-pattern https://chat.openai.com/c/574c8562-2f17-4d1c-8a02-9d1c65f1f6c3","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"CPP-Concurrency-01-概述","slug":"Concurrency/CPP-Concurrency-01-概述","date":"2022-08-16T14:43:29.000Z","updated":"2023-08-15T14:37:41.484Z","comments":true,"path":"2022/08/16/Concurrency/CPP-Concurrency-01-概述/","link":"","permalink":"http://xboom.github.io/2022/08/16/Concurrency/CPP-Concurrency-01-%E6%A6%82%E8%BF%B0/","excerpt":"","text":"前面学了 C的基本概念，由于并发编程内容比较多，这里单独学习《C Concurrency In Action - second Edition 2019》并发编程 问题 1：什么是并发 在一个双核机器(具有两个处理核心)上，每个任务可以在各自的处理核心上执行。在单核机器上做任务切换时，每个任务的块交织进行。单核交织进行任务的时候中间有一小段分隔(任务切换也是需要时间的) 为了实现交织进行，系统每次从一个任务切换到另一个时都需要切换一次上下文(context switch)，进行上下文的切换时，操作系统必须为当前运行的任务保存CPU的状态和指令指针，并计算出要切换到哪个任务，并为即将切换到的任务重新加载处理器状态。然后，CPU可能要将新任务的指令和数据的内存载入到缓存中，这会阻止CPU执行任何指令，从而造成的更多的延迟。 问题 2：如何实施并发 第一种，每个进程只要一个线程，同时运行多个进程。进程中的所有线程共享地址空间，需要保证数据一致性 第二种，每个进程有多个线程。独立进程可以通过进程间常规的通信方式(信号、套接字、文件、管道)。缺点是 相比线程间通信，它的速度慢，每个进程所需的固定开销(时间启动、内部资源管理进程)。但是编码更容易切安全 问题 3：什么时候不用并发 不使用并发的唯一原因就是收益比不上成本。使用并发的代码在很多情况下难以理解，因此编写和维护的多线程代码就会直接产生脑力成本，同时额外的复杂性也可能引起更多的错误。 例子 123456#include &lt;iostream&gt;using namespace std;int main()&#123; cout &lt;&lt; \"Hello World\\n\";&#125; 与启动一个独立的线程 123456789101112#include &lt;iostream&gt;#include &lt;thread&gt; using namespace std;void hello() &#123; cout &lt;&lt; \"Hello Concurrent World\\n\";&#125;int main()&#123; thread t(hello); // 2 t.join(); // 3&#125; 序号说明： 标准C++库中对多线程支持的声明在新的头文件中 每个线程都必须具有一个初始函数(initial function)，新线程的执行从此开始。对于应用程序来说，初始线程是main()，对于其他线程，可以在 std::thread 对象的构造函数中指定 初始线程拉起新线程之后，会同时继续执行。如果不等待新线程结束，就将自顾自地继续运行到main()的结束，有可能发生在新线程运行之前。所以调用 join() 的等待新线程结束 这里包含了一些问题，待后续解答 进程与线程有什么区别 创建线程的时候发生了什么 线程是如何管理的 线程是如何通信的 参考链接 《C++ Concurrency In Action - second Edition 2019》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"Concurrency","slug":"Concurrency","permalink":"http://xboom.github.io/tags/Concurrency/"}]},{"title":"设计模式-02-单例模式","slug":"Design Patterns/设计模式-02-单例模式","date":"2022-08-15T14:58:29.000Z","updated":"2023-07-23T07:23:30.852Z","comments":true,"path":"2022/08/15/Design Patterns/设计模式-02-单例模式/","link":"","permalink":"http://xboom.github.io/2022/08/15/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-02-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"一个类只允许创建一个对象(或实例)，那这个类就是一个单例类，这种设计模式就叫做单例设计模式(Singleton Design Pattern) 单例模式分为 饿汉式 和 懒汉式 两种实现 饿汉式：初始化的时候已经创建好实例 懒汉式：只有在调用的时候才会初始化 构建的时候注意： 构造函数是私有访问权限(防止被其他地方重新构建) 构建的时候考虑并发情况 考虑是否支持延迟加载 源码路径：https://github.com/XBoom/DesignPatterns/tree/main/01_Singleton 饿汉式 代码实现 123456789101112131415// 饿汉模式，直接在程序初始化的时候赋值type HungrySingleton struct &#123;&#125;var hungrySingleton *HungrySingletonfunc init() &#123; hungrySingleton = new(HungrySingleton)&#125;// GetHungryInstance 获取饿汉实例func GetHungryInstance() *HungrySingleton &#123; return hungrySingleton&#125; 单元测试 1234567891011121314151617func TestGetHungryInstance(t *testing.T) &#123; ins1 := GetHungryInstance() ins2 := GetHungryInstance() if ins1 != ins2 &#123; t.Fatal(\"instance is not equal\") &#125;&#125;func BenchmarkGetHungryInstance(b *testing.B) &#123; b.RunParallel(func(pb *testing.PB) &#123; for pb.Next() &#123; if GetHungryInstance() != GetHungryInstance() &#123; b.Errorf(\"test fail\") &#125; &#125; &#125;)&#125; 性能测试结果 12345pkg: DesignPatterns/01_Singletoncpu: Intel(R) Core(TM) i7-8559U CPU @ 2.70GHzBenchmarkGetHungryInstanceBenchmarkGetHungryInstance-8 1000000000 0.2271 ns/opPASS 懒汉式 代码实现 12345678910111213type LazybonesSingleton struct &#123;&#125;var once sync.Oncevar lazybonesSingleton *LazybonesSingleton// GetLazybonesInstance 获取懒汉实例func GetLazybonesInstance() *LazybonesSingleton &#123; once.Do(func() &#123; lazybonesSingleton = new(LazybonesSingleton) &#125;) return lazybonesSingleton&#125; 单元测试 1234567891011121314151617func TestGetLazybonesInstance(t *testing.T) &#123; ins1 := GetLazybonesInstance() ins2 := GetLazybonesInstance() if ins1 != ins2 &#123; t.Fatal(\"instance is not equal\") &#125;&#125;func BenchmarkGetLazybonesInstance(b *testing.B) &#123; b.RunParallel(func(pb *testing.PB) &#123; for pb.Next() &#123; if GetLazybonesInstance() != GetLazybonesInstance() &#123; b.Errorf(\"test fail\") &#125; &#125; &#125;)&#125; 性能测试结果 12345pkg: DesignPatterns/01_Singletoncpu: Intel(R) Core(TM) i7-8559U CPU @ 2.70GHzBenchmarkGetLazybonesInstanceBenchmarkGetLazybonesInstance-8 1000000000 0.7035 ns/opPASS 加了 Sync.Once 这里性能会低一点 完了吗？没有。可以明显看到 饿汉模式不支持延迟初始化 但 懒汉模式性能也不高，不如将两者结合起来，实现 双重检测 1234567891011121314151617// HungryLazybones 懒汉模式与饿汉模式结合type HungryLazybones struct &#123;&#125;var hungryLazybones *HungryLazybonesvar onc sync.Once// GetHungryLazybonesInstance 获取单例实例func GetHungryLazybonesInstance() *HungryLazybones &#123; if hungryLazybones == nil &#123; onc.Do(func() &#123; hungryLazybones = new(HungryLazybones) &#125;) &#125; return hungryLazybones&#125; 另外在 Java 中还有两种方式来实现单例模式 方法 1：通过静态方法实现单例模式 12345678910111213public class IdGenerator &#123; private AtomicLong id = new AtomicLong(0); private IdGenerator() &#123;&#125; private static class SingletonHolder&#123; private static final IdGenerator instance = new IdGenerator(); &#125; public static IdGenerator getInstance() &#123; return SingletonHolder.instance; &#125; public long getId() &#123; return id.incrementAndGet(); &#125;&#125; SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。insance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。这种实现方法既保证了线程安全，又能做到延迟加载 方法 2：枚举类型 1234567public enum IdGenerator &#123; INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() &#123; return id.incrementAndGet(); &#125;&#125; 基于枚举类型的单例实现。通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。 问题 如何实现集群的单例模式 接获取原子锁(防止多个服务同时使用单例对象) 判断单例在共享区域是否存在，不存在则构建单例对象 将单例对象序列化并存储到外部共享存储区 使用单例对象 将对象显示地将对象从内存中删除 释放原子锁 总结 可拓展性差，如果需要多实例对象可能比较麻烦，适用于单实例对象 可测试性差，因为是唯一实例，进行多场景修改实例进行测试可能会比较麻烦 参考文档 https://lailin.xyz/post/singleton.html https://github.com/senghoo/golang-design-pattern/blob/master/03_singleton/singleton.go","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"设计模式-01-概述","slug":"Design Patterns/设计模式-01-概述","date":"2022-08-14T14:58:29.000Z","updated":"2023-05-27T10:35:39.090Z","comments":true,"path":"2022/08/14/Design Patterns/设计模式-01-概述/","link":"","permalink":"http://xboom.github.io/2022/08/14/Design%20Patterns/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-01-%E6%A6%82%E8%BF%B0/","excerpt":"","text":"设计模式是软件设计中常见问题的典型解决方，尝试着学习一下它到底是怎样的 6大设计原则 SOLID 原则 单一职责原则 (Single responsibility principle, SRP)，每个实体功能都应该专注于一件事上。同时做两件以上的事情，不但阅读性降低，出错时也更难找到问题点。另外多个职责耦合在一起，会影响复用性 开放封闭原则 (Open-Close principle, OCP)，通过增加新代码来扩展系统的功能，而不是通过修改原本已经存在的代码来扩展系统的功能。当未来需求有异动时，如果为了新需求而改动原有代码，可能会造成其他调用原本代码时发生非预期错误 里氏替换原则 (Liskov substitution principle, LSP)，任何基类可以出现的地方，子类一定可以出现(替换)。子类可以扩展父类的功能，但不能改变父类原有的功能。只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。 实现 OCP 原则的关键步骤就是抽象化，而基类与子类的继承关系是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。 接口隔离原则 (Interface segregation principle, ISP)，使用多个隔离的小接口，比使用单个整合在一起的大接口要好。为了方便维护和升级已有的代码，需要尽量降低类之间的耦合度，降低依赖，降低耦合 依赖反转原则 (Dependency inversion principle, DIP)，要依赖于抽象，而不依赖于具体。即高低阶层代码都依赖一个抽象类，在抽象类中定义所依赖的方法，并由子类去实现 最少知识原则(Least knowledge principle, LKP)，实体之间的联系(通信/交流)应当尽量少。即一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。这样当一个模块修改时，就会尽量少的影响其他的模块，扩展会相对容易（主要还是解耦思想)。 DRY 原则 Do not repeat yourself 设计模式分类 设计模式根据其目的来分类： 创建型模式提供创建对象的机制， 增加已有代码的灵活性和可复用性，注重如何初始化一个或一群实体 结构型模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效，注重实体之间互相组合取代继承 行为模式负责对象间的高效沟通和职责委派，注重分配每个实体的功能，建立联系沟通。 创建型模式 单例模式(Singleton Design Pattern) 简单工厂模式(Simple Factory Pattern) 工厂方法模式(Factory Method Pattern) 抽象工厂模式(Abstract Factory) 建造者模式(Builder Design Pattern) 原型模式(Prototype Design Pattern) 结构性模式 代理模式(Proxy Design Pattern) 桥接模式(Bridge Design Pattern) 装饰器模式(Decorator Design Pattern) 适配器模式(Adapter Design Pattern) 门面模式(Facade Design Pattern) 组合模式(Composite Design Pattern) 享元模式(Flyweight Design Pattern) 行为模式 观察者模式(Observer Design Pattern) 模板方法模式(Template Design Pattern) 策略模式(Strategy Method Design Pattern) 职责链模式(Chain Of Responsibility Design Pattern) 状态模式(State Design Pattern) 迭代器模式(Iterator Design Pattern) 访问者模式(Visitor Design Pattern) 备忘录模式(Memento Design Pattern) 命令模式(Command Design Pattern) 解释器模式(Interpreter Design Pattern) 中介者模式(Mediator Design Pattern) 其中个人觉得以下模式常见且实用，可以好好把握 模板方法模式 组合模式 桥接模式 原型模式 单例模式 策略模式 职责链模式 状态模式 迭代器模式 参考文档 https://lailin.xyz/post/go-design-pattern.html https://docs.microsoft.com/zh-cn/azure/architecture/patterns/ https://github.com/senghoo/golang-design-pattern https://github.com/mohuishou/go-design-pattern https://github.com/idootop/Design-Patterns-Dart","categories":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"}],"tags":[{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"}]},{"title":"CPP-15-14新特性","slug":"C++/CPP-15-14新特性","date":"2022-08-14T14:43:29.000Z","updated":"2023-08-13T16:39:16.414Z","comments":true,"path":"2022/08/14/C++/CPP-15-14新特性/","link":"","permalink":"http://xboom.github.io/2022/08/14/C++/CPP-15-14%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"返回值类型推导 先看一段代码： 123456789101112#include &lt;iostream&gt;using namespace std;auto func(int i) &#123; return i;&#125;int main() &#123; cout &lt;&lt; func(4) &lt;&lt; endl; return 0;&#125; 使用C++11编译： 12345~/test$ g++ test.cc -std=c++11test.cc:5:16: error: ‘func’ function uses ‘auto’ type specifier without trailing return typeauto func(int i) &#123; ^test.cc:5:16: note: deduced return type only available with -std=c++14 or -std=gnu++14 返回值类型推导也可以用在模板中： 12345678910#include &lt;iostream&gt;using namespace std;template&lt;typename T&gt; auto func(T t) &#123; return t; &#125;int main() &#123; cout &lt;&lt; func(4) &lt;&lt; endl; cout &lt;&lt; func(3.4) &lt;&lt; endl; return 0;&#125; 注意：函数内如果有多个return语句，它们必须返回相同的类型，否则编译失败 12345auto func(bool flag) &#123; if (flag) return 1; else return 2.3; // error&#125;// inconsistent deduction for auto return type: ‘int’ and then ‘double’ 如果return语句返回初始化列表，返回值类型推导也会失败 123auto func() &#123; return &#123;1, 2, 3&#125;; // error returning initializer list&#125; 如果函数是虚函数，不能使用返回值类型推导 1234struct A &#123; // error: virtual function cannot have deduced return type virtual auto func() &#123; return 1; &#125;&#125; 返回类型推导可以用在前向声明中，但是在使用它们之前，翻译单元中必须能够得到函数定义 123456auto f(); // declared, not yet definedauto f() &#123; return 42; &#125; // defined, return type is intint main() &#123; cout &lt;&lt; f() &lt;&lt; endl;&#125; 返回类型推导可用在递归函数中，但递归调用必须以至少一个返回语句作为先导，以便编译器推导出返回型 123456auto sum(int i) &#123; if (i == 1) return i; // return int else return sum(i - 1) + i; // ok&#125; 在C++11中，lambda表达式参数需要使用具体的类型声明： 1auto f &#x3D; [] (int a) &#123; return a; &#125; 在C++14中，对此进行优化，lambda表达式参数可以直接是auto： 123auto f = [] (auto a) &#123; return a; &#125;;cout &lt;&lt; f(1) &lt;&lt; endl;cout &lt;&lt; f(2.3f) &lt;&lt; endl; 变量模板 12345678template&lt;class T&gt;constexpr T pi = T(3.1415926535897932385L);int main() &#123; cout &lt;&lt; pi&lt;int&gt; &lt;&lt; endl; // 3 cout &lt;&lt; pi&lt;double&gt; &lt;&lt; endl; // 3.14159 return 0;&#125; 别名模板 1234567891011121314151617template&lt;typename T, typename U&gt;struct A &#123; T t; U u;&#125;;template&lt;typename T&gt;using B = A&lt;T, int&gt;;int main() &#123; B&lt;double&gt; b; b.t = 10; b.u = 20; cout &lt;&lt; b.t &lt;&lt; endl; cout &lt;&lt; b.u &lt;&lt; endl; return 0;&#125; constexpr的限制 C11中constexpr函数可以使用递归，在C14中可以使用局部变量和循环 123constexpr int factorial(int n) &#123; // C++14 和 C++11均可 return n &lt;= 1 ? 1 : (n * factorial(n - 1));&#125; 在C++14中可以这样做： 1234567constexpr int factorial(int n) &#123; // C++11中不可，C++14中可以 int ret = 0; for (int i = 0; i &lt; n; ++i) &#123; ret += i; &#125; return ret;&#125; C++11中constexpr函数必须必须把所有东西都放在一个单独的return语句中，而constexpr则无此限制 123constexpr int func(bool flag) &#123; // C++14 和 C++11均可 return 0;&#125; 在C++14中可以这样： 1234constexpr int func(bool flag) &#123; // C++11中不可，C++14中可以 if (flag) return 1; else return 0;&#125; deprecated C++14中增加了deprecated标记，修饰类、变、函数等，当程序中使用到了被其修饰的代码时，编译时被产生警告，用户提示开发者该标记修饰的内容将来可能会被丢弃，尽量不要使用 123456struct [[deprecated]] A &#123; &#125;;int main() &#123; A a; return 0;&#125; 当编译时，会出现如下警告： 1234567~/test$ g++ test.cc -std=c++14test.cc: In function ‘int main()’:test.cc:11:7: warning: ‘A’ is deprecated [-Wdeprecated-declarations] A a; ^test.cc:6:23: note: declared here struct [[deprecated]] A &#123; 二进制字面量 C++14引入了二进制字面量，也引入了分隔符，防止看起来眼花 12int a = 0b0001'0011'1010;double b = 3.14'1234'1234'1234; std::exchange 123456789int main() &#123; std::vector&lt;int&gt; v; std::exchange(v, &#123;1,2,3,4&#125;); cout &lt;&lt; v.size() &lt;&lt; endl; for (int a : v) &#123; cout &lt;&lt; a &lt;&lt; \" \"; &#125; return 0;&#125; 可以看看 exchange 的实现 123456template&lt;class T, class U = T&gt;constexpr T exchange(T&amp; obj, U&amp;&amp; new_value) &#123; T old_value = std::move(obj); obj = std::forward&lt;U&gt;(new_value); return old_value;&#125; new_value的值给了obj，而没有对new_value赋值 std::quoted C++14引入std::quoted用于给字符串添加双引号 123456int main() &#123; string str = \"hello world\"; cout &lt;&lt; str &lt;&lt; endl; cout &lt;&lt; std::quoted(str) &lt;&lt; endl; return 0;&#125; 编译&amp;输出： 1234~/test$ g++ test.cc -std=c++14~/test$ ./a.outhello world\"hello world\" 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-14-11新特性","slug":"C++/CPP-14-11新特性","date":"2022-08-12T14:43:29.000Z","updated":"2023-08-13T16:39:18.178Z","comments":true,"path":"2022/08/12/C++/CPP-14-11新特性/","link":"","permalink":"http://xboom.github.io/2022/08/12/C++/CPP-14-11%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"类型推导 C++11引入了auto和decltype关键字，使用他们可以在编译期就推导出变量或者表达式的类型，方便开发者编码也简化了代码 auto：让编译器在编译器就推导出变量的类型，可以通过=右边的类型推导出变量的类型 1auto a = 10; // 10是int型，可以自动推导出a是int decltype：相对于auto用于推导变量类型，而decltype则用于推导表达式类型，这里只用于编译器分析表达式的类型，表达式实际不会进行运算 123cont int &amp;i = 1;int a = 2;decltype(i) b = 2; // b是const int&amp; 右值引用 右值引用是 C++11 引入的一个重要特性，它允许程序员有效地处理临时（即将销毁的）对象，并支持移动语义，从而提高性能和资源利用率。右值引用通常与移动语义一起使用，用于优化对象的传递、复制和销毁操作 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;class MyString &#123;public: MyString(const char* str) : data(new char[strlen(str) + 1]) &#123; cout &lt;&lt; \"point 2\" &lt;&lt; endl; strcpy(data, str); &#125; // 移动构造函数 MyString(MyString&amp;&amp; other) : data(other.data) &#123; cout &lt;&lt; \"point 1\" &lt;&lt; endl; other.data = nullptr; &#125; ~MyString() &#123; delete[] data; &#125; void Show() &#123; cout &lt;&lt; data &lt;&lt; endl; &#125;private: char* data;&#125;;int main() &#123; MyString str1(\"Hello, world!\"); str1.Show(); // 使用 std::move 调用移动构造函数 MyString str2 = std::move(str1); // str1.Show(); //segmentation fault str2.Show(); return 0;&#125; 输出结果 1234point 2Hello, world!point 1Hello, world! 移动构造函数是一种特殊的构造函数，用于实现从一个对象（通常是临时对象或右值）移动数据到另一个对象，而不是进行深拷贝。移动构造函数的存在可以显著提高性能，特别是对于大型资源，如动态分配的内存 移动构造函数通常采用右值引用作为参数，即 Type&amp;&amp;，其中 Type 是类的类型。在移动构造函数内部，我们可以执行数据指针的转移、资源的所有权转移等操作，以实现高效的移动语义 列表初始化 在C++11中可以直接在变量名后面加上初始化列表来进行对象的初始化 1234567Time::Time(int h, int m)&#123; hours = h; minutes = m;&#125;Time coding(2, 40) 封装 C++11 新增了 std::function &amp; std::bind &amp; Lambda表达式 std::function std::function 是一种通用、多态的函数封装。可以对任何可以调用的目标实体进行存储、复制、和调用操作，这些目标实体包括普通函数、Lambda表达式、函数指针、以及其它函数对象等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566//代码出自链接：http://www.jellythink.com/archives/771#include &lt;functional&gt;#include &lt;iostream&gt;using namespace std;std::function&lt;int(int)&gt; Functional;// 普通函数int TestFunc(int a)&#123; return a;&#125;// Lambda表达式auto lambda = [](int a)-&gt;int&#123; return a; &#125;;// 仿函数(functor)class Functor&#123;public: int operator()(int a) &#123; return a; &#125;&#125;;// 1.类成员函数// 2.类静态函数class TestClass&#123;public: int ClassMember(int a) &#123; return a; &#125; static int StaticMember(int a) &#123; return a; &#125;&#125;;int main()&#123; // 普通函数 Functional = TestFunc; int result = Functional(10); cout &lt;&lt; \"普通函数：\"&lt;&lt; result &lt;&lt; endl; // Lambda表达式 Functional = lambda; result = Functional(20); cout &lt;&lt; \"Lambda表达式：\"&lt;&lt; result &lt;&lt; endl; // 仿函数 Functor testFunctor; Functional = testFunctor; result = Functional(30); cout &lt;&lt; \"仿函数：\"&lt;&lt; result &lt;&lt; endl; // 类成员函数 TestClass testObj; Functional = std::bind(&amp;TestClass::ClassMember, testObj, std::placeholders::_1); result = Functional(40); cout &lt;&lt; \"类成员函数：\"&lt;&lt; result &lt;&lt; endl; // 类静态函数 Functional = TestClass::StaticMember; result = Functional(50); cout &lt;&lt; \"类静态函数：\"&lt;&lt; result &lt;&lt; endl; return 0;&#125; 运行结果 12345普通函数：10Lambda表达式：20仿函数：30类成员函数：40类静态函数：50 转换后的std::function对象的参数能转换为可调用实体的参数；可调用实体的返回值能转换为std::function对象的返回值。 std::function对象最大的用处就是在实现函数回调，但注意，它不能被用来检查相等或者不相等，但是可以与NULL或者nullptr进行比较 std::bind std::bind 是 C++11 标准引入的函数模板，位于 &lt;functional&gt; 头文件中。它用于创建一个新的可调用对象(函数对象或函数指针)，将一个函数与其参数绑定起来，可以实现参数的预绑定、参数重排序以及参数传递等功能 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;functional&gt;using namespace std;// 原始函数void greet(const string &amp;name, const string &amp;message, const string &amp;result) &#123; cout &lt;&lt; \"Hello, \" &lt;&lt; name &lt;&lt; \"! \" &lt;&lt; message &lt;&lt; \", \"&lt;&lt; result &lt;&lt;endl;&#125;int main() &#123; // 使用 bind 创建一个新的函数对象，并绑定第一个参数为 \"Alice\" auto greetAlice = bind(greet, \"Alice\", placeholders::_1, \"Bye\"); auto greetBob = bind(greet, \"Bob\", placeholders::_1, placeholders::_2); // 调用新的函数对象 greetAlice(\"How are you?\"); // Output: Hello, Alice! How are you? greetBob(\"Nice to meet you\", \"heihei\"); return 0;&#125; 运行结果 12Hello, Alice! How are you?, ByeHello, Bob! Nice to meet you, heihei 其中 placeholders::_1 表示参数被保留为占位符 Lambda 表达式 Lambda 表达式是 C++11 标准引入的一种匿名函数形式，它允许你在代码中内联定义一个简单的函数，无需显式命名。Lambda 表达式通常用于需要一个函数对象（函数符）的地方，例如作为函数参数、STL 算法的谓词、或者用于创建自定义的排序规则 Lambda 表达式的基本语法如下 123[capture](parameters) -&gt; return_type &#123; // 函数体&#125; capture 是用于捕获外部变量的列表。它可以为空，表示不捕获任何变量，也可以是 [&amp;]（捕获所有变量引用）、[=]（捕获所有变量拷贝）、[var1, var2]（捕获特定变量）等形式。 parameters 是传递给 Lambda 表达式的参数列表。 return_type 是 Lambda 表达式的返回类型。可以省略，编译器会自动推断。 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;int main() &#123; vector&lt;int&gt; numbers = &#123;3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5&#125;; // 使用 Lambda 表达式作为 STL 算法的谓词进行排序 sort(numbers.begin(), numbers.end(), [](int a, int b) &#123; return a &lt; b; &#125;); // 使用 Lambda 表达式打印排序后的结果 cout &lt;&lt; \"Sorted numbers:\"; for (int num : numbers) &#123; cout &lt;&lt; \" \" &lt;&lt; num; &#125; cout &lt;&lt; endl; return 0;&#125; 另外 捕获引用[&amp;]与捕获复制[=]的区别 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;using namespace std;int main() &#123; int x = 10, y = 20; int m = 100, n = 200; // 使用 Lambda 表达式引用捕获外部变量 x 和 y auto lambda1 = [&amp;]() &#123; cout &lt;&lt; \"x: \" &lt;&lt; x &lt;&lt; \", y: \" &lt;&lt; y &lt;&lt; endl; x += 5; y += 10; &#125;; lambda1(); // 调用 Lambda 表达式 auto lambda2 = [=]() mutable &#123; cout &lt;&lt; \"x: \" &lt;&lt; m &lt;&lt; \", y: \" &lt;&lt; n &lt;&lt; endl; m += 5; n += 10; &#125;; lambda2(); // 在调用 Lambda 表达式后，x 和 y 的值被修改 cout &lt;&lt; \"After lambda call - x: \" &lt;&lt; x &lt;&lt; \", y: \" &lt;&lt; y &lt;&lt; endl; cout &lt;&lt; \"After lambda call - m: \" &lt;&lt; m &lt;&lt; \", n: \" &lt;&lt; n &lt;&lt; endl; return 0;&#125; 这里不同的是，尝试对以值方式(copy captured)捕获的变量进行赋值操作。默认情况下，Lambda 表达式的参数和以值方式捕获的变量都是不可变的，即不能被修改 希望在 Lambda 表达式内部修改这些变量，需要将 Lambda 表达式标记为可变mutable 另外已值方式捕获的变量不会影响 main 函数的 m 和 n, 因为值捕获创建了变量的拷贝 模板改进 C++11关于模板有一些细节的改进： 模板的右尖括号 模板的别名 函数模板的默认模板参数 模板的右尖括号(Template Right Angle Brackets)： 在 C11 之前，当使用嵌套的模板类型时，右尖括号 &gt;&gt; 可能会被解释为右移操作符。C11 引入了对 &gt;&gt; 在模板中的正确解析，以避免这种歧义。这样可以更容易地编写嵌套的模板类型，例如容器嵌套容器。 1std::vector&lt;std::vector&lt;int&gt;&gt; matrix; // 在 C++11 及以后版本中正确解析 模板的别名(Template Aliases)： C++11 引入了模板的别名，可以使用 using 关键字来定义模板别名，从而为模板类型创建简洁易读的名称。这对于复杂的模板类型特别有用 1234template &lt;typename T&gt;using Vector = std::vector&lt;T&gt;; // 定义模板别名 VectorVector&lt;int&gt; numbers; // 等同于 std::vector&lt;int&gt; 函数模板的默认模板参数(Default Template Arguments for Function Templates)： C++11 允许在函数模板中为模板参数指定默认值，从而使调用函数模板时可以省略特定的模板参数。这可以简化模板函数的调用，并提高代码的可读性 12345678910template &lt;typename T = int&gt;void printValue(T value) &#123; std::cout &lt;&lt; value &lt;&lt; std::endl;&#125;// 调用函数模板时可以省略模板参数，使用默认类型 intprintValue(42);// 也可以显式指定模板参数类型printValue&lt;double&gt;(3.14); 并发 有点多，后面单独弄一章单线程 智能指针 智能指针(Smart Pointers)用于管理动态分配的内存资源，有助于避免内存泄漏、多重释放等常见的内存错误 C++11引入了三种主要类型的智能指针： std::shared_ptr： 共享指针，用于多个智能指针共享同一个资源。资源在最后一个std::shared_ptr离开其作用域时释放 1234#include &lt;memory&gt;std::shared_ptr&lt;int&gt; sharedPtr = std::make_shared&lt;int&gt;(42);std::shared_ptr&lt;int&gt; anotherSharedPtr = sharedPtr; // 共享资源 std::unique_ptr： 独占指针，用于唯一拥有一个资源，保证了资源的独占性和自动释放 1234#include &lt;memory&gt;std::unique_ptr&lt;int&gt; uniquePtr = std::make_unique&lt;int&gt;(42);// std::unique_ptr&lt;int&gt; anotherUniquePtr = uniquePtr; // 错误，无法复制，但可以通过std::move移动 std::weak_ptr： 弱指针，用于共享资源的控制，但不会增加资源的引用计数。通常与std::shared_ptr一起使用，以防止循环引用 12345#include &lt;memory&gt;std::shared_ptr&lt;int&gt; shared = std::make_shared&lt;int&gt;(42);std::weak_ptr&lt;int&gt; weak = shared; // 弱指针std::shared_ptr&lt;int&gt; sharedAgain = weak.lock(); // 尝试获取shared_ptr 基于范围的for循环 123456789vector&lt;int&gt; vec;for (auto iter = vec.begin(); iter != vec.end(); iter++) &#123; // before c++11 cout &lt;&lt; *iter &lt;&lt; endl;&#125;for (int i : vec) &#123; // c++11基于范围的for循环 cout &lt;&lt; \"i\" &lt;&lt; endl;&#125; 委托构造函数 委托构造函数允许在同一个类中一个构造函数调用另外一个构造函数，可以在变量初始化时简化操作，通过代码来感受下委托构造函数的妙处： 不使用委托构造函数： 12345678910111213141516171819struct A &#123; A()&#123;&#125; A(int a) &#123; a_ = a; &#125; A(int a, int b) &#123; // 好麻烦 a_ = a; b_ = b; &#125; A(int a, int b, int c) &#123; // 好麻烦 a_ = a; b_ = b; c_ = c; &#125; int a_; int b_; int c_;&#125;; 使用委托构造函数： 123456789101112struct A &#123; A()&#123;&#125; A(int a) &#123; a_ = a; &#125; A(int a, int b) : A(a) &#123; b_ = b; &#125; A(int a, int b, int c) : A(a, b) &#123; c_ = c; &#125; int a_; int b_; int c_;&#125;; 继承构造函数 继承构造函数可以让派生类直接使用基类的构造函数，如果有一个派生类，希望派生类采用和基类一样的构造方式，可以直接使用基类的构造函数，而不是再重新写一遍构造函数，老规矩，看代码： 不使用继承构造函数： 1234567891011121314151617181920212223struct Base &#123; Base() &#123;&#125; Base(int a) &#123; a_ = a; &#125; Base(int a, int b) : Base(a) &#123; b_ = b; &#125; Base(int a, int b, int c) : Base(a, b) &#123; c_ = c; &#125; int a_; int b_; int c_;&#125;;struct Derived : Base &#123; Derived() &#123;&#125; Derived(int a) : Base(a) &#123;&#125; // 好麻烦 Derived(int a, int b) : Base(a, b) &#123;&#125; // 好麻烦 Derived(int a, int b, int c) : Base(a, b, c) &#123;&#125; // 好麻烦&#125;;int main() &#123; Derived a(1, 2, 3); return 0;&#125; 使用继承构造函数 123456789101112131415161718192021struct Base &#123; Base() &#123;&#125; Base(int a) &#123; a_ = a; &#125; Base(int a, int b) : Base(a) &#123; b_ = b; &#125; Base(int a, int b, int c) : Base(a, b) &#123; c_ = c; &#125; int a_; int b_; int c_;&#125;;struct Derived : Base &#123; using Base::Base;&#125;;int main() &#123; Derived a(1, 2, 3); return 0;&#125; 只需要使用using Base::Base继承构造函数，就免去了很多重写代码的麻烦 nullptr nullptr是c11用来表示空指针新引入的常量值，在c中如果表示空指针语义时建议使用nullptr而不要使用NULL，因为NULL本质上是个int型的0，其实不是个指针 12345678910111213void func(void *ptr) &#123; cout &lt;&lt; \"func ptr\" &lt;&lt; endl;&#125;void func(int i) &#123; cout &lt;&lt; \"func i\" &lt;&lt; endl;&#125;int main() &#123; func(NULL); // 编译失败，会产生二义性 func(nullptr); // 输出func ptr return 0;&#125; final &amp; override c++11关于继承新增了两个关键字， final用于修饰一个类，表示禁止该类进一步派生和虚函数的进一步重载， override用于修饰派生类中的成员函数，标明该函数重写了基类函数，如果一个函数声明了override但父类却没有这个虚函数，编译报错 示例代码1： 1234567891011121314struct Base &#123; virtual void func() &#123; cout &lt;&lt; \"base\" &lt;&lt; endl; &#125;&#125;;struct Derived : public Base&#123; void func() override &#123; // 确保func被重写 cout &lt;&lt; \"derived\" &lt;&lt; endl; &#125; void fu() override &#123; // error，基类没有fu()，不可以被重写 &#125;&#125;; 示例代码2： 1234567891011struct Base final &#123; virtual void func() &#123; cout &lt;&lt; \"base\" &lt;&lt; endl; &#125;&#125;;struct Derived : public Base&#123; // 编译失败，final修饰的类不可以被继承 void func() override &#123; cout &lt;&lt; \"derived\" &lt;&lt; endl; &#125;&#125;; default c++11引入default特性，多数时候用于声明构造函数为默认构造函数，如果类中有了自定义的构造函数，编译器就不会隐式生成默认构造函数 123456789struct A &#123; int a; A(int i) &#123; a = i; &#125;&#125;;int main() &#123; A a; // 编译出错 return 0;&#125; 上面代码编译出错，因为没有匹配的构造函数，因为编译器没有生成默认构造函数，而通过default，程序员只需在函数声明后加上“=default;”，就可将该函数声明为 defaulted 函数，编译器将为显式声明的 defaulted 函数自动生成函数体 12345678910struct A &#123; A() = default; int a; A(int i) &#123; a = i; &#125;&#125;;int main() &#123; A a; return 0;&#125; delete c++中，如果开发人员没有定义特殊成员函数，那么编译器在需要特殊成员函数时候会隐式自动生成一个默认的特殊成员函数，例如拷贝构造函数或者拷贝赋值操作符，如下代码： 123456789101112struct A &#123; A() = default; int a; A(int i) &#123; a = i; &#125;&#125;;int main() &#123; A a1; A a2 = a1; // 正确，调用编译器隐式生成的默认拷贝构造函数 A a3; a3 = a1; // 正确，调用编译器隐式生成的默认拷贝赋值操作符&#125; 有时候想禁止对象的拷贝与赋值，可以使用delete修饰，如下 1234567891011121314struct A &#123; A() = default; A(const A&amp;) = delete; A&amp; operator=(const A&amp;) = delete; int a; A(int i) &#123; a = i; &#125;&#125;;int main() &#123; A a1; A a2 = a1; // 错误，拷贝构造函数被禁用 A a3; a3 = a1; // 错误，拷贝赋值操作符被禁用&#125; delele函数在c++11中很常用，std::unique_ptr就是通过delete修饰来禁止对象的拷贝的 explicit explicit专用于修饰构造函数，表示只能显式构造，不可以被隐式转换，根据代码看explicit的作用： 不用explicit： 12345678910struct A &#123; A(int value) &#123; // 没有explicit关键字 cout &lt;&lt; \"value\" &lt;&lt; endl; &#125;&#125;;int main() &#123; A a = 1; // 可以隐式转换 return 0;&#125; 使用 explicit 1234567891011struct A &#123; explicit A(int value) &#123; cout &lt;&lt; \"value\" &lt;&lt; endl; &#125;&#125;;int main() &#123; A a = 1; // error，不可以隐式转换 A aa(2); // ok return 0;&#125; constexpr 首先说一下 const const的修饰的变量不可更改 1const int value = 5; 指针使用const，从右向左读，即可知道const究竟修饰的是指针还是指针所指向的内容 12char *const ptr; // 指针本身是常量const char* ptr; // 指针指向的变量为常量 在函数参数中使用const，一般会传递类对象时会传递一个const的引用或者指针，这样可以避免对象的拷贝，也可以防止对象被修改 12class A&#123;&#125;;void func(const A&amp; a); const修饰类的成员变量，表示是成员常量，不能被修改，可以在初始化列表中被赋值 1234567class A &#123; const int value = 5;&#125;;class B &#123; const int value; B(int v) : value(v)&#123;&#125;&#125;; 修饰类成员函数，表示在该函数内不可以修改该类的成员变量 123class A&#123; void func() const;&#125;; 修饰类对象，类对象只能调用该对象的const成员函数 12345class A &#123; void func() const;&#125;;const A a;a.func(); const只表示read only的语义，只保证了运行时不可以被修改，但它修饰的仍然有可能是个动态变量 constexpr修饰的才是真正的常量，它会在编译期间就会被计算出来，整个运行过程中都不可以被改变，constexpr可以用于修饰函数，这个函数的返回值会尽可能在编译期间被计算出来当作一个常量，但是如果编译期间此函数不能被计算出来，那它就会当作一个普通函数被处理 123456789101112#include&lt;iostream&gt;using namespace std;constexpr int func(int i) &#123; return i + 1;&#125;int main() &#123; int i = 2; func(i);// 普通函数 func(2);// 编译期间就会被计算出来&#125; enum class 不带作用域的枚举代码： 123456789101112131415161718enum AColor &#123; kRed, kGreen, kBlue&#125;;enum BColor &#123; kWhite, kBlack, kYellow&#125;;int main() &#123; if (kRed == kWhite) &#123; cout &lt;&lt; \"red == white\" &lt;&lt; endl; &#125; return 0;&#125; 不带作用域的枚举类型可以自动转换成整形，且不同的枚举可以相互比较，代码中的红色居然可以和白色比较，这都是潜在的难以调试的bug，而这种完全可以通过有作用域的枚举来规避 有作用域的枚举代码： 123456789101112131415161718enum class AColor &#123; kRed, kGreen, kBlue&#125;;enum class BColor &#123; kWhite, kBlack, kYellow&#125;;int main() &#123; if (AColor::kRed == BColor::kWhite) &#123; // 编译失败 cout &lt;&lt; \"red == white\" &lt;&lt; endl; &#125; return 0;&#125; 使用带有作用域的枚举类型后，对不同的枚举进行比较会导致编译失败，消除潜在bug，同时带作用域的枚举类型可以选择底层类型，默认是int，可以改成char等别的类型。 12345enum class AColor : char &#123; kRed, kGreen, kBlue&#125;; 平时编程过程中使用枚举，一定要使用有作用域的枚举取代传统的枚举 非受限联合体 首先说明一下 POD类型 在C中，POD(Plain Old Data)类型没有用户自定义的构造函数、析构函数或虚函数，并且可以通过内存拷贝进行操作。C11引入了更严格的定义，将POD类型划分为三种：POD、POD Trivial、POD标准布局。这些类型通常适用于与C语言库交互、内存布局和二进制数据处理等情况 基本数据类型： 整数类型（如int、char、long）、浮点类型（如float、double）等是POD类型 12int x = 42;float y = 3.14; 结构体和类： 简单的结构体和类，只包含POD类型的成员且没有自定义构造函数、析构函数和虚函数，也可以是POD类型 12345678910struct Point &#123; int x; int y;&#125;;class Rectangle &#123;public: int width; int height;&#125;; C数组： C风格的数组也是POD类型 1int array[5] = &#123;1, 2, 3, 4, 5&#125;; std::vector、std::array和动态内存分配 就不是 联合体： 简单的联合体也可以是POD类型，但要注意成员之间的内存布局 1234union Data &#123; int intValue; float floatValue;&#125;; 枚举： 枚举类型在某些情况下可以被视为POD类型，但也可能受到枚举的底层类型和使用方式的影响 1enum Color &#123; Red, Green, Blue &#125;; 比如 123456789101112131415161718192021222324252627//1 包含成员函数enum class Color &#123; Red, Green, Blue // 无法包含成员函数，否则不是POD类型 // void print() &#123;&#125;&#125;;//2. 包含虚函数enum class Shape &#123; Circle, Square // 无法包含虚函数，否则不是POD类型 // virtual void draw() &#123;&#125;&#125;;//3. 继承class Base &#123;public: int x;&#125;;enum class Derived : int, Base &#123; // 不是POD类型，因为继承了Base类 A, B&#125;; 大体上可以理解为对象可以直接memcpy的类型 c11之前union中数据成员的类型不允许有非POD类型，而这个限制在c11被取消，允许数据成员类型有非POD类型 123456789struct A &#123; int a; int *b;&#125;;union U &#123; A a; // 非POD类型 c++11之前不可以这样定义联合体 int b;&#125;; sizeof c++11中sizeof可以用的类的数据成员上，看代码： c++11前： 12345678910struct A &#123; int data[10]; int a;&#125;;int main() &#123; A a; cout &lt;&lt; \"size \" &lt;&lt; sizeof(a.data) &lt;&lt; endl; return 0;&#125; c++11后： 123456789struct A &#123; int data[10]; int a;&#125;;int main() &#123; cout &lt;&lt; \"size \" &lt;&lt; sizeof(A::data) &lt;&lt; endl; return 0;&#125; 想知道类中数据成员的大小在c++11中是不是方便了许多，而不需要定义一个对象，在计算对象的成员大小 assertion c++11引入static_assert声明，在编译期间检查，如果第一个参数值为false，则打印message，编译失败。 1static_assert(true/false, message); 自定义字面量 c11可以自定义字面量，平时c中都或多或少使用过chrono中的时间，例如： 12std::this_thread::sleep_for(std::chrono::milliseconds(100)); // 100msstd::this_thread::sleep_for(std::chrono::seconds(100)); // 100s 其实没必要这么麻烦，也可以这么写： 12std::this_thread::sleep_for(100ms); // c++14里可以这么使用，这里只是举个自定义字面量使用的例子std::this_thread::sleep_for(100s); 这就是自定义字面量的使用，示例如下： 12345678struct mytype &#123; unsigned long long value;&#125;;constexpr mytype operator\"\" _mytype ( unsigned long long n ) &#123; return mytype&#123;n&#125;;&#125;mytype mm = 123_mytype;cout &lt;&lt; mm.value &lt;&lt; endl; 基础数值类型 c++11新增了几种数据类型：long long、char16_t、char32_t等 long long： 这是一种整数类型，用于表示更大范围的整数值。在某些平台上，long long 的范围要比传统的 int 或 long 类型更大，长度至少具有64位。 1long long bigNumber = 123456789012345LL; // 后缀 LL 表示 long long 类型 char16_t 和 char32_t： 这些是字符类型，用于表示更宽字符集的字符。在国际化和 Unicode 支持方面，它们非常有用，可以用来存储更多种类的字符 12char16_t unicodeChar16 = u'\\u03A9'; // 使用 u 前缀表示 char16_t 类型 至少具有16位char32_t unicodeChar32 = U'\\U0001F60A'; // 使用 U 前缀表示 char32_t 类型 至少具有32位 计算实际长度 123456789#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; \"Size of long long: \" &lt;&lt; sizeof(long long) &lt;&lt; \" bytes\" &lt;&lt; std::endl; std::cout &lt;&lt; \"Size of char16_t: \" &lt;&lt; sizeof(char16_t) &lt;&lt; \" bytes\" &lt;&lt; std::endl; std::cout &lt;&lt; \"Size of char32_t: \" &lt;&lt; sizeof(char32_t) &lt;&lt; \" bytes\" &lt;&lt; std::endl; return 0;&#125; 不同的编译器和平台可能会有不同的结果 正则表达式 c++11引入了regex库更好的支持正则表达式 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;iterator&gt;#include &lt;regex&gt;#include &lt;string&gt;int main() &#123; std::string s = \"I know, I'll use2 regular expressions.\"; // 忽略大小写 std::regex self_regex(\"REGULAR EXPRESSIONS\", std::regex_constants::icase); if (std::regex_search(s, self_regex)) &#123; std::cout &lt;&lt; \"Text contains the phrase 'regular expressions'\\n\"; &#125; std::regex word_regex(\"(\\\\w+)\"); // 匹配字母数字等字符 auto words_begin = std::sregex_iterator(s.begin(), s.end(), word_regex); auto words_end = std::sregex_iterator(); std::cout &lt;&lt; \"Found \" &lt;&lt; std::distance(words_begin, words_end) &lt;&lt; \" words\\n\"; const int N = 6; std::cout &lt;&lt; \"Words longer than \" &lt;&lt; N &lt;&lt; \" characters:\\n\"; for (std::sregex_iterator i = words_begin; i != words_end; ++i) &#123; std::smatch match = *i; std::string match_str = match.str(); if (match_str.size() &gt; N) &#123; std::cout &lt;&lt; \" \" &lt;&lt; match_str &lt;&lt; '\\n'; &#125; &#125; std::regex long_word_regex(\"(\\\\w&#123;7,&#125;)\"); // 超过7个字符的单词用[]包围 std::string new_s = std::regex_replace(s, long_word_regex, \"[$&amp;]\"); std::cout &lt;&lt; new_s &lt;&lt; '\\n';&#125; chrono c++11关于时间引入了chrono库，用于提供时间处理和时钟操作的支持 duration 用于表示时间间隔，例如秒、毫秒、微秒等 time_point 用于表示特定时间点 clocks 于获取时间信息 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;chrono&gt;int main() &#123; // 获取当前时间点 auto start = std::chrono::high_resolution_clock::now(); // 执行一些耗时操作 for (int i = 0; i &lt; 1000000; ++i) &#123; // Do something &#125; // 获取当前时间点 auto end = std::chrono::high_resolution_clock::now(); // 计算时间差 std::chrono::duration&lt;double&gt; duration = end - start; // 输出执行时间 std::cout &lt;&lt; \"Time taken: \" &lt;&lt; duration.count() &lt;&lt; \" seconds\" &lt;&lt; std::endl; return 0;&#125;//Time taken: 0.0013827 seconds 参考链接 https://github.com/0voice/cpp_new_features/blob/main/吐血整理：C%2B%2B11新特性.md https://blog.csdn.net/wangshubo1989/article/details/49134235","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-13-代码重用","slug":"C++/CPP-13-代码重用","date":"2022-08-10T14:43:29.000Z","updated":"2023-08-10T04:04:51.928Z","comments":true,"path":"2022/08/10/C++/CPP-13-代码重用/","link":"","permalink":"http://xboom.github.io/2022/08/10/C++/CPP-13-%E4%BB%A3%E7%A0%81%E9%87%8D%E7%94%A8/","excerpt":"","text":"前一章节说明的是公有继承，其实还有 包含、私有继承和保护继承，它们实现了 has-a 关系，即新的类将包含另一个类的对象 包含 首先了解一下valarray类，是由头文件valarray支持的。这个类用于处理 数值(或具有类似特性的类)，支持数组等操作。valarray被定义为一个模板类，以便能够处理不同的数据类型。 模板特性意味着声明对象时，必须指定具体的数据类型。因此，使用valarray类来声明一个对象时，需要在标识符valarray后面加上一对尖括号，并在其中包含所需的数据类型 1234567valarray&lt;int&gt; q_values; //an array of intvalarray&lt;double&gt; weight; //an array of double, size 0valarray&lt;int&gt; v2(8); //an array of 8 int elementvalarray&lt;int&gt; v3(10, 8); //an array of 8 int elements, each set to 10double gpa[5] = &#123;3.1, 3.5, 3.8, 2.9, 3.3&#125;;valarray&lt;double&gt; v4(gpa, 4); //an array of 4 elements initialized to the first 4 elements of gpa 首先声明一个类的定义 studentc.h 12345678910111213141516171819202122232425262728293031323334353637// studentc.h -- defining a Student class using containment#ifndef STUDENTC_H_#define STUDENTC_H_#include &lt;iostream&gt;#include &lt;string&gt; #include &lt;valarray&gt;using namespace std;class Student&#123; private: typedef valarray&lt;double&gt; ArrayDb; string name; // contained object ArrayDb scores; // contained object // private method for scores output ostream &amp; arr_out(ostream &amp; os) const;public: Student() : name(\"Null Student\"), scores() &#123;&#125; explicit Student(const string &amp; s) : name(s), scores() &#123;&#125; explicit Student(int n) : name(\"Nully\"), scores(n) &#123;&#125; Student(const string &amp; s, int n) : name(s), scores(n) &#123;&#125; Student(const string &amp; s, const ArrayDb &amp; a) : name(s), scores(a) &#123;&#125; Student(const char * str, const double * pd, int n) : name(str), scores(pd, n) &#123;&#125; ~Student() &#123;&#125; double Average() const; const string &amp; Name() const; double &amp; operator[](int i); double operator[](int i) const;// friends // input friend istream &amp; operator&gt;&gt;(istream &amp; is, Student &amp; stu); // 1 word friend istream &amp; getline(istream &amp; is, Student &amp; stu); // 1 line // output friend ostream &amp; operator&lt;&lt;(ostream &amp; os, const Student &amp; stu);&#125;;#endif 这里用到了 explicit，它的作用是：用来修饰类的构造函数，被修饰的构造函数的类，不能发生相应的隐式类型转换，只能以显示的方式进行类型转换 explicit 关键字只能用于类内部的构造函数声明上 explicit 关键字作用于单个参数的构造函数 其次，string、valarray都算是被包含的类，被包含对象的接口不是公有的，但可以在类方法中使用它(这个好理解，就是 string、valarray 的函数执行范围在 student 内部一样) 私有继承 使用私有继承，基类的公有成员和保护成员都将成为派生类的私有成员。而使用公有继承，基类的公有方法将成为派生类的公有方法，基类使用 private 修饰 12345class Student : private string, private valarray&lt;double&gt;&#123; public: ...&#125;; 使用私有继承时，只能在派生类的方法中使用基类的方法。但有时候可能希望基类工具是公有的。可以使用对象来调用方法也能 使用 类名和作用域解析运算符 来调用方法 1234567891011double Student::Average() const&#123; if(scores.size() &gt; 0) return scores.sum()/scores.size(); else return 0;&#125;double Student::Average() const&#123; if(ArrayDb:size() &gt; 0) return ArrayDb::sum()/ArrayDb::size(); else return 0;&#125; 如果要使用基类对象本身呢？可以通过强制转换，因为继承类就是从基类派生出来的，所以可以强制转换 1234const string &amp; Student::Name() const&#123; return (const string &amp;) *this;&#125; 这里指向用于调用该方法的 Student 对象中的继承而来的 string 对象 如果要访问基类的友元函数呢？用类名显式地限定函数名不适合于友元函数，这是因为友元不属于类。然而，可以通过显式地转换为基类来调用正确的函数 1234ostream &amp; operator&lt;&lt; (ostream &amp; os, const Student &amp; stu)&#123; os &lt;&lt; \"Scores for \" &lt;&lt; (const String &amp;) stu &lt;&lt; \":\\n\";&#125; 在私有继承中，在不进行显式类型转换的情况下，不能将指向派生类的引用或指针赋给基类引用或指针 保护继承是私有继承的变体。保护继承在列出基类时使用关键字 protected：，各继承方式的权限转换如下 特征 公有继承 保护继承 私有继承 公有成员变成 派生类的公有成员 派生类的保护成员 派生类的私有成员 保护成员变成 派生类的保护成员 生类的保护成员 派生类的私有成员 私有成员变成 只能通过基类接口访问 只能通过基类接口访问 只能通过基类接口访问 能否隐式向上转换 是 是(但只能在派生类中) 否 类模板 C++的类模板为生成通用的类声明提供方法 比如我们有一个类 123456789101112131415typedef unsigned long Item;class Stack&#123;private: enum &#123;MAX = 10&#125;; // constant specific to class Item items[MAX]; // holds stack items int top; // index for top stack itempublic: Stack(); bool isempty(); bool isfull(); bool push(const Item &amp; item); // add item to stack bool pop(Tiem &amp; item); // pop top into item&#125;;#endif 采用模板时，将使用模板定义替换Stack声明，使用模板成员函数替换Stack的成员函数。和模板函数一样，模板类以下面这样的代码开头： 1template &lt;class Type&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// stacktp.h -- a stack template#ifndef STACKTP_H_#define STACKTP_H_template &lt;class Type&gt;class Stack&#123;private: enum &#123;MAX = 10&#125;; // constant specific to class Type items[MAX]; // holds stack items int top; // index for top stack itempublic: Stack(); bool isempty(); bool isfull(); bool push(const Type &amp; item); // add item to stack bool pop(Type &amp; item); // pop top into item&#125;;template &lt;class Type&gt;Stack&lt;Type&gt;::Stack()&#123; top = 0;&#125;template &lt;class Type&gt;bool Stack&lt;Type&gt;::isempty()&#123; return top == 0;&#125;template &lt;class Type&gt;bool Stack&lt;Type&gt;::isfull()&#123; return top == MAX;&#125;template &lt;class Type&gt;bool Stack&lt;Type&gt;::push(const Type &amp; item)&#123; if (top &lt; MAX) &#123; items[top++] = item; return true; &#125; else return false;&#125;template &lt;class Type&gt;bool Stack&lt;Type&gt;::pop(Type &amp; item)&#123; if (top &gt; 0) &#123; item = items[--top]; return true; &#125; else return false; &#125;#endif 使用模板类则需要实例化 12Stack&lt;int&gt; kernels;Stack&lt;string&gt; colonels; 使用模板时主要注意，注意指针可能会失败，因为仅仅创建指针但是并没有分配内存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// stcktp1.h -- modified Stack template#ifndef STCKTP1_H_#define STCKTP1_H_template &lt;class Type&gt;class Stack&#123;private: enum &#123;SIZE = 10&#125;; // default size int stacksize; Type * items; // holds stack items int top; // index for top stack itempublic: explicit Stack(int ss = SIZE); Stack(const Stack &amp; st); ~Stack() &#123; delete [] items; &#125; bool isempty() &#123; return top == 0; &#125; bool isfull() &#123; return top == stacksize; &#125; bool push(const Type &amp; item); // add item to stack bool pop(Type &amp; item); // pop top into item Stack &amp; operator=(const Stack &amp; st);&#125;;template &lt;class Type&gt;Stack&lt;Type&gt;::Stack(int ss) : stacksize(ss), top(0) //分配空间&#123; items = new Type [stacksize];&#125;template &lt;class Type&gt;Stack&lt;Type&gt;::Stack(const Stack &amp; st) //分配空间&#123; stacksize = st.stacksize; top = st.top; items = new Type [stacksize]; for (int i = 0; i &lt; top; i++) items[i] = st.items[i];&#125;template &lt;class Type&gt;bool Stack&lt;Type&gt;::push(const Type &amp; item)&#123; if (top &lt; stacksize) &#123; items[top++] = item; return true; &#125; else return false;&#125;template &lt;class Type&gt;bool Stack&lt;Type&gt;::pop(Type &amp; item)&#123; if (top &gt; 0) &#123; item = items[--top]; return true; &#125; else return false;&#125;template &lt;class Type&gt;Stack&lt;Type&gt; &amp; Stack&lt;Type&gt;::operator=(const Stack&lt;Type&gt; &amp; st)&#123; if (this == &amp;st) return *this; delete [] items; stacksize = st.stacksize; top = st.top; items = new Type [stacksize]; for (int i = 0; i &lt; top; i++) items[i] = st.items[i]; return *this; &#125;#endif 使用逻辑如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;#include &lt;cstdlib&gt; // for rand(), srand()#include &lt;ctime&gt; // for time()#include \"stcktp1.h\"using namespace std;const int Num = 10;int main()&#123; srand(time(0)); // randomize rand() cout &lt;&lt; \"Please enter stack size: \"; int stacksize; cin &gt;&gt; stacksize; // create an empty stack with stacksize slots Stack&lt;const char *&gt; st(stacksize); // in basket const char * in[Num] = &#123; \" 1: Hank Gilgamesh\", \" 2: Kiki Ishtar\", \" 3: Betty Rocker\", \" 4: Ian Flagranti\", \" 5: Wolfgang Kibble\", \" 6: Portia Koop\", \" 7: Joy Almondo\", \" 8: Xaverie Paprika\", \" 9: Juan Moore\", \"10: Misha Mache\" &#125;; // out basket const char * out[Num]; int processed = 0; int nextin = 0; while (processed &lt; Num) &#123; if (st.isempty()) st.push(in[nextin++]); else if (st.isfull()) st.pop(out[processed++]); else if (rand() % 2 &amp;&amp; nextin &lt; Num) // 50-50 chance st.push(in[nextin++]); else st.pop(out[processed++]); &#125; for (int i = 0; i &lt; Num; i++) cout &lt;&lt; out[i] &lt;&lt; endl; cout &lt;&lt; \"Bye\\n\"; return 0; &#125; 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-12-类继承","slug":"C++/CPP-12-类继承","date":"2022-08-08T14:43:29.000Z","updated":"2023-08-10T04:04:43.417Z","comments":true,"path":"2022/08/08/C++/CPP-12-类继承/","link":"","permalink":"http://xboom.github.io/2022/08/08/C++/CPP-12-%E7%B1%BB%E7%BB%A7%E6%89%BF/","excerpt":"","text":"从一个类派生出另一个类时，原始类称为基类，继承类称为派生类。被称为 is-a(is-a-kind-of)关系 首先声明一个基类 tabtenn0.h 1234567891011121314151617181920#ifndef TABTENN0_H_#define TABTENN0_H_#include &lt;string&gt;using namespace std;class TableTennisPlayer&#123;private: string firstname; string lastname; bool hasTable;public: TableTennisPlayer(const string &amp; fn = \"none\", const string &amp; ln = \"none\", bool ht = false); void Name() const; bool HashTable() const &#123;return hasTable;&#125;; void ResetTable(bool v) &#123;hasTable = v;&#125;;&#125;;#endif 接着实现基类 tabtenn0.cpp 12345678910#include &lt;iostream&gt;#include \"tabtenn0.h\"TableTennisPlayer::TableTennisPlayer(const string &amp; fn, const string &amp; ln, bool ht) : firstname(fn),lastname(ln), hasTable(ht)&#123;&#125;void TableTennisPlayer::Name() const&#123; cout &lt;&lt; lastname &lt;&lt; \", \" &lt;&lt; firstname;&#125; 构造函数使用了成员初始化列表语法,也可以使用普通格式 1234567TableTennisPlayer::TableTennisPlayer(const string &amp; fn, const string &amp; ln, bool ht)&#123; firstname = fn; lastname = ln; hasTable = ht;&#125; 接着使用这个类 usett0.cpp 12345678910111213141516#include &lt;iostream&gt;#include \"tabtenn0.h\"using namespace std;int main(void)&#123; TableTennisPlayer player1(\"Chuck\", \"Blizzard\", true); TableTennisPlayer player2(\"Tara\", \"Boomdea\", false); player1.Name(); if(player1.HashTable()) cout &lt;&lt; \": has a table.\\n\"; else cout &lt;&lt; \": hasn't a table.\\n\"; player2.Name(); if(player2.HashTable()) cout &lt;&lt; \": has a table.\\n\"; else cout &lt;&lt; \": hasn't a table.\\n\"; return 0;&#125; 运行结果是 12Blizzard, Chuck: has a table.Boomdea, Tara: hasn't a table. 接着声明一个派生类 tabtenn1.h 12345678910111213#include &lt;iostream&gt;#include \"tabtenn0.h\"class RatedPlayer : public TableTennisPlayer&#123; private: unsigned int rating; public: RatedPlayer(unsigned int r = 0, const string &amp; fn = \"none\", const string &amp; ln = \"none\", bool ht = false); RatedPlayer(unsigned int r, const TableTennisPlayer &amp; tp); unsigned int Rating() const &#123;return rating;&#125;; void ResetRating(unsigned int r) &#123;rating = r;&#125;&#125;; 特殊的声明头 public TableTennisPlayer 表明TableTennisPlayer是一个公有基类，这被称为公有派生。 派生类对象包含基类对象。 使用公有派生，基类的公有成员将成为派生类的公有成员；基类的私有部分也将成为派生类的一部分，但只能通过基类的公有和保护方法访问 派生类对象可以使用基类的方法 派生类需要自己的构造函数 派生类可以根据需要添加额外的数据成员和成员函数 现在实现派生类 tabtenn1.cpp 12345678#include &lt;iostream&gt;#include \"tabtenn1.h\"using namespace std;RatedPlayer::RatedPlayer(unsigned int r, const string &amp; fn, const string &amp; ln, bool ht) :TableTennisPlayer(fn, ln, ht)&#123; rating = r;&#125; :TableTennisPlayer(fn,ln,ht)是成员初始化列表。它是可执行的代码，调用TableTennisPlayer构造函数 如果不调用基类的构造函数呢? 1234RatedPlayer::RatedPlayer(unsigned int r, const string &amp; fn, const string &amp; ln, bool ht)&#123;&#123; rating = r;&#125; 必须首先创建基类对象，如果不调用基类构造函数，程序将使用默认的基类构造函数，所以等效于 1234RatedPlayer::RatedPlayer(unsigned int r, const string &amp; fn, const string &amp; ln, bool ht) :TableTennisPlayer()&#123; rating = r;&#125; 也可以显示的调用基类构造函数 1234RatedPlayer::RatedPlayer(unsigned int r, const TableTennisPlayer &amp; tp) :TableTennisPlayer(tp)&#123; rating = r;&#125; 注意这里使用了TableTennisPlayer(tp) 但是并没有定义，这是因为 如果需要使用复制构造函数但又没有定义，编译器将自动生成一个 也可以对派生类使用成员初始化列表语法 123RatedPlayer::RatedPlayer(unsigned int r, const TableTennisPlayer &amp; tp) :TableTennisPlayer(tp), rating(r)&#123;&#125; 总结一下派生类的构造函数 首先创建基类对象 派生类构造函数应通过成员初始化列表将基类信息传递给基类构造函数； 派生类构造函数应初始化派生类新增的数据成员 派生类构造函数应初始化派生类新增的数据成员 多态 希望同一个方法在派生类和基类中的行为是不同的，即同一个方法的行为随上下文而异。有两种重要的机制可用于实现多态公有继承： 在派生类中重新定义基类的方法 使用虚方法 123456789101112131415161718192021222324252627282930313233343536373839404142// brass.h -- bank account classes#ifndef BRASS_H_#define BRASS_H_#include &lt;string&gt;// Brass Account Classclass Brass&#123;private: std::string fullName; long acctNum; double balance;public: Brass(const std::string &amp; s = \"Nullbody\", long an = -1, double bal = 0.0); void Deposit(double amt); virtual void Withdraw(double amt); double Balance() const; virtual void ViewAcct() const; virtual ~Brass() &#123;&#125;&#125;;//Brass Plus Account Classclass BrassPlus : public Brass&#123;private: double maxLoan; double rate; double owesBank;public: BrassPlus(const std::string &amp; s = \"Nullbody\", long an = -1, double bal = 0.0, double ml = 500, double r = 0.11125); BrassPlus(const Brass &amp; ba, double ml = 500, double r = 0.11125); virtual void ViewAcct()const; virtual void Withdraw(double amt); void ResetMax(double m) &#123; maxLoan = m; &#125; void ResetRate(double r) &#123; rate = r; &#125;; void ResetOwes() &#123; owesBank = 0; &#125;&#125;;#endif 使用delete释放由new分配的对象的代码说明了为何基类应包含虚析构函数(虽然有时好像并不需要析构函数) 如果析构函数不是虚的，则将只调用对应于指针类型的析构函数。意味着只有Brass的析构函数被调用，即使指针指向的是一个BrassPlus对象。 如果析构函数是虚的，将调用相应对象类型的析构函数。因此，如果指针指向的是BrassPlus对象，将调用BrassPlus的析构函数，然后自动调用基类的析构函数 因此，使用虚析构函数可以确保正确的析构函数序列被调用，如果 BrassPlus包含一个执行某些操作的析构函数，则Brass必须有一个虚析构函数，即使该析构函数不执行任何操作 虚函数 首先，虚函数 与 纯虚函数 是不同的 虚函数是为了允许用基类的指针来调用子类的这个函数，不代表函数为不被实现的函数 纯虚函数是为了，起到一个规范的作用，规范继承这个类的程序员必须实现这个函数，代表函数没有被实现 12345678910111213141516171819202122class A&#123;public: virtual void foo() &#123; cout&lt;&lt;\"A::foo() is called\"&lt;&lt;endl; &#125;&#125;;class B:public A&#123;public: void foo() &#123; cout&lt;&lt;\"B::foo() is called\"&lt;&lt;endl; &#125;&#125;;int main(void)&#123; A *a = new B(); a-&gt;foo(); // 在这里，a虽然是指向A的指针，但是被调用的函数(foo)却是B的! return 0;&#125; 一个类函数的调用并不是在编译时刻被确定的，而是在运行时刻被确定的。由于编写代码的时候并不能确定被调用的是基类的函数还是哪个派生类的函数，即动态联编，所以被成为&quot;虚&quot;函数 纯虚函数是在基类中声明的虚函数，它在基类中没有定义，但要求任何派生类都要定义自己的实现方法。在基类中实现纯虚函数的方法是在函数原型后加 =0 1virtual void funtion1()=0 这里就衍生出了另外一个概念 抽象类（接口），带有纯虚函数的类为抽象类。 抽象类只能作为基类来使用，其纯虚函数的实现由派生类给出。 如果派生类中没有重新定义纯虚函数，而只是继承基类的纯虚函数，则这个派生类仍然还是一个抽象类。 如果派生类中给出了基类纯虚函数的实现，则该派生类就不再是抽象类了，它是一个可以建立对象的具体的类 抽象类是不能定义对象的 虚函数原理 编译器处理虚函数的方法是：给每个对象添加一个隐藏成员。隐藏成员中保存了一个指向函数地址数组的指针。这种数组称为虚函数表(virtual function table，vtbl)。虚函数表中存储了为类对象进行声明的虚函数的地址。 例如，基类对象包含一个指针，该指针指向基类中所有虚函数的地址表。派生类对象将包含一个指向独立地址表的指针。如果派生类提供了虚函数的新定义，该虚函数表将保存新函数的地址；如果派生类没有重新定义虚函数，该vtbl将保存函数原始版本的地址。如果派生类定义了新的虚函数，则该函数的地址也将被添加到 vtbl 中。注意，无论类中包含的虚函数是1个还是10个，都只需要在对象中添加1个地址成员，只是表的大小不同而已 使用虚函数时，在内存和执行速度方面有一定的成本，包括： 每个对象都将增大，增大量为存储地址的空间； 对于每个类，编译器都创建一个虚函数地址表(数组)； 对于每个函数调用，都需要执行一项额外的操作，即到表中查找地址 虚函数需要注意的 在基类方法的声明中使用关键字virtual可使该方法在基类以及所有的派生类(包括从派生类派生出来的类)中是虚的。 如果使用指向对象的引用或指针来调用虚方法，程序将使用为对象类型定义的方法，而不使用为引用或指针类型定义的方法。这称为 动态联编或晚期联编。这种行为非常重要，因为这样基类指针或引用可以指向派生类对象。 如果定义的类将被用作基类，则应将那些要在派生类中重新定义的类方法声明为虚的。 构造函数不能是虚函数。创建派生类对象时，将调用派生类的构造函数，而不是基类的构造函数，然后，派生类的构造函数将使用基类的 一个构造函数，这种顺序不同于继承机制。因此，派生类不继承基类的构造函数，所以将类构造函数声明为虚的没什么意义。 析构函数应当是虚函数，除非类不用做基类 友元不能是虚函数，因为友元不是类成员，而只有成员才能是虚函数 如果重新定义继承的方法，应确保与原来的原型完全相同，但如果返回类型是基类引用或指针，则可以修 改为指向派生类的引用或指针(这种例外是新出现的)。这种特性被称为返回类型协变(covariance of return type) 1234567891011class Dwelling&#123;public: virtual Dwelling &amp; build(int n);&#125;;class Hovel : public Dwelling&#123;public: virtual Hovel &amp; build(int n);&#125; 这种只适用于返回值，而不适用于参数 如果基类声明被重载了，则应在派生类中重新定义所有的基类版本 123456789101112131415class Dwelling&#123;public: virtual void showperks(int a) const; virtual void showperks(double x) const; virtual void showperks() const;&#125;;class Hovel : public Dwelling&#123;public: virtual void showperks(int a) const; virtual void showperks(double x) const; virtual void showperks() const;&#125; 如果只重新定义一个版本，则另外两个版本将被隐藏，派生类对象将无法使用它们。注意，如果不需要修改，则新定义可只调用基类版本 派生类引用或指针转换为基类引用或指针被称为向上强制转换(upcasting)，使公有继承不需要进行显式类型转换 将基类指针或引用转换为派生类指针或引用称为向下强制转(downcasting)。如果不使用显式类型转换，则向下强 制转换是不允许的 在继承中还有一个 protected 需要注意 在类外只能用公有类成员来访问protected部分中的类成员。private和protected之间的区别只有在基类派生的类中才会表现出来。派生类的成员可以直接访问基类的protected成员，但不能直接访问基类的private私有成员 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-11-使用类","slug":"C++/CPP-11-使用类","date":"2022-08-07T13:39:29.000Z","updated":"2023-08-10T04:04:40.096Z","comments":true,"path":"2022/08/07/C++/CPP-11-使用类/","link":"","permalink":"http://xboom.github.io/2022/08/07/C++/CPP-11-%E4%BD%BF%E7%94%A8%E7%B1%BB/","excerpt":"","text":"运算符重载 运算符重载是一种形式的 C++ 多态 用户能够定义多个名称相同但特征标(参数列表)不同的函数的。称为函数重载或函数多态，旨在能够用同名的函数来完成相同的基本操作，即使这种操作被用于不同的数据类型 首先实现时间操作 file1: mytime0.h，实现类的声明 1234567891011121314151617#ifndef MYTIME0_H_#define MYTIME0_H_class Time&#123; private: int hours; int minutes; public: Time(); Time(int h, int m = 0); void AddMin(int m); void AddHr(int h); void Reset(int h = 0, int m = 0); Time Sum(const Time &amp; t) const; void Show() const;&#125;;#endif file2: usetime0.cpp 实现类的成员函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include \"mytime0.h\"using namespace std;Time::Time()&#123; hours = minutes = 0;&#125;Time::Time(int h, int m)&#123; hours = h; minutes = m;&#125;void Time::AddMin(int m)&#123; minutes += m; hours += minutes / 60; minutes %= 60;&#125;void Time::AddHr(int h)&#123; hours += h;&#125;void Time::Reset(int h, int m)&#123; hours = h; minutes = m;&#125;Time Time::Sum(const Time &amp; t) const&#123; Time sum; sum.minutes = minutes + t.minutes; sum.hours = hours + t.hours + sum.minutes / 60; sum.minutes %= 60; return sum;&#125;void Time::Show() const&#123; cout &lt;&lt; hours &lt;&lt; \" hours, \" &lt;&lt; minutes &lt;&lt; \" minutes\" &lt;&lt; endl;&#125; file3:usertime0.cpp 使用类 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include \"mytime0.h\"using namespace std;int main()&#123; Time planning; Time coding(2, 40); Time fixing(5, 55); Time total; cout &lt;&lt; \"planning time = \"; planning.Show(); cout &lt;&lt; \"coding time = \"; coding.Show(); cout &lt;&lt; \"fixing time = \"; fixing.Show(); total = coding.Sum(fixing); cout &lt;&lt; \"coding.Sum(fixing) = \"; total.Show(); return 0;&#125; 编译运行如下 123456789/*g++ -Wall -Werror -std=c++11 -c src/mytime0.cpp -o build/mytime0.og++ -Wall -Werror -std=c++11 -c src/usetime0.cpp -o build/usetime0.og++ build/mytime0.o build/usetime0.o -o demo*/planning time = 0 hours, 0 minutescoding time = 2 hours, 40 minutesfixing time = 5 hours, 55 minutescoding.Sum(fixing) = 8 hours, 35 minutes 添加一个加法运算符 1234567891011121314Time operator+(const Time &amp; t) const;Time Time::operator+(const Time &amp; t) const&#123; Time sum; sum.minutes = minutes + t.minutes; sum.hours = hours + t.hours + sum.minutes / 60; sum.minutes %= 60; return sum;&#125;total = coding + fixing;cout &lt;&lt; \"coding + fixing = \";total.Show(); 运行结果与 sum 函数一样 12coding.Sum(fixing) = 8 hours, 35 minutescoding + fixing = 8 hours, 35 minutes 对象连续相加也是允许的 12345total = coding + fixing + coding + planning;cout &lt;&lt; \"all + =\";total.Show();//输出结果 all + =11 hours, 15 minutes 重载的限制也是有的 重载后的运算符必须至少有一个操作数是用户定义的类型，将防止用户为标准类型重载运算符 使用运算符时不能违反运算符原来的句法规则。例如，不能将求模运算符(%)重载成使用一个操作数 不能创建新运算符。例如，不能定义operator **( )函数来表示求幂 不能重载下面的运算符 sizeof：sizeof运算符 .：成员运算符。 . *：成员指针运算符。 ::：作用域解析运算符。 ?:：条件运算符。 typeid：一个RTTI运算符。 const_cast：强制类型转换运算符。 dynamic_cast：强制类型转换运算符。 reinterpret_cast：强制类型转换运算符。 static_cast：强制类型转换运算符。 友元 C控制对类对象私有部分的访问。通常，公有类方法提供唯一的访问途径，但是有时候这种限制太严格，C提供了另外一种形式的访问权限：友元 友元有3种： 友元函数 友元类 友元成员函数 为何需要友元？为类重载二元运算符时(带两个参数的运算符)常常需要友元，通过让函数成为类的友元，可以赋予该函数与类的成员函数相同的访问权限 比如Time 类重载操作符号* 12345678910Time operator+(double n) const;Time Time::operator+(double n) const&#123; Time sum; sum.minutes = minutes * n % 60; sum.hours = hours + minutes * n / 60; sum.hours = sum.hours % 24; return sum;&#125; 上述可以 使用 A = B * 0.75 但是无法用来实现 A = 0.75 * B，这样不就和普通的 * 不一样了吗 友元就是为了解决这个问题，步骤如下 将其原型放在类声明中，并在原型声明前加上关键字 friend 12friend Time operator*(double m, const Time &amp; t); //gose in class declaration//friend Time operator*(double n, const Time &amp; t) const; //x 非成员函数上不允许使用类型限定符 虽然operator *( )函数是在类声明中声明的，但它不是成员函数，因此不能使用成员运算符来调用； 虽然operator *( )函数不是成员函数，但它与成员函数的访问权限相同 注意，不能使用 类型限定符，因为 此时的 opeartor* 不是成员函数 是编写函数定义。因为它不是成员函数，所以不要使用 Time::限定符。另外，不要在定义中使用关键字friend 12345678Time operator*(double m, const Time &amp; t) //friend not used in definition&#123; Time sum; sum.minutes = minutes * m % 60; sum.hours = hours + minutes * m / 60; sum.hours = sum.hours % 24; return sum;&#125; 有了上述声明和定义后，就可以使用 A = 2.75 * B 了 12345678Time operator*(double m, const Time &amp; t) //friend not used in definition&#123; Time sum; sum.minutes = t.minutes % 60; sum.hours = t.hours + t.minutes * m / 60; sum.hours = sum.hours % 24; return sum;&#125; 由于不是成员函数，所以不能直接使用 sum.minutes = minutes % 60; 12total = coding * 0.75; //错误，没有与这些操作数匹配的 \"*\" 运算符total = 0.25 * coding; 所以，上述的友元函数并不能两者都支持，还是需要一个重载的成员函数 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-10-对象和类","slug":"C++/CPP-10-对象和类","date":"2022-08-06T16:00:29.000Z","updated":"2023-08-10T04:04:36.792Z","comments":true,"path":"2022/08/07/C++/CPP-10-对象和类/","link":"","permalink":"http://xboom.github.io/2022/08/07/C++/CPP-10-%E5%AF%B9%E8%B1%A1%E5%92%8C%E7%B1%BB/","excerpt":"","text":"声明一个类 123456789101112131415161718class Stock&#123;private: string company; long shares; double share_val; double total_val; void set_tot() &#123; total_val = shares * share_val; &#125;public: void acquire(const string &amp;co, long n, double pr); void buy(long num, double price); void sell(long num, double price); void update(double price); void show();&#125;; 关键字private和public描述了对类成员的访问控制。使用类对象的程序都可以直接访问公有部分，但只能通过公有成员函数来访问对象的私有成员，默认是private私有成员 成员函数的函数头使用作用域运算符解析（::）来指出函数所属的类。例如，update() 成员函数的函数头如下 1void Stock::update(double price); 构造与析构 123456789int year = 2001; struct thing &#123; char *pn; int m; &#125;; thing amabob = &#123;\"wodget\", -23&#125;; //Stock hot = &#123;\"stock host\", 200, 50.23&#125;;&#125; 上述代码中 thing 结构体可以直接声明对象，但是 Stock 无法实例化对象，会提示 没有与参数列表匹配的构造函数 &quot;Stock::Stock&quot; 实例，原因是数据的访问状态是私有的，程序不能直接访问。这个时候就可以用构造函数 1234567891011121314Stock(const string &amp;co, long n, double pr)&#123; company = co; if(n &lt; 0) &#123; shares = 0; &#125; else &#123; shares = n; &#125; share_val = pr;&#125;Stock hot = &#123;\"stock host\", 200, 50.23&#125;;Stock niu = Stock&#123;\"stock host\", 200, 50.23&#125;;Stock *nnm = new Stock&#123;\"stock host\", 200, 50.23&#125;; 上述三种方式使用构造函数，但是需要一个特殊的地方 12//类 \"Stock\" 不存在默认构造函数Stock st; 也就是说 Stock st;其实是使用的默认构造函数，如果不编写构造函数的时候是可以的，但是如果声明了构造函数，那么就不能直接声明类实例。为了解决这个问题可以将已有构造函数使用默认值 1234Stock(const string &amp;co = \"Error\", long n = 2, double pr = 3.145)&#123; //...&#125; 也可以使用一个默认构造函数 1Stock(); 由于只能有一个默认构造函数，所以上述两种方法只能用一种 当程序过期的时候，程序将自动调用一个特殊的成员函数–析构函数，与构造函数不同的是，析构函数没有参数 12345//~Stock();~Stock()&#123; cout &lt;&lt; \"Bye, \" &lt;&lt; company &lt;&lt; \"!\\n\";&#125; 调用析构函数时机由编译器决定，通常不显示地调用析构函数 如果创建的是静态存储类对象，则其析构函数将在程序结束时被调用 如果创建的是自动存储类对象，则其析构函数将在程序执行完代码块自动被调用 如果对象是通过 new 创建时，则它将驻留在栈内存或自由存储区中，当使用 delete 来释放内存时，其析构函数将自动被调用 由于在类过期时析构函数将自动被调用，因此必须有一个析构函数。如果没有提供析构函数，编译器将隐式地声明一个默认析构函数 可以使用对象数组声明多个对象 1234567const int STKS = 4;Stock stocks[STKS] = &#123; Stock&#123;\"boject 1\", 12, 111&#125;, Stock&#123;\"boject 2\", 12, 111&#125;, Stock&#123;\"boject 3\", 12, 111&#125;, Stock&#123;\"boject 4\", 12, 111&#125;,&#125;; const 成员函数 12const Stock land = Stock&#123;\"Kluudgehorn Properties\"&#125;;land.show(); 提示 对象含有与成员 函数 &quot;Stock::show&quot; 不兼容的类型限定符C/C++(1086)，stock11.cpp(8, 5): 对象类型是: const Stock，因为 代码无法确保调用对象不被修改——调用对象和const一样， 不应被修改。 为了保证不会修改调用对象，C++的解决方法是将const关键字 放在函数的括号后面 123456void show() const;void Stock::show() const&#123; cout &lt;&lt; \"Number of shares purchased\" &lt;&lt; endl;&#125; this指针 有时候方法可能涉及到两个对象，在这种情况下需要使用C++的this指针 例如：该函数隐式地访问一个对象，而显式地访问另一个对象，比较之后返回其中一个对象的引用 1const Stock &amp; topval(const Stock &amp; s) const; 括号中的const表明，该函数不会修改被显式地访问的对象；而括号后的const表明，该函数不会修改被隐式地访问的对象。 由于该函数返回了两个const对象之一的引用，因此返回类型也应为 const 引用 假设要对Stock对象stock1和stock2进行比较，并返回其中一个则 12top = stock1.topval(stock2);top = stock2.topval(stock1); 第一种格式隐式地访问stock1，而显式地访问stock2；第二种格式显式地访问stock1，而隐式地访问无论使用哪一 种方式，都将对这两个对象进行比较，并返回其中一个对象 12345const Stock &amp; Stock:topval(const Stock &amp; s) const&#123; if(s.total_val &gt; total_val) return s; //return argument object else return *this; //invoking object&#125; 使用被称为this的特殊指针。this指针指向用来调用成员函数的对象(this被作为隐藏参数传递给方法)。 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"算法之美-滑动窗口","slug":"Algorithms To Live By/算法之美-滑动窗口","date":"2022-08-01T09:42:45.000Z","updated":"2022-08-01T09:42:45.000Z","comments":true,"path":"2022/08/01/Algorithms To Live By/算法之美-滑动窗口/","link":"","permalink":"http://xboom.github.io/2022/08/01/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"","text":"在TCP原理中使用滑动窗口进行流量控制，平时的算法中也会使用双指针的滑动窗口进行问题处理。这里看看滑动窗口的具体原理 实现原理 滑动窗口不仅仅可以通过长度来限制流量，还能通过在窗口中保存最近一段时间数据，以此来进行更加复杂的实时调整 为什么不使用链表：使用切片构建类似环的结构，有效节省空间 技术内幕 算法实现中的类似滑动窗口框架为 1234567891011121314151617string s, t;// 在 s 中寻找 t 的「最小覆盖子串」int left = 0, right = 0;string res = s;while(right &lt; s.size()) &#123; window.add(s[right]); right++; // 如果符合要求，移动 left 缩小窗口 while (window 符合要求) &#123; // 如果这个窗口的子串更短，则更新 res res = minLen(res, window); window.remove(s[left]); left++; &#125;&#125;return res; 看看go-zero中滑动窗口的实现 代码路径：core/collection/rollingwindow.go 对象定义 123456789101112131415type ( //自定义可选参数 RollingWindowOption func(rollingWindow *RollingWindow) // 滑动窗口结构 RollingWindow struct &#123; lock sync.RWMutex //窗口读写锁 size int //窗口大小 win *window //窗口对洗那个 interval time.Duration //时间间隔 offset int //当前位置 ignoreCurrent bool //忽略当前 lastTime time.Duration // start time of the last bucket &#125;) 槽 窗口中的每个槽可用于存储更加复杂的数据进行最近一段时间数据统计 123456789101112131415//槽type Bucket struct &#123; Sum float64 Count int64&#125;func (b *Bucket) add(v float64) &#123; b.Sum += v b.Count++&#125;func (b *Bucket) reset() &#123; b.Sum = 0 b.Count = 0&#125; 窗口操作 使用切片与取余进行类似环形操作 1234567891011121314//增加 % 类型唤醒操作func (w *window) add(offset int, v float64) &#123; w.buckets[offset%w.size].add(v) &#125;func (w *window) reduce(start, count int, fn func(b *Bucket)) &#123; for i := 0; i &lt; count; i++ &#123; fn(w.buckets[(start+i)%w.size]) &#125;&#125;func (w *window) resetBucket(offset int) &#123; w.buckets[offset%w.size].reset()&#125; 滑动窗口操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//窗口移动func (rw *RollingWindow) span() int &#123; offset := int(timex.Since(rw.lastTime) / rw.interval) //移动格子 if 0 &lt;= offset &amp;&amp; offset &lt; rw.size &#123; return offset &#125; return rw.size //当偏移量超过窗口大小，那么直接返回窗口大小(不需要移动offset的原因抛弃旧窗口即可)&#125;//更新偏移量func (rw *RollingWindow) updateOffset() &#123; span := rw.span() //窗口移动位置 if span &lt;= 0 &#123; return &#125; offset := rw.offset //当前窗口起始起始 for i := 0; i &lt; span; i++ &#123; //重置过期的槽 rw.win.resetBucket((offset + i + 1) % rw.size) &#125; rw.offset = (offset + span) % rw.size //更新起始位置 now := timex.Now() //当前时间 // align to interval time boundary rw.lastTime = now - (now-rw.lastTime)%rw.interval //最新移动时间&#125;//滑动窗口更新槽func (rw *RollingWindow) Add(v float64) &#123; rw.lock.Lock() defer rw.lock.Unlock() rw.updateOffset() //更新索引 rw.win.add(rw.offset, v) //增加值&#125;//对窗口进行 fn 操作func (rw *RollingWindow) Reduce(fn func(b *Bucket)) &#123; rw.lock.RLock() defer rw.lock.RUnlock() var diff int span := rw.span() // ignore current bucket, because of partial data if span == 0 &amp;&amp; rw.ignoreCurrent &#123; diff = rw.size - 1 &#125; else &#123; diff = rw.size - span &#125; if diff &gt; 0 &#123; offset := (rw.offset + span + 1) % rw.size rw.win.reduce(offset, diff, fn) &#125;&#125;","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"CPP-09-编译访问","slug":"C++/CPP-09-编译访问","date":"2022-07-31T14:07:29.000Z","updated":"2023-08-10T04:04:33.884Z","comments":true,"path":"2022/07/31/C++/CPP-09-编译访问/","link":"","permalink":"http://xboom.github.io/2022/07/31/C++/CPP-09-%E7%BC%96%E8%AF%91%E8%AE%BF%E9%97%AE/","excerpt":"","text":"C++允许甚至鼓励程序员将组件函数放在独立的文件中，甚至可以单独编译这些文件，然后将它们链接成可执行的程序 可以将原来的程序分成三部分 头文件：包含结构声明和使用这些结构的函数的原型。 源代码文件：包含与结构有关的函数的代码。 源代码文件：包含调用与结构相关的函数的代码。 请不要将函数定义或变量声明放到头文件中，例如，如果在头文件包含一个函数定义，然后在其他两个文件(属于同一个程序)中包含该头文件，则同一个程序中将包含同一个函数的两个定义，除非函数是内联的，否则将出错。下面列出了头文件中常包含的内容 函数原型 使用#define或const定义的符号常量 结构声明 类声明 模板声明 内联函数 查找头文件规则 如果文件名包含在尖括号&lt; &gt;中，则 C++编译器将在存储标准头文件的主机系统的文件系统中查找； 如果文件名包含在双引号&quot; &quot;中，则编译器将首先查找当前的工作目录或源代码目录(或其他目录，这取决于编译器)。如果没有在那里找到头文件，则将在标准位置查找。因此在包含自己的头文件时，应使用&quot; &quot;而不是&lt; &gt; 头文件定义结构体定于与与结构体相关的函数原型 1234567891011121314151617181920#ifndef COORDIN_H_#define COORDIN_H_struct polar&#123; double distance; //distance from origin double angle; //direction from origin&#125;;struct rect&#123; double x; //horizontal distance from origin double y; //vertical distance from origin&#125;;//prototypespolar rect_to_polar(rect xypos);void show_polar(polar dapos);#endif 在同一个文件中只能将同一个头文件包含一次。这个规则很容易在不知情的情况下将头文件包含多次，所以使用 #ifndef，这种方法并不能防止编译器将文件包含两次，而只是让它忽略除第一次包含之外的所有内容。 12345#ifndef XXXX#define XXXX// place include file content#endif 为什么上面使用 COORDIN_H_这么奇怪的名字？ 根据include文件名来选择名称，并加上一些下划线，以创建一个在其他地方不太可能被定义的名称 file1.cpp 调用与结构相关的函数的代码 1234567891011121314151617#include &lt;iostream&gt;#include &lt;cmath&gt;#include \"coordin.h\"using namespace std;polar rect_to_polar(rect xypos)&#123; polar result; result.distance = sqrt(xypos.x * xypos.x + xypos.y * xypos.y); return result;&#125;void show_polar(polar dapos)&#123; const double RAD_TO_DEG = 57.295; cout &lt;&lt; \"show:\" &lt;&lt; dapos.angle * RAD_TO_DEG &lt;&lt; endl;&#125; file2.cpp 定义与结构体相关的函数定义 1234567891011121314#include &lt;iostream&gt;#include \"coordin.h\"using namespace std;int main()&#123; rect rplace; polar pplace; rplace.x = 10.1; rplace.y = 20.2; pplace = rect_to_polar(rplace); //转换 show_polar(pplace); //打印 return 0;&#125; 查看编译过程 12345#make clean &amp;&amp; makerm -rf build/*.o demog++ -Wall -Werror -std=c++11 -c src/file1.cpp -o build/file1.og++ -Wall -Werror -std=c++11 -c src/file2.cpp -o build/file2.og++ build/file1.o build/file2.o -o demo 当函数被调用时，其自动变量将被加入到栈中，栈顶指针指向变量后面的下一个可用的内存单元。函数结束时，栈 顶指针被重置为函数被调用前的值，从而释放新变量使用的内存 被调用的函数根据其形参描述来确定每个参数的地 址。 函数fib( )被调用时，传递一个2字节的int和一个 4字节的long。函数地址 0x1089 将名称 real 和tell同参数关联起来，函数调用将其参数的值放在栈顶，然后重新设置栈顶指针 当fib( )结束时，栈顶指针重新指向以前的位置。新值没有被删除，但不再被标记，它们所占据的空间将被下一个将值加入到栈中的函数调用所使用 C++也为静态存储持续性变量提供了3种链接性 123456789101112...int global = 1000; //static duration, external linkagestatic int one_file = 50; //static duration, internal linkage int main()&#123; ...&#125;void funct1(int n)&#123; static int count = 0; //static duration, no linkage int llama = 0;&#125; 外部链接性(可在其他文件中访问) 内部链接性(只能在当前文件中访问) 无链接性(只能在当前函数或代码块中访问) 编译器将分配固定的内存块来存储所有的静态变量，这些变量在整个程序执行期间一直存在。另外，如果没有显式地 初始化静态变量，编译器将把它设置为0 零初始化和常量表达式初始化被统称为静态初始化，在编译器处理文件(翻译单元)时初始化变量。动态初始化意味着变量将在编译后初始化。 C++提供了两种变量声明。 一种是定义声明(defining declaration)或简称为定义(definition)，它给变量分配存储空间； 另一种是引用声(referencing declaration)或简称为声明(declaration)，它不给变量分配存储空间，因为它引用已有的变量。 引用声明使用关键字extern，且不进行初始化；否则，声明为定义，分配存储空间 123double up; //definition, up is 0extren int blem; //blem defined elsewhereextern char gr = 'z'; //definition because initialized cv 限定符 代表 const 和 volatile const 表明内存被初始化后，程序便不 能再对它进行修改 volatile表明即使程序代码没有对内存单元进行修改，其值也可能发生变化 mutable 即使结构(或类)变量为 const，其某个成员也可以被修改 123456789struct data&#123; char name[30]; mutable int accesses;&#125;;const data veep = &#123;\"hello\", 12&#125;;strcpy(veep.name, \"hello world\"); //not allowed \"const char *\" 类型的实参与 \"char *\" 类型的形参不兼容veep.accesses++; //allowed 全局const定义就像使用了static说明符一样，下面两者的内存存储是一样的 12const int fingers = 10;static const int fingers = 10; C++不允许在一个函数中定义另外一个函数，因此所有函数的存储持续性都自动为静态的，在整个程序执行期间都是一直存在的。函数的链接性为外部的，即可以在文件间共享。 可以使用关键字 extern 来指定函数在另一个文件中定义的(可选) 可以使用 static 将函数的连接性设置为内部的，使之只能在一个文件中使用 命名空间 第一个文件 namesp.h 12345678910111213141516171819202122232425262728#include &lt;string&gt;using namespace std;namespace pers&#123; struct Person &#123; string fname; string lname; &#125;; void getPerson(Person &amp;); void showPerson(const Person &amp;);&#125;;namespace debts&#123; using namespace pers; struct Debt &#123; Person name; double amount; &#125;; void getDebt(Debt &amp;); void showDebt(const Debt &amp;); double sumDebts(const Debt ar[], int n);&#125;; 第二个文件 namesp.cpp 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include \"namesp.h\"using namespace std;namespace pers&#123; void getPerson(Person &amp; rp) &#123; rp.fname = \"hello\"; rp.lname = \"world\"; &#125; void showPerson(const Person &amp;rp) &#123; cout &lt;&lt; rp.fname &lt;&lt; \", \" &lt;&lt; rp.lname &lt;&lt; endl; &#125;&#125;;namespace debts&#123; void getDebt(Debt &amp; rd) &#123; getPerson(rd.name); cout &lt;&lt; rd.amount &lt;&lt; endl; &#125; void showDebt(const Debt &amp; rd) &#123; showPerson(rd.name); cout &lt;&lt; rd.name.fname &lt;&lt; rd.amount &lt;&lt; endl; &#125; double sumDebts(const Debt ar[], int n) &#123; double total = 0; for(int i = 0; i &lt; n; i++) total += ar[i].amount; return total; &#125;&#125;; 第三个文件 namespp.cpp 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include \"namesp.h\"void other(void);void another(void);int main(void)&#123; using debts::Debt; using debts::showDebt; Debt golf = &#123;&#123;\"hello\", \"world\"&#125;, 120.0&#125;; showDebt(golf); other(); another(); return 0;&#125;void other(void)&#123; using namespace debts; //debts 包含 pers，所以这里可以使用 Person Person dg = &#123;\"Doodles\", \"Glister\"&#125;; showPerson(dg); cout &lt;&lt; endl; Debt zippy[3]; for(int i = 0; i &lt; 3; i++) getDebt(zippy[i]); for(int i = 0; i &lt; 3; i++) showDebt(zippy[i]); cout &lt;&lt; sumDebts(zippy, 3) &lt;&lt; endl;&#125;void another(void)&#123; using pers::Person; //使用具体结构 Person collector = &#123;\"Milo\", \"RightShift\"&#125;; pers::showPerson(collector); //使用命名空间调用 cout &lt;&lt; endl;&#125; 命名空间建议 使用在已命名的名称空间中声明的变量，而不是使用外部全局变量 使用在已命名的名称空间中声明的变量，而不是使用静态全局变量 如果开发了一个函数库或类库，将其放在一个名称空间中。事实上，C当前提倡将标准函数库放在名称空间std中，这种做法扩展到了来自C语言中的函数。例如，头文件math.h是与C语言兼容的， 没有使用名称空间，但C头文件cmath应将各种数学库函数放在名称空间std中 不要在头文件中使用using编译指令。 这样做掩盖了要让哪些名称可用； 包含头文件的顺序可能影响程序的行为。如果非 要使用编译指令using，应将其放在所有预处理器编译指令#include 之后。 导入名称时，首选使用作用域解析运算符或using声明的方法 对于using声明，首选将其作用域设置为局部而不是全局 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-08-函数探幽","slug":"C++/CPP-08-函数探幽","date":"2022-07-29T14:35:29.000Z","updated":"2023-08-10T04:04:30.032Z","comments":true,"path":"2022/07/29/C++/CPP-08-函数探幽/","link":"","permalink":"http://xboom.github.io/2022/07/29/C++/CPP-08-%E5%87%BD%E6%95%B0%E6%8E%A2%E5%B9%BD/","excerpt":"","text":"内联函数 编译过程的最终产品是可执行程序——由一组机器语言指令组成。 运行程序时，操作系统将这些指令载入到计算机内存中，因此每条指令都有特定的内存地址。计算机随后将逐步执行这些指令。有时(如有循环或分支语句时)，将跳过一些指令，向前或向后跳到特定地址。常规函数调用也使程序跳到另一个地址(函数的地址)，并在函数结束时返回 执行到函数调用指令时， 程序将在函数调用后立即存储该指令的内存地址，并将函数参数复制到 堆栈（为此保留的内存块），跳到标记函数起点的内存单元，执行函数代码（也许还需将返回值放入到寄存器中），然后跳回到地址被保存的指令处。来回跳跃并记录跳跃位置意味着以前使用函数时，需要一定的开销。 对于内联代码，程序无需跳到另一个位置处执行代码，再跳回来。所以内联函数的运行速度比常规函数稍快，但代价是需要占用更多内存 要使用内联函数，需要采取以下措施 在函数声明前加上关键字 inline 在函数定义前加上关键字 inline 123456inline double square(int x) &#123; return x * x;&#125;cout &lt;&lt; square(2) &lt;&lt; endl; //4 引用变量 引用是已定义的变量的别名(另一个名称) 1234567891011int rats = 100;int nums = 10;int &amp; rats_back = rats;//int &amp; rats_back; //声明引用变量时进行初始化cout &lt;&lt; rats &lt;&lt; \" \" &lt;&lt; rats_back &lt;&lt; endl;rats = 101;cout &lt;&lt; rats &lt;&lt; \" \" &lt;&lt; rats_back &lt;&lt; endl;rats_back = nums;cout &lt;&lt; rats &lt;&lt; \" \" &lt;&lt; rats_back &lt;&lt; \" \" &lt;&lt; nums &lt;&lt; endl;nums = 1;cout &lt;&lt; rats &lt;&lt; \" \" &lt;&lt; rats_back &lt;&lt; \" \" &lt;&lt; nums &lt;&lt; endl; 运行结果 1234100 100101 10110 10 1010 10 1 使用 int &amp; rats_back = rats; 声明引用变量，即指向 int 变量的引用 必须在声明引用变量时进行初始化 一旦与某个变量关联起来，就将一直效忠于它 引用经常被用作函数参数，使得函数中的变量名成为调用程序中的变量的别名。这种传递参数的方法称为按引用传递。按引用传递允许被调用的函数能够访问调用函数中的变量。C++新增的这项特性是对C语言的超越，C语言只能按值传递 1234567891011121314151617181920212223242526272829303132void swapr(int &amp; a, int &amp; b) //a, b are aliases for ints&#123; int temp; temp = a; a = b; b = temp;&#125;void swapp(int * p, int * q) //p, q are addreesses of ints&#123; int temp; temp = *p; *p = *q; *q = temp;&#125;void swapv(int a, int b) //a, b are new variables&#123; int temp; temp = a; a = b; b = temp;&#125;int wallet1 = 300;int wallet2 = 350;swapr(wallet1, wallet2); //引用传递cout &lt;&lt; wallet1 &lt;&lt; \" \" &lt;&lt; wallet2 &lt;&lt; endl;swapp(&amp;wallet1, &amp;wallet2);cout &lt;&lt; wallet1 &lt;&lt; \" \" &lt;&lt; wallet2 &lt;&lt; endl;swapv(wallet1, wallet2);cout &lt;&lt; wallet1 &lt;&lt; \" \" &lt;&lt; wallet2 &lt;&lt; endl; 运行结果 123350 300300 350300 350 swapp在函数使用p和q的整个过程中使用解除 引用运算符*，所以跟 swapv 效果一样 这就是需要注意的地方，通过引用传递可能修改源值 1234567891011121314151617double cube(double a)&#123; a = a * a; return a;&#125;double refcube(double &amp;ra)&#123; ra = ra * ra; return ra;&#125;double a = 2;recout &lt;&lt; cube(a) &lt;&lt; \" \" &lt;&lt; a &lt;&lt; endl; //4 2cout &lt;&lt; refcube(a) &lt;&lt; \" \" &lt;&lt; a &lt;&lt; endl; //4 4refcube(a + 0.5); //candidate function not viable: expects an lvalue for 1st argument 可以通过 const 不允许函数修改 1double refcube(const double &amp;ra); 所以，将引用参数声明为常量数据的引用的理由有三个: 使用const可以避免无意中修改数据的编程错误； 使用const使函数能够处理const和非const实参，否则将只能接受非const数据； 使用const引用使函数能够正确生成并使用临时变量。 结构体的引用变量 1234567891011121314151617181920212223242526272829struct student&#123; string name; int age; double weight;&#125;;void Add(student &amp; t)&#123; t.name = \"hello\";&#125;student Change(student t)&#123; t.name = \"nihao\"; return t;&#125;student t = student&#123; \"world\", 18, 77.5&#125;;Add(t);cout &lt;&lt; t.name &lt;&lt; \" \" &lt;&lt; t.age &lt;&lt; \" \" &lt;&lt; t.weight &lt;&lt; endl;student t1 = Change(t);cout &lt;&lt; t.name &lt;&lt; \" \" &lt;&lt; t.age &lt;&lt; \" \" &lt;&lt; t.weight &lt;&lt; endl;cout &lt;&lt; t1.name &lt;&lt; \" \" &lt;&lt; t1.age &lt;&lt; \" \" &lt;&lt; t1.weight &lt;&lt; endl; 运行结果 123hello 18 77.5hello 18 77.5nihao 18 77.5 什么时候使用引用 如果数据对象很小，如内置数据类型或小型结构，则按值传递。 如果数据对象是数组，则使用指针，因为这是唯一的选择，并将指针声明为指向const的指针。 如果数据对象是较大的结构，则使用const指针或const引用，以提高程序的效率。这样可以节省复制结构所需的时间和空间。 如果数据对象是类对象，则使用const引用。类设计的语义常常要求使用引用，这是C++新增这项特性的主要原因。因此，传递类对象 参数的标准方式是按引用传递。 对于修改调用函数中数据的函数: 如果数据对象是内置数据类型，则使用指针 如果看到诸如 fixit(&amp;x)，则该函数将修改 x。 如果数据对象是数组，则只能使用指针。 如果数据对象是结构，则使用引用或指针 如果数据对象是类对象，则使用引用 默认参数 123456int add(int a, int b = 1)&#123; return a + b;&#125;cout &lt;&lt; add(1) &lt;&lt; \" \" &lt;&lt; add(1, 3) &lt;&lt; endl; //2 4 函数重载 函数多态是C++在C语言的基础上新增的功能。 默认参数使得能够使用不同数目的参数调用同一个函数， 函数多态(函数重载)让您能够使用多个同名的函数 如果两个函数的参数数目和类型相同，同时参数的排列顺序也相同，则它们的特征标相同，而变量名是无关紧要的。 C++允许定义名称相同的函数，条件是它们的特征标不同。如果参数数目(和/或)参数类型不同，则特征标也不同。 123456789101112131415161718192021222324252627282930void add(double a, double b)&#123; cout &lt;&lt; \"double: \" &lt;&lt; (a + b) &lt;&lt; endl;&#125; void add(long a, long b)&#123; cout &lt;&lt; \"long: \" &lt;&lt; (a + b) &lt;&lt; endl;&#125;void add(int a, int b)&#123; cout &lt;&lt; \"int: \" &lt;&lt; (a + b) &lt;&lt; endl;&#125;void add(int a, double b)&#123; cout &lt;&lt; \"int double: \" &lt;&lt; (a + b) &lt;&lt; endl;&#125;void add(int a, long b, double c)&#123; cout &lt;&lt; \"int long double: \" &lt;&lt; (a + b + c) &lt;&lt; endl;&#125;add(1.0, 2.0);add(1, 2);add(1L, 2L);add(1, 2.0);add(1, 2L, 3.0); 运行结果 12345double: 3int: 3long: 3int double: 3int long double: 6 函数模板 需要多个对不同类型使用同一种算法的函数时，可使用模板 12template &lt;typename AnyType&gt;template &lt;class AnyType&gt; 123456789101112131415161718template &lt;typename T&gt; void Swap(T &amp;a, T &amp;b)&#123; T temp; temp = a; a = b; b = temp;&#125;int i = 10;int j = 20;Swap(i, j);cout &lt;&lt; i &lt;&lt; \" \" &lt;&lt; j &lt;&lt; endl;double x = 24.5;double y = 81.7;Swap(x, y);cout &lt;&lt; x &lt;&lt; \" \" &lt;&lt; y &lt;&lt; endl; 输出结果 1220 1081.7 24.5 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-07-函数","slug":"C++/CPP-07-函数","date":"2022-07-29T12:17:29.000Z","updated":"2023-08-10T04:04:27.115Z","comments":true,"path":"2022/07/29/C++/CPP-07-函数/","link":"","permalink":"http://xboom.github.io/2022/07/29/C++/CPP-07-%E5%87%BD%E6%95%B0/","excerpt":"","text":"函数的定义如下 1234567891011void functionName(parameterList) &#123; statement(s); return; //optional&#125;typeName functionName(parameterList) &#123; statement(s); return value; //value is type cast to type typeName&#125; 为什么有时候会需要原型？ 编译器在搜索文件的剩余部分时将必须停止对main( )的编译。函数可能并不在文件中，C++允许将一个程序放 在多个文件中，单独编译这些文件，然后再将它们组合起来。在这种情况下，编译器在编译main( )时，可能无权访问函数代码。如果函数位于库中，情况也将如此。避免使用函数原型的唯一方法是，在首次使用函数之前定义它，但这并不总是可行的 12double cube(double); //OKvoid say_bye(...); 函数原型不要求提供变量名，有类型列表就足够了 不指定参数列表时应使用省略号 参数数组 C通常 按值传递 参数，这意味着将 数值参数传递给函数，而后者将其赋给一个新的变量。出于简化的目的，C标准使用参数(argument)来表示实参，使用参量(parameter)来表示形参 但是在传递数组的时候，传递的是指针(数组的地址，也是值传递)，将数组名解释为其第一个元素的地址 1int sum_arr(int arr[], int n) //arr = array, n = size 所以可以直接使用指针 1int sum_arr(int *arr, int n) 但也不能完全相等，因为 int * arr 还可以表示为一个整形的指针 因为数组作为函数参数传递的是指针，意味着函数内部可能修改数组内容，为了防止组数在函数内被修改可以使用const 保护数组 1void show_array(const double arr[], int num); const 意味着arr不能被修改，否则编译器将报错 指针与const const 用法有两种 让指针指向一个常量对象，可以防止使用该指针来修改所指向的值 将指针本身声明为常量，这样可以防止改变指针指向的位置 首先，声明一个指向常量的指针 123456int age = 39;const int *pt = &amp;age;const int num = 40;//*pt += 1;cout &lt;&lt; *pt &lt;&lt; \" \" &lt;&lt; age &lt;&lt; \" \" &lt;&lt; num &lt;&lt; endl; pt，是一个指针(*pt)，指针的类型是 const int，也就是 *pt 指向了一个常量，不能使用 *pt 来修改它 123*pt = 1; //INVALID because pt points to a const intage = 2;num = 4; //cannot assign to variable 'num' with const-qualified type 'const int' 这个声明意味着 *pt 指向了一个常量，不能通过 pt 修改，但并不意味着 age 是一个常量 假设有一个由const数据组成的数组，则禁止将常量数组的地址赋给非常量指针将意味着不能将数组名作为参数传递给使用非常量形参的函数： 1234const int months[12] = &#123;31, 28, 31, 30, 31&#125;;int sum(int arr[], int n);int j = sum(months, 12); //not allowedmonths[0] = 1; //cannot assign to variable 'months' with const-qualified type 'const int[12]' 尽可能使用 const 将指针参数声明为指向常量数据的指针有两条理由： 这样可以避免由于无意间修改数据而导致的编程错误； 使用const使得函数能够处理const和非const实参，否则将只能接受非const数据。 如果条件允许，则应将指针形参声明为指向const的指针 12345int age = 39;int num = 40;const int *pt = &amp;age;pt = &amp;num;cout &lt;&lt; *pt &lt;&lt; endl; //40 声明中的const只能防止修改pt指向的值(这里为39)，而不能防止修改pt的值 12345int age = 39;int num = 40;int * const pt = &amp;age; pt = &amp;num; //cannot assign to variable 'pt' with const-qualified type 'int *const'cout &lt;&lt; *pt &lt;&lt; endl; 其他参数 将二维数组作为参数的函数，必须牢记，数组名被视为其地址，因此，相应的形参是一个指针，就像一维数组一样。比较难处理的 是如何正确地声明指针 123456int data[3][4] = &#123;&#123;1, 2, 3, 4&#125;, &#123;9, 8, 7, 6&#125;, &#123;2, 4, 6, 8&#125;&#125;;int total = sum(data, 3);int sum(int arr[][4], int size);data[r][c] == *(*(data + r) + c) //same thing 其中 arr[] 表示的就是一个数组的地址，然后 int arr[][4]表示二维数组，sum 只能接收 4 列的二维数组，但是行数却没有确定 将字符串作为数组 1234567891011121314151617181920unsigned int c_in_str(const char * str, char ch);int main() &#123; char m[15] = \"hello world\"; //char * str = \"hello world\"; //error const char * n = \"hello world\"; cout &lt;&lt; c_in_str(m, 'o') &lt;&lt; \" \" &lt;&lt; c_in_str(n, 'e') &lt;&lt; endl; //2 1 return 0;&#125;unsigned int c_in_str(const char * str, char ch)&#123; unsigned int count = 0; while(*str) //quit wheen *str is '\\0' &#123; if(*str == ch) count++; str++; &#125; return count;&#125; 注意 **字符串需要在 c++11 中赋值给 const char **，否则 ISO C++11 does not allow conversion from string literal to 'char *' 函数无法返回一 个字符串，但可以返回字符串的地址，这样做的效率更高 12345678910111213141516char * buildstr(char c, int n);int main() &#123; char * result = buildstr('h', 5); cout &lt;&lt; result &lt;&lt; endl; //hhhhh delete [] result; return 0;&#125;char * buildstr(char c, int n)&#123; char * temp = new char [n + 1]; temp[n] = '\\0'; while(n-- &gt; 0) temp[n] = c; return temp;&#125; 注意事项 删除的时候别忘了 delete [] result 构建指定长度的字符数组，长度为 n + 1，且最后设置 \\0 函数指针结构 12345678910111213141516171819struct polar&#123; int x; int y;&#125;;void show_polar(const polar * po)&#123; cout &lt;&lt; \"position x: \" &lt;&lt; po-&gt;x &lt;&lt; \" y: \" &lt;&lt; po-&gt;y &lt;&lt; endl;&#125;int main() &#123; polar * p = new polar; p-&gt;x = 100; p-&gt;y = 200; show_polar(p); return 0;&#125; 输出结果：position x: 100 y: 200 函数指针 函数的地址是存储其机器语言代码的内存的开始地址，可以编写将另一个函数的地 址作为参数的函数。这样第一个函数将能够找到第二个函数，并运行它 使用函数指针必须 获取函数的地址，使用函数名(后面不跟参数)即可 声明一个函数指针，声明应像函数原型那样指出有关函数的信息 使用函数指针来调用函数 12double pam(int); //prototypedouble (*pf)(int); //pf pointts to a function that take one int argumeent and that returns type double 定义函数原型，然后使用 (*func_point) 替换 func_name 即可 1234567891011121314151617181920212223int sum(int x, int y)&#123; return x + y;&#125;int del(int x, int y)&#123; return x - y;&#125;int mix(int x,int y, int (*ptr)(int y, int z))&#123; return ptr(x, y);&#125;int (*ptr)(int x, int y);ptr = sum;cout &lt;&lt; ptr(1, 2) &lt;&lt; endl;ptr = del;cout &lt;&lt; ptr(1, 2) &lt;&lt; endl;cout &lt;&lt; mix(1, 2, sum) &lt;&lt; endl;cout &lt;&lt; mix(1, 2, del) &lt;&lt; endl;cout &lt;&lt; mix(1, 2, ptr) &lt;&lt; endl; 运行结果 123453-13-1-1 表示函数指针数组 1const double * (*pa[3])(const double *, int) = &#123;f1, f2, f3&#125;; 自动类型推断只能用于单值初始化， 而不能用于初始化列表，所以 1234567const double *px = pa[0](av, 3);auto pb = pa;auto pc = &amp;pa;const double *py = (*pb[1])(av, 3);//*pd[3] //an array of 3 ponter//(*pd)[3] // a pointer to an array of 3 elementsconst double *(*(*pd)[3])(const double*, int) = &amp;pa; 1234567891011121314151617181920const double * f1(const double *ar, int n)&#123; return ar;&#125;const double *f2(const double ar[], int n)&#123; return ar + 1;&#125;const double *f3(const double ar[], int n)&#123; return ar + 2;&#125;double av[3] = &#123;1112.3, 1542.6, 2227.9&#125;;const double * (*p1)(const double *, int) = f1;auto p2 = f2;cout &lt;&lt; (*p1)(av, 3) &lt;&lt; \": \" &lt;&lt; *(*p1)(av, 3) &lt;&lt; endl; cout &lt;&lt; p2(av, 3) &lt;&lt;\": \" &lt;&lt; *p2(av, 3) &lt;&lt; endl; 输出结果 120x7ff7b6fc8440: 1112.30x7ff7b6fc8448: 1542.6 太过于复杂，可以使用 typedef 进行简化 12345typedef const double *(*p_fun)(const double ar[], int n);double av[3] = &#123;1112.3, 1542.6, 2227.9&#125;;p_fun p1 = f1;cout &lt;&lt; *p1(av, 3) &lt;&lt; endl; 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-06-指针","slug":"C++/CPP-06-指针","date":"2022-07-29T07:17:29.000Z","updated":"2023-08-10T04:04:22.978Z","comments":true,"path":"2022/07/29/C++/CPP-06-指针/","link":"","permalink":"http://xboom.github.io/2022/07/29/C++/CPP-06-%E6%8C%87%E9%92%88/","excerpt":"","text":"指针是一个变量，其存储的是值的地址，而不是值本身 1int * p_updates; * p_updates 的类型为 int，不是指针 p_updates 的类型是 int *，是指向 int 的指针 指针在地址中的存储格式 12int ducks = 12;int *birddog = &amp;ducks; ducks 的地址是 0x1000，存储的值是 12。而 birddog 的地址是 0x1006，存储的值就是 ducks 的地址 0x1000 指针变量的声明 12345678910int a = 12;int *p1, p2, *p3;p1 = &amp;a;p3 = &amp;a;*p1 = 100;p2 = 200;cout &lt;&lt; a &lt;&lt;endl;cout &lt;&lt; p1 &lt;&lt; \" \" &lt;&lt; *p1 &lt;&lt; endl;cout &lt;&lt; &amp;p2 &lt;&lt; \" \" &lt;&lt; p2 &lt;&lt; endl;cout &lt;&lt; p3 &lt;&lt; \" \" &lt;&lt; *p3 &lt;&lt; endl; 输出结果 12341000x7ff7b7a6b458 1000x7ff7b7a6b44c 2000x7ff7b7a6b458 100 从结果来看： 声明的时候 int *p1, p2;，声明的是两种类型 当两个指针指向同一个地址时，修改一个会影响另外一个 既然是地址，是不是可以直接给地址复制，答案是可以的 123int * p3;//p3 = 0x7ff7b7a6b458; //incompatible integer to pointer conversion assigning to 'int *' from 'long'p3 = (int *)0x7ff7b7a6b458 不能直接将整型复制给指针，可通过类型转换赋值给 p3，但是会段错误(Segmentation fault)，即未知的地址 new 在C语言中，可以 用库函数malloc( )来分配内存；在C中仍然可以这样做，但C还有new运算符 12345int night = 1000;int *pt = new int;*pt = 1001;cout &lt;&lt; night &lt;&lt; \" \" &lt;&lt; *pt &lt;&lt; endl;cout &lt;&lt; sizeof(pt) &lt;&lt; \" \" &lt;&lt; sizeof(*pt) &lt;&lt; endl; 运行结果 121000 10018 4 地址本身只指出了对象存储地址的开始，而没有指出其类型(使用的字节数)。而通过 变量值都存储在被称为栈(stack)的内存区域中，而new从被称为堆(heap)或自由存储区(free store)的内存区域分配内存 在 C中，值为0的指针被称为空指针(null pointer)。C确保空指针不会指向有效的数据 归还或释放(free)的内存可供程序的其他部分使用。使用 delete 时，后面要加上指向内存块的指针 12345int *pt = new int;cout &lt;&lt; pt &lt;&lt; endl;delete(pt);cout &lt;&lt; pt &lt;&lt; endl;delete(pt); //malloc: Double free of object 0x7f822cf05ce0 运行结果 120x7f93c3705ce00x7f93c3705ce0 使用 delete 仅会释放内存块，但是不会将指针的地址置为空 new 与 delete 需要配合使用，否则将发生内存泄露(memory leak) 不能重复释放已经释放的内存块 delete 不能删除一个不是 new 分配的内存 123int a = 10;int *pt = &amp;a;delete pt; delete 能删除一个空指针 12int *pt2 = nullptr;delete pt2; 动态数组 在编译时给数组分配内存被称为静态联编(static binding)，在编写程序时指定数组的长度，意味着数组是在编译时加入到程序中的 还可以在程序运行时选择数组的长度，被称为动态联编(dynamic binding)，程序将在运行时确定数组的长度，这种数组叫作动态数组(dynamic array) 在C++中，创建动态数组，只要将数组的元素类型和元素数目告诉new即可 123int *psome = new int [10];delete []psome;//delete psome; //'delete' applied to a pointer that was allocated with 'new[]'; 方括号告诉程序，应释放整个数组，而不仅仅是指针指向的元素。 注意delete和指针之间的方括号 如果使用new时，不带方括号，则使用delete时，也不应带方括号。 如果使用new时带方括号，则使用delete 时也应带方括号 123456789101112double * p1 = new double [3];p1[0] = 0.2;p1[1] = 0.3;p1[2] = 0.8;cout &lt;&lt; \"p1[1] is \" &lt;&lt; p1[1] &lt;&lt; endl;p1 = p1 + 1;cout &lt;&lt; \"Now p1[0] is \" &lt;&lt; p1[0] &lt;&lt; endl;cout &lt;&lt; \"p1[1] is \" &lt;&lt; p1[1] &lt;&lt; endl;p1 = p1 - 1;delete [] p1;//cout &lt;&lt; \"p1[0] is \" &lt;&lt; p1[0] &lt;&lt; endl;return 0; 运行结果 123p1[1] is 0.3Now p1[0] is 0.3p1[1] is 0.8 注意在释放指针的时候执行了 p1 = p1 - 1;否则会提示错误 error for object 0x7fe25e705ce8: pointer being freed was not allocated 将整数变量加 1 后，其值将增加1；但将指针变量加1后，增加的量等于它指向的类型的字节数。将指向double的指针加1后，如果系统对double使用8个字节存储，则数值将增加8；将指向short的指针加1后，如果系统对short使用2 个字节存储，则指针值将增加2 12345678double wages[3] = &#123;10000.0, 20000.0, 30000.0&#125;;short stacks[3] = &#123;3, 2, 1&#125;;double * pw = wages;short * ps = &amp;stacks[0];short * ps2 = stacks;cout &lt;&lt; *(pw + 1) &lt;&lt; endl;cout &lt;&lt; *(ps + 1) &lt;&lt; endl;cout &lt;&lt; *(ps2 + 1) &lt;&lt; endl; 运行结果 1232000022 数组名被解释为其第一个元素的地址，而对数组名应用地址运算符时，得到的是整个数组的地址： 总结 不要使用delete来释放不是new分配的内存 不要使用delete释放同一个内存块两次 如果使用new [ ]为数组分配内存，则应使用delete [ ]来释放。 如果使用new为一个实体分配内存，则应使用delete（没有方括 号）来释放。 对空指针应用delete是安全的。 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-05-结构","slug":"C++/CPP-05-结构","date":"2022-07-28T16:20:29.000Z","updated":"2023-08-10T04:04:18.466Z","comments":true,"path":"2022/07/29/C++/CPP-05-结构/","link":"","permalink":"http://xboom.github.io/2022/07/29/C++/CPP-05-%E7%BB%93%E6%9E%84/","excerpt":"","text":"结构体 结构是一种比数组更灵活的数据格式，一个结构可以存储多种类型的数据 123456struct inflatable&#123; char name[20]; float volume; double price;&#125;; 定义之后就可以使用结构体声明变量，与C不同的是，C++允许在声 明结构变量时省略关键字struct 12inflatable hat; //okstruct inflatable mainframe; //ok 与数组一样，C++11也支持将列表初始化用于结构，且等号 = 是可选的 1234567891011inflatable guest =&#123; \"Glorious Gloria\", 1.88, 29.99&#125;;inflatable local = &#123;\"Audacious Arthur\", 3.12&#125;;inflatable other&#123;&#125;;//cout &lt;&lt; guest &lt;&lt; endl; //没有与这种操作相匹配的 &lt;&lt; 运算符cout &lt;&lt; other.name &lt;&lt; endl; cout &lt;&lt; local.price &lt;&lt; endl; 运行结果 120 所以 = 是可选的 如果为设置值的情况下，默认为0值 同理在复制过程中，结构体也是值复制 123456789101112131415inflatable guest =&#123; \"Glorious Gloria\", 1.88, 29.99&#125;;inflatable other&#123;&#125;;other = guest;//other.name = \"hello wrold\"; //表达式必须是可修改的左值strcpy_s(other.name, sizeof(other.name) - 1, guest.name);other.price = 1.2;cout &lt;&lt; other.price&lt;&lt; endl;cout &lt;&lt; guest.price&lt;&lt; endl; 运行结果 121.22.99 结构体的复制也是值复制，修改并不会影响另外一个结构体对象 为什么使用 strcpy_s 而不是使用 strncpy 或者 strcpy，可以参考 https://www.yuankang.top/2023/07/28/C++/CPP-04-string类/ 中的说明 结构体变量还有其他创建方式 声明两个结构体变量 123456struct inflatable&#123; char name[20]; float volume; double price;&#125; infla1, infla2; //声明两个结构体变量 直接初始化结构体变量 123456789struct perks&#123; int key_number; char car[12];&#125; per = &#123; 7, \"Packard\"&#125;; 声明结构体变量(无法重复声明一样结构体变量，不建议使用) 12345struct // no tag&#123; int x; //2 member int y;&#125; position; //a structure variable 共用体 共用体(union)是一种数据格式，它能够存储不同的数据类型， 但只能同时存储其中的一种类型 12345678910111213141516union one4all&#123; int int_val; long long_val; double double_val;&#125;;one4all pail = &#123;&#125;;pail.bool_val = true;cout &lt;&lt; pail.bool_val &lt;&lt; endl;cout &lt;&lt; pail.int_val &lt;&lt; endl;cout &lt;&lt; pail.double_val &lt;&lt; endl;pail.double_val = 1.1;cout &lt;&lt; pail.bool_val &lt;&lt; endl;cout &lt;&lt; pail.int_val &lt;&lt; endl;cout &lt;&lt; pail.double_val &lt;&lt; endl; 运行结果 123456114.94066e-3240-17179869181.1 从运行结果可以看出 未设置共用体情况下，共用体的字段值是未知的 当设置一个字段的情况下，另外一个字段将失效 由于共用体每次只能存储一个值，因此它必须 有足够的空间来存储最大的成员，所以，共用体的长度为其最大成员的 长度 将结构体与共用体结合 12345678910111213141516struct widget&#123; char brand[20]; int type; union id &#123; long id_num; char id_char[20]; &#125; id_val;&#125;;widget prize;if(prize.type == 1) cin &gt;&gt; prize.id_val.id_num;else cin &gt;&gt; prize.id_val.id_char; 可以通过匿名共用体的方式减少名称调用，其成员将成为相同地址处的变量。每次只有一个成员是当前的成员 12345678910111213141516struct widget&#123; char brand[20]; int type; union &#123; long id_num; char id_char[20]; &#125;;&#125;;widget prize;if(prize.type == 1) cin &gt;&gt; prize.id_num;else cin &gt;&gt; prize.id_char; 共用体常用于(但并非只能用于)节省内存 枚举 C++的enum提供了另一种创建符号常量的方式，这种方式可以代替const，使用 enum的句法与使用结构相似 123456789101112131415enum spectrum&#123; red, orange, yellow, greed, blue&#125;;spectrum band;band = yellow;cout &lt;&lt; band &lt;&lt; endl;//band = 2000; //不能将 \"int\" 类型的值分配到 \"spectrum\" 类型的实体band = spectrum(1);cout &lt;&lt; band &lt;&lt; \" \"&lt;&lt; (band == orange) &lt;&lt; endl; 输出结果 1221 1 enum 可以直接通过枚举序号获取 band = spectrum(1); 枚举类型可以比较默认从 0 开始 更进一步，可以为枚举设置值 1234567891011121314enum spectrum&#123; red = 9, orange = 10, yellow = 100, greed = 4, blue = 5&#125;;spectrum band;band = red;cout &lt;&lt; band &lt;&lt; endl;band = spectrum(99);cout &lt;&lt; band &lt;&lt; \" \"&lt;&lt; (band == orange) &lt;&lt; endl; 输出结果 12999 0 可以直接使用 spectrum(99) 声明一个新的枚举类型值 枚举类型设置的值并没有顺序要求 枚举值的最大值 = 枚举量的最大值。找到大于这个最大值的、最小的2的幂，将它减去1，得到的便是取 值范围的上限。 既能顺便设置值，又能不设置值，name 1234567891011enum spectrum&#123; red, orange = 0, yellow = 1, greed = 1, blue&#125;;cout &lt;&lt; (red == orange) &lt;&lt; (orange == yellow) &lt;&lt; endl;cout &lt;&lt; red &lt;&lt; orange &lt;&lt; yellow &lt;&lt; greed &lt;&lt; blue &lt;&lt; endl; 输出结果 121000112 默认是从 0 开始的 枚举可以设置成相同的值 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-04-string类","slug":"C++/CPP-04-string类","date":"2022-07-27T16:14:29.000Z","updated":"2023-08-10T04:04:15.172Z","comments":true,"path":"2022/07/28/C++/CPP-04-string类/","link":"","permalink":"http://xboom.github.io/2022/07/28/C++/CPP-04-string%E7%B1%BB/","excerpt":"","text":"要使用string类，必须在程序中包含头文件string。string类位于名称空间std中 123456string name1 = \"hello\";char name2[6] = \"hello\";string name3 = \"hello\";//char name4[5] = \"hello\"; //\"const char [6]\" 类型的值不能用于初始化 \"char [5]\" 类型的实体if(name1 == name3) cout &lt;&lt; \"name1 == name3 \" &lt;&lt; endl;if(name2 == name3) cout &lt;&lt; \"name2 == name3\" &lt;&lt; endl; 这里有两个注意点 字符串可以通过 == 比较，且 name1 == name3 字符串可以通过 char[] 表示，且相等 字符数组后面有一个 ‘\\0’ C++11也允许将列表初始化用于C-风格字符串和string对象： 1234567891011char charr1[20];char charr2[20] = \"jaguar\";string str1;string str2 = \"panther\";cout &lt;&lt; \"Enter a kind of feline: \";cin &gt;&gt; charr1;cout &lt;&lt; \"Enter another kind of feline: \";cin &gt;&gt; str1;cout &lt;&lt; charr1 &lt;&lt; \" \" &lt;&lt; charr2 &lt;&lt; \" \" &lt;&lt; str1 &lt;&lt; \" \" &lt;&lt; str2 &lt;&lt; endl;cout &lt;&lt; charr2[2] &lt;&lt; \" \" &lt;&lt; str2[2] &lt;&lt; endl; 输出结果 1234Enter a kind of feline: helloEnter another kind of feline: worldhello jaguar world pantherg n 所以 可以使用C-风格字符串来初始化string对象。 可以使用cin来将键盘输入存储到string对象中。 可以使用cout来显示string对象。 可以使用数组表示法来访问存储在string对象中的字符。 对于一个数组，返回这个数组占的总空间，所以 sizeof(name2) 取的额是字符串name2占的总空间 头文件cstring（以前为string.h）提供了字符串操作函数 12char *strcpy(char *__dst, const char *__src); //copy __src to __dstchar *strcat(char *__s1, const char *__s2); //append contents of charr2 to char1 接着看看实际操作 1234567891011121314151617181920string str1 = \"hello\";string str2 = str1;cout &lt;&lt; str1 &lt;&lt; \" \" &lt;&lt; str2 &lt;&lt; endl;str2[0] = '0';cout &lt;&lt; str1 &lt;&lt; \" \" &lt;&lt; str2 &lt;&lt; endl;char name1[6] = \"world\";char name2[5];strcpy(name2, name1);cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; name2 &lt;&lt; endl;name2[0] = '0';cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; name2 &lt;&lt; endl;//strcpy(name1, str1); //不存在从 \"std::__1::string\" 到 \"const char *\" 的适当转换函数char name3[6] = \"xboom\";char name4[6];strcpy(name4, name3);cout &lt;&lt; name3 &lt;&lt; \" \" &lt;&lt; name3 &lt;&lt; endl;name3[0] = '0';cout &lt;&lt; name4 &lt;&lt; \" \" &lt;&lt; name3 &lt;&lt; endl; 运行结果 123456hello hellohello 0ello world 0orldxboom xboomxboom 0boom 可以得出几点： strcpy 是复制，不会影响原来的字符串 string 类可以直接使用 = 复制，并且也不会影响原来的 string 转换为 const char *，因为 const char * 是一个指向字符常量的指针，类型不匹配 使用 strcpy 注意目标空间大于等于源目标，否则导致目标缓冲区异常，导致崩溃或未知错误 计算字符长度也是不一样的 1234char name1[6] = \"world\";char name2[6] = &#123;'h','e','l','l','o','\\0'&#125;;string name3 = \"hello\";cout &lt;&lt; strlen(name1) &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt;\" \"&lt;&lt; name3.size() &lt;&lt; endl; 输出结果 15 5 5 1234567char name[20];string str;string str2;cin.getline(name, 20);getline(cin, str);cin &gt;&gt; str2;cout &lt;&lt; name &lt;&lt; \" \" &lt;&lt; str &lt;&lt; \" \" &lt;&lt; str2 &lt;&lt; endl; 运行结果 1234hello world #输入hello world #输入hello world #输入hello world hello world hello getline() 是 C++ 标准库中的一个自由函数（不是成员函数），用于从标准输入流 cin 中读取一行文本，并将其存储到 std::string 对象 str 中 cin.getline() 是 istream 类的成员函数，用于从标准输入流 cin 中读取一行文本，并将其存储到字符数组 name 中 C++11 新增了原始字符串，字符表示的就是自己 12cout &lt;&lt; R\"(Jim \"King\" Tutt users \"\\n\" instead of endl.)\" &lt;&lt; '\\n';cout &lt;&lt; R\"+*(\"(Who wouldn't?)\", she whispered.)+*\" &lt;&lt; endl; 输入结果 12Jim \"King\" Tutt users \"\\n\" instead of endl.\"(Who wouldn't?)\", she whispered. 原始字符串将 （和）用作定界符，并使用前缀 R 来标识原始字符串 也可以使用 R&quot;+*( 表示原始字符串的开头的时候，必须使用 )+*&quot; 标识原始字符串的结尾 最后看一波 strcpy 的源码实现 123456789101112char* strcpy(char* dest, const char* src) &#123; char* tmp = dest; // 保存目标字符串的起始地址，用于返回复制后的目标字符串的指针 while (*src != '\\0') &#123; *dest = *src; // 复制源字符串的字符到目标字符串 ++dest; ++src; &#125; *dest = '\\0'; // 在目标字符串的末尾添加 null 终止符 return tmp; // 返回复制后的目标字符串的指针&#125; src 为指针常量不会被修改 定义一个指针 tmp 并将其指向目标字符串 dest 的起始地址。这样在复制结束后，可以通过 tmp 指针找到复制后的目标字符串的起始地址，并返回该指针 实际的使用中会发现 strcpy 几乎不是用，原因是 strcpy当一直遍历到 src 结束才会停止写入 dest，当 src 长度大于 dest 的时候，会导致溢出 就提到了另外函数 strncpy 123456789101112char *strncpy(char *dest, const char *src, size_t count)&#123; char *tmp = dest; while (count) &#123; //仅复制指定大小的长度 if ((*tmp = *src) != 0) //如果没有移动到字符串末尾‘\\0’ src++; tmp++; count--; &#125; return dest; &#125; 这里有几个关键的问题 *temp != 0 进行判断是否结束，不是 \\0 吗？ '0'代表ASCII值为48的数字零字符，而'\\0'代表空字符(null terminator)，其ASCII值为0 如果 src_len &gt;= count 会怎样？ src 的全部内容都回被copy 到 dest 中，但是 dest 末尾并不是\\0 如果 src_len &lt; count ， tmp++ 直到 count == 0，也就是说超过 count 的部分不变 1234char arr[6] = \"world\";char dest[12] = \"hello world\";strncpy(dest, arr, 8);cout &lt;&lt; dest &lt;&lt; endl; 输出结果 world, 是因为将'\\0' 也复制进去了，那是不是当超过 src_len &lt; count 就可以结束了 在使用 strncpy 又会有一个新的提示 This function or variable may be unsafe. Consider using strncpy_s instead. To disable deprecation, use _CRT_SECURE_NO_WARNINGS. See online help for details 下面是 strcpy_s 的实现 12345678910111213141516171819202122232425262728_FUNC_PROLOGUEerrno_t __cdecl _FUNC_NAME(_CHAR *_DEST, size_t _SIZE, const _CHAR *_SRC)&#123; _CHAR *p; size_t available; _VALIDATE_STRING(_DEST, _SIZE); //验证目标字符串 _DEST 的合法性 //验证源字符串 _SRC 的合法性。它可能会检查 _SRC 是否为有效的指针，并确保 _DEST 和 _SIZE 参数在一定条件下都是合法的 _VALIDATE_POINTER_RESET_STRING(_SRC, _DEST, _SIZE); p = _DEST; available = _SIZE; //_SRC 到头或者数量达到目标就停止复制 while ((*p++ = *_SRC++) != 0 &amp;&amp; --available &gt; 0) &#123; &#125; //达到复制长度，说明 _DEST 还没存入'\\0', 即 src_len &gt;= cout 的情况 if (available == 0) &#123; //将_DEST 全部重置为 '\\0' _RESET_STRING(_DEST, _SIZE); //返回_DEST 长度不够存储_SIZE 的错误 _RETURN_BUFFER_TOO_SMALL(_DEST, _SIZE); &#125; //src_len &lt; count 的情况 //否则天车工剩下的部分为 '\\0' _FILL_STRING(_DEST, _SIZE, _SIZE - available + 1); _RETURN_NO_ERROR;&#125; 也就是 strcpy_s 除了字符校验还解决了当 dest_len &lt; cout 时候的溢出 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-03-数组","slug":"C++/CPP-03-数组","date":"2022-07-26T17:05:29.000Z","updated":"2023-08-10T04:04:11.798Z","comments":true,"path":"2022/07/27/C++/CPP-03-数组/","link":"","permalink":"http://xboom.github.io/2022/07/27/C++/CPP-03-%E6%95%B0%E7%BB%84/","excerpt":"","text":"数组 声明数组的通用格式如下 存储在每个元素中的值的类型 数组名 数组中的元素数 1typeName arrayName[arraySize]; 包含了每个元素的值类型、数组名、数组中的元素个数，arraySize 必须是整形常量（如10）或 const值，也可以是常量表达式（如 8 * sizeof(int)），即其中所有的值在编译时都是已知的 数组的初始化有以下几种 1234int cards[4] = &#123;1, 2, 3, 4&#125;;int hand[4];hand[4] = &#123;5, 6, 7, 8&#125;;hand = cards; 另外还有特殊的初始化 1234float hotelTips[5] = &#123;5.0, 2.5&#125;; //只初始化hotelTips的前两个元素long totals[500] = &#123;0&#125;; //全部是0long nums[500] = &#123;1&#125;; //第一个是1，其他都是0long sum[500] = &#123;&#125;; //??? 如果只对数组的一部分进行初始化，则编译器将把其他元素设置为 0 如果不指定数组的元素个数，编译器将计算元素个数 12short things[] = &#123;1, 5, 3, 8&#125;; //编译器将使things数组包含4个元素int num_elements = sizeof(things)/sizeof(short); C++11将使用大括号的初始化(列表初始化)作为一 种通用初始化方式，可用于所有类型，C++11 中的列表初始化新增了一些功能 初始化数组时，可省略等号 = 可不在大括号内包含任何东西，这将把所有元素都设置为 列表初始化禁止缩窄转换 123456double earnings[4] &#123;1.2e4, 1.6e4, 1.1e4, 1.7e4&#125;;unsigned int cunts[10] = &#123;&#125;; //all elements set to 0float balances[100] &#123;&#125;; //all elements set to 0long plifs[] = &#123;25, 92, 3.0&#125;; //not allowedchar slifs[4] &#123;'h', 'i', 1232311, '\\0'&#125;; //not allowedchar tlifs[4] &#123;'h', 'i', 112, '\\0'&#125;; //allowed C++标准模板库STL提供了一种数组替代品—模板类vector，而 C++11新增了模板类array 字符数组 字符串是存储在内存的连续字节中的一系列字符，意味着可以将字符串存储在char数组中，其中每个字符都位于自己的数组元素中，但注意以 以空字符（null character）结尾！！！！ 123char dog[10] = &#123;'h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd'&#125;; //not a stringchar cat[10] = &#123;'h', 'e', 'l', 'l', 'o', 'b', 'a', 'b', 'y', '\\0'&#125;; //a stringchar cat[10] = \"hellobaby\"; //等价第 2 行 所以字符常量 'S' 与字符串常量 &quot;S&quot; 是不等价的，&quot;S&quot; 表示的是两个字符（字符S和\\0）组成的字 符串。更糟糕的是，&quot;S&quot;实际上表示的是字符串所在的内存地址，以下是非法的 1char shirt_size = \"S\"; //illegal type mismatch 另外，跟数组的初始化类似，字符数组也会为没有初始化的自动设置值 为了在数组中使用字符串，可以有两种办法 将数组初始化为字符串常量 char cat[10] = &quot;hellobaby&quot;; 将键盘或文件输入读入到数组中，如下 123456const int Size = 15;char name1[Size];char name2[Size] = \"Hello World!\";cin &gt;&gt; name1;cout &lt;&lt; \"name1 len \" &lt;&lt; strlen(name1) &lt;&lt; \" size \" &lt;&lt; sizeof(name1) &lt;&lt; endl;cout &lt;&lt; name1 &lt;&lt; \" | \" &lt;&lt; name2; 输出结果 123nihaoname1 len 5 size 15nihao | Hello World! 可以看到 strlen 返回的是字符串的长度，但是 sizeof 返回的数组的长度。另外 strlen 只计算可见的字符，而不把空字符计算在内。 如果输入长度超过字符数组长度怎么办？ 1234567const int Size = 5;char name1[Size];char name2[Size];cin &gt;&gt; name1;cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; strlen(name1) &lt;&lt; endl;cin &gt;&gt; name2;cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl; 运行结果 123ni hello world #输入ni 2hello 5 这里有两个注意的地方 name1 是 ni 而不是 ni he ,这是因为cin 使用空白(空格、制表符和换行符)来确定字符串的结束位置 没有执行(用户输入) cin &gt;&gt; name2;，cin 把 hello 放入 name1 中并添加空字符。然后输出队列中剩余的 world 直接输入到 name2 中，所以感觉不到输入第二次 要是就是想将 ni hello world 保存到一个字符串中怎么办？ getline() 函数读取整行，通过换行符来确定输入结尾 1234567const int Size = 5;char name1[Size];char name2[Size];cin.getline(name1, Size);cin.getline(name2, Size);cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; strlen(name1) &lt;&lt; endl;cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl; 输出结果 123ni hello world #输入ni h 4 0 这里不一样的是 输入超过字符数组长度的会被自动截断 如果输入超过长度，那么后面的会等于直接输入'\\0' get()与 getline() 不同的是，get() 并不再读取并丢弃换行符，而是将其留在输入队列中 1234567const int Size = 5;char name1[Size];char name2[Size];cin.get(name1, Size);cin.get(name2, Size);cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; strlen(name1) &lt;&lt; endl;cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl; 输出结果 123ni hello world #输入ni h 4ello 4 可以将上述逻辑合并到一起 123456const int Size = 20;char name1[Size];char name2[Size];cin.getline(name1, Size).getline(name2, Size);cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; strlen(name1) &lt;&lt; endl;cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl; 运行结果 1234hello world #输入hello #输入hello world 11hello 5 当读取空行后将设置失效位 (failbit)。这意味着接下来的输入将被阻断，但可以用下面的命令来cin.clear()恢复输入，从上次结尾的地方继续读取 12345678const int Size = 5;char name1[Size];char name2[Size];cin.getline(name1, Size);cin.clear();cin.getline(name2, Size);cout &lt;&lt; name1 &lt;&lt; \" \" &lt;&lt; strlen(name1) &lt;&lt; endl;cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl; 运行结果 123ni hello world #输入ni h 4ello 4 如果手动设置字符数组指定位置为空字符，那么将导致字符串被截断 12345const int Size = 15;char name2[Size] = \"Hello World!\";cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl;name2[4] = '\\0';cout &lt;&lt; name2 &lt;&lt; \" \" &lt;&lt; strlen(name2) &lt;&lt; endl; 输出结果 12Hello World! 12Hell 4 最后一种输入的异常是混合输入 123456789cout &lt;&lt; \"Waht year was your house built?\\n\";int year;cin &gt;&gt; year;cout &lt;&lt; \"Wat is its street address?\\n\";char address[80];cin.getline(address, 80);cout &lt;&lt; \"Year build: \" &lt;&lt; year &lt;&lt; endl;cout &lt;&lt; \"Address: \" &lt;&lt; address &lt;&lt; endl;cout &lt;&lt; \"Done! \\n\"; 运行结果是 123456Waht year was your house built?1966 #输入Wat is its street address?Year build: 1966Address: Done! 用户根本没有输入地址的机会。问题在于，当cin读取年份，将回车键生成的换行符留在了输入队列中。后面的cin.getline( )看到换行符后，将认为是一个空行，并将一个空字符串赋给address数组。 解决办法是，在读取地址之前先读取并丢弃换行符。可以通过 空参数的cin.get() 或者 cin.clear() 解决 数组的替代 12vector&lt;typeName&gt; vt(n_elem);array&lt;typeName, n_elem&gt; arr; 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-02-数据处理","slug":"C++/CPP-02-数据处理","date":"2022-07-26T15:58:29.000Z","updated":"2023-08-10T04:04:08.528Z","comments":true,"path":"2022/07/26/C++/CPP-02-数据处理/","link":"","permalink":"http://xboom.github.io/2022/07/26/C++/CPP-02-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","excerpt":"","text":"简单变量 12int num;num = 5; 程序将找到一块能够存储整数的内存，将该内存单元标记为num，并将5复制到该内存单元中；然后，可在程序中使用num来访问该内存单元 C++还有另一种C语言没有的初始化语法 1int num(5); 这些变量都是声明了类型了，计算机如何知道没有声明类型的常量，比如 1cout &lt;&lt; \"Year = \" &lt;&lt; 1492 &lt;&lt; \"\\n\"; 除非有理由存储为其他类型，否则 C++ 将整型常量存储为 int 类型，这里的其他理由是指 使用了特殊的后缀来表示特定的类型 太大，不能存储为 int const 首先来看 #define 符号常量–预处理方式 1#define INT_MAX 32767 在C++编译过程中，首先将源代码传递给预处理器。在这里，#define和#include一样，是一个预处理器编译指令。告诉预处理器：在程序中查找INT_MAX，并将所有的 INT_MAX都替换为32767 const比 #define 好。 const 能够明确指定类型。 可以使用C++的作用域规则将定义限制在特定的函数或文件中 可以将const用于更复杂的类型，比如数组或者结构 d.dddE+n指的是将小数点向右移n位 d.dddE～n指的是将小数点向左移n位，8.33E～4 表示8.33/10^4，即0.000833 类型转换 转换 潜在的问题 将较大的浮点类型转换为娇小的浮点类型，如将 double 转换为 float 精度(有效数位)降低，值可能超出目标类型的 取值范围，在这种情况下，结果将是不确定的 将浮点类型转换为整型 小数部分丢失，原来的值可能超出目标类型的取 值范围，在这种情况下，结果将是不确定的 将较大的整型转换为较小的整 型，如将long转换为short 原来的值可能超出目标类型的取值范围，通常只 复制右边的字节 将 0 赋给bool变量时，将被转换为false；而非零值将被转换为true。 C++11制定了校验关系 如果有一个操作数的类型是long double，则将另一个操作数转换为long double 如果有一个操作数的类型是long double，则将另一个操作数转换为long double 否则，如果有一个操作数的类型是float，则将另一个操作数转换为float 否则，说明操作数都是整型，因此执行整型提升 在这种情况下，如果两个操作数都是有符号或无符号的，且其中一个操作数的级别比另一个低，则转换为级别高的类型。 如果一个操作数为有符号的，另一个操作数为无符号的，且 无符号操作数的级别比有符号操作数高，则将有符号操作数转换为无符 号操作数所属的类型 否则，如果有符号类型可表示无符号类型的所有可能取值， 则将无符号操作数转换为有符号操作数所属的类型。 否则，将两个操作数都转换为有符号类型的无符号版本。 也可以{ }方式初始化时进行的转换 123int ch(77);int cm = &#123;78&#125;;cout &lt;&lt; ch &lt;&lt; \" \"&lt;&lt; cm; //77 78 强制类型转换不会修改变量本身，而是创建一个新的、指定类型的值，可以在表达式中使用这个值，常见的转换表达式如下 12(typeName) vartypeName (var) C++还引入了4个强制类型转换运算符 123456int ch(77);char cm = (char)ch;bool flag = bool(ch);bool cf = static_cast&lt;bool&gt;(ch); cout &lt;&lt; ch &lt;&lt; \" \"&lt;&lt; cm &lt;&lt; \" \" &lt;&lt; flag &lt;&lt; \" \" &lt;&lt; cf; //77 M 1 1 auto C++11新增了一个工具，让编译器能够根据初始值的类型推断变量的类型。为此，它重新定义了auto的含义 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;typeinfo&gt;#include &lt;string&gt;using namespace std;template&lt;typename T&gt; string type_name() &#123; string name = typeid(T).name(); return name;&#125;int main() &#123; int num = 42; double pi = 3.14159; char letter = 'A'; bool is_true = true; auto name = 1e6+7; cout &lt;&lt; \"Type of num: \" &lt;&lt; type_name&lt;decltype(num)&gt;() &lt;&lt; endl; cout &lt;&lt; \"Type of pi: \" &lt;&lt; type_name&lt;decltype(pi)&gt;() &lt;&lt; endl; cout &lt;&lt; \"Type of letter: \" &lt;&lt; type_name&lt;decltype(letter)&gt;() &lt;&lt; endl; cout &lt;&lt; \"Type of is_true: \" &lt;&lt; type_name&lt;decltype(is_true)&gt;() &lt;&lt; endl; cout &lt;&lt; \"Type of name: \" &lt;&lt; type_name&lt;decltype(name)&gt;() &lt;&lt; endl; return 0;&#125; 输出结果 12345Type of num: iType of pi: dType of letter: cType of is_true: bType of name: d 总结 C++的基本类型分为两组： 由存储为整数的值组成，整型之间通过存储值时使用的内存量及有无符号来区分，从最小到最大依次是：bool、char、signed char、 unsigned char、short、unsigned short、int、unsigned int、long、unsigned long以及 由存储为浮点格式的值组成。分别是float、double和long double。 C11新增了long long和unsigned long long，还有一种wchar_t 类型，它在这个序列中的位置取决于实现。C11新增了类型char16_t 和char32_t，分别存储16和32位的字符编码 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"CPP-01-HelloWorld","slug":"C++/CPP-01-HelloWorld","date":"2022-07-26T14:58:29.000Z","updated":"2023-08-10T04:04:05.427Z","comments":true,"path":"2022/07/26/C++/CPP-01-HelloWorld/","link":"","permalink":"http://xboom.github.io/2022/07/26/C++/CPP-01-HelloWorld/","excerpt":"","text":"最近由于部门调整，需要改变编码语言，这里重新学习一下《C++ Primer Plus》，建立了一个 Demo 方便自己编译代码 https://github.com/XBoom/CPP-Learn.git 来看第一个 C++ Demo 12345678910#include&lt;iostream&gt;int main()&#123; using namespace std; cout &lt;&lt; \"Come up and C++ me some time.\"; cout &lt;&lt; endl; cout &lt;&lt; \"You won't regret it!\"; return 0;&#125; 头文件 #include 编译指令让预处理器将 iostream 文件的内容添加到程序中，取代程序中的代码行 #include &lt;iostream&gt;。原始文件没有被修改，只是将源代码文件和iostream组合成一个复合文件。在编译后使用 头文件类型 约定 示例 说明 C++ 旧式风格 以.h结尾 iostream.h c++程序可以使用 C 旧式风格 以.h结尾 math.h c、c++程序可以使用 C++ 新式风格 没有拓展名 iostream C++ 程序可以使用，使用 namespace std 转换后的 C 加上前缀c，没有扩展名 cmath C++ 程序可以使用，可以使用不是C的特 性，如namespace std 命名空间 如果使用iostream，而不是iostream.h，则应使用下面的名称空间编译指令来使iostream中的定义对程序可用，叫做 using编译指令 1using namespace std; 名称空间支持是一项C++特性，旨在让编写大型程序包含多个项目现有的代码组合起来的程序时更容易 但是如果多个项目包含相同的函数，那么就可以通过指定命名空间调用。 按照这种方式，类、函数和变量便是C++编译器的标准组件，它们都被放置在名称空间std中。仅当头文件没有扩展名h时，情况才是如此。在iostream中定义的用于输出的cout变量实际上是 std::cout，而endl实际上是std::endl。因此，可以省略编译指令using编码 123std::cout &lt;&lt; \"Come up and C++ me some time.\";std::cout &lt;&lt; endl;std::cout &lt;&lt; \"You won't regret it!\"; 而让程序能够访问命名空间的方法有很多中 将 using namespace std；放在函数定义之前，文件中所有的函数都能够使用名称空间std中所有的元素 将 using namespace std；放在特定函数定义中，该函数能够使命名约定用名称空间std中的所有元素 在特定的函数中使用类似 using std::cout;这样的编译指令，而不是 using namespace std;，让该函数能够使用指定的元素，如cout 完全不使用编译指令using，而在需要使用名称空间std中的元素 时，使用前缀std::，如下所示： cout &amp;&amp; cin 1cout &lt;&lt; \"Come up and C++ me some time.\"; &lt;&lt; 符号表示该语句将把这个字符串发送给 cout；该符号指出了信息流动的路径，是一个预定义的对象，知道如何显示字符串、数字和单个字符 插入运算符 &lt;&lt; 看上去就像按位左移运算符 &lt;&lt; ，这是一个运算符重载的例子，通过重载，同一个运算符将有不同的含义 12int num;cin &gt;&gt; num; 输入时，cin 使用 &gt;&gt; 运算符从输入流中抽取字符 12345678910111213#include &lt;iostream&gt;using namespace std;int main() &#123; char ch; char cm = 'M'; char cn = 77; cout &lt;&lt; \"Enteer a character: \" &lt;&lt; endl; cin &gt;&gt; ch; cout.put('!'); cout.put('.'); cout &lt;&lt; \" ch \" &lt;&lt; ch &lt;&lt; \" cm \" &lt;&lt; cm &lt;&lt;\" cn \" &lt;&lt; cn &lt;&lt; endl;&#125; 上面的输出结果 123Enteer a character: M!. ch M cm M cn M 其中 cout.put(char c)的作用是存放一个 char 变量 函数原型 函数原型之于函数就像变量声明之于变量 — 指出涉及的类型 1double sqrt(double); //function prototype 对于C库中 的每个函数，都在一个或多个头文件中提供了其原型，C 和 C 将库函数的这两项特性 (原型和定义)分开了。库文件中包含了函数的编译代码，而头文件中则包含了原型 参考链接 《C++ Primer Plus》","categories":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"}]},{"title":"算法-A071-前缀异或和","slug":"Algorithm/算法-A071-前缀异或和","date":"2022-07-24T06:48:35.000Z","updated":"2023-09-28T16:21:38.681Z","comments":true,"path":"2022/07/24/Algorithm/算法-A071-前缀异或和/","link":"","permalink":"http://xboom.github.io/2022/07/24/Algorithm/%E7%AE%97%E6%B3%95-A071-%E5%89%8D%E7%BC%80%E5%BC%82%E6%88%96%E5%92%8C/","excerpt":"","text":"前缀异或和问题是通过 前缀和 + 状态压缩 + 哈希表来处理连续 xxx 问题，发现是有规律的 通过前缀异或和获取每个位置的状态 使用哈希表记录每个状态第一次出现的位置 如果状态小于一定数量，可以通过状态压缩(BitMask)记录 问题 问题1：统计美丽子数组数目 12345678910111213141516171819class Solution &#123;public: long long beautifulSubarrays(vector&lt;int&gt;&amp; nums) &#123; long long res = 0; //计算前缀异或和！！！ vector&lt;int&gt; temp(nums.size() + 1); for(int i = 0; i &lt; nums.size(); i++) &#123; temp[i + 1] = temp[i] ^ nums[i]; //temp[i] 表示包含了 nums[i] 的异或和 &#125; //计算数据对总数！！！ unordered_map&lt;int, int&gt;dict; for(int x : temp) &#123; //printf(\"%d %d \\n\", x, dict[x]); res += dict[x]++; //这样算，每次 加1 类似 1 + 2 + 3 + n -&gt; 计算 n个数字一共有多少个数据对 &#125; return res; &#125;&#125;; 这里有两个知识点： 子数组的异或和等于前缀和的异或和 AB ^ A = B 注意 res += dict[mod]++;,可以理解为动态计算 1 + 2 + 3 + .... + n 问题2：和可被 K 整除的子数组 12345678910111213141516class Solution &#123;public: int subarraysDivByK(vector&lt;int&gt;&amp; nums, int k) &#123; unordered_map&lt;int, int&gt; dict; dict[0] = 1; int res = 0; int s = 0; for(int i = 0; i &lt; nums.size(); i++) &#123; s += nums[i]; //当被除数为负数时取模结果为负数，需要纠正(保证取余一定为正) int mod = (s % k + k) % k; res += dict[mod]++; &#125; return res; &#125;&#125;; 这里的知识点表示： C++取余是向0取整，所以就有 1234567/4= 17/(-4)= -17%4= 37%(-4)= 3(-7)/4= -1(-7)%4= -3 一个负数取余可能出现模为负数的情况，可以通过 mod = (s % k + k) % k 解决 问题3：找出最长的超赞子字符串 123456789101112131415161718192021222324252627class Solution &#123;public: int longestAwesome(string s) &#123; int temp = 0; unordered_map&lt;int, int&gt; dict; int l = 0; dict[0] = -1; for(int i = 0; i &lt; s.size(); i++) &#123; temp ^= (1 &lt;&lt; (s[i] - '0')); //status1 与 status2 两个状态相同，那么所有的都出现了偶数次 if(dict.count(temp) != 0) &#123; l = max(l, i - dict[temp]); &#125; else &#123; dict[temp] = i; &#125; //status1 与 status2 只差一位不同，找出这一位 for(int k = 0; k &lt; 10; k++) &#123; if(dict.count(temp ^ (1 &lt;&lt; k))) &#123; l = max(l, i - dict[temp ^ (1 &lt;&lt; k)]); &#125; &#125; &#125; return l; &#125;&#125;; 注意点： 为什么在查找 只差一位的情况时没有进行存储，因为当前前缀异或和的值是 temp 总结 遇到奇偶个数校验，想到 XOR 遇到有限的参数(小于20个)表状态， 想到状态压缩 （bitmask） 遇到求最长的连续子串使得和为k 想到 前缀和 + 哈希表 记录第一次出现某一状态的位置。 顺序练习 两数之和 和为K的子数组 和可被K整除的子数组 使数组和能被 P 整除 连续的子数组和 连续数组 面试题 17.05. 字母与数字 最美子字符串的数目 和相同的二元子数组 每个元音包含偶数次的最长子字符串 找出最长的超赞子字符串","categories":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/categories/Algorithms/"}],"tags":[{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/tags/Algorithms/"}]},{"title":"有趣的算法-老鼠毒药问题","slug":"Interesting Algorithm/有趣的算法-老鼠毒药","date":"2022-07-21T02:00:35.000Z","updated":"2022-07-21T02:00:35.000Z","comments":true,"path":"2022/07/21/Interesting Algorithm/有趣的算法-老鼠毒药/","link":"","permalink":"http://xboom.github.io/2022/07/21/Interesting%20Algorithm/%E6%9C%89%E8%B6%A3%E7%9A%84%E7%AE%97%E6%B3%95-%E8%80%81%E9%BC%A0%E6%AF%92%E8%8D%AF/","excerpt":"","text":"问题描述 有 1000 个一模一样的瓶子，其中有 999 瓶是普通的水，有一瓶是毒药。任何喝下毒药的生物都会在一星期之后死亡。现在，你只有 10 只小白鼠和一星期的时间，如何检验出哪个瓶子里有毒药？ 问题分析 这个题目需要面对几个问题 因为毒药会在一个星期后生效，而只有一星期的时间，那么只能让老鼠多喝几瓶最后根据老鼠中毒情况判断出哪一瓶有毒 要喝完这一千瓶，那么每只老鼠要喝很多，如果老鼠们喝的不交叉，那么即使老鼠中毒，也无法区分它喝的哪一瓶有毒 如果它们喝的有交叉，就需要每一瓶都存在部分老鼠喝了部分老鼠没有喝，且每一瓶喝与不喝的老鼠都不一样 使用二进制思想(当时就是没想到) 10个老鼠相当于10位的二进制位，可以表达的最大数量为1024 老鼠 1 2 3 4 5 6 7 8 9 10 0 0 0 0 0 0 0 0 0 0 如果给瓶子从1开始编号到1000，那么根据数字的二进制位，如果位数上是1的对应的老鼠就要喝掉这一瓶。根据老鼠的生存情况，就可以推断出哪一瓶有毒，假如：500是有毒的，那么它的二进制为 0111110100那么， 老鼠 1 2 3 4 5 6 7 8 9 10 500 0 1 1 1 1 1 0 1 0 0 508 0 1 1 1 1 1 1 1 0 0 510 0 1 1 1 1 1 1 1 1 0 512 0 1 1 1 1 1 1 1 1 1 也就是说：第2、3、4、5、6、8只老鼠会死掉。而喝了508，510、512的老鼠都没有事情 参考文档 https://blog.csdn.net/qq_43827595/article/details/104154716","categories":[{"name":"Interesting Algorithm","slug":"Interesting-Algorithm","permalink":"http://xboom.github.io/categories/Interesting-Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"有趣的算法-赛马问题","slug":"Interesting Algorithm/有趣的算法-赛马问题","date":"2022-07-09T12:06:06.000Z","updated":"2022-07-09T12:06:06.000Z","comments":true,"path":"2022/07/09/Interesting Algorithm/有趣的算法-赛马问题/","link":"","permalink":"http://xboom.github.io/2022/07/09/Interesting%20Algorithm/%E6%9C%89%E8%B6%A3%E7%9A%84%E7%AE%97%E6%B3%95-%E8%B5%9B%E9%A9%AC%E9%97%AE%E9%A2%98/","excerpt":"","text":"问题描述 64匹马，8个跑道，问最少比赛多少场，可以选出跑得最快的4匹马。每场比赛每个跑道只允许一匹马，且不存在并列情形 问题分析 第一步：先让马儿跑起来，首先将马儿分批次赛跑，一共需要进行8次赛跑。假设结果如下： 分组 第一名 第二名 第三名 第四名 第五名 第六名 第七名 第八名 A A1 A2 A3 A4 A5 A6 A7 A8 B B1 B2 B3 B4 B5 B6 B7 B8 C C1 C2 C3 C4 C5 C6 C7 C8 D D1 D2 D3 D4 D5 D6 D7 D8 E E1 E2 E3 E4 E5 E6 E7 E8 F F1 F2 F3 F4 F5 F6 F7 F8 G G1 G2 G3 G4 G5 G6 G7 G8 H H1 H2 H3 H4 H5 H6 H7 H8 可直接排除各组最后四名赛马，剩余64-4*8=32匹赛马待定 第二步：将每一组中的第一名进行赛跑(如果每一组选多个参加赛马，那样就存在重复比赛)，需要进行1次赛跑。假设结果如下： 分组 第一名 第二名 第三名 第四名 第五名 第六名 第七名 第八名 1 A1 B1 C1 D1 E1 F1 G1 H2 可直接排除各组最后四名赛马，也就是后四组全部淘汰，剩余 32 - 4 * 4 = 16，其中第一名已经知道就是A1 需要注意的是：这里还可以继续排除 因为在头名争夺中 D1只能排第四，所以D1最快也是第四，D组剩余被淘汰 同理C组最多只有2名在前4 同理B组最多只有1名在前4 分组 第一名 第二名 第三名 第四名 A A1 A2 A3 A4 B B1 B2 B3 B4 C C1 C2 C3 C4 D D1 D2 D3 D4 所以剩余需要确认的数量为 16 - 1 - 3 - 2 - 1 = 9。 第三步：剩余的9匹赛马中需要选出8匹马再次进行一次赛马 这里是否有一匹特殊的马，不需要参与赛跑进行这一次赛马就能得出结果？ 排除A组中的3匹马中的一匹，那么除非B1都输或者都赢，否则B1以及后面的排名不确定。也就是排除前面的对后面影响较大 排除D1/C2，那么可能无法确认D1与C2谁是第四 排除C1与排除其他一样，可能无法确认自己和它身后的排名 所以，这里最好从D1与C2中排除一个进行赛跑，如果这一轮得出结果，那么就不用跑。如果没有得出结论就再跑一次 因为所有赛马的第一名已经确认是第一，所以剩下的比赛就是确认 2 - 4 名次，C1要是所有赛马的前四名，这次必须跑入前三。 第一种可能：C1第三名或者第三名之后，那么比赛结束，一共经过了 8 + 1 + 1 = 10 赛出前四名 第二种可能：C1排在第二名，也就是说 C2 和 D1 无法确认谁是第四个，那么就需要加赛一场。排除前三，剩余的马再比一场。一共经过了 8 + 1 + 1 + 1 = 11 参考文档 https://zhuanlan.zhihu.com/p/103572219","categories":[{"name":"Interesting Algorithm","slug":"Interesting-Algorithm","permalink":"http://xboom.github.io/categories/Interesting-Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"分布式事务-07-通知型事务","slug":"Distributed Transaction/分布式事务-07-通知型事务","date":"2022-06-07T14:58:29.000Z","updated":"2023-06-26T06:56:56.001Z","comments":true,"path":"2022/06/07/Distributed Transaction/分布式事务-07-通知型事务/","link":"","permalink":"http://xboom.github.io/2022/06/07/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-07-%E9%80%9A%E7%9F%A5%E5%9E%8B%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"刚性事务属于 CAP 理论中的 CP 组合，会有性能上限，无法满足高并发场景的需求。基于 BASE 理论，柔性事务方案被提出用于保证事务数据的最终一致性。柔性事务本质是对 XA 协议的妥协，它通过降低强一致性要求，从而降低数据库资源锁定时间，提升可用性，允许有中间状态，要求最终一致性，也就是 AP 组合。柔性事务分为通知型和补偿型： 通知型事务都是异步的，包含有：可靠消息、最大努力通知两种 补偿型事务都是同步的，包含有：AT、TCC、Saga 可靠消息 可靠消息方案是指当事务发起方(消息发送者)执行完成本地事务后并发出一条消息，事务参与方(消息接收者)一定能够接收消息并处理事务成功 此方案强调的是一旦消息发给事务参与方，则最终事务要达到一致。这其中有两个关键问题： 本地事务与消息发送的原子性问题：若事务发起方在本地事务执行成功，则消息必须发出(可以是立即发送，也可以是异步发送)，否则就无消息 事务参与方接收消息的可靠性问题 参与方必须能够从 MQ 接收到消息，如果接收消息失败可以重复接收消息 参与方要解决消息重复消费的问题(消费处理的幂等性) 可靠消息方案适合执行周期长且实时性要求不高的场景。引入消息机制后，原先同步的事务操作变为基于消息写/读的异步操作，避免了同步阻塞，也实现了服务间的解耦。一般有基于本地消息表和基于消息中间件这两种实现式 本地消息表 本地消息表核心思路是：将分布式事务拆分成本地事务进行处理 优点： 从应用设计开发的角度实现了消息数据的可靠性，消息数据的可靠性不依赖于消息中间件，弱化了对 MQ 中间件特性的依赖，方案轻量，容易实现。 缺点： 需设计 DB 消息表，同时还需要一个后台任务，不断扫描本地消息。导致消息的处理和业务逻辑耦合额外增加业务方的负担 事务消息 为了解决本地消息表与业务耦合的问题，利用消息队列的事务消息的能力，可理解为将本地消息表移动到了 MQ 内部 事务消息发送步骤如下： 发送方将半事务消息发送至 RocketMQ MQ 将消息持久化成功之后，向发送方返回 ACK 确认消息已经发送成功，此时消息为半事务消息 发送方开始执行本地事务逻辑 发送方根据本地事务执行结果向服务端提交二次确认（Commit 或是 Rollback），服务端收到 Commit 状态则将半事务消息标记为可投递，订阅方最终将收到该消息；服务端收到 Rollback 状态则删除半事务消息，订阅方将不会接受该消息 事务消息回查步骤如下： 在断网或者是应用重启的特殊情况下，上述发送步骤的步骤 4 提交的二次确认最终未到达服务端，经过固定时间后服务端将对该消息发起消息回查 发送方收到消息回查后，需要检查对应消息的本地事务执行的最终结果 发送方根据检查得到的本地事务的最终状态再次提交二次确认，服务端仍按照发送步骤的步骤 4 对半事务消息进行操作 最大努力通知 最大努力通知型的目标是 事务发起方尽量将业务处理结果通知到参与方。适用于一些最终一致性时间敏感度低，且参与方的处理结果不影响发起方的处理结果的这类通知类的业务场景。如短信供应商的回执通知： 最大努力通知型的实现方案，一般符合以下两个特点： 消息重复通知，在业务活动发起方完成业务处理之后，向参与方发送消息，参与方可能没有接收到通知，此时要发起方有一定的机制对消息重复通知（通常是发起方调用参与方的 http 接口，而且会协商一个 N 次 通知的上限） 定期校对，事务发起方提供消息校对的接口，如果事务参与方没有接收到发起方发送的消息，可以调用事务发起方提供的接口主动获取消息 总结 可靠性的保障方不同 可靠消息方案中，发起方需要保证将消息发出去，并且将消息发到参与方，消息的可靠性关键由发起方来保证 最大努力通知方案中，发起方尽最大努力将业务处理结果通知给参与方，但参与者是可能接收不到消息，此时需要接收通知方主动调用发起通知方的接口查询业务处理结果，消息通知的可靠性关键在参与方 两者的业务应用场景不同 可靠消息方案 关注的是整体业务处理的事务一致，以异步的方式完成整个业务处理，通常是内部系统之间的调用 最大努力通知 关注的是业务处理后的通知事务，即将业务处理结果可靠的通知出去 技术解决方向不同 可靠消息方案 要解决消息从发出到接收的一致性，即消息发出并且被接收到 最大努力通知方案 无法保证消息从发出到接收的一致性，只提供消息接收的可靠性机制。可靠机制是，尽最大努力将消息通知给参与方，当消息无法被参与方接收时，由参与方主动查询消息（业务处理结果） 参考文档 https://developer.aliyun.com/article/1192549#slide-22","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"分布式事务-06-AT","slug":"Distributed Transaction/分布式事务-06-AT","date":"2022-06-06T14:58:29.000Z","updated":"2023-06-26T06:56:51.740Z","comments":true,"path":"2022/06/06/Distributed Transaction/分布式事务-06-AT/","link":"","permalink":"http://xboom.github.io/2022/06/06/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-06-AT/","excerpt":"","text":"AT模式是Seata最主推的分布式事务且基于XA演进而来的解决方案，是一种改进的二阶段提交，主要有三个角色：TM、RM和TC，其中TM和RM作为Seata的客户端和业务集成，TC作为Seata服务器独立部署 在AT模式下，数据库资源被当做RM，访问RM时，Seata会对请求进行拦截；每个本地事务提交时，RM会向TC(Transaction Coordinator，事务协调器)注册一个分支事务，用户只需关注自己的 业务SQL ，用户的 业务SQL 作为一阶段，Seata 框架会自动生成事务的二阶段提交和回滚操作 流程 第一阶段 当访问 RM 的时候，会拦截解析 SQL 语句，保存 “before image” 执行 SQL 更新业务数据 接着保存操作后的 “after image” 插入回滚日志 undo_log 向TC注册分支事务，申请全局锁，并将其纳入到该XID对应的全局事务范围。 提交本地事务 向 TC 汇报本地事务结果 第二阶段 事务提交 如果在第一阶段所有的分支事务已经提交，则TC决定全局事务提交，此时只需要清理UNDO_LOG日志即可，相比较XA模式，不需要TC触发所有分支事务的提交 具体的流程为： 分支事务收到TC的提交请求之后放入异步队列中，马上返回提交成功的结果 这里不需要同步返回的原因是：TC不需要知道分支事务的结果，因为仅仅只是一步删除UNDO_LOG记录的操作，即使不成功也不会结果造成影响，所以采用异步是有效的方式 从异步队列中执行分支提交请求，清理undo_log日志，这里并不需要分支事务的提交了，因为第一阶段中已经提交过了 理解起来就是：AT模式下的全局事务的提交只需要清理UNDO_LOG记录就行，不需要管分支(本地)事务的提交结果！ 事务回滚 在第一阶段的分支事务中任何一个分支事务执行失败，都会进入全局事务回滚流程，回滚主要是依赖UNDO_LOG中的记录进行补偿的 具体的流程如下： 接收到TC的回滚请求后，开启本地事务用来执行回滚操作 本地事务分支开始进行对查找UNDO_LOG记录，通过XID+branch ID查到UNDO_LOG记录； 数据校验(对比更新后镜像数据与当前数据)，拿到rollback_for中afterImage镜像数据与当前业务表中数据比较，不同的话，比如afterImage镜像数据拿出的amount理论上为99，但是实际上amount=98，则说明已经被当前全局事务外的某个操作做了修改(实际上由于全局锁的存在，并不会存在其他全局事务对业务数据进行更新)，那么事务不进行回滚。 第二步中对比结果是相等的话，就采用beforeImage和SQL的相关信息进行回滚，即 1UPDATE rep SET acoumt&#x3D;100 WHERE id&#x3D;1 删除UNDO_LOG记录 提交本地事务 将本地事务的回滚执行结果报告给TC 隔离性保证 在AT模式下，多个全局事务操作同一张表时，它的事务隔离性保证是基于全局锁来实现的 写隔离，在第一阶段本地事务提交之前，必须确保拿到全局锁，如果拿不到全局锁则一直等待尝试，超出最大尝试次数则放弃全局锁的获取，并回滚释放本地锁（在本地事务开始之前就获到本地锁，这里的本地锁概念是数据库锁，比如对某行记录的行锁） 读隔离，Seata AT事务模式的默认全局隔离级别是Read Uncommit，在这种隔离级别下，所有事务都可以看到其它未提交事物的执行结果产生脏读，这在最终一致性的事务模型是被允许的，并且大部分是分布式事务是接受脏读的。 总结 优点： 原子性，AT协议保证了分布式事务的原子性，要么所有 RM 都成功提交事务，要么所有 RM 都回滚事务。 数据一致性，AT协议确保了分布式事务的一致性，所有 RM 在提交阶段只有在所有其他 RM 也准备好提交时才会提交事务。 灵活性，AT协议可以适应各种分布式环境和参与者的异构性，因为它没有对具体的数据库或资源管理器实施特定要求。 缺点： 同步阻塞，AT协议在准备和提交阶段都需要等待 RM 的响应，因此可能会引入同步阻塞，影响事务的性能和吞吐量。 单点故障，在AT协议中，TC 是关键的中心节点，如果 TC 发生故障，整个分布式事务的执行将受到影响。 数据不一致风险，在AT协议中，即使在准备阶段所有参与者都准备就绪，但在提交阶段仍然存在参与者无法提交成功的情况，这可能导致数据的不一致。 参考文档 https://seata.io/zh-cn/blog/seata-at-tcc-saga.html https://www.cnblogs.com/jian0110/p/14925087.html https://zhuanlan.zhihu.com/p/78599954","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"分布式事务-05-SAGA","slug":"Distributed Transaction/分布式事务-05-SAGA","date":"2022-06-05T14:58:29.000Z","updated":"2023-06-26T06:56:48.872Z","comments":true,"path":"2022/06/05/Distributed Transaction/分布式事务-05-SAGA/","link":"","permalink":"http://xboom.github.io/2022/06/05/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-05-SAGA/","excerpt":"","text":"SAGE核心思想是将长事务拆分为多个本地短事务，由Saga事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作 如何实现补偿(提前准备回滚语句) dtm 的SAGA模式与Seata的SAGA在设计理念上是不一样的 流程 已跨行转账的业务为例，转出(TransOut)和转入(TransIn)分别在不同的微服务里，一个成功完成的SAGA事务典型的时序图如下 成功 失败 代码如下： 12345req := &amp;gin.H&#123;\"amount\": 30&#125; // 微服务的请求Bodysaga := dtmcli.NewSaga(DtmServer, shortuuid.New()). Add(qsBusi+\"/TransOut\", qsBusi+\"/TransOutCompensate\", req). Add(qsBusi+\"/TransIn\", qsBusi+\"/TransInCompensate\", req)err := saga.Submit() 构建了事务的请求Body 构建一个事务包含了 DtmServer为DTM服务的地址 shortuuid.New() 事务请求ID 添加一个TransOut的子事务，每个事务都包含了正向操作与补偿操作(逆向操作) 正向操作为url: qsBusi+&quot;/TransOut&quot;， 逆向操作为url: qsBusi+&quot;/TransOutCompensate&quot; 添加一个TransIn的子事务 正向操作为url: qsBusi+&quot;/TransIn&quot; 逆向操作为url: qsBusi+&quot;/TransInCompensate&quot; 提交saga事务 问题1：是如何进行补偿的 问题2：补偿失败是如何处理的 答：在补偿操作遇见失败时，会不断进行重试，直到成功。(TM重启了怎么办) 问题3：sage事务是同步返回结果还是异步任务处理的 问题4：当RM1执行成功，RM2执行失败的同时RM1崩溃了，TM会如何处理 问题5：当服务崩溃大量事务堆积在TM上，TM如何支持短时间大量事务重试 补偿 补偿的情况有几种 第一种情况，子事务 A - B - C 中 C 失败，需要对 A - B 进行补偿操作，如何保存处理单个分布式事务的子事务顺序补偿问题 B 先回滚，然后是A 第二种情况，子事务 A - B - C 在执行过程中 A1 - B1 - C1 也在执行，A - B - C 中 C失败，A1 - B1 - C1 中 B1 失败，那么如何处理 单个服务的补偿分为 已执行 、未执行、执行中(结果未知)，那么补偿又是如何处理的 补偿执行顺序 DTM 的SAGA事务在1.10.0及之前，补偿操作是并发执行的，1.10.1之后，是根据用户指定的分支顺序，进行回滚的。 普通SAGA，未打开并发选项，那么SAGA事务的补偿分支是完全按照正向分支的反向顺序进行补偿的。 并发SAGA，补偿分支也会并发执行，补偿分支的执行顺序与指定的正向分支顺序相反。假如并发SAGA指定A分支之后才能执行B，那么进行并发补偿时，DTM保证A的补偿操作在B的补偿操作之后执行 Demo 一个用户出行旅游的应用，收到一个用户出行计划，需要预定去三亚的机票，三亚的酒店，返程的机票。要求： 两张机票和酒店要么都预定成功，要么都回滚（酒店和航空公司提供了相关的回滚接口） 预订机票和酒店是并发的，避免串行的情况下，因为某一个预定最后确认时间晚，导致其他的预定错过时间 预定结果的确认时间可能从1分钟到1天不等 首先，根据要求1创建一个saga事务，这个saga包含三个分支，预定去三亚机票，预定酒店，预定返程机票 1234saga := dtmcli.NewSaga(DtmServer, gid). Add(Busi+\"/BookTicket\", Busi+\"/BookTicketRevert\", bookTicketInfo1). Add(Busi+\"/BookHotel\", Busi+\"/BookHotelRevert\", bookHotelInfo2). Add(Busi+\"/BookTicket\", Busi+\"/BookTicketRevert\", bookTicketBackInfo3) 接着，根据要求2，让saga并发执行（默认是顺序执行） 1saga.EnableConcurrent() 最后，根据要求3，由于不是即时响应，所以不能够让预定操作等待第三方的结果，而是提交预定请求后，就立即返回状态-进行中 123456789101112saga.RetryInterval = 60saga.Submit()// ........func bookTicket() string &#123; order := loadOrder() if order == nil &#123; // 尚未下单，进行第三方下单操作 order = submitTicketOrder() order.save() &#125; order.Query() // 查询第三方订单状态 return order.Status // 成功-SUCCESS 失败-FAILURE 进行中-ONGOING&#125; 分支事务未完成，dtm会重试我们的事务分支，把重试间隔指定为1分钟，这里订票结果不应当采用指数退避算法重试，否则最终用户不能及时收到通知。在bookTicket中，返回结果ONGOING，当dtm收到这个结果时，会采用固定间隔重试，这样能及时通知到用户 并发 并发SAGA通过EnableConcurrent()打开，当saga提交后，多个事务分支之间是并发执行。DTM也支持指定事务分支之间的依赖关系，可以指定特定任务A执行完成之后才能够执行任务B 并发SAGA如果出现回滚，那么所有回滚的补偿操作会全部并发执行，不再考虑前面的任务依赖。 由于并发SAGA的正向操作和补偿操作都是并发执行的，因此更容易出现空补偿和悬挂情况，需要参考DTM的子事务屏障环节妥善处理 部分无法回滚 12345678saga := dtmcli.NewSaga(DtmServer, shortuuid.New()). Add(Busi+\"/CanRollback1\", Busi+\"/CanRollback1Revert\", req). Add(Busi+\"/CanRollback2\", Busi+\"/CanRollback2Revert\", req). Add(Busi+\"/UnRollback1\", \"\", req). Add(Busi+\"/UnRollback2\", \"\", req). EnableConcurrent(). AddBranchOrder(2, []int&#123;0, 1&#125;). // 指定step 2，需要在0，1完成后执行 AddBranchOrder(3, []int&#123;0, 1&#125;) // 指定step 3，需要在0，1完成后执行 指定Step 2，3 中的 UnRollback 操作，必须在Step 0，1 完成后执行 这样也能处理 第一个事务输出是第二个事务的输入怎么办的问题 超时回滚 saga属于长事务，因此持续的时间跨度很大，可能是100ms到1天，因此saga没有默认的超时时间。 dtm支持saga事务单独指定超时时间，到了超时时间，全局事务就会回滚。 1saga.TimeoutToFail = 1800 在saga事务中，设置超时时间一定要注意，这类事务里不能够包含无法回滚的事务分支，因为超时回滚时，已执行的无法回滚的分支，数据就是错的 参考文档 https://dtm.pub/practice/saga.html https://dtm.pub/deploy/maintain.html https://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"分布式事务-04-TCC","slug":"Distributed Transaction/分布式事务-04-TCC","date":"2022-06-04T14:58:29.000Z","updated":"2023-06-26T06:56:45.258Z","comments":true,"path":"2022/06/04/Distributed Transaction/分布式事务-04-TCC/","link":"","permalink":"http://xboom.github.io/2022/06/04/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-04-TCC/","excerpt":"","text":"TCC是Try、Confirm、Cancel三个词语的缩写，TCC分为3个阶段 Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性） Confirm 阶段：如果所有分支的Try都成功了，则走到Confirm阶段。Confirm真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源 Cancel 阶段：如果所有分支的Try有一个失败了，则走到Cancel阶段。Cancel释放 Try 阶段预留的业务资源 流程 成功 失败 假如Confirm/Cancel操作遇见失败会怎么样？按照Tcc模式的协议，Confirm/Cancel操作是要求最终成功的，遇见失败的情况，都是由于临时故障或者程序bug。 总结 优点： TCC 只适合短事务 允许开发人员根据业务需求灵活定义每个阶段的操作逻辑。可使TCC模式适用于各种复杂的分布式事务场景。 TCC模式通过将事务拆分为Try、Confirm和Cancel三个阶段，避免了传统的锁机制带来的并发性能问题 由于无锁设计和阶段化的执行，TCC模式具有良好的并发性能。各个参与者可以并行执行Try和Confirm阶段的操作，减少了事务冲突和等待时间。 TCC模式对于 RM 的故障和系统崩溃有一定的容错性。在Confirm阶段，事务的确认操作将确保事务的最终提交；而在Cancel阶段，事务的撤销操作将恢复资源到事务之前的状态，保证了系统的可靠性和故障恢复能力。 缺点： 相对于传统的ACID事务模型，TCC模式的实现相对复杂。需要开发人员仔细设计和实现每个阶段的操作逻辑 由于TCC模式的阶段执行方式，可能存在部分阶段执行成功而后续阶段失败的情况。这可能导致数据的不一致性，需要通过应用程序层面的补偿机制或人工干预来解决。 不同 RM 之间需要进行额外的网络通信来协调事务的执行。这可能增加系统的网络开销和延迟。 参考文档 https://dtm.pub/guide/theory.html#事务 https://blog.csdn.net/yeyazhishang/article/details/80758354","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"分布式事务-03-三阶段提交","slug":"Distributed Transaction/分布式事务-03-三阶段提交","date":"2022-06-03T14:58:29.000Z","updated":"2023-06-26T06:56:41.467Z","comments":true,"path":"2022/06/03/Distributed Transaction/分布式事务-03-三阶段提交/","link":"","permalink":"http://xboom.github.io/2022/06/03/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-03-%E4%B8%89%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/","excerpt":"","text":"针对两阶段提交存在的问题，三阶段提交协议通过引入一个 预询盘 阶段，以及超时策略来减少整个集群的阻塞时间，提升系统性能？ 为啥加一个流程就能减少阻塞时间，接着往下看 针对2PC存在的问题 3PC 做了如下改变 引入超时机制。同时在 TM 和 RM 中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 超时机制： 对于 TM 来说如果在指定时间内没有收到所有 RM 的应答，则可以自动退出 WAIT 状态，并向所有参与者发送 rollback 通知。 对于 RM 来说如果位于 READY 状态，但是在指定时间内没有收到TM的第二阶段通知，则不能武断地执行 rollback 操作，因为TM 可能发送的是 commit 通知，这个时候执行 rollback 就会导致数据不一致。 互询机制，让 RM A 去询问其他 RM B 的执行情况。 如果 B 执行了 rollback 或 commit 操作，则 A 可以大胆的与 B 执行相同的操作； 如果 B 此时还没有到达 READY 状态，则可以推断出 TM 发出的肯定是 rollback 通知； 如果 B 同样位于 READY 状态，则 A 可以继续询问另外的参与者。 只有当所有的 RM 都位于 READY 状态时，此时两阶段提交协议无法处理，将陷入长时间的阻塞状态(TM 可能崩溃了) 3PC把2PC的准备阶段再次一分为二：预询盘(can_commit)、预提交(pre_commit)以及事务提交(do_commit) 流程 第一阶段 预询盘 TM 向各 RM 发送 CanCommit 的请求，询问是否可以执行事务提交操作，并开始等待各 RM 的响应 RM 收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 Yes 响应，并进入预备状态，否则反馈 No 异常情况： 部分 RM 收到 canCommit 指令后，直接反馈不能开始事务，TM 向所有参与者发送 abort 请求。 TM 等待反馈超时，向所有 RM 发送 abort 请求。 RM 预检没问题，但一直无法接收到下一步的的指令(反馈 can commit 超时 或接收RM 的下一步指令超时)，自省中断事务的中断 RM 收到来自 TM 的 abort 请求之后执行事务中断 第二阶段 预提交 这个阶段类似2PC 的第一阶段，这个环节根据阶段 1 的参与者的反馈不同，而执行不同的逻辑： 1）如果任意一个 RM 在阶段 1 向 TM 反馈了 No 响应，或者 TM 等待 RM 反馈超时，那么就会中断事务，中断执行逻辑如下： TM 向所有 RM 发送 abort 请求 RM 无论是收到来自 TM 的 abort 请求，还是等待超时，都执行事务中断 2）另一种情况，如果 TM 接收到各RM 反馈都是 Yes，那么才执行事务预提交，执行逻辑如下： TM 向各RM 发送 preCommit 请求，并进入 prepared 阶段 各 RM 接收到 preCommit 请求后执行事务操作，并将 Undo 和 Redo 信息记录到事务日记中，但事务不提交 如果各 RM 都成功执行了事务操作，那么反馈给TM Ack 响应，同时等待最终指令，提交 commit 或者终止 abort 第三阶段 事务提交 这个阶段类似 2PC 的第二个阶段，这个环节根据阶段2 RM 不同反馈，而执行不同的逻辑： 1）假设 TM 正常工作，并且有任意一个 RM 在阶段 2 反馈 No，或者在等待RM 的反馈超时后，都会主动中断事务 TM 向所有RM 节点发送 abort 请求 RM 接收到 abort 请求后，利用 undo 日志执行事务回滚，并在完成事务回滚后释放占用的资源后，向 TM 发送 ack 信息，反馈事务回滚结果 TM 接收到所有 RM 反馈的 ACK 消息之后，完成事务的中断 2）假设 TM 正常工作，接收到了所有 RM 的 ack 响应，那么它将从预提交阶段进入提交状态 TM 向所有 RM 发送 doCommit 请求 RM 收到 doCommit 请求后，正式提交事务，并在完成事务提交后释放占用的资源，向 TM 发送 ACK 信息，反馈事务提交结果 TM 接收到所有参与者 ack 信息，整个事务完成 异常情况： RM 收到 PreCommit 指令并正常执行事务，给 TM 反馈 preCommit 完成后，如果未能等到下一步的 doCommit 指令超时了，会自主提交事务。因为询问阶段是一致通过的，执行到这个阶段整个事务成功的概率已经很高了 部分 RM 执行 preCommit 异常，部分参与者执行 preCommit 正常，但此时 TM 挂了，那么通过 RM 的超时自醒机制，就出现部分参与者提交，部分参与者回滚，出现数据不一致。 TM 发送了 abort 指令，RM 超时未收到指令就提交了事务，其他 RM 收到了协调者发送的 abort 指令后执行了回滚，也会出现数据不一致。 Keidar 和 Dolev (1998) 建议使用增强型三阶段提交 (E3PC) 协议来消除此问题。E3PC 协议需要至少三个往返才能完成，这将有很长的延迟才能完成每笔交易。 总结 相较于2PC，3PC引入超时机制与质询机制，增加了额外的通信开销，增加了系统网络负载与延迟 异常情况仍然无法避免数据不一致的情况 依赖于时间同步，3PC的正确执行依赖于系统中所有 RM 和 TM 的时间同步。如果时间同步不准确，可能导致超时机制失效或误判，影响事务的最终一致性。 参考文档 https://pdai.tech/md/arch/arch-z-transection.html#分布式事务方案之刚性事务","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"分布式事务-02-两阶段提交","slug":"Distributed Transaction/分布式事务-02-两阶段提交","date":"2022-06-02T14:58:29.000Z","updated":"2023-06-26T06:56:38.419Z","comments":true,"path":"2022/06/02/Distributed Transaction/分布式事务-02-两阶段提交/","link":"","permalink":"http://xboom.github.io/2022/06/02/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-02-%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4/","excerpt":"","text":"理解两阶段提交，需要首先了解 XA 协议。 XA定义：XA是由X/Open组织提出的分布式事务的规范。主要定义了**(全局)事务管理器TM和(局部)资源管理器(RM)**之间的接口，主流的关系型 数据库产品都是实现了XA接口的 两阶段提交(Two-phase Commit Protocol，简称 2PC)针对的是刚性事务，将一个分布式的事务过程拆分成两个阶段： 投票 和 事务提交 ，是一个非常经典的强一致、中心化的原子提交协议 流程 AP 向 TM 提交请求，发起分布式事务 第一阶段 准备阶段（Prepare Phase） 准备阶段，主要目的在于打探数据库集群中的各个 RM 是否能够正常的执行事务，具体步骤如下： TM 向所有的 RM 发送事务预处理请求 REQUEST-TO-PREPARE，并等待 RM 反馈事务执行结果(准备结果) RM 收到请求之后，执行询问发起为止的所有事务操作，并将 Undo 信息和 Redo 信息写入日志 RM 响应 TM 发起的询问。如果 RM 第二步 执行成功，则返回PREPARED；否则返回 NO。同时阻塞等待 TM 的后续指令 第一阶段准备阶段也被称作投票阶段，即各 RM 投票是否要继续接下来的提交操作 这里就会存在一些问题： TM 发送请求部分 RM 没有收到 答：RM 会阻塞等待，而 TM 也会因为收不到回复而阻塞等待。解决方案，提供超时机制， TM超时回滚或咨询事务执行结果逻辑 RM 收到请求是如何准备的 答：特殊处理机制，看下面的 MySQL 执行 XA 命令 RM 收到请求之后执行了准备操作但是没有响应 答：TM 会阻塞，解决方案，提供超时机制， TM超时回滚或咨询事务执行结果逻辑 TM 等待响应部分或全部超时如何处理 答：TM 会阻塞，解决方案，提供超时机制， TM超时回滚或咨询事务执行结果逻辑 可惜的是，当 TM 无法收到所有 RM 的回复的时候，TM 会陷入阻塞！！需要超时逻辑进行下一步处理 在具体应用中超时/失败场景可以根据实际的系统需要而进行方案设计，这里的协议仅仅做了流程上的考量 超时重试：TM 可以尝试重新发送准备请求给失败的 RM，设定一个合理的超时时间。如果在超时时间内收到了 RM 的准备就绪通知，那么可以继续进行提交阶段。如果超时后仍未收到响应，可以将 RM 标记为失败，并回滚事务。 回滚事务：如果准备请求发送失败，可以将事务标记为回滚，并通知所有 RM 回滚事务。这样可以确保所有 RM 处于一致的状态，即使其中一部分 RM 未能接收到准备请求。 异常处理：如果准备请求的发送失败是由于网络故障、RM 崩溃或其他不可预见的错误引起的，可以捕获异常并进行相应的处理。可以根据具体的系统需求，选择适当的策略，如重试、回滚或者向管理员报告问题 RM 支持 XA 协议，在收到 TM 请求之后，可以根据 MySQL 命令看看 RM 是如何处理的 12345678910111213141516171819// 第一阶段// 1.1 通过 XA START 和 XA END 来包裹 用户业务SQLmysql&gt; XA START 'transfer_money';Query OK, 0 rows affected (0.00 sec)mysql&gt; UPDATE account SET money = money -100 where id = 1 ;Query OK, 1 row affected (0.00 sec)mysql&gt; XA END 'transfer_money';Query OK, 0 rows affected (0.00 sec)// 1.2 通过 XA PREPARE 通知 RM 一阶段就绪mysql&gt; XA PREPARE 'transfer_money';Query OK, 0 rows affected (0.00 sec)// 第二阶段，通过 XA COMMIT 完成二阶段的提交mysql&gt;mysql&gt; XA COMMIT 'transfer_money';Query OK, 0 rows affected (0.00 sec) 第二阶段 提交阶段（Commit Phase） 在第一阶段 TM 收到所有 RM 返回成功的情况下，流程如下所示 如果所有的 RM 都回复的是PREPARED， 那么 TM 向所有 RM 发送COMMIT 消息； 否则 TM 向所有回复PREPARED的 RM 发送ABORT消息； RM 如果收到 TM 发来的COMMIT消息则提交，ABORT消息则回滚，并向 TM 发送DONE消息以确认 不懂就问： 部分 RM 并没有收到 TM 的提交或者回滚、 答：数据可能出现不一致。解决方案，可以引入咨询机制查看事务是否正常，阻塞时间更长 TM 等待 RM 的提交响应超时如何处理。 答：TM 无法知道执行结果，可能出现不一致，解决方案，可以引入咨询机制查看事务是否正常，阻塞时间更长 TM 作为事务协调者，是否存在单点故障以及性能瓶颈。 答：可以通过协商选举一个出来(如果能保存未提交事务让备TM 继续咨询协调更好) 当TM 等待响应部分超时或者失败，TM 均认为 RM 无法成功执行事务，为了整个集群数据的一致性，向各个 RM 发送事务回滚通知： TM 向各个 RM 发送事务 rollback 通知，请求回滚事务 RM 收到事务回滚通知之后执行 rollback 操作，然后释放占有的资源 RM 向 TM 返回事务 rollback 结果信息(发送也可能失败) 不懂就问： 部分RM 并没有收到回滚通知怎么办？ 答：所有处于执行了操作但是未提交状态的 RM 都会陷入阻塞情况. 总结 优点： 两阶段提交支持不同数据库的分布式事务，比如一个是 MySQL，另外一个是 Oracle 业务无侵入：XA 模式将是业务无侵入的，不给应用设计和开发带来额外负担 数据库的支持广泛：XA 协议被主流关系型数据库广泛支持，不需要额外的适配即可使用 缺点： 同步阻塞，无论是在第一阶段的过程中,还是在第二阶段,所有 RM 资源和 TM 资源都是被锁住的,只有当所有 RM 准备完毕，TM 才会通知进行全局提交，RM 进行本地事务提交后才会释放资源。这样的过程会比较漫长，对性能影响比较大。 单点故障，一旦 TM 发生故障。RM 会一直阻塞下去。尤其在第二阶段，TM 发生故障，那么所有的 RM 还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是 TM 挂掉，可以重新选举一个TM ，但是无法解决因为 TM 宕机导致的 RM 处于阻塞状态的问题） 数据不一致。当只有部分 RM 收到commit请求，会导致整个分布式系统便出现了数据部一致性的现象。 脑裂，当 TM 发出 commit 消息之后宕机，而接收到这条消息的 RM 同时也宕机了。那么即使 TM 通过选举协议产生了新的TM ，这条事务的状态也是不确定的，没人知道事务是否被已经提交 参考文档 https://pdai.tech/md/arch/arch-z-transection.html#分布式事务方案之刚性事务 https://zh.wikipedia.org/wiki/二阶段提交 https://blog.51cto.com/u_15287666/2989395 https://www.cnblogs.com/qdhxhz/p/11167025.html","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"分布式事务-00-前言","slug":"Distributed Transaction/分布式事务-01-前言","date":"2022-06-01T14:58:29.000Z","updated":"2023-06-26T06:56:34.742Z","comments":true,"path":"2022/06/01/Distributed Transaction/分布式事务-01-前言/","link":"","permalink":"http://xboom.github.io/2022/06/01/Distributed%20Transaction/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-01-%E5%89%8D%E8%A8%80/","excerpt":"","text":"这里开一个新坑就是分布式事务，为了学这一块内容准备这样做 学习分布式事务前的基本知识 围绕这些解决方案的实现原理是什么 不同的解决方案与不同的实现原理进行对比 看看都有哪些解决方案，以及它们的优缺点 CAP 分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。这被称为CAP理论 C 一致性(Consistency)：要求分布式系统中的所有副本或节点在任何时刻都具有相同的数据值。当系统接收到更新请求后，所有节点必须保证在一定时间内达到一致的状态(强一致性) A 可用性(Availability)：要求分布式系统在任何时刻都能够响应用户的请求并提供正常的服务。即使在节点故障或网络分区等情况下，系统仍然能够继续运行并提供服务 P 分区容错性(Partition tolerance)：指系统能够继续运行并保持一致性和可用性，即使在不可避免的网络分区(分布式系统中的消息丢失或延迟)发生时 根据上面三者可以组合的情况有 CA(一致性和可用性)：系统追求强一致性和高可用性，但在发生网络分区时，系统会停止对外服务，直到分区问题解决。关系型数据库通常采用这种方案。 AP(可用性和分区容错性)：系统追求高可用性和分区容错性，即使发生网络分区，系统仍然可以继续运行，但可能导致数据的一致性问题。例如，大规模分布式系统如互联网应用中的NoSQL数据库常采用这种方案。 CP(一致性和分区容错性)：在这种方案中，系统追求强一致性和分区容错性，但在发生网络分区时，可能会牺牲可用性。这意味着系统可能在网络分区期间无法提供服务。例如，一些分布式数据库系统采用这种方案。 CAP的一致性指的是强一致性 CAP中分布式事务的一致性是多个节点状态的一致性，而ACID中事务的一致性指的是DB的约束定义的前后一致性 根据 CAP 各个的描述可以得出系统三者不可能同时满足，需要在系统设计时根据情况舍弃 BASE BASE是Basically Available（基本可用）、**Soft state（软状态）和Eventually consistent（最终一致性）**三个短语的简写，BASE是基于CAP定理逐步演化而来，对CAP中一致性和可用性权衡的结果，核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性(Eventual consistency) Basically Available(基本可用): 指分布式系统在出现不可预知故障的时候，允许损失部分可用性(系统仍然可用) Soft state(软状态): 弱状态也称为软状态，和硬状态相对，是指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 Eventually consistent(最终一致性): 强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性 BASE理论提出通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。 分布式的理论基础是CAP，分布式系统中，P(分区容错)是必选项，所以只能在AP或者CP中选择。 分布式理论的CP -&gt; 刚性事务，遵循ACID，对数据要求强一致性 分布式理论的AP+BASE -&gt; 柔性事务，遵循BASE，允许一定时间内不同节点的数据不一致，但要求最终一致 刚性事务基于分布式理论的CP，遵循ACID，对数据要求强一致性。包括 2PC、3PC 柔性事务基于分布式理论的AP，遵循BASE，允许一定时间内不同节点的数据不一致，但要求最终一致。它们又可以分为两种 基于补偿的有 TCC 与 SAGA 基于最终一致性的有 本地消息、事务消息、尽最大努力通知 整体分布式事务实现原理根据资源在分布式事务的角色可以分为： AP 应用程序处理器，一般是发起分布式事务的一方 RM 资源管理器，负责管理和控制分布式系统中的特定资源或数据库。资源管理器协调事务的执行，包括处理事务的提交和回滚，以及确保数据的一致性和完整性。 TM 事务管理器，负责协调和管理分布式事务的执行。事务管理器协调不同资源管理器的操作，以确保所有参与的资源在事务提交或回滚时保持一致性，并提供事务的ACID TC 事务协调者，维护全局和分支事务的状态，驱动全局事务提交或回滚(Seata 引入) 根据分布式事务的组成分为 事务分支：每个服务管理的事务组成部分，称为事务分支，RM 服务会在全局事务上，注册一个事务分支 分支操作：对于RM服务，在TCC事务模式下，会实现Try/Confirm/Cancel三个操作，多个分支操作配合完成一个分支事务 本地事务：RM 服务可能访问一个数据库，创建一个本地事务，也可能访问多个数据库，创建多个本地事务 找了一下分布式事务的实现方案，发现两种 DTM 与 SEATA 特性 DTM SEATA 语言 Go、Java、python、php、c#… Java、Go、Python 异常处理 子事务屏障自动处理 手动处理 TCC事务 ✓ ✓ XA事务 ✓ ✓ AT事务 建议使用XA ✓ SAGA事务 支持并发 状态机模式 二阶段消息 ✓ ✗ 单服务多数据源 ✓ ✗ 通信协议 HTTP、gRPC dubbo、gRPC 仓库 https://github.com/dtm-labs/dtm.git https://github.com/seata/seata.git 接下来异常介绍几种分布式事务的实现原理、优缺点以及适用场景 二阶段提交 三阶段提交 TCC SAGA AT 通知型事务 二阶段消息(DTM) 异常处理优化(DTM) Percolator 参考文档 https://dtm.pub/guide/theory.html#事务 https://blog.csdn.net/yeyazhishang/article/details/80758354 https://xiaomi-info.github.io/2020/01/02/distributed-transaction/ https://segmentfault.com/a/1190000040321750 http://seata.io/zh-cn/docs/overview/what-is-seata.html","categories":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"}],"tags":[{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"}]},{"title":"MySql-10-MVCC","slug":"MySql/MySql-10-MVCC","date":"2022-03-01T07:47:24.000Z","updated":"2023-05-27T15:42:23.777Z","comments":true,"path":"2022/03/01/MySql/MySql-10-MVCC/","link":"","permalink":"http://xboom.github.io/2022/03/01/MySql/MySql-10-MVCC/","excerpt":"","text":"MVCC(Multi Version Concurrency Control)即多版本并发控制，MySQL 中InnoDB中实现了事务(多版本并发控制MVCC+锁)， 通过MVCC解决隔离性问题。 具体来说： 隔离性：MySQL 中使用两种锁机制，分别是行级锁和表级锁。在事务中，当一个事务对某个数据进行修改时，MySQL 会对这个数据进行加锁，其他事务就不能对这个数据进行修改，从而保证隔离性。 原子性：MySQL 还会将事务的操作记录到日志中，如果事务执行失败，可以通过日志进行回滚，保证了原子性； 持久性：通过日志进行恢复，保证了持久性。 首先明白两个概念 当前读：读取的是记录最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读。 快照读：像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读； 那么 当前读、快照读和MVCC的关系？ MVCC多版本并发控制指的是 “维持一个数据的多个版本，使得读写操作没有冲突” 这么一个概念，仅仅是一个理想概念 快照读就是MySQL为我们实现MVCC理想模型的其中一个具体非阻塞读功能。而相对而言，当前读就是悲观锁的具体功能实现 要说的再细致一些，快照读本身也是一个抽象概念，再深入研究。MVCC模型在MySQL中的具体实现则是由 4个隐式字段，undo日志 ，Read View 等去完成的，具体可以看下面的MVCC实现原理 在 MVCC 中事务的所有写操作(INSERT、UPDATE、DELETE)为数据行新增一个最新的版本快照，而读操作是去读旧版本的快照，也就是说，读操作和写操作是分离的，二者之间没有依赖、互斥关系 MVCC是由MySQL数据库InnoDB存储引擎实现的，并非是由MySQL本身实现的，不同的存储引擎，对MVCC都有不同的实现标准 MVCC 解决了什么问题，有什么好处？ 数据库的并发场景有三种, 分别为： 读-读：不存在任何问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在更新丢失问题，比如第一类更新丢失，第二类更新丢失 MVCC是为了解决数据库采用悲观锁这样性能不佳的形式去解决读-写冲突问题，而提出的解决方案。在数据库中，因为MVCC可以： MVCC + 悲观锁 MVCC解决读写冲突，悲观锁解决写写冲突 MVCC + 乐观锁 MVCC解决读写冲突，乐观锁解决写写冲突 这种组合的方式就可以最大程度的提高数据库并发性能，并解决读写冲突，和写写冲突导致的问题 实现原理 MVCC模型在MySQL中的具体实现则是由 4个隐式字段，undo日志 ，Read View 等去完成的，每行记录除了自定义的字段外，还有数据库隐式定义的四个字段 DB_ROW_ID 6byte, 隐含的自增ID(隐藏主键)，如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引 DB_TRX_ID 6byte, 最近修改(修改/插入)事务ID：记录创建这条记录/最后一次修改该记录的事务ID DB_ROLL_PTR 7byte, 回滚指针，指向这条记录的上一个版本(存储于rollback segment里） DELETED_BIT 1byte, 记录被更新或删除并不代表真的删除，而是删除flag变了 这四个字段是记录在 InnoDB 存储引擎的聚簇索引中的数据行中。InnoDB 存储引擎使用聚簇索引来组织数据，并将数据行按照聚簇索引的顺序存储在磁盘上。在聚簇索引中，每个数据行包含了完整的记录信息，包括字段和相关的事务信息 需要明确两点 读操作对数据库也是一项事务，但它的事务id不是 根据 max_trx_id 生成的，而是根据 trx_id 地址计算而来的 如果事务B对于事务A来说是不可见的，就需要顺着修改记录的版本链，从回滚指针开始往前遍历，直到找到第一个对于事务A来说是可见的事务ID，或者遍历完版本链也未找到（表示这条记录对事务A不可见） undo log undo log 在 一条sql语句 中会在更新前产生一条 undo log 记录，作用就是保护事务失败之后回滚到历史版本，它一共有三种类型 Insert undo log ：插入一条记录时，至少把这条记录的主键值记下来，回滚的时候只需要把这个主键值对应的记录删掉 Update undo log：修改一条记录时，至少把修改这条记录前的旧值都记录下来，回滚时再把这条记录更新为旧值 Delete undo log：删除一条记录时，至少把这条记录中的内容都记下来，回滚时把由这些内容组成的记录插入到表中 删除操作都只是设置一下老记录的DELETED_BIT，并不真正将过时的记录删除。 为了节省磁盘空间，InnoDB有专门的purge线程来清理 DELETED_BIT = true 的记录。为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的DELETED_BIT为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。 查询操作(SELECT) 并不会修改任何用户记录，所以并不会记录相应的undo log 执行流程： 一个有个事务插入person表插入了一条新记录，记录如下，name为Jerry, age为24岁，隐式主键是1，事务ID和回滚指针，我们假设为NULL 来一个事务1对该记录的name做出修改，改为Tom 在事务1修改该行(记录)数据时，数据库会先对该行加排他锁 然后把该行数据拷贝到undo log中，作为旧记录，即在undo log中有当前行的拷贝副本 拷贝完毕后，修改该行name为Tom，并且修改隐藏字段的事务ID为当前事务1的ID, 我们默认从1开始，之后递增，回滚指针指向拷贝到undo log的副本记录，即表示我的上一个版本就是它 事务提交后，释放锁 又来一个事务2修改同一条记录，将age改为30 在事务2修改该行数据时，数据库也先为该行加锁 然后把该行数据拷贝到undo log中，作为旧记录，发现该行记录已经有undo log了，那么最新的旧数据作为链表的表头，插在该行记录的undo log最前面 修改该行age为30岁，并且修改隐藏字段的事务ID为当前事务2的ID, 那就是2，回滚指针指向刚刚拷贝到undo log的副本记录 事务提交，释放锁 不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，即链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录(向图中的第一条insert undo log，其实在事务提交之后可能就被删除丢失了，该undo log的节点可能是会purge线程清除掉） Read View Read View 就是事务进行快照读操作的时候生产的，每个事务都有自己的 Read View（读视图）。每个事务的读视图是独立的，用于确保事务在读取数据时能够看到一致的数据快照。虽然每个事务有自己的读视图，但是在相同的事务隔离级别下，所有事务的读视图都是基于相同的全局一致性视图(Global Consistent View)。全局一致性视图是当前系统中所有已提交的事务形成的一个一致的数据快照 在 读已提交 和 可重复读 的隔离级别中，事务在启动的时候会创建一个读视图(Read View)，用它来记录当前系统的活跃事务信息，通过读视图来进行本事务之间的可见性判断。 其中有四个重要的字段： creator_trx_id：表示生成读视图的事务的事务ID m_ids：表示在生成读视图时，当前系统中活跃着的事务ID列表(未提交的事务ID列表) min_trx_id：表示在生成读视图时，当前系统中活跃着的最小事务ID max_trx_id：表示在生成读视图时，系统应该分配给下一个事务的事务ID(事务 ID 是累计递增分配的，所以后面分配的事务ID一定会比前面的大) 需要注意下一个事务ID的值，并不是事务ID列表中的最大值+1，而是当前系统中已存在过的事务的最大值+1。例如当前数据库中活跃的事务有(1,2)，此时事务2提交，同时又开启了新事务，在生成的读视图中，下一个事务ID的值为3 通过将版本链与读视图两者结合起来，来进行并发事务间可见性的判断，判断规则如下（假设现在要判断事务A是否可以访问到事务B的修改记录） 若 事务B的 当前事务ID DB_TRX_ID 小于 事务A中最小事务ID min_trx_id，代表事务B是在事务A生成读视图之前就已经提交了的，所以事务B对于事务A来说是可见的。 若事务B的 当前事务ID DB_TRX_ID 大于或等于 事务A下一个事务ID，代表事务B是在事务A生成读视图之后才开启，所以事务B对于事务A来说是不可见的。 若事务B的当前事务ID DB_TRX_ID在事务A的最小事务ID和下一个事务ID之间**(左闭右开，[最小事务ID, 下一个事务ID))**，需要分两种情况讨论： 若事务B的当前事务ID在事务A的事务ID列表中，代表创建事务A时事务B还是活跃的，未提交，所以事务B对于事务A来说是不可见的。 若事务B的当前事务ID不在事务A的事务ID列表中，代表创建事务A时事务B已经提交，所以事务B对于事务A来说是可见的。 读视图的创建时机，事务在启动时会创建一个读视图（Read View），而开启一个事务有两种方式，通过这两种方式开启事务，创建读视图的时机也是不同的： 如果是以 begin/start transaction 方式开启事务，读视图会在执行第一个快照读语句时创建 如果以 start transaction with consistent snapshot 方式开启事务，同时便会创建读视图 整体流程 当事务2对某行数据执行了快照读，数据库为该行数据生成一个Read View读视图，假设当前事务ID为2，此时还有事务1和事务3在活跃中，事务4在事务2快照读前一刻提交更新了，所以Read View记录了系统当前活跃事务1，3的ID，维护在一个列表上，假设我们称为m_ids 事务1 事务2 事务3 事务4 事务开始 事务开始 事务开始 事务开始 … … … 修改且已提交 进行中 快照读 进行中 … … … Read View 不仅仅会通过一个列表m_ids来维护事务2执行快照读那刻系统正活跃的事务ID(事务4已经提交，所以不活跃) min_trx_id = 1， max_trx_id = 4 + 1 = 5 m_ids集合的值是1,3 事务4修改过该行记录，并在事务2执行快照读前，就提交了事务，当前该行当前数据的undo log如下图所示； 事务2在快照读该行记录的时候，就会拿该行记录的DB_TRX_ID去跟min_trx_id,max_trx_id和活跃事务ID列表(m_ids)进行比较，判断当前事务2能看到该记录的版本是哪个 DB_TRX_ID = 4 &gt; min_trx_id = 1 且 DB_TRX_ID = 4 &lt; min_trx_id = 5，也就是可能在范围内，可能是活跃的，也可能不活跃 DB_TRX_ID = 4 不在 m_ids = [1, 3] 中，所以符合可见性条件 所以事务4修改后提交的最新结果对事务2快照读时是可见的，所以事务2能读到的最新数据记录是事务4所提交的版本，而事务4提交的版本也是全局角度上最新的版本 也正是Read View生成时机的不同，从而造成RC(read commited),RR(Repeatable read)级别下快照读的结果的不同 RR是如何在RC级的基础上解决不可重复读的？ 当前读和快照读在RR级别下的区别： 表1： 事务A 事务B 开启事务 开启事务 快照读(无影响)查询金额为500 快照查询金额为500 更新金额为400 提交事务 select 快照读 金额为500 select lock in share mode 当前读 金额为 400 表2： 事务A 事务B 开启事务 开启事务 快照读(无影响)查询金额为500 更新金额为400 提交事务 select 快照读 金额为400 select lock in share mode 当前读 金额为 400 表1与表2的区别是： 表1的事务B在事务A修改金额前快照读过一次金额数据 表2的事务B在事务A修改金额前没有进行过快照读 事务中快照读的结果是非常依赖该事务首次出现快照读的地方，即某个事务中首次出现快照读的地方非常关键，它有决定该事务后续快照读结果的能力 删除和更新是一样的，如果事务B的快照读是在事务A操作之后进行的，事务B的快照读也是能读取到最新的数据的 RC,RR级别下的InnoDB快照读有什么不同？ 正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同 在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见； 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见 而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因 在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。 参考链接 https://mp.weixin.qq.com/s/ct74vpWfKwhBJG6cafpBLw https://segmentfault.com/a/1190000023827696 https://www.modb.pro/db/73927 https://pdai.tech/md/db/sql-mysql/sql-mysql-mvcc.html","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"k8s入门5-容器网络之网络栈","slug":"K8S/k8s入门5-容器网络","date":"2022-02-20T16:35:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门5-容器网络/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A85-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/","excerpt":"","text":"容器网络 一个 Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的 网络栈包括：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。这些要素就构成了一个进程发起和响应网络请求的基本环境 回环设备是指拿一个大的镜像文件，如xxx.iso或xxx.img等，在此文件内建立一个文件系统，此文件就像一个新的磁盘或光盘设备一样使用。回环可以理解成回复重用，在已有设备上建立文件来模拟物理块设备 一个容器可以声明直接使用宿主机的网络栈（–net=host），即不开启 Network Namespace 1$ docker run –d –net=host --name nginx-host nginx 在这种情况下，容器启动后，直接监听的就是宿主机的 80 端口 优点：可以为容器提供良好的网络性能，不需要进行网络转发操作 缺点：不可避免地引入共享网络资源的问题，比如端口冲突 在大多数情况下，还是希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口 问题1：这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？ 在 Linux 中，能够起到虚拟交换机作用的网络设备是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上 Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信 问题2：如果将容器连接到网桥上? 答案是使用Veth Pair，它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里，使得 Veth Pair 被用作连接不同 Network Namespace 的“网线” 容器里 eth0 的网卡，正是一个 Veth Pair 设备在容器里的这一端 通过 route 命令查看 nginx-1 容器的路由表，可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则） 再查看Veth Pair 设备的另一端(宿主机的网络信息)，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth5d20cf2 Mac 无法查看，可以通过上面的host模型进行查看 1docker run -d --net=host --name nginx-host nginx 通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上 如果再在这台宿主机上启动另外一个容器，将会发现一个新的名字 veth7e63610 也插在docker0网桥上 如果在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址（172.17.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的，这是因为： nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机 同一个宿主机上的不同容器通过docker0网桥进行通信 要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址 ARP(Address Resolution Protocol)，是通过三层的 IP 地址找到对应的二层 MAC 地址的协议。 这个 eth0 网卡是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上，变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包，全部交给对应的网桥。 docker0 在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器。 有了目的 MAC 地址，nginx-1 容器的 eth0 网卡就 ping 包发出去。而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth5d20cf2 虚拟网卡上，然后直接流入到了 docker0 网桥里。 docker0 继续扮演二层交换机的角色。docker0 网桥根据数据包的目的 MAC 地址，在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：veth7e63610，然后把数据包发往这个端口。而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。 nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。 在实际的数据传递时，上述数据的传递过程在网络协议栈的不同层次，都有 Linux 内核 Netfilter 参与其中。通过打开 iptables 的 TRACE 功能查看到数据包的传输过程，在 系统日志 /var/log/syslog 或者 var/log/messages 里看到数据包传输的日志，具体方法如下所示 1234# 在宿主机上执行$ iptables -t raw -A OUTPUT -p icmp -j TRACE$ iptables -t raw -A PREROUTING -p icmp -j TRACE docker0 网桥的工作方式可以理解为，在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换 在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里 当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3， 请求数据包首先经过 docker0 网桥出现在宿主机上。 然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理 这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。 当遇到容器连不通“外网”的时候，先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常 问题4：如果在另外一台宿主机（比如：10.168.0.3）上，也有一个 Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢？ 在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了 创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上 构建这种容器网络的核心在于：需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络，被称为：Overlay Network（覆盖网络） 容器跨主机网络 Flannel 为了解决这个容器“跨主通信”的问题，出现了那么多的容器网络方案。Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现 Flannel 支持三种后端实现，也代表了三种容器跨主网络的主流实现方法 VXLAN host-gw UDP UDP 假设有两台宿主机： 宿主机 Node1 上有容器 container-1，它的 IP 地址是 100.96.1.2，docker0 网桥的地址是：100.96.1.1/24 宿主机 Node2 上有容器 container-2，它的 IP 地址是 100.96.2.3，docker0 网桥的地址是：100.96.2.1/24 第一步：container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。 第二步：由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。 第三步：这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示： 123456# 在Node 1上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2 可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。 flannel0 的设备类型是一个 TUN 设备，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能是在操作系统内核和用户应用程序之间传递 IP 包 当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。 反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向 当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。 那么 flannelId 又是如何知道这个IP地址对应的容器运行在Node 2上面呢？ 在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示： 1234$ etcdctl ls /coreos.com/network/subnets/coreos.com/network/subnets/100.96.1.0-24/coreos.com/network/subnets/100.96.2.0-24/coreos.com/network/subnets/100.96.3.0-24 所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3 12$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24&#123;\"PublicIP\":\"10.168.0.3\"&#125; 而对于 flanneld 来说，只要 Node 1 和 Node 2 是互通的，那么 flanneld 作为 Node 1 上的一个普通进程，就一定可以通过上述 IP 地址（10.168.0.3）访问到 Node 2，这没有任何问题 **flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。**这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。 当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。 通过这样一个普通的、宿主机之间的 UDP 通信，一个 UDP 包就从 Node 1 到达了 Node 2。而 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包。 而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。根据我前面讲解的 TUN 设备的原理，这正是一个从用户态向内核态的流动方向（Flannel 进程向 TUN 设备发送数据包），所以 Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示： 123456# 在Node 2上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.3 由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。 接下来的流程，docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里。而 container-2 返回给 container-1 的数据包，则会经过与上述过程完全相反的路径回到 container-1 中。 需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可： 12$ FLANNEL_SUBNET=100.96.1.1/24$ dockerd --bip=$FLANNEL_SUBNET ... 以上，就是基于 Flannel UDP 模式的跨主通信的基本原理了，如下所示。 可以看到，Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。 上述 UDP 模式有严重的性能问题，所以已经被废弃了。 相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示： 可以看到： 第一次，用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态； 第二次，IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程； 第三次，flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。 此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。这也正是造成 Flannel UDP 模式性能不好的主要原因 VXLAN 即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network） VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。 而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。 而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。 上述基于 VTEP 设备进行“隧道”通信的流程，如下所示： 可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。那么，与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。也就是说，来到了“隧道”的入口。为了方便叙述，我接下来会把这个 IP 包称为“原始 IP 包”。 为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1 这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。从图 3 的 Flannel VXLAN 模式的流程图中我们可以看到，10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。为了方便叙述，接下来我会把 Node 1 和 Node 2 上的 flannel.1 设备分别称为“源 VTEP 设备”和“目的 VTEP 设备”。而这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。 “源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。 目的 VTEP 设备 的 MAC 地址是什么？ 此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示： 123# 在Node 1上$ ip neigh show dev flannel.110.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT 这条记录的意思非常明确，即：IP 地址 10.1.16.0，对应的 MAC 地址是 5e:f8:4f:00:e3:37。 可以看到，最新版本的 Flannel 并不依赖 L3 MISS 事件和 ARP 学习，而会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录，直接下放到其他每台宿主机上 有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示： 可以看到，Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。需要注意的是，上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。所以图中的 Inner IP Header 字段，依然是 container-2 的 IP 地址，即 10.1.16.3。 但是，上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Outer Ethernet Frame）。 为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。 所以，跟 UDP 模式类似，在宿主机看来，它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备，发起了一次普通的 UDP 链接。它哪里会知道，这个 UDP 包里面，其实是一个完整的二层数据帧。这是不是跟特洛伊木马的故事非常像呢？不过，不要忘了，一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对应的宿主机地址是什么。也就是说，这个 UDP 包该发给哪台宿主机呢？在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。 不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示： 123# 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:375e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent 可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即：发往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node 2，UDP 包要发往的目的地就找到了。 所以接下来的流程，就是一个正常的、宿主机网络上的封包工作。 我们知道，UDP 包是一个四层数据包，所以 Linux 内核会在它前面加上一个 IP 头，即原理图中的 Outer IP Header，组成一个 IP 包。并且，在这个 IP 头里，会填上前面通过 FDB 查询出来的目的主机的 IP 地址，即 Node 2 的 IP 地址 10.168.0.3。然后，Linux 内核再在这个 IP 包前面加上二层数据帧头，即原理图中的 Outer Ethernet Header，并把 Node 2 的 MAC 地址填进去。这个 MAC 地址本身，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护。这时候，我们封装出来的“外部数据帧”的格式，如下所示： 这样，封包工作就宣告完成了。接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。以上，就是 Flannel VXLAN 模式的具体工作原理了。 Flannel 能负责保证二层网络（MAC 地址）的连通性吗？ CNI网络插件 UDP与VXLAN都需要容器连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作 网络插件是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。 Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0 以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已 Kubernetes 为 Flannel 分配的子网范围是 10.244.0.0/16。这个参数可以在部署的时候指定 1$ kubeadm init --pod-network-cidr=10.244.0.0/16 也可以在部署完成后，通过修改 kube-controller-manager 的配置文件来指定。这时候，假设 Infra-container-1 要访问 Infra-container-2（也就是 Pod-1 要访问 Pod-2），这个 IP 包的源地址就是 10.244.0.2，目的 IP 地址是 10.244.1.3。而此时，Infra-container-1 里的 eth0 设备，同样是以 Veth Pair 的方式连接在 Node 1 的 cni0 网桥上。所以这个 IP 包就会经过 cni0 网桥出现在宿主机上 此时，Node 1 上的路由表，如下所示： 12345678# 在Node 1上$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.244.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni010.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 因为我们的 IP 包的目的 IP 地址是 10.244.1.3，所以它只能匹配到第二条规则，也就是 10.244.1.0 对应的这条路由规则。可以看到，这条规则指定了本机的 flannel.1 设备进行处理。并且，flannel.1 在处理完后，要将 IP 包转发到的网关（Gateway），正是“隧道”另一端的 VTEP 设备，也就是 Node 2 的 flannel.1 设备。所以，接下来的流程，就跟上一篇文章中介绍过的 Flannel VXLAN 模式完全一样了 需要注意的是，CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。而此时，如果你用 docker run 单独启动一个容器，那么 Docker 项目还是会把这个容器连接到 docker0 网桥上。所以这个容器的 IP 地址，一定是属于 docker0 网桥的 172.17.0.0/16 网段。 Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面： 一方面，Kubernetes 项目并没有使用 Docker 的网络模型（CNM），所以它并不希望、也不具备配置 docker0 网桥的能力； 另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关。 Kubernetes 创建一个 Pod 的第一步，就是创建并启动一个 Infra 容器，用来“hold”住这个 Pod 的 Network Namespace。所以，CNI 的设计思想，就是：Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。 那么它是怎么完成这个网络栈的配置的呢？ 在部署 Kubernetes 的时候，有一个步骤是安装 kubernetes-cni 包，它的目的就是在宿主机上安装 CNI 插件所需的基础可执行文件，在安装完成后，你可以在宿主机的 /opt/cni/bin 目录下看到它们，如下所示： 1234567891011121314$ ls -al /opt/cni/bin/total 73088-rwxr-xr-x 1 root root 3890407 Aug 17 2017 bridge-rwxr-xr-x 1 root root 9921982 Aug 17 2017 dhcp-rwxr-xr-x 1 root root 2814104 Aug 17 2017 flannel-rwxr-xr-x 1 root root 2991965 Aug 17 2017 host-local-rwxr-xr-x 1 root root 3475802 Aug 17 2017 ipvlan-rwxr-xr-x 1 root root 3026388 Aug 17 2017 loopback-rwxr-xr-x 1 root root 3520724 Aug 17 2017 macvlan-rwxr-xr-x 1 root root 3470464 Aug 17 2017 portmap-rwxr-xr-x 1 root root 3877986 Aug 17 2017 ptp-rwxr-xr-x 1 root root 2605279 Aug 17 2017 sample-rwxr-xr-x 1 root root 2808402 Aug 17 2017 tuning-rwxr-xr-x 1 root root 3475750 Aug 17 2017 vlan 这些 CNI 的基础可执行文件，按照功能可以分为三类： 第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件。比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan。我在前面提到过的 Flannel、Weave 等项目，都属于“网桥”类型的 CNI 插件。所以在具体的实现中，它们往往会调用 bridge 这个二进制文件。 第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。 第三类，是由 CNI 社区维护的内置 CNI 插件。比如： flannel，就是专门为 Flannel 项目提供的 CNI 插件； tuning，是一个通过 sysctl 调整网络设备参数的二进制文件； portmap，是一个通过 iptables 配置端口映射的二进制文件； bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。 从这些二进制文件中，我们可以看到，如果要实现一个给 Kubernetes 用的容器网络方案，其实需要做两部分工作，以 Flannel 项目为例： 首先，实现这个网络方案本身。这一部分需要编写的，其实就是 flanneld 进程里的主要逻辑。比如，创建和配置 flannel.1 设备、配置宿主机路由、配置 ARP 和 FDB 表里的信息等等。然后，实现该网络方案对应的 CNI 插件。这一部分主要需要做的，就是配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。 由于 Flannel 项目对应的 CNI 插件已经被内置了，所以它无需再单独安装。而对于 Weave、Calico 等其他项目来说，我们就必须在安装插件的时候，把对应的 CNI 插件的可执行文件放在 /opt/cni/bin/ 目录下。 实际上，对于 Weave、Calico 这样的网络方案来说，它们的 DaemonSet 只需要挂载宿主机的 /opt/cni/bin/，就可以实现插件可执行文件的安装了。你可以想一下具体应该怎么做，就当作一个课后小问题留给你去实践了。 接下来，你就需要在宿主机上安装 flanneld（网络方案本身）。而在这个过程中，flanneld 启动后会在每台宿主机上生成它对应的 CNI 配置文件（它其实是一个 ConfigMap），从而告诉 Kubernetes，这个集群要使用 Flannel 作为容器网络方案。 这个 CNI 配置文件的内容如下所示： 12345678910111213141516171819$ cat /etc/cni/net.d/10-flannel.conflist &#123; \"name\": \"cbr0\", \"plugins\": [ &#123; \"type\": \"flannel\", \"delegate\": &#123; \"hairpinMode\": true, \"isDefaultGateway\": true &#125; &#125;, &#123; \"type\": \"portmap\", \"capabilities\": &#123; \"portMappings\": true &#125; &#125; ]&#125; Soft multi-tenancy 为什么说 Kubernetes 只有 soft multi-tenancy？ 参考链接 《极客时间-深入剖析 Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门13-容器网络之Service和Ingress","slug":"K8S/k8s入门13-容器网络之Service和Ingress","date":"2022-02-20T16:35:06.000Z","updated":"2022-02-20T16:36:12.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门13-容器网络之Service和Ingress/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A813-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8BService%E5%92%8CIngress/","excerpt":"","text":"Service 暴露给外界的三种方法中有一个叫作 LoadBalancer 类型的 Service，它会为你在 Cloud Provider（比如：Google Cloud 或者 OpenStack）里创建一个与该 Service 对应的负载均衡服务 由于每个 Service 都要有一个负载均衡服务，为什么没有一个内置一个全局的负载均衡器。通过访问的 URL，把请求转发给不同的后端 Service。 这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。可以说是 Service 的“Service”。 举个例子，假如我现在有这样一个站点：https://cafe.example.com。其中， https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。 https://cafe.example.com/tea，对应的则是“茶水点餐系统”。 这两个系统，分别由名叫 coffee 和 tea 这样两个 Deployment 来提供服务 如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？ 在 Kubernetes 里就需要通过 Ingress 对象来描述，如下所示： 123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: cafe-ingressspec: tls: - hosts: - cafe.example.com secretName: cafe-secret rules: - host: cafe.example.com http: paths: - path: /tea backend: serviceName: tea-svc servicePort: 80 - path: /coffee backend: serviceName: coffee-svc servicePort: 80 在 Kubernetes 里，文件的 rules 字段叫作：IngressRule IngressRule 的 Key 就叫做：host。它必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。 而 host 字段定义的值就是这个 Ingress 的入口。当用户访问 cafe.example.com 的时候，实际上访问到的是这个 Ingress 对象。这样，Kubernetes 就能使用 IngressRule 来对你的请求进行下一步转发。 接下来 IngressRule 规则的定义，则依赖于 path 字段。你可以简单地理解为，这里的每一个 path 都对应一个后端 Service。所以在我们的例子里，我定义了两个 path，它们分别对应 coffee 和 tea 这两个 Deployment 的 Service（即：coffee-svc 和 tea-svc） 所以所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。而这个代理服务对应的转发规则，就是 IngressRule。跟 Nginx、HAproxy 等项目的配置文件的写法是一致的。Kubernetes 的用户就无需关心 Ingress 的具体细节了。选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可 这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。 以最常用的 Nginx Ingress Controller 为例，在用 kubeadm 部署的 Bare-metal 环境中，实践 Ingress 机制的使用过程。 部署 Nginx Ingress Controller 的方法非常简单，如下所示： 1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml 在mandatory.yaml这个文件里，正是 Nginx 官方为你维护的 Ingress Controller 的定义。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: ... spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE - name: http valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 上述是使用 nginx-ingress-controller 镜像的 Pod。 注意: 这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数{POD_NAMESPACE}。而这个信息，当然是通过 Downward API 拿到的，即：Pod 的 env 字段里的定义（env.valueFrom.fieldRef.fieldPath） 而这个 Pod 本身，就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。 而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置 此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。可以看到，一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门12-容器网络之Service调试访问","slug":"K8S/k8s入门12-容器网络之Service调试访问","date":"2022-02-20T16:34:52.000Z","updated":"2022-02-20T16:36:17.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门12-容器网络之Service调试访问/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A812-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8BService%E8%B0%83%E8%AF%95%E8%AE%BF%E9%97%AE/","excerpt":"","text":"Service 的访问入口，其实就是每台宿主机上由 kube-proxy 生成的 iptables 规则，以及 kube-dns 生成的 DNS 记录。Service 的访问信息在 Kubernetes 集群之外是无效的 如何从外部(Kubernetes 集群之外)，访问到 Kubernetes 里创建的 Service 外部访问Service 最常用的办法：NodePort 123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: my-nginx labels: run: my-nginxspec: type: NodePort ports: - nodePort: 8080 targetPort: 80 protocol: TCP name: http - nodePort: 443 protocol: TCP name: https selector: run: my-nginx 在 Service 定义中声明它的类型是，type=NodePort。然后，在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口 如果不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，可通过 kube-apiserver 的–service-node-port-range 参数来修改它 这时候，要访问这个 Service，你只需要访问 1&lt;任何一台宿主机的IP地址&gt;:8080 就可以访问到某一个被代理的 Pod 的 80 端口了。NodePort 模式下，kube-proxy 要做的就是在每台宿主机上生成这样一条 iptables 规则： 1-A KUBE-NODEPORTS -p tcp -m comment --comment \"default/my-nginx: nodePort\" -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了。需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示： 1-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE 可以看到，这条规则设置在 POSTROUTING 检查点，也就是说，它给即将离开这台主机的 IP 包，进行了一次 SNAT 操作，将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。 当然，这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行（否则普通的 IP 包就被影响了）。而 iptables 做这个判断的依据，就是查看该 IP 包是否有一个“0x4000”的“标志”。你应该还记得，这个标志正是在 IP 包被执行 DNAT 操作之前被打上去的。 可是，为什么一定要对流出的包做 SNAT操作呢？ 这里的原理其实很简单，如下所示： 123456789 client \\ ^ \\ \\ v \\ node 1 &lt;--- node 2 | ^ SNAT | | ---&gt; v |endpoint 当一个外部的 client 通过 node 2 的地址访问一个 Service 的时候，node 2 上的负载均衡规则，就可能把这个 IP 包转发给一个在 node 1 上的 Pod。这里没有任何问题。 而当 node 1 上的这个 Pod 处理完请求之后，它就会按照这个 IP 包的源地址发出回复。 可是，如果没有做 SNAT 操作的话，这时候，被转发来的 IP 包的源地址就是 client 的 IP 地址。所以此时，Pod 就会直接将回复发给client。对于 client 来说，它的请求明明发给了 node 2，收到的回复却来自 node 1，这个 client 很可能会报错。 所以，在上图中，当 IP 包离开 node 2 之后，它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址。这样，Pod 在处理完成之后就会先回复给 node 2（而不是 client），然后再由 node 2 发送给 client。 当然，这也就意味着这个 Pod 只知道该 IP 包来自于 node 2，而不是外部的 client。对于 Pod 需要明确知道所有请求来源的场景来说，这是不可以的。 所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。 而这个机制的实现原理也非常简单：这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示： 123456789 client ^ / \\ / / \\ / v X node 1 node 2 ^ | | | | vendpoint 当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。 从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 LoadBalancer 类型的 Service，如下所示： 123456789101112---kind: ServiceapiVersion: v1metadata: name: example-servicespec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer 在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。 而第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 ExternalName。举个例子： 1234567kind: ServiceapiVersion: v1metadata: name: my-servicespec: type: ExternalName externalName: my.database.example.com 在上述 Service 的 YAML 文件中，我指定了一个 externalName=my.database.example.com 的字段。而且你应该会注意到，这个 YAML 文件里不需要指定 selector。 这时候，当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。 此外，Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址，比如下面这个例子： 1234567891011121314kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 在上述 Service 中，我为它指定的 externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。不过，在这里 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点。你可以想一想这是为什么。 实际上，在理解了 Kubernetes Service 机制的工作原理之后，很多与 Service 相关的问题，其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。 比如，当你的 Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常： 1234567# 在一个Pod里执行$ nslookup kubernetes.defaultServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local 如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。 而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints： 123$ kubectl get endpoints hostnamesNAME ENDPOINTShostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 需要注意的是，如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。 而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示： 12345678910I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container \"/kube-proxy\"I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier.I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns-tcp\" to [10.244.1.3:53]I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns\" to [10.244.1.3:53]I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for \"default/kubernetes:https\" to [10.240.0.2:443]I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from masterI1027 22:14:54.040048 5063 proxier.go:294] Adding new service \"default/kubernetes:https\" at 10.0.0.1:443/TCPI1027 22:14:54.040154 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns\" at 10.0.0.10:53/UDPI1027 22:14:54.040223 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns-tcp\" at 10.0.0.10:53/TCP 如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。而一个 iptables 模式的 Service 对应的规则，我在上一篇以及这一篇文章里已经全部介绍到了，它们包括： KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应； KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应； KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致； 如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。 通过查看这些链的数量、转发目的地址、端口、过滤条件等信息，你就能很容易发现一些异常的蛛丝马迹。 当然，还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己。这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。关于 Hairpin 的原理我在前面已经介绍过，这里就不再赘述了。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。 其中，在 hairpin-veth 模式下，你应该能看到 CNI 网桥对应的各个 VETH 设备，都将 Hairpin 模式设置为了 1，如下所示： 123$ for d in &#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;veth*&#x2F;hairpin_mode; do echo &quot;$d &#x3D; $(cat $d)&quot;; done&#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;veth4bfbfe74&#x2F;hairpin_mode &#x3D; 1&#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;vethfc2a18c5&#x2F;hairpin_mode &#x3D; 1 而如果是 promiscuous-bridge 模式的话，你应该看到 CNI 网桥的混杂模式（PROMISC）被开启，如下所示： 12$ ifconfig cni0 |grep PROMISCUP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1 总结： 从外部访问Service的三种方式 NodePort LoadBalancer External Name 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护 Kubernetes 里面的 Service 和 DNS 机制，也都不具备强多租户能力。比如，在多租户情况下，每个租户应该拥有一套独立的 Service 规则（Service 只应该看到和代理同一个租户下的 Pod）。再比如 DNS，在多租户情况下，每个租户应该拥有自己的 kube-dns（kube-dns 只应该为同一个租户下的 Service 和 Pod 创建 DNS Entry） 为什么Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点？ 因为k8s只是在集群中的每个节点上创建了一个 externalIPs 与kube-ipvs0网卡的绑定关系. 若流量都无法路由到任意的一个k8s节点,那自然无法将流量转给具体的service 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门11-容器网络之服务发现","slug":"K8S/k8s入门11-容器网络之服务发现","date":"2022-02-20T16:34:37.000Z","updated":"2022-02-20T16:36:23.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门11-容器网络之服务发现/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A811-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。 一个Service例子： 123456789101112apiVersion: v1kind: Servicemetadata: name: hostnamesspec: selector: app: hostnames ports: - name: default protocol: TCP port: 80 targetPort: 9376 使用 selector 字段来声明这个 Service 只代理携带了 app=hostnames 标签的 Pod。Service 的 80 端口，代理的是 Pod 的 9376 端口 然后应用的Deployment为： 1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: hostnamesspec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: - name: hostnames image: k8s.gcr.io/serve_hostname ports: - containerPort: 9376 protocol: TCP 这个应用的作用就是每次访问 9376 端口时，返回它自己的 hostname。 被 selector 选中的 Pod，称为 Service 的 Endpoints，可使用 kubectl get ep 命令看到它们，如下所示： 123$ kubectl get endpoints hostnamesNAME ENDPOINTShostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉 通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod ： 12345678910111213$ kubectl get svc hostnamesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhostnames ClusterIP 10.0.1.175 &lt;none&gt; 80/TCP 5s$ curl 10.0.1.175:80hostnames-0uton$ curl 10.0.1.175:80hostnames-yp2kp$ curl 10.0.1.175:80hostnames-bvc05 这个 VIP 地址是 Kubernetes 自动为 Service 分配的。 通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。这种方式称为：ClusterIP 模式的 Service Service 是由 kube-proxy 组件，加上 iptables 来共同实现的 举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示： 1-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \"default/hostnames: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。 10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。 KUBE-SVC-NWV5X2332I4OT4T3 规则，实际上是一组规则的集合，如下所示： 123-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -j KUBE-SEP-57KPRZ3JQVENLNBR 这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。 而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。 这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。 通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示 123456789-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.3.6:9376-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.1.7:9376-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.2.3:9376 可以看到，这三条链，其实是三条 DNAT 规则。但在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。而 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。 这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的 kube-proxy - IPVS模式 kube-proxy 通过 iptables 处理 Service 的过程，需要在宿主机上设置相当多的 iptables 规则，并在控制循环里不断地刷新这些规则来确保它们始终是正确的 当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。 解决办法：IPVS模式的Service IPVS 模式的工作原理：当创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示： 123456# ip addr ... 73：kube-ipvs0：&lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff inet 10.0.1.175/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示： 12345678# ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.102.128.4:80 rr -&gt; 10.244.3.6:9376 Masq 1 0 0 -&gt; 10.244.1.7:9376 Masq 1 0 0 -&gt; 10.244.2.3:9376 Masq 1 0 0 三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。 相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。 不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。所以，在大规模集群里，建议为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。 service 与 DNS 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。 对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：…svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。 而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：…svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。 此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：…pod.cluster.local。这条记录指向 Pod 的 IP 地址。而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：…svc.cluster.local。这条记录也指向 Pod 的 IP 地址。 但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain 字段，那么这时候 Pod 的 A 记录就会变成：…svc.cluster.local，比如： 12345678910111213141516171819202122232425262728apiVersion: v1kind: Servicemetadata: name: default-subdomainspec: selector: name: busybox clusterIP: None ports: - name: foo port: 1234 targetPort: 1234---apiVersion: v1kind: Podmetadata: name: busybox1 labels: name: busyboxspec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - \"3600\" name: busybox 在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。 在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。 总结： 当服务（Pod）的 IP 地址是不固定的且没办法提前获知时，该如何通过一个固定的方式访问到这个 Pod 呢？ ClusterIP 模式的 Service 为你提供的，就是一个 Pod 的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过 Label 确定的。 Headless Service 提供的是一个 Pod 的稳定的 DNS 名字，并且名字是可以通过 Pod 名字和 Service 名字拼接出来的。 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门10-容器网络之多租户","slug":"K8S/k8s入门10-容器网络之多租户","date":"2022-02-20T16:34:25.000Z","updated":"2022-02-20T16:36:30.000Z","comments":true,"path":"2022/02/21/K8S/k8s入门10-容器网络之多租户/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A810-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E7%A7%9F%E6%88%B7/","excerpt":"","text":"什么是多租户：一种软件架构规范，运行在服务器上的单一实例，可服务多个客户或组织（租户）,一个满足多租户规范的软件应用需要对数据和配置进行隔离，每一个租户都有自己虚拟的实例 Kubernetes 的网络模型，以及前面这些网络方案的实现，都只关注网络的“连通”，却不关心“隔离”，那么 Kubernetes 的网络方案对“隔离”是如何考虑的？ NetworkPolicy 在 Kubernetes 里，网络隔离能力的定义，是依靠一种专门的 API 对象来描述的，即：NetworkPolicy 12345678910111213141516171819202122232425262728293031323334apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 Kubernetes 里的 Pod 默认都是“允许所有”（Accept All）的，即：Pod 可以接收来自任何发送方的请求；或者，向任何接收方发送请求。而如果你要对这个情况作出限制，就必须通过 NetworkPolicy 对象来指定 而在上面这个例子里，你首先会看到 podSelector 字段。它的作用，就是定义这个 NetworkPolicy 的限制范围，比如：当前 Namespace 里携带了 role=db 标签的 Pod 而如果你把 podSelector 字段留空： 12spec: podSelector: &#123;&#125; 那么这个 NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod。而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。而 NetworkPolicy 定义的规则，其实就是“白名单”。 例如，在我们上面这个例子里，我在 policyTypes 字段，定义了这个 NetworkPolicy 的类型是 ingress 和 egress，即：它既会影响流入（ingress）请求，也会影响流出（egress）请求。 然后，在 ingress 字段里，我定义了 from 和 ports，即：允许流入的“白名单”和端口。其中，这个允许流入的“白名单”里，我指定了三种并列的情况，分别是：ipBlock、namespaceSelector 和 podSelector。 而在 egress 字段里，我则定义了 to 和 ports，即：允许流出的“白名单”和端口。这里允许流出的“白名单”的定义方法与 ingress 类似。只不过，这一次 ipblock 字段指定的，是目的地址的网段。 综上所述，这个 NetworkPolicy 对象，指定的隔离规则如下所示： 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。 Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：a. default Namespace 里的，携带了 role=fronted 标签的 Pod；b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。 Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。 需要注意的是，定义一个 NetworkPolicy 对象的过程，容易犯错的是“白名单”部分（from 和 to 字段） 12345678910... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ... 像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。 而下面这个例子，虽然看起来类似，但是它定义的规则却完全不同： 12345678910... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 注意看，这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。 此外，如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用，你的 CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的。 在具体实现上，凡是支持 NetworkPolicy 的 CNI 网络插件，都维护着一个 NetworkPolicy Controller，通过控制循环的方式对 NetworkPolicy 对象的增删改查做出响应，然后在宿主机上完成 iptables 规则的配置工作。 在 Kubernetes 生态里，目前已经实现了 NetworkPolicy 的网络插件包括 Calico、Weave 和 kube-router 等多个项目，但是并不包括 Flannel 项目。 所以说，如果想要在使用 Flannel 的同时还使用 NetworkPolicy 的话，你就需要再额外安装一个网络插件，比如 Calico 项目，来负责执行 NetworkPolicy。 安装 Flannel + Calico 的流程非常简单，你直接参考这个文档一键安装即可 网络隔离 以三层网络插件为例(比如 Calico 和 kube-router)，分析一下这部分的原理。简单的NetworkPolicy 策略 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db ingress: - from: - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: tcp port: 6379 可以看到，我们指定的 ingress“白名单”，是任何 Namespace 里，携带 project=myproject 标签的 Namespace 里的 Pod；以及 default Namespace 里，携带了 role=frontend 标签的 Pod。允许被访问的端口是：6379。而被隔离的对象，是所有携带了 role=db 标签的 Pod。 那么这个时候，Kubernetes 的网络插件就会使用这个 NetworkPolicy 的定义，在宿主机上生成 iptables 规则。这个过程，我可以通过如下所示的一段 Go 语言风格的伪代码来为你描述： 1234567for dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址 for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址 for port, protocol := range ingress.ports &#123; iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT &#125; &#125;&#125; 可以看到，这是一条最基本的、通过匹配条件决定下一步动作的 iptables 规则。 这条规则的名字是 KUBE-NWPLCY-CHAIN，含义是：当 IP 包的源地址是 srcIP、目的地址是 dstIP、协议是 protocol、目的端口是 port 的时候，就允许它通过（ACCEPT）。 而正如这段伪代码所示，匹配这条规则所需的这四个参数，都是从 NetworkPolicy 对象里读取出来的 Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。 在设置好上述“隔离”规则之后，网络插件还需要想办法，将所有对被隔离 Pod 的访问请求，都转发到上述 KUBE-NWPLCY-CHAIN 规则上去进行匹配。并且，如果匹配不通过，这个请求应该被“拒绝”。 在 CNI 网络插件中，上述需求可以通过设置两组 iptables 规则来实现。 第一组规则，负责“拦截”对被隔离 Pod 的访问请求。生成这一组规则的伪代码，如下所示： 1234567for pod := range 该Node上的所有Pod &#123; if pod是networkpolicy.spec.podSelector选中的 &#123; iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN ... &#125;&#125; IPTABLE iptables 规则使用到了内置链：FORWARD。它是什么意思呢？说到这里，我就得为你稍微普及一下 iptables 的知识了。实际上，iptables 只是一个操作 Linux 内核 Netfilter 子系统的“界面”。顾名思义，Netfilter 子系统的作用，就是 Linux 内核里挡在“网卡”和“用户态进程”之间的一道“防火墙”。它们的关系，可以用如下的示意图来表示： 可以看到，这幅示意图中，IP 包“一进一出”的两条路径上，有几个关键的“检查点”，它们正是 Netfilter 设置“防火墙”的地方。在 iptables 中，这些“检查点”被称为：链（Chain）。这是因为这些“检查点”对应的 iptables 规则，是按照定义顺序依次进行匹配的。这些“检查点”的具体工作原理，可以用如下所示的示意图进行描述： 可以看到，当一个 IP 包通过网卡进入主机之后，它就进入了 Netfilter 定义的流入路径（Input Path）里。在这个路径中，IP 包要经过路由表路由来决定下一步的去向。而在这次路由之前，Netfilter 设置了一个名叫 PREROUTING 的“检查点”。在 Linux 内核的实现里，所谓“检查点”实际上就是内核网络协议栈代码里的 Hook（比如，在执行路由判断的代码之前，内核会先调用 PREROUTING 的 Hook）。 而在经过路由之后，IP 包的去向就分为了两种： 第一种，继续在本机处理； 第二种，被转发到其他目的地 我们先说一下 IP 包的第一种去向。这时候，IP 包将继续向上层协议栈流动。在它进入传输层之前，Netfilter 会设置一个名叫 INPUT 的“检查点”。到这里，IP 包流入路径（Input Path）结束。 接下来，这个 IP 包通过传输层进入用户空间，交给用户进程处理。而处理完成后，用户进程会通过本机发出返回的 IP 包。这时候，这个 IP 包就进入了流出路径（Output Path）。此时，IP 包首先还是会经过主机的路由表进行路由。 路由结束后，Netfilter 就会设置一个名叫 OUTPUT 的“检查点”。然后，在 OUTPUT 之后，再设置一个名叫 POSTROUTING“检查点”。你可能会觉得奇怪，为什么在流出路径结束后，Netfilter 会连着设置两个“检查点”呢？ 这就要说到在流入路径里，路由判断后的第二种去向了。在这种情况下，这个 IP 包不会进入传输层，而是会继续在网络层流动，从而进入到转发路径（Forward Path）。在转发路径中，Netfilter 会设置一个名叫 FORWARD 的“检查点”。而在 FORWARD“检查点”完成后，IP 包就会来到流出路径。而转发的 IP 包由于目的地已经确定，它就不会再经过路由，也自然不会经过 OUTPUT，而是会直接来到 POSTROUTING“检查点”。所以说，POSTROUTING 的作用，其实就是上述两条路径，最终汇聚在一起的“最终检查点”。 需要注意的是，在有网桥参与的情况下，上述 Netfilter 设置“检查点”的流程，实际上也会出现在链路层（二层），并且会跟我在上面讲述的网络层（三层）的流程有交互。 这些链路层的“检查点”对应的操作界面叫作 ebtables。所以，准确地说，数据包在 Linux Netfilter 子系统里完整的流动过程，其实应该如下所示（这是一幅来自Netfilter 官方的原理图，建议你点击图片以查看大图）： 可以看到，我前面为你讲述的，正是上图中绿色部分，也就是网络层的 iptables 链的工作流程。 另外，你应该还能看到，每一个白色的“检查点”上，还有一个绿色的“标签”，比如：raw、nat、filter 等等。 在 iptables 里，这些标签叫作：表。比如，同样是 OUTPUT 这个“检查点”，filter Output 和 nat Output 在 iptables 里的语法和参数，就完全不一样，实现的功能也完全不同。 所以说，iptables 表的作用，就是在某个具体的“检查点”（比如 Output）上，按顺序执行几个不同的检查动作（比如，先执行 nat，再执行 filter）。 在理解了 iptables 的工作原理之后，我们再回到 NetworkPolicy 上来。这时候，前面由网络插件设置的、负责“拦截”进入 Pod 的请求的三条 iptables 规则，就很容易读懂了： 123iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAINiptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN... 其中，第一条 FORWARD 链“拦截”的是一种特殊情况：它对应的是同一台宿主机上容器之间经过 CNI 网桥进行通信的流入数据包。其中，–physdev-is-bridged 的意思就是，这个 FORWARD 链匹配的是，通过本机上的网桥设备，发往目的地址是 podIP 的 IP 包。 kube-router 其实是一个简化版的 Calico，它也使用 BGP 来维护路由信息，但是使用 CNI bridge 插件负责跟 Kubernetes 进行交互。 而第二条 FORWARD 链“拦截”的则是最普遍的情况，即：容器跨主通信。这时候，流入容器的数据包都是经过路由转发（FORWARD 检查点）来的。 不难看到，这些规则最后都跳转（即：-j）到了名叫 KUBE-POD-SPECIFIC-FW-CHAIN 的规则上。它正是网络插件为 NetworkPolicy 设置的第二组规则。而这个 KUBE-POD-SPECIFIC-FW-CHAIN 的作用，就是做出“允许”或者“拒绝”的判断。这部分功能的实现，可以简单描述为下面这样的 iptables 规则： 12iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAINiptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable 可以看到，首先在第一条规则里，我们会把 IP 包转交给前面定义的 KUBE-NWPLCY-CHAIN 规则去进行匹配。按照我们之前的讲述，如果匹配成功，那么 IP 包就会被“允许通过”。 而如果匹配失败，IP 包就会来到第二条规则上。可以看到，它是一条 REJECT 规则。通过这条规则，不满足 NetworkPolicy 定义的请求就会被拒绝掉，从而实现了对该容器的“隔离”。 以上，就是 CNI 网络插件实现 NetworkPolicy 的基本方法了。当然，对于不同的插件来说，上述实现过程可能有不同的手段，但根本原理是不变的。 NetworkPolicy 实际上只是宿主机上的一系列 iptables 规则，Kubernetes 负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力 它使得指定的 Namespace（比如 my-namespace）里的所有 Pod，都不能接收任何 Ingress 请求。 job，cronjob这类计算型pod不需要也不应该对外提供服务，可以拒绝所有流入流量，提高系统安全。 参考链接 《极客时间-深入剖析 Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"算法之美-令牌桶算法","slug":"Algorithms To Live By/算法之美-令牌桶算法","date":"2022-02-20T16:22:31.000Z","updated":"2022-07-21T01:53:50.000Z","comments":true,"path":"2022/02/21/Algorithms To Live By/算法之美-令牌桶算法/","link":"","permalink":"http://xboom.github.io/2022/02/21/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E4%BB%A4%E7%89%8C%E6%A1%B6%E7%AE%97%E6%B3%95/","excerpt":"","text":"在高并发系统中有三把利器用来保护系统：缓存、降级和限流 其中常用的限流算法有 **漏桶算法 **和 令牌桶算法 漏桶算法 把请求比作是水，水来了都先放进桶里，并以限定的速度出水，当水来得过猛而出水不够快时就会导致水直接溢出，即拒绝服务 漏斗有一个进水口 和 一个出水口，出水口以一定速率出水，并且有一个最大出水速率： 在漏斗中没有水的时候， 如果进水速率小于等于最大出水速率，那么，出水速率等于进水速率，此时，不会积水 如果进水速率大于最大出水速率，那么，漏斗以最大速率出水，此时，多余的水会积在漏斗中 在漏斗中有水的时候 出水口以最大速率出水 如果漏斗未满，且有进水的话，那么这些水会积在漏斗中 如果漏斗已满，且有进水的话，那么这些水会溢出到漏斗之外 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package ratelimit // import \"go.uber.org/ratelimit\"import ( \"time\" \"sync/atomic\" \"unsafe\")type state struct &#123; last time.Time sleepFor time.Duration&#125;type atomicLimiter struct &#123; state unsafe.Pointer //用于存储上一次执行的时间以及需要sleep的时间 padding [56]byte //是一个无意义的填充数据，为了提高性能，避免 cpu 缓存的 false sharing perRequest time.Duration //只单位，默认为秒 maxSlack time.Duration //松弛时间，也就是可以允许的突发流量的大小, 默认是 Pre/10 clock Clock //时钟，用于在测试的时候可以 mock 掉不使用真实的时间&#125;// newAtomicBased returns a new atomic based limiter.func newAtomicBased(rate int, opts ...Option) *atomicLimiter &#123; // TODO consider moving config building to the implementation // independent code. config := buildConfig(opts) perRequest := config.per / time.Duration(rate) l := &amp;atomicLimiter&#123; perRequest: perRequest, maxSlack: -1 * time.Duration(config.slack) * perRequest, clock: config.clock, &#125; initialState := state&#123; last: time.Time&#123;&#125;, sleepFor: 0, &#125; atomic.StorePointer(&amp;l.state, unsafe.Pointer(&amp;initialState)) return l&#125;// Take blocks to ensure that the time spent between multiple// Take calls is on average time.Second/rate.func (t *atomicLimiter) Take() time.Time &#123; var ( newState state // 状态 taken bool // 用于表示原子操作是否成功 interval time.Duration // 需要 sleep 的时间 ) for !taken &#123; // 如果 CAS 操作不成功就一直尝试 now := t.clock.Now() //获取当前时间 previousStatePointer := atomic.LoadPointer(&amp;t.state) // load 出上一次调用的时间 oldState := (*state)(previousStatePointer) newState = state&#123; last: now, sleepFor: oldState.sleepFor, &#125; // 如果 last 是零值的话，表示之前就没用过，直接保存返回即可 if oldState.last.IsZero() &#123; taken = atomic.CompareAndSwapPointer(&amp;t.state, previousStatePointer, unsafe.Pointer(&amp;newState)) continue &#125; // sleepFor 是需要睡眠的时间，由于引入了松弛时间，所以 sleepFor 可能是一个 // maxSlack ~ 0 之间的一个值，所以这里需要将现在的需要 sleep 的时间和上一次 // sleepFor 的值相加 newState.sleepFor += t.perRequest - now.Sub(oldState.last) // 如果距离上一次调用已经很久了，sleepFor 可能会是一个很小的值 // 最小值只能是 maxSlack 的大小 if newState.sleepFor &lt; t.maxSlack &#123; newState.sleepFor = t.maxSlack &#125; // 如果 sleepFor 大于 0 的话，计算出需要 sleep 的时间 // 然后将 state.sleepFor 置零 if newState.sleepFor &gt; 0 &#123; newState.last = newState.last.Add(newState.sleepFor) interval, newState.sleepFor = newState.sleepFor, 0 &#125; // 保存状态 taken = atomic.CompareAndSwapPointer(&amp;t.state, previousStatePointer, unsafe.Pointer(&amp;newState)) &#125; t.clock.Sleep(interval) return newState.last&#125; 令牌桶算法 令牌桶算法的原理是系统以恒定的速率产生令牌，然后把令牌放到令牌桶中，令牌桶有一个容量，当令牌桶满了的时候，再向其中放令牌，那么多余的令牌会被丢弃；当想要处理一个请求的时候，需要从令牌桶中取出一个令牌，如果此时令牌桶中没有令牌，那么则拒绝该请求 源码：https://github.com/beefsack/go-rate/blob/master/rate.go 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package rateimport ( \"container/list\" \"sync\" \"time\")// A RateLimiter limits the rate at which an action can be performed. It// applies neither smoothing (like one could achieve in a token bucket system)// nor does it offer any conception of warmup, wherein the rate of actions// granted are steadily increased until a steady throughput equilibrium is// reached.type RateLimiter struct &#123; limit int interval time.Duration mtx sync.Mutex times list.List //双向链表&#125;// New creates a new rate limiter for the limit and interval.func New(limit int, interval time.Duration) *RateLimiter &#123; lim := &amp;RateLimiter&#123; limit: limit, interval: interval, &#125; lim.times.Init() return lim&#125;// Wait blocks if the rate limit has been reached. Wait offers no guarantees// of fairness for multiple actors if the allowed rate has been temporarily// exhausted.func (r *RateLimiter) Wait() &#123; for &#123; ok, remaining := r.Try() if ok &#123; break &#125; time.Sleep(remaining) &#125;&#125;// Try returns true if under the rate limit, or false if over and the// remaining time before the rate limit expires.func (r *RateLimiter) Try() (ok bool, remaining time.Duration) &#123; r.mtx.Lock() defer r.mtx.Unlock() now := time.Now() if l := r.times.Len(); l &lt; r.limit &#123; r.times.PushBack(now) return true, 0 &#125; frnt := r.times.Front() if diff := now.Sub(frnt.Value.(time.Time)); diff &lt; r.interval &#123; return false, r.interval - diff &#125; frnt.Value = now r.times.MoveToBack(frnt) return true, 0&#125; 漏桶VS令牌桶 漏桶算法 能够强行限制数据的传输速率， 令牌桶算法 在能够限制数据的平均传输速率外，只要桶中存在令牌，就允许突发地传输数据直到达到用户配置的门限，所以也适合于具有突发特性的流量 参考链接 https://www.cnblogs.com/xuwc/p/9123078.html https://www.jianshu.com/p/d6250493308b https://segmentfault.com/a/1190000015967922","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"Mysql-08-备份与恢复","slug":"MySql/MySql-08-备份与恢复","date":"2022-02-15T07:47:24.000Z","updated":"2023-05-28T09:05:39.404Z","comments":true,"path":"2022/02/15/MySql/MySql-08-备份与恢复/","link":"","permalink":"http://xboom.github.io/2022/02/15/MySql/MySql-08-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/","excerpt":"","text":"概述 根据备份方法的不同，备份可以分为： 热备 Hot Backup：指数据库运行中直接备份，对正在运行的数据库没有任何影响，也称在线备份(Online Backup) 冷备 Code Backup: 指备份操作是在数据库停止的情况下，也称离线备份(Offline Backup) 温备份 Warm Backup: 指数据库运行中进行的备份操作，但是对当前数据库操作有影响。如加一个全局读锁保证数据一致性 按照备份后的文件的内容可以分为： 逻辑备份： 指备份出的文件内容是可读的，内容一般是由一条条SQL语句或表内容实际数据组成 一般用于数据库升级与迁移 缺点是 恢复所需要的时间往往比较长 裸文件备份: 复制数据库的物理文件，即可以是在数据库运行中的复制，也可以是数据库停止运行时直接的数据文件复制。 恢复时间往往比逻辑备份短的多 按照备份数据库的内容可分为： 完全备份：对数据库进行一个完全的备份 增量备份：在上次完全备份的基础上，对于更改的数据进行备份 日志备份：主要指对MySQL数据库二进制日志的备份 MySQL数据库复制(replication)的原理就是异步实时的将二进制日志重做传送并应用到从(slave/standby)数据库 冷备 对于InnoDB存储引擎的冷备，只需要备份MySQL数据库的frm文件，共享表空间文件，独立表空间文件(*.idb)，重做日志。 另外建议定期备份MySQL数据库的配置文件my.cnf 最好将本地产生的备份存放到一台远程服务器中，却被不会应为本地数据库的宕机而影响备份文件的使用 优点： 备份简单，只需要复制相关文件 恢复简单，只需要把文件恢复到指定位置即可 备份文件可在不同操作系统，不同MySQL版本上恢复 恢复速度快，不需要执行任何SQL语句，也不需要重建索引 缺点: InnoDB存储引擎冷备的文件通常比逻辑文件大很多，因为表空间存放着很多其他数据，如undo段，插入缓冲等信息 冷备跨平台可能存在操作系统，MySQL的版本，文件大小写敏感和浮点数格式都可能称为问题 热备 ibbackup是InnoDB存储引擎官方提供的热备工具，可以同时备份MyISAM存储引擎。原理是： 记录备份开始时，InnoDB存储引擎重做日志文件检查点的LSN 复制共享表空间文件以及独立表空间文件 记录复制完表空间文件后，InnoDB存储引擎重做日志文件检查点LSN 复制在备份时产生的重做日志 优点： 在线备份，不阻塞任何的SQL语句 备份性能好，备份的实质是复制数据库文件和重做日志文件 支持压缩备份，通过选项，支持不同级别的压缩 跨平台 逻辑备份 123456789101112131415161718#1. 备份所有数据库mysqldump --all-databases &gt; dump.sql#2. 备份指定数据库mysqldump --databases db1 db2 db3 &gt;dump.sql#3. 导出指定数据库指定条件的数据(test数据库，表a)mysqldump --single-transaction --where&#x3D;&#39;b&gt;2&#39; test a &gt; a.sql#4. 备份恢复(直接执行mysql语句)mysql -uroot -p &lt; test_backup.sql#5. 执行使用source导出逻辑备份文件mysql&gt; source &#x2F;home&#x2F;mysql&#x2F;test_bakcup.sql# 二进制日志备份与恢复#1. 推荐二进制日志的服务器配置log-bin &#x3D; mysql-binsync_binlog &#x3D; 1innodb_suuport_xa &#x3D; 1#2. 使用mysqlbinlog恢复日志shell&gt; mysqlbinlog binlog.00001 &gt; &#x2F;tmp&#x2F;statements.sql 注意： mysqldump 无法导出视图，需要独立导出视图的定义或者备份视图定义 frm文件 备份二进制日志文件前，可以通过FLUSH LOGS 命令来生成一个新的二进制日志文件，然后备份 快照备份 指通过文件系统的快照功能进行数据库备份，MySQL本身不支持快照功能 复制(replication) replication的工作原理可以分为3个步骤： 主服务器(master)把数据更改记录到二进制日志(binlog)中 从服务器(slave)把主服务器的二进制日志复制到自己的中继日志(relay log)中 从服务器重做中继日志中的日志，把更改应用到自己的数据库 复制不是完全实时地进行同步，而是异步实时 主服务器上有一个线程负责发送二进制日志 从服务器有2个线程 I/O线程，负责读取主服务器的二进制日志，并将其保存为中继日志 SQL线程负责复制执行中继日志 在主从架构下，当主服务器误操作发生，从库也会跟着执行，这该怎样恢复？ 可对从服务器上的数据库所在分区做快照，避免误操作对复制造成的影响，然后根据二进制日志进行point-in-time的恢复，因此快照+复制的备份架构如下： 参考文献 《MySql技术内幕》","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"算法之美-剑指","slug":"Algorithms To Live By/算法之美-剑指","date":"2022-02-14T16:27:12.000Z","updated":"2022-07-21T01:56:08.000Z","comments":true,"path":"2022/02/15/Algorithms To Live By/算法之美-剑指/","link":"","permalink":"http://xboom.github.io/2022/02/15/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E5%89%91%E6%8C%87/","excerpt":"","text":"数据结构 链表 从尾到头打印链表 题目 1234567输入一个链表的头节点，按链表从尾到头的顺序返回每个节点的值（用数组返回）。如输入&#123;1,2,3&#125;的链表如下图:返回一个数组为[3,2,1]0 &lt;= 链表长度 &lt;= 10000 解答 123456789101112131415161718192021/*** struct ListNode &#123;* int val;* struct ListNode *next;* ListNode(int x) :* val(x), next(NULL) &#123;* &#125;* &#125;;*/class Solution &#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; vector&lt;int&gt; result; while(head) &#123; result.push_back(head-&gt;val); head = head-&gt;next; &#125; reverse(result.begin(), result.end()); return result; &#125;&#125;; 反转链表 题目 给定一个单链表的头结点pHead(该头节点是有值的，比如在下图，它的val是1)，长度为n，反转该链表后，返回新链表的表头。 数据范围： 0\\leq n\\leq10000≤n≤1000 要求：空间复杂度 O(1)O(1) ，时间复杂度 O(n)O(n) 。 如当输入链表{1,2,3}时， 经反转后，原链表变为{3,2,1}，所以对应的输出为{3,2,1}。 以上转换过程如下图所示： 解答 1234567891011121314151617181920212223/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* ReverseList(ListNode* pHead) &#123; ListNode *pre = nullptr; ListNode *cur = pHead; ListNode *nex = nullptr; // 这里可以指向nullptr，循环里面要重新指向 while (cur) &#123; nex = cur-&gt;next; cur-&gt;next = pre; pre = cur; cur = nex; &#125; return pre; &#125;&#125;; 合并两个排序的链表 题目 输入两个递增的链表，单个链表的长度为n，合并这两个链表并使新链表中的节点仍然是递增排序的。 数据范围： 0 \\le n \\le 10000≤n≤1000，-1000 \\le 节点值 \\le 1000−1000≤节点值≤1000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 如输入{1,3,5},{2,4,6}时，合并后的链表为{1,2,3,4,5,6}，所以对应的输出为{1,2,3,4,5,6} 解答 12345678910111213141516171819202122232425262728/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) &#123; ListNode * pre = new ListNode(-1); ListNode * cur = pre; while(pHead1 &amp;&amp; pHead2) &#123; if(pHead1-&gt;val &lt; pHead2-&gt;val) &#123; cur-&gt;next = pHead1; pHead1 = pHead1-&gt;next; &#125; else &#123; cur-&gt;next = pHead2; pHead2 = pHead2-&gt;next; &#125; cur = cur-&gt;next; &#125; if(!pHead1) cur-&gt;next = pHead2; if(!pHead2) cur-&gt;next = pHead1; return pre-&gt;next; &#125;&#125;; 两个链表的第一个公共结点 题目 输入两个无环的单向链表，找出它们的第一个公共结点，如果没有公共节点则返回空。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的） 数据范围： n \\le 1000n≤1000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如，输入{1,2,3},{4,5},{6,7}时，两个无环的单向链表的结构如下图所示： 输入分为是3段，第一段是第一个链表的非公共部分，第二段是第二个链表的非公共部分，第三段是第一个链表和二个链表的公共部分。 后台会将这3个参数组装为两个链表，并将这两个链表对应的头节点传入到函数FindFirstCommonNode里面，用户得到的输入只有pHead1和pHead2 解答 1234567891011121314151617181920/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) &#123; ListNode *ta = pHead1; ListNode *tb = pHead2; while (ta != tb) &#123; ta = ta ? ta-&gt;next : pHead2; tb = tb ? tb-&gt;next : pHead1; &#125; return ta; &#125;&#125;; 链表中环的入口结点 题目 给一个长度为n链表，若其中包含环，请找出该链表的环的入口结点，否则，返回null。 数据范围： n\\le10000n≤10000，1&lt;=结点值&lt;=100001&lt;=结点值&lt;=10000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如，输入{1,2},{3,4,5}时，对应的环形链表如下图所示： 12345678910111213141516171819202122232425262728/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; ListNode *fast = pHead; ListNode *slow = pHead; while(fast &amp;&amp; fast-&gt;next) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; if(fast == slow) break; &#125; if(!fast || !fast-&gt;next) return nullptr; fast = pHead; while(fast != slow) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return fast; &#125;&#125;; 链表中倒数最后k个结点 题目 输入一个长度为 n 的链表，设链表中的元素的值为 ai ，返回该链表中倒数第k个节点。 如果该链表长度小于k，请返回一个长度为 0 的链表。 数据范围：0 \\leq n \\leq 10^50≤n≤105，0 \\leq a_i \\leq 10^90≤a**i≤109，0 \\leq k \\leq 10^90≤k≤109 要求：空间复杂度 O(n)O(n)，时间复杂度 O(n)O(n) 进阶：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如输入{1,2,3,4,5},2时，对应的链表结构如下图所示： 其中蓝色部分为该链表的最后2个结点，所以返回倒数第2个结点（也即结点值为4的结点）即可，系统会打印后面所有的节点来比较。 解答 12345678910111213141516171819202122232425262728/** * struct ListNode &#123; * int val; * struct ListNode *next; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * &#125;; */class Solution &#123;public: /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param pHead ListNode类 * @param k int整型 * @return ListNode类 */ ListNode* FindKthToTail(ListNode* pHead, int k) &#123; ListNode* r = pHead; while (k-- &amp;&amp; r) r = r-&gt;next; // 移动右侧指针造成 k 的距离差 if (k &gt;= 0) return nullptr; // 此时说明 k 比链表长度长 ListNode* l = pHead; while (r) r = r-&gt;next, l = l-&gt;next; // 两个指针一起移动找到倒数第 k 个节点 return l; &#125;&#125;; 删除链表中重复的节点 题目 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 数据范围：链表长度满足 0 \\le n \\le 1000 \\0≤n≤1000 ，链表中的值满足 1 \\le val \\le 1000 \\1≤val≤1000 进阶：空间复杂度 O(n)*O*(n) ，时间复杂度 O(n) *O*(n) 例如输入{1,2,3,3,4,4,5}时，对应的输出为{1,2,5}，对应的输入输出链表如下图所示： 例如 输入：{1,2,3,3,4,4,5},输出：{1,2,5} 输入：{1,1,1,8}，输出：{8} 解答 123456789101112131415161718192021222324252627282930313233343536/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* deleteDuplication(ListNode* pHead) &#123; if (!pHead) return NULL; ListNode* slow = new ListNode(-1), *fast = new ListNode(-1), *dummy = new ListNode(-1); dummy-&gt;next = pHead; // 初始化两个指针 slow = dummy; //虚拟指针 fast = dummy-&gt;next; //头指针 while (fast) &#123; // 遇到重复则只更新fast节点 while (fast-&gt;next &amp;&amp; fast-&gt;val == fast-&gt;next-&gt;val) &#123; fast = fast-&gt;next; &#125; // 遇到重复 if (slow-&gt;next != fast) &#123; //如果不相等，说明中间slow 与 fast有相同值的节点 slow-&gt;next = fast-&gt;next; fast = slow-&gt;next; &#125; else &#123; // 没有重复 fast = fast-&gt;next; slow = slow-&gt;next; &#125; &#125; return dummy-&gt;next; &#125;&#125;; 参考链接 https://www.nowcoder.com/ta/coding-interviews","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"MySql-06-集群","slug":"MySql/MySql-06-集群","date":"2022-02-11T07:47:24.000Z","updated":"2023-05-27T15:42:32.563Z","comments":true,"path":"2022/02/11/MySql/MySql-06-集群/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql-06-%E9%9B%86%E7%BE%A4/","excerpt":"","text":"高可用方案 共享存储：SAN/NAS 操作系统实时数据块恢复：DRBD架构(MySQL+DRBD+Heartbeat) 主从复制架构 主从复制(一主多从) MMM架构(双主多从) MHA架构(多主多从) 数据库高可用架构 GRT(MySQL Group Replication) Galera MySQL Cluster 和 PXC MySQL Cluster(ndb存储引擎比较复杂，并没有大规模使用) PXC(Percona XtraDB Cluster) 常见方案介绍 方案1 主从架构 主从复制的模式有哪些？https://zhuanlan.zhihu.com/p/307288925 怎么保证数据库一致性 方案2 MHA架构 MHA: Master Hight Availability Manager and Toolsfor MySQL 生产环境MySQL数据库集群MHA上线实施方案 MHA(Master High Availability Manager and Toolsfor MySQL)目前在Mysql高可用方面是一个相对成熟的解决方案。它是日本的一位MySQL专家采用Perl语言编写的一个脚本管理工具，该工具仅适用于MySQLReplication 环境，目的在于维持Master主库的高可用性。 MHA是基于标准的MySQL复制(异步/半同步)。 MHA是由管理节点(MHA Manager)和数据节点(MHA Node)两部分组成。 MHA Manager可以单独部署在一台独立机器,也可以部署在一台slave上。 方案3 MMM架构 MySQL-MMM实现MySQL高可用 MMM，全称为Master-Master replication manager for Mysql，是一套支持双主故障切换和双主日常管理的脚本程序，MMM使用Perl语言开发。主要用来监控和管理MySQL Master-Master(双)复制。特别适合DBA做维护等需要主从复制的场景，通过双主架构避免了重复搭建从库的麻烦。虽然叫做双主复制，但是业务上同一时刻只允许对一个主进行写入，另一台备选主上提供部分读服务，以加速在主主切换时备选主的预热 MMM优缺点 优点：高可用性，扩展性好，出现故障自动切换，对于主主同步，在同一时间只提供一台数据库写操作，保证的数据的一致性。 缺点：Monitor节点是单点，可以结合Keepalived实现高可用 方案4 DRBD架构 https://www.linbit.com/en/drbd-community/drbd-download/ 读写分离方案 客户端解决方案(应用层) 常用：DDL、 Sharding-Jdbc (常用shardding-jdbc) 优点： 程序自动完成，数据源方便管理 不需要维护，因为没用中间件 理论支持任何数据库 （sql标准） 缺点： 增加了开发成本、代码有入侵 不能做到动态增加数据源 程序员开发完成，运维参与不了。 中间件解决方案（代理层） 常用：mysql proxy、mycat、altas (常用mycat) 优点： 数据增加了都程序没用任何影响 应用层（程序）不需要管数据库方面的事情 增加数据源不需要重启程序 缺点： 程序依赖中间件，导致切换数据库变的困难 增加了proxy 性能下降 增加了维护工作、高可用问题 主从复制 Mysql为了保证数据库一致性，引入了集中复制类型，分别是 异步复制 半同步复制 全同步复制 主从复制整理分为以下三个步骤 主库将数据库的变更操作记录到Binlog日志文件(Master服务器对数据库更改操作记录在Binlog中，BinlogDump Thread接到写入请求后，读取Binlog信息推送给Slave的I/O Thread) 从库读取主库中的Binlog日志文件信息写入到从库的Relay Log中继日志中(Slave的I/O Thread将读取到的Binlog信息写入到本地Relay Log中) 从库读取中继日志信息在从库中进行Replay,更新从库数据信息(Slave的SQL Thread检测到Relay Log的变更请求，解析relay log中内容在从库上执行) 主库上并发的修改操作在从库上只能串行化执行，因为只有一个SQL线程来重放中继日志，这也是很多工作负载的性能瓶颈所在 异步复制 mysql主从复制存在的问题： 主库宕机后，数据可能丢失 从库只有一个SQL Thread，主库写压力大，复制很可能延时 半同步复制 为了提升数据安全，MySQL让Master在某一个时间点等待Slave节点的 ACK（Acknowledgecharacter）消息，接收到ACK消息后才进行事务提交，这也是半同步复制的基础，MySQL从5.5版本开始引入了半同步复制机制来降低数据丢失的概率。 介绍半同步复制之前先快速过一下 MySQL 事务写入碰到主从复制时的完整过程，主库事务写入分为 4个步骤： InnoDB Redo File Write (Prepare Write) Binlog File Flush &amp; Sync to Binlog File InnoDB Redo File Commit（Commit Write） Send Binlog to Slave 当Master不需要关注Slave是否接受到Binlog Event时，即为传统的主从复制。 当Master需要在第三步等待Slave返回ACK时，即为 after-commit（Master先提交，等Slave ACK以后再，回复客户端），半同步复制（MySQL 5.5引入）。 当Master需要在第二步等待 Slave 返回 ACK 时，即为 after-sync（Master先写binlog，等Slave ACK以后再提交），增强半同步（MySQL 5.7引入）。 下图是 MySQL 官方对于半同步复制的时序图，主库等待从库写入 relay log 并返回 ACK 后才进行Engine Commit。 MySQL Group Replication MySQL Group Replication是建立在已有MySQL复制框架的基础之上，通过新增Group Replication Protocol协议及Paxos协议的实现，形成的整体高可用解决方案 与原有复制方式相比，MGR主要增加了状态机一致性顺序复制和certify这两个环境 MySQL事务通过before_commit钩子进入MGR，before_commit位于MYSQL_BIN_LOG::commit()函数中，具体是在进入事务组提交MYSQL_BIN_LOG::ordered_commit()之前，这就意味着执行到before_commit这个钩子时，事务还未提交，产生的Binlog还未写入Binlog文件中，事务GTID还未产生 分布式数据库事务 分布式数据库实现分布式事务的主流方法还是2PC 如上图所示，当分布式事务提交时，会选择其中的一个数据分片作为协调者在所有数据分片上执行两阶段提交协议。由于所有数据分片都是通过 Paxos 复制日志实现多副本高可用的，当主副本发生宕机后，会由同一数据分片的备副本转换为新的主副本继续提供服务，所以可以认为参与者和协调者都是保证高可用不宕机的（多数派存活），绕开了协调者宕机的问题。 在参与者高可用的实现前提下，可以对协调者进行了“无状态”的优化。在标准的两阶段提交中，协调者要通过记录日志的方法持久化自己的状态，否则如果协调者和参与者同时宕机，协调者恢复后可能会导致事务提交状态不一致。但是如果我们认为参与者不会宕机，那么协调者并不需要写日志记录自己的状态。 所以在第一阶段所有参与者都回复prepare完成以后，即可以反馈事务提交成功，提升了2PC的效率 由于存在多副本，只要保证在prepare阶段，验证事务执行没有错误，协调者发出commit指令后，就可以乐观的认为，事务执行成功并反馈给事务发起者。相信commit消息会被多数副本收到，多数副本收到消息以后，剩下的就交给他们自己同步 在上图中（绿色部分表示写日志的动作），左侧为标准两阶段提交协议，用户感知到的提交时延是4次写日志耗时以及2次 RPC 的往返耗时；由于少了协调者的写日志耗时以及提前了应答客户端的时机，用户感知到的提交时延是1次写日志耗时以及1次 RPC 的往返耗时 参考链接 MySQL高可用集群方案 MySQL 主从复制","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"Protocol-02-Quic","slug":"Protocol/Protocol-02-Quic","date":"2022-02-09T07:20:49.000Z","updated":"2023-09-08T15:37:53.044Z","comments":true,"path":"2022/02/09/Protocol/Protocol-02-Quic/","link":"","permalink":"http://xboom.github.io/2022/02/09/Protocol/Protocol-02-Quic/","excerpt":"","text":"TCP协议在创建链接前会进行三次握手。如果增加传输层协议(TLS)，则握手次数更多 QUIC协议 可以在1到2个数据包(取决于连接服务是未知还是已知)内，完成连接的创建(包括TLS) QUIC 非常类似于在 UDP 上实现的 TCP + TLS + HTTP/2。相比于 TCP，流控功能在用户空间而不在内核空间，可以不受限于 CUBIC 或是 BBR，而是可以自由选择甚至根据应用场景自由调整优化 为什么不修改TCP协议? TCP 是在操作系统内核和中间件固件中实现的 这么好为什么没有大规模使用？ 可能会被路由封杀UDP 443端口（ 这正是QUIC 部署的端口）； UDP包过多，由于QS限定，会被服务商误认为是攻击，UDP包被丢弃； 无论是路由器还是防火墙目前对QUIC都还没有做好准备。 QUIC 优点 QUIC 与现有 TCP + TLS + HTTP/2 方案相比，有以下几点主要特征： 利用缓存，显著减少连接建立时间； 改善拥塞控制，拥塞控制从内核空间到用户空间； 没有 head of line 阻塞的多路复用； 前向纠错，减少重传； 连接平滑迁移，网络状态的变更不会影响连接断线 TCP 的拥塞控制实际上包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复 QUIC 协议当前默认使用了 TCP 协议的 Cubic 拥塞控制算法，同时也支持 CubicBytes, Reno, RenoBytes, BBR, PCC 等拥塞控制算法。 从拥塞算法本身来看，QUIC 只是按照 TCP 协议重新实现了一遍，那么 QUIC 协议到底改进在哪些方面呢？ 可插拔 就是能够非常灵活地生效，变更和停止 应用程序层面就能实现不同的拥塞控制算法，不需要操作系统，不需要内核支持。而传统的 TCP 拥塞控制，必须要端到端的网络协议栈支持，才能实现控制效果。 即使是单个应用程序的不同连接也能支持配置不同的拥塞控制。能提供更加有效的拥塞控制。比如 BBR 适合，Cubic 适合； 程序不需要停机和升级就能实现拥塞控制的变更 单调递增的 Packet Number TCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达 QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 sequence number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。而 TCP 呢，重传 segment 的 sequence number 和原始的 segment 的 Sequence Number 保持不变，也正是由于这个特性，引入了 Tcp 重传的歧义问题 如上图，超时事件 RTO 发生后，客户端发起重传，然后接收到了 Ack 数据。由于序列号一样，这个 Ack 数据到底是原始请求的响应还是重传请求的响应呢？不好判断。 如果算成原始请求的响应，但实际上是重传请求的响应（上图左），会导致采样 RTT 变大。如果算成重传请求的响应，但实际上是原始请求的响应，又很容易导致采样 RTT 过小。 由于 Quic 重传的 Packet 和原始 Packet 的 Pakcet Number 是严格递增的，所以很容易就解决了这个问题 如上图所示，RTO 发生后，根据重传的 Packet Number 就能确定精确的 RTT 计算。如果 Ack 的 Packet Number 是 N+M，就根据重传请求计算采样 RTT。如果 Ack 的 Pakcet Number 是 N，就根据原始请求的时间计算采样 RTT，没有歧义性。 但是单纯依靠严格递增的 Packet Number 肯定是无法保证数据的顺序性和可靠性。QUIC 又引入了一个 Stream Offset 的概念。 即一个 Stream 可以经过多个 Packet 传输，Packet Number 严格递增，没有依赖。但是 Packet 里的 Payload 如果是 Stream 的话，就需要依靠 Stream 的 Offset 来保证应用数据的顺序。 假设 Packet N 丢失了，发起重传，重传的 Packet Number 是 N+2，但是它的 Stream 的 Offset 依然是 x，这样就算 Packet N + 2 是后到的，依然可以将 Stream x 和 Stream x+y 按照顺序组织起来，交给应用程序处理。 不允许 Reneging Reneging: 接收方丢弃已经接收并且上报给 SACK 选项的内容。TCP 协议不鼓励这种行为，但是协议层面允许这样的行为。主要是考虑到服务器资源有限，比如 Buffer 溢出，内存不够等情况。 Reneging 对数据重传会产生很大的干扰。因为 Sack 都已经表明接收到了，但是接收端事实上丢弃了该数据。 QUIC 在协议层面禁止 Reneging，一个 Packet 只要被 Ack，就认为它一定被正确接收，减少了这种干扰。 更多的ACK块 TCP 的 Sack 选项能够告诉发送方已经接收到的连续 Segment 的范围，方便发送方进行选择性重传。 由于 TCP 头部最大只有 60 个字节，标准头部占用了 20 字节，所以 Tcp Option 最大长度只有 40 字节，再加上 Tcp Timestamp option 占用了 10 个字节 [25]，所以留给 Sack 选项的只有 30 个字节。每一个 Sack Block 的长度是 8 个，加上 Sack Option 头部 2 个字节，也就意味着 Tcp Sack Option 最大只能提供 3 个 Block。 Quic Ack Frame 可同时提供 256 个 Ack Block，在丢包率比较高的网络下，更多的 Sack Block 可以提升网络的恢复速度，减少重传量。 Ack Delay 时间 Tcp 的 Timestamp 选项存在一个问题：只回显发送方的时间戳，但没有计算接收端接收到 segment 到发送 Ack 该 segment 的时间。这个时间可以简称为 Ack Delay 这样就会导致 RTT 计算误差。如下图： TCP 的 RTT 计算：RTT = timestamp2 - timestamp1 Quic 的RTT 计算：RTT = timestamp2 - timestamp1 - Ack Delay 当然RTT的具体计算需要采样，参考历史数据平滑计算 SRTT = SRTT + α(RTT - SRTT) RTO = β * SRTT + α * DevRTT 基于 stream 和 connection 级别的流量控制 QUIC 的流量控制类似 HTTP2，即在 Connection 和 Stream 级别提供了两种流量控制 Connection 可以类比一条 TCP 连接，Stream 可以认为就是一条 HTTP 请求。多路复用意味着在一条 Connetion 上会同时存在多条 Stream。既需要对单个 Stream 进行控制，又需要针对所有 Stream 进行总体控制。 QUIC 实现流量控制的原理比较简单： 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。 QUIC 的流量控制和 TCP 有点区别，TCP 为了保证可靠性，窗口左边沿向右滑动时的长度取决于已经确认的字节数。如果中间出现丢包，就算接收到了更大序号的 Segment，窗口也无法超过这个序列号。 但 QUIC 不同，就算此前有些 packet 没有接收到，它的滑动只取决于接收到的最大偏移字节数 针对Stream: 可用窗口 = 最大窗口数 - 接收到的最大偏移数 针对Connection: 可用窗口 = stream1 可用窗口 + stream2 可用窗口+ …… + streamN 可用窗口 STGW 也在连接和 Stream 级别设置了不同的窗口数。可以在内存不足或者上游处理性能出现问题时，通过流量控制来限制传输速率，保障服务可用性 没有对头阻塞的多路复用 QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势，很大程度上缓解甚至消除了队头阻塞的影响。 QUIC 一个连接的多个 stream 之间没有依赖。假如 stream2 丢了一个 udp packet，不会影响 其他stream 的处理。 多路复用是 HTTP2 最强大的特性，能够将多条请求在一条 TCP 连接上同时发出去。但也恶化了 TCP 的一个问题，队头阻塞。 HTTP2 在一个 TCP 连接上同时发送 4 个 Stream。其中 Stream1 已经正确到达，并被应用层读取。但是 Stream2 的第三个 tcp segment 丢失了，TCP 为了保证数据的可靠性，需要发送端重传第 3 个 segment 才能通知应用层读取接下去的数据，虽然这个时候 Stream3 和 Stream4 的全部数据已经到达了接收端，但都被阻塞住了 不仅如此，由于 HTTP2 强制使用 TLS，还存在一个 TLS 协议层面的队头阻塞 上面两段存在歧义TODO ??? gRPC 基于 HTTP2 但是并没有TLS Record 是 TLS 协议处理的最小单位，最大不超过 16K，Nginx 默认的大小就是 16K。由于一个 record 必须经过数据一致性校验才能进行加解密，所以一个 16K 的 record，就算丢了一个字节，也会导致已接收的 15.99K 数据无法处理，因为不完整 那 QUIC 多路复用为什么能避免上述问题呢？ QUIC 最基本的传输单元是 Packet，不会超过 MTU 的大小，整个加密和认证过程都是基于 Packet 的，不会跨越多个 Packet。这样就能避免 TLS 协议存在的队头阻塞； Stream 之间相互独立，比如 Stream2 丢了一个 Pakcet，不会影响 Stream3 和 Stream4。不存在 TCP 队头阻塞 当然，并不是所有的 QUIC 数据都不会受到队头阻塞的影响，比如 QUIC 当前也是使用 Hpack 压缩算法 [10]，由于算法的限制，丢失一个头部数据时，可能遇到队头阻塞。 总体来说，QUIC 在传输大量数据时，比如视频，受到队头阻塞的影响很小。 为什么压缩之后就出现对头阻塞了？ TODO 加密认证的报文 TCP 协议头部没有经过任何加密和认证，所以在传输过程中很容易被中间网络设备篡改，注入和窃听。比如修改序列号、滑动窗口。这些行为有可能是出于性能优化，也有可能是主动攻击。 但是 QUIC 的 packet 可以说是武装到了牙齿。除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。 这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。 如下图所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。 连接迁移 一条 TCP 连接是由四元组标识的（源 IP，源端口，目的 IP，目的端口）。什么叫连接迁移呢？就是当其中任何一个元素发生变化时，这条连接依然维持着，能够保持业务逻辑不中断。当然这里面主要关注的是客户端的变化，因为客户端不可控并且网络环境经常发生变化，而服务端的 IP 和端口一般都是固定的。 比如大家使用手机在 WIFI 和 4G 移动网络切换时，客户端的 IP 肯定会发生变化，需要重新建立和服务端的 TCP 连接。 又比如大家使用公共 NAT 出口时，有些连接竞争时需要重新绑定端口，导致客户端的端口发生变化，同样需要重新建立 TCP 连接。 那 QUIC 是如何做到连接迁移呢？任何一条 QUIC 连接不再以 IP 及端口四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。 由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。 其他 此外，QUIC 还能实现前向冗余纠错，在重要的包比如握手消息发生丢失时，能够根据冗余信息还原出握手消息。 QUIC 还能实现证书压缩，减少证书传输量，针对包头进行验证等 QUIC原理 代码实现：https://github.com/lucas-clemente/quic-go 参考链接 http://www.52im.net/thread-2816-1-1.html https://docs.google.com/document/d/1F2YfdDXKpy20WVKJueEf4abn_LVZHhMUMS5gX6Pgjl4/edit# 网络编程懒人入门(十)：一泡尿的时间，快速读懂QUIC协议 让互联网更快：新一代QUIC协议在腾讯的技术实践分享 七牛云技术分享：使用QUIC协议实现实时视频直播0卡顿！ https://hungryturbo.com/HTTP3-explained/quic/为什么需要QUIC.html#回顾http-2 https://zhuanlan.zhihu.com/p/32553477","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"Quic","slug":"Quic","permalink":"http://xboom.github.io/tags/Quic/"}]},{"title":"算法之美-LRU","slug":"Algorithms To Live By/算法之美-LRU","date":"2022-01-12T16:05:08.000Z","updated":"2022-07-24T06:48:35.000Z","comments":true,"path":"2022/01/13/Algorithms To Live By/算法之美-LRU/","link":"","permalink":"http://xboom.github.io/2022/01/13/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-LRU/","excerpt":"","text":"问题背景 LRU(Least Recently Used)，最近最少使用。根据时间维度来选择将要淘汰的元素，即删除掉最长时间没被访问的元素。 实现一个LRU算法需要解决的问题： 一个存储最近使用元素的结构，能够快速定位到元素(map) 如何计算元素被最少使用了，当添加或更新元素的时候方便查询最少使用元素(双向链表) 最近最少使用存在数量上限。(容量capacity) 实现原理 双向链表按照被使用的顺序存储了这些键值对，靠近头部的键值对是最近使用的，而靠近尾部的键值对是最久未使用的。 哈希表即为普通的哈希映射（HashMap），通过缓存数据的键映射到其在双向链表中的位置 删除节点 删除的时候链表和map中的都需要删除 新增节点 注意： list节点中也要存储Map的key 技术内幕 代码路径：core/collection/cache.go 对象定义 12345678910111213lru interface &#123; add(key string) //新增 remove(key string) //删除&#125;emptyLru struct&#123;&#125;keyLru struct &#123; limit int //容量限制 evicts *list.List //双向链表 elements map[string]*list.Element //map根据key存储双向链表位置 onEvict func(key string) //自定义触发函数&#125; 删除元素 12345678910111213141516171819func (klru *keyLru) remove(key string) &#123; if elem, ok := klru.elements[key]; ok &#123; //通过map判断元素是否存在 klru.removeElement(elem) //删除节点 &#125;&#125;func (klru *keyLru) removeOldest() &#123; elem := klru.evicts.Back() //获取尾部节点 if elem != nil &#123; klru.removeElement(elem) &#125;&#125;func (klru *keyLru) removeElement(e *list.Element) &#123; klru.evicts.Remove(e) //删除双向链表节点 key := e.Value.(string) delete(klru.elements, key) //删除map中节点 klru.onEvict(key) //触发自定义逻辑&#125; 添加元素 123456789101112131415func (klru *keyLru) add(key string) &#123; if elem, ok := klru.elements[key]; ok &#123; //如果元素存在则将节点移动到链表头 klru.evicts.MoveToFront(elem) return &#125; // Add new item elem := klru.evicts.PushFront(key) //放入链表头部 klru.elements[key] = elem //存放节点 // Verify size not exceeded if klru.evicts.Len() &gt; klru.limit &#123; //如果节点数量超过限制，则删除尾部节点 klru.removeOldest() &#125;&#125; 参考链接 https://talkgo.org/t/topic/2280","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"算法之美-链表问题","slug":"Algorithms To Live By/算法之美-链表问题","date":"2022-01-11T16:05:08.000Z","updated":"2022-07-24T05:31:45.000Z","comments":true,"path":"2022/01/12/Algorithms To Live By/算法之美-链表问题/","link":"","permalink":"http://xboom.github.io/2022/01/12/Algorithms%20To%20Live%20By/%E7%AE%97%E6%B3%95%E4%B9%8B%E7%BE%8E-%E9%93%BE%E8%A1%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"17.从尾到头打印链表 https://www.acwing.com/problem/content/18/ 题目 12345678910输入一个链表的头结点，按照 从尾到头 的顺序返回节点的值。返回的结果用数组存储。数据范围0≤ 链表长度 ≤1000。样例输入：[2, 3, 5]返回：[5, 3, 2] 解答 12345678910111213141516171819&#x2F;** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; *&#x2F;class Solution &#123;public: vector&lt;int&gt; printListReversingly(ListNode* head) &#123; vector&lt;int&gt; result; while(head) &#123; result.push_back(head-&gt;val); head &#x3D; head-&gt;next; &#125; return vector&lt;int&gt;(result.rbegin(), result.rend()); &#125;&#125;; 相关知识 - 反向迭代器 begin() 返回一个迭代器，它指向容器的第一个元素 end() 返回一个迭代器，它指向容器的最后一个元素的下一个位置 rbegin() 返回一个逆序迭代器，它指向容器的最后一个元素 rend() 返回一个逆序迭代器，它指向容器的第一个元素前面的位置 应用场景： 1234&#x2F;&#x2F;sorts v in &quot;normal&quot; ordersort(v.begin(), v.end());&#x2F;&#x2F;sorts in reversesort(v.rbegin(), v.rend()); 28.O(1)删除链表结点 题目 https://www.acwing.com/problem/content/85/ 123456789101112给定单向链表的一个节点指针，定义一个函数在O(1)时间删除该结点。假设链表一定存在，并且该节点一定不是尾节点。数据范围链表长度 [1,500]。样例输入：链表 1-&gt;4-&gt;6-&gt;8 删掉节点：第2个节点即6（头节点为第0个节点）输出：新链表 1-&gt;4-&gt;8 解答 123456789101112131415161718&#x2F;** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; *&#x2F;class Solution &#123;public: void deleteNode(ListNode* node) &#123; ListNode* nxt &#x3D; node-&gt;next; node-&gt;val &#x3D; nxt-&gt;val; node-&gt;next &#x3D; nxt-&gt;next; delete nxt; &#125;&#125;; 相关知识 12//1. auto: 在声明变量的时候可根据变量初始值的数据类型自动为该变量选择与之匹配的数据类型//2. delete TODO 29.删除链表重复节点 题目 1234567891011121314在一个排序的链表中，存在重复的节点，请删除该链表中重复的节点，重复的节点不保留。数据范围链表中节点 val 值取值范围 [0,100]。链表长度 [0,100]。样例1输入：1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5输出：1-&gt;2-&gt;5样例2输入：1-&gt;1-&gt;1-&gt;2-&gt;3输出：2-&gt;3 注意重复的节点也不保留 解答 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplication(ListNode* head) &#123; ListNode * dummy = new ListNode(-1); //定义一个头指针(防止head为空) dummy-&gt;next = head; ListNode * pre = dummy; //双指针前指针 while(pre-&gt;next) &#123; ListNode * nxt = pre-&gt;next; //双指针尾指针 //一直循环，直到头指针与尾指针值不一致 //注意这里pre是从dummy开头的，而这题是要删除所有重复的节点，所以有时候pre—&gt;next需要一起删除 //所以这里每次都是比较 pre-&gt;next 与 nxt while(nxt &amp;&amp; pre-&gt;next-&gt;val == nxt-&gt;val) nxt = nxt-&gt;next; //经过上述流转出现 pre-&gt;next-&gt;val != nxt-&gt;val //1. 如果两者相邻，那么pre向后移动一个节点 //2. 如果两者不相邻，那么pre与nxt之间都是重复节点 if (pre-&gt;next-&gt;next == nxt) pre = pre-&gt;next; else pre-&gt;next = nxt; &#125; return dummy-&gt;next; &#125;&#125;; 重点：是每次使用 pre-&gt;next 与 nxt进行比较，那样即使删除也是删除pre-&gt;next 与 nxt之间的结点 33.链表中倒数第N个节点 题目 12345678910111213输入一个链表，输出该链表中倒数第 k 个结点。注意：k &gt;= 1;如果 k 大于链表长度，则返回 NULL;数据范围链表长度 [0,30]。样例输入：链表：1-&gt;2-&gt;3-&gt;4-&gt;5 ，k=2输出：4 解答 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* findKthToTail(ListNode* pListHead, int k) &#123; //使用快慢指针 ListNode *fast = pListHead; ListNode *slow = pListHead; //快指针先走k步，如果fast为空或者为空都没有走到k个，说明链表数目小于k for (int i = 0; i &lt; k; i++) &#123; if (fast) fast = fast-&gt;next; //这里k-1 是因为 i = 0 即为第一个节点 fast 需要走 i = k - 1 步 if (!fast &amp;&amp; i &lt; k - 1) return NULL; &#125; while (fast) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return slow; &#125;&#125;; 重点：快慢指针 34.链表中环的入口节点 题目 https://www.acwing.com/problem/content/86/ 12345678910111213141516给定一个链表，若其中包含环，则输出环的入口节点。若其中不包含环，则输出null。数据范围节点 val 值取值范围 [1,1000]。链表长度 [0,500]。样例如下图给定如上所示的链表：[1, 2, 3, 4, 5, 6]2注意，这里的2表示编号是2的节点，节点编号从0开始。所以编号是2的节点就是val等于3的节点。则输出环的入口节点3. 证明： 如上图所示，a 是起点，b 是环的入口，c 是两个指针的第一次相遇点，ab 距离 x，bc 距离是 y 存在快慢指针 first 与 second，其中second的速度是first的2倍 想法1： 当 first 走到 b 时， second 已经从 b 开始在环上走了 x 步，可能多余n圈，距离 b 还差 y 步 所以 second 走的路程 2x + y = x + n 圈 ==&gt; x + y = n 圈，那么从 c 走 x 步 也能到达b 想法2： 用 z 表示从 c 点顺时针走到 b 的距离。则第一次相遇时 second 所走的距离是 x+(y+z)∗n+y，其中n表示圈数，所以 12x+(y+z)∗n+y = 2(x+y) //second是 first路程的两倍x = (n-1) * (y+z) + z 所以其实 x == z ，相遇之后节点距离入环扣与链表头到入环扣距离相等 解答 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *entryNodeOfLoop(ListNode *head) &#123; if (!head || !head-&gt;next) return NULL; //初始节点判断 //快慢指针 ListNode * first = head; ListNode * second = head; while (first &amp;&amp; second)&#123; //外循环防止节点为空，找到第一个相遇的地方 first = first-&gt;next; second = second-&gt;next; if(second) second = second-&gt;next; else return NULL; if (first == second) &#123; //当相遇则再次相遇说走的路程就是换入口处 first = head; while (first != second) &#123; first = first-&gt;next; second = second-&gt;next; &#125; return first; &#125; &#125; return NULL; &#125;&#125;; 重点是：从第一次相遇的地方走x步就能到达入环扣 35.反转链表 题目 123456789101112定义一个函数，输入一个链表的头结点，反转该链表并输出反转后链表的头结点。思考题：请同时实现迭代版本和递归版本。数据范围链表长度 [0,30]。样例输入:1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出:5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 解答 非递归 12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; ListNode * second = head; ListNode * first = nullptr; //头部空指针 while(second) &#123; ListNode * pre = second-&gt;next; second-&gt;next = first; first = second; second = pre; &#125; return first; &#125;&#125;; 递归 12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if(!head || !head-&gt;next ) return head; //1 - 2 - 3 - 4 ListNode *tail = reverseList(head-&gt;next); //反转head-&gt;next 链表, 原链表的尾节点tail //为什么不用记录前一个节点进行转换， //因为 递归中 head-&gt;next 已经经过了反转，后面只需要反转 head 与 head-&gt;next的关系就可以了 head-&gt;next-&gt;next = head; //将 这里直接使用 head-&gt;next-&gt;next 不用再声明变量 head-&gt;next = nullptr; // nullptr &lt;- 1 &lt;- 2 return tail; //其实这里一直返回的是原链表的尾节点 &#125;&#125;; 36.合并两个排序的链表 题目 https://www.acwing.com/problem/content/34/ 123456789输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。数据范围链表长度 [0,500]。样例输入：1-&gt;3-&gt;5 , 2-&gt;4-&gt;5输出：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;5 解答 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* merge(ListNode* l1, ListNode* l2) &#123; ListNode * dummy = new ListNode(-1); //新建头部的保护结点dumm ListNode * cur = dummy; //当前指针 while(l1 &amp;&amp; l2) &#123; if(l1-&gt;val &gt; l2-&gt;val) &#123; cur-&gt;next = l2; l2 = l2-&gt;next; &#125;else&#123; cur-&gt;next = l1; l1 = l1-&gt;next; &#125; cur = cur-&gt;next; &#125; //二元判断，防止两个链表数目不一致 cur-&gt;next = (l1 != nullptr ? l1 : l2); return dummy-&gt;next; &#125;&#125;; 两个链表各遍历一次，所以时间复杂度为O(n) 48. 复杂链表的复刻 题目 https://www.acwing.com/problem/content/89/ 123456789请实现一个函数可以复制一个复杂链表。在复杂链表中，每个结点除了有一个指针指向下一个结点外，还有一个额外的指针指向链表中的任意结点或者null。注意：函数结束后原链表要与输入时保持一致。数据范围链表长度 [0,500]。 解答 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition for singly-linked list with a random pointer. * struct ListNode &#123; * int val; * ListNode *next, *random; * ListNode(int x) : val(x), next(NULL), random(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *copyRandomList(ListNode *head) &#123; //开始遍历添加cur的复制点np ListNode * p = head; while(p) &#123; //1 - 2 --&gt; 1 - 1 - 2 复制节点并添加到节点后面 auto * np = new ListNode(p-&gt;val); auto * nxt = p-&gt;next; p-&gt;next = np; np-&gt;next = nxt; p = nxt; &#125; //如果存在 cur 有 random 则 p-&gt;next 是被复制的节点 p = head; while (p) &#123; if(p-&gt;random) //p 是旧节点,则 p-&gt;next 是新节点 p-&gt;next-&gt;random = p-&gt;random-&gt;next; //这里也是随机指定，和原来的不一定相等 p = p-&gt;next-&gt;next; &#125; ListNode * dummy = new ListNode(-1); //保护头节点 ListNode * cur = dummy; p = head; while(p) &#123; //1 - 1 - 2 - 2 - 3 - 3 cur-&gt;next = p-&gt;next; //新链表 cur = cur-&gt;next; p-&gt;next = p-&gt;next-&gt;next; //恢复旧指针 p = p-&gt;next; &#125; return dummy-&gt;next; &#125;&#125;; 重点是：p-&gt;next-&gt;random = p-&gt;random-&gt;next; //如果有random，则它是原节点。将新节点指一个random 49.二叉搜索树与双向链表 https://www.acwing.com/problem/content/87/ 题目 1234567891011输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。注意：需要返回双向链表最左侧的节点。例如，输入上图中左边的二叉搜索树，则输出右边的排序双向链表。数据范围树中节点数量 [0,500]。 解答 在中序递归遍历的基础，用一个pre指针保存中序遍历的前一个结点。遍历顺序就是双线链表的建立顺序； 每一个结点访问时它的左子树肯定被访问过了，所以放心大胆的改它的left指针，不怕树断掉； 同理，pre指向的结点保存的数肯定小于当前结点，所以其左右子树肯定都访问过了，所以其right指针也可以直接改。 最后需要一直向左找到双向链表的头结点。 1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode * pre = NULL; //pre记录当前节点指针 TreeNode* convert(TreeNode* root) &#123; dfs(root); while(root &amp;&amp; root-&gt;left) root = root-&gt;left; //遍历找到第一个节点 return root; &#125; //深度优先遍历 void dfs(TreeNode* root) &#123; if(!root) return; dfs(root-&gt;left); root-&gt;left = pre; //左节点就是前节点 if(pre) pre-&gt;right = root; //前节点的后继节点就是自己 pre = root; dfs(root-&gt;right); &#125;&#125;; 重点是：利用深度优先遍历 + pre节点记录 66. 两个链表的第一个公共结点 题目 https://www.acwing.com/problem/content/62/ 12345678910111213141516输入两个链表，找出它们的第一个公共结点。当不存在公共节点时，返回空节点。数据范围链表长度 [1,2000]。样例给出两个链表如下所示：A： a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3输出第一个公共节点c1 不要用两层循环暴力破解！！！ 解答 不同部分为a和b，公共部分为c；a + c + b = b + c + a;让两个一起走，a走到头就转向b， b走到头转向a，则在公共部分相遇 12345678910111213141516171819202122232425/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *findFirstCommonNode(ListNode *headA, ListNode *headB) &#123; ListNode * A = headA; ListNode * B = headB; while(A != B) &#123; //A遍历完开始走到B if(A) A = A-&gt;next; else A = headB; //B遍历完开始走到A if(B) B = B-&gt;next; else B = headA; &#125; return A; &#125;&#125;; 826. 单链表 题目 1234567891011121314151617181920212223242526272829303132333435363738实现一个单链表，链表初始为空，支持三种操作：向链表头插入一个数；删除第 k 个插入的数后面的数；在第 k 个插入的数后插入一个数。现在要对该链表进行 M 次操作，进行完所有操作后，从头到尾输出整个链表。注意:题目中第 k 个插入的数并不是指当前链表的第 k 个数。例如操作过程中一共插入了 n 个数，则按照插入的时间顺序，这 n 个数依次为：第 1 个插入的数，第 2 个插入的数，…第 n 个插入的数。输入格式第一行包含整数 M，表示操作次数。接下来 M 行，每行包含一个操作命令，操作命令可能为以下几种：H x，表示向链表头插入一个数 x。D k，表示删除第 k 个插入的数后面的数（当 k 为 0 时，表示删除头结点）。I k x，表示在第 k 个插入的数后面插入一个数 x（此操作中 k 均大于 0）。输出格式共一行，将整个链表从头到尾输出。数据范围1≤M≤100000所有操作保证合法。输入样例：10H 9I 1 1D 1D 0H 6I 3 6I 4 5I 4 5I 3 4D 6输出样例：6 4 6 5 解答 静态链表 1451. 单链表快速排序 题目 12345678910111213给定一个单链表，请使用快速排序算法对其排序。要求：期望平均时间复杂度为 O(nlogn)，期望额外空间复杂度为 O(logn)。思考题： 如果只能改变链表结构，不能修改每个节点的val值该如何做呢？数据范围链表中的所有数大小均在 int 范围内，链表长度在 [0,10000]。输入样例：[5, 3, 2]输出样例：[2, 3, 5] 解答 首先复习以下快速排序 123456789101112131415161718192021222324252627282930313233343536//用于默写test中国暖的算法#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e6 + 1;int nums[N]; //用于存储数据int n;void quick_sort(int nums[], int l, int r)&#123; if(l &gt;= r) return; int mid = nums[l + r &gt;&gt; 1]; int i = l - 1; int j = r + 1; while(i &lt; j) &#123; do i++; while (nums[i] &lt; mid); do j--; while (nums[j] &gt; mid); if (i &lt; j) swap(nums[i], nums[j]); //表示num[i] 在左边但是数据却比 num[j] 大 &#125; quick_sort(nums, l, i); quick_sort(nums, i + 1, r); return;&#125;int main()&#123; scanf(\"%d\", &amp;n); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;nums[i]); quick_sort(nums, 0, n - 1); for(int i = 0; i &lt; n; i++) printf(\"%d \", nums[i]); return 0;&#125; 则链表排序为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e5 + 1;int n;struct ListNode&#123; int val; ListNode *next; ListNode(int x) : val(x), next(NULL) &#123;&#125;&#125;;void quickSort(ListNode *head, ListNode *tail)&#123; if (head != tail) &#123; int key = head-&gt;val; ListNode *p = head, *q = p-&gt;next; while (q != tail) &#123; if (q-&gt;val &lt; key) &#123; p = p-&gt;next; swap(p-&gt;val, q-&gt;val); &#125; q = q-&gt;next; &#125; if (p != head) swap(head-&gt;val, p-&gt;val); quickSort(head, p); quickSort(p-&gt;next, tail); &#125;&#125;ListNode *quickSortList(ListNode *head)&#123; if (!head) return head; quickSort(head, NULL); return head;&#125;int main()&#123; ListNode * head = new ListNode(-1); ListNode * cur = head; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; int val; scanf(\"%d\", &amp;val); ListNode * node = new ListNode(val); cur-&gt;next = node; cur = cur-&gt;next; &#125; ListNode * result = quickSortList(head-&gt;next); while (result) &#123; printf(\"%d\", result-&gt;val); result = result-&gt;next; &#125; return 0;&#125; 1560. 反转链表 题目 https://www.acwing.com/problem/content/1562/ 1234567891011121314151617181920212223242526272829303132333435363738394041给定一个常数 K 和一个单链表 L，请你在单链表上每 K 个元素做一次反转，并输出反转完成后的链表。如果链表最后一部分不足 K 个元素，则最后一部分不翻转。例如，假设 L 为 1→2→3→4→5→6，如果 K=3，则你应该输出 3→2→1→6→5→4；如果 K=4，则你应该输出 4→3→2→1→5→6。### 补充1、本题中可能包含不在链表中的节点，这些节点无需考虑。### 输入格式第一行包含头节点地址，总节点数量 N 以及常数 K。节点地址用一个 5 位非负整数表示（可能有前导 0），NULL 用 −1 表示。接下来 N 行，每行描述一个节点的信息，格式如下：`Address Data Next`其中 Address 是节点地址，Data 是一个整数，Next 是下一个节点的地址。### 输出格式将重新排好序的链表，从头节点点开始，依次输出每个节点的信息，格式与输入相同。### 数据范围1≤N≤105,1≤K≤N### 输入样例：00100 6 400000 4 9999900100 1 1230968237 6 -133218 3 0000099999 5 6823712309 2 33218### 输出样例：00000 4 3321833218 3 1230912309 2 0010000100 1 9999999999 5 6823768237 6 -1 参考链接 https://www.acwing.com/problem/","categories":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"}],"tags":[{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"}]},{"title":"MQ-Kafka4-日志存储","slug":"MQ/MQ-Kafka4-日志存储","date":"2022-01-06T15:35:06.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka4-日志存储/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka4-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/","excerpt":"","text":"文件目录 为了防止Log过大，Kafka映日了日志分段(LogSegment)的概念，将Log 切分为多个LogSegment。 Log在物理上只已文件夹的形式存储，而每个LogSegment对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件(比如以&quot;.txnindex&quot;为后缀的事务索引文件) Log对应的是一个命名形式为 &lt;topic&gt;-&lt;partition&gt;的文件夹，假设有一个名为 &quot;topic-log&quot;的主题，此主题具有4个分区，那么实际物理存储上表现为：“topic-log-0”、“topic-log-1”、“topic-log-2”、“topic-log-3” 向Log 中追加消息时是顺序写入的，只有最后一个LogSegment 才能执行写入操作，在此之前所有的LogSegment 都不能写入数据 为了方便描述，将最后一个LogSegment 称为activeSegment ，表示当前活跃的日志分段。随着消息的不断写入，当 activeSegment 满足一定的条件时，就创建新的activeSegment，并将消息追加到新的activeSegment 为了便于消息的检索，每个LogSegment 中的日志文件（以“ .log ”为文件后缀）都有对应的两个索引文件： 偏移量索引文件，以“ .index ”为文件后缀 时间戳索引文件，以“ .timeindex ”为文件后缀 每个LogSegment 都有一个基准偏移量baseOffset，用来表示当前LogSegment中第一条消息的offset 。偏移量是一个64 位的长整型数，日志文件和两个索引文件都是根据基准偏移量（ baseOffset ）命名的，名称固定为20 位数字，没有达到的位数则用0 填充。比如第一个LogSegment 的基准偏移量为0 ，对应的日志文件为00000000000000000000.log 第2 个 LogSegment 对应的基准位移是133 ，也说明了该LogSegment 中的第一条消息的偏移量为133 ＇同时可以反映出第一个LogSegment 中共有133 条消息(偏移量从0 至132的消息） 消费者提交的位移是保存在Kafka 内部的主题consumer offsets中的，初始情况下这个主题并不存在，当第一次有消费者消费消息时会自动创建这个主题 Kafka文件目录布局 每一个根目录都会包含最基本的4个检查点文件（ xxx-checkpoint ）和meta.propties 文件。在创建主题的时候，如果当前broker中不止配置了一个根目录，那么会挑选分区数最少的那个根目录来完成本次创建任务 日志格式 消息集称为 Record Batch，其内部可包含一条或多条消息 在消息压缩的情形下， Record Batch Header 部分（参见图5-7 左部， 从first offset 到 records count 字段）是不被压缩的，而被压缩的是records 字段中的所有内容。生产者客户端中的ProducerBatch 对应这里的RecordBatch,而ProducerRecord 对应这里的Record Record Record包含： length ：消息总长度。 attributes ： 弃用，但还是在消息格式中占据1B 的大小， 以备未来的格式扩展。 timestamp delta ： 时间戳增量。通常一个time stamp 需要占用8 个字节，如果像这里一样保存与RecordBatch 的起始时间戳的差值，则可以进一步节省占用的字节数。 offset delta ： 位移增量。保存与RecordBatc h 起始位移的差值，可以节省占用的字节数 headers ：这个字段用来支持应用级别的扩展，包含key和value ，一个Record 里面可以包含0 至多个Header RecodeBatch包含： first offset ：表示当前RecordBatch 的起始位移。 length ：计算从partition leader epoeh 字段开始到末尾的长度。 partition leader epoeh ：分区leader 纪元，可以看作分区leader 的版本号或更新次数 magic ：消息格式的版本号，对v2 版本而言， magie 等于2 。 attributes ：消息属性(2B) 低 3 位表示压缩格式， 第4 位表示时间戳类型； 第5 位表示此RecordBatch 是否处于事务中，0 表示非事务， l 表示事务。 第6 位表示是否是控制消息(ControlBatch)。0 表示非控制消息，而 1 表示是控制消息，控制消息用来支持事务功能。 last offset delta: RecordBatch 中最后一个Record 的offset 与自rst offset 的差值。主要被broker 用来确保RecordBatch 中Record 组装的正确性。 first timestamp: RecordBatch 中第一条Record 的时间戳。 max timestamp: RecordBatch 中最大的时间戳， 一般情况下是指最后一个Record的时间戳，和last offset delta 的作用一样，用来确保消息组装的正确性。 produeer id: PID ，用来支持幂等和事务 日志索引 每个日志分段文件对应了两个索引文件，主要用来提高查找消息的效率。 偏移量索引文件用来建立消息偏移量（ offset ）到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置； 时间戳索引文件则根据指定的时间戳（ timestamp ）来查找对应的偏移量信息 Kafka 中的索引文件以 稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引页。每当写入一定量（由broker 端参数log.index.interval.bytes 指定，默认值为4096 ，即4KB ）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应地可以增加或缩小索引项的密度。 稀疏索引通过 MappedByteBuffer 将索引文件映射到内存中，以加快索引的查询速度。 偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。 时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。 当日志分段达到一定条件则创建新的日志分段，一定条件包括： 当前日志分段文件的大小超过了broker 端参数log.segment.bytes 配置的值，默认值1GB 当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.hours参数配置的值。如果同时配置了log.roll.ms 和log.roll.hours 参数，那么log.roll.ms 的优先级高。默认情况下，只配置了log.roll.hours 参数，其值为168，即7天。 偏移量索引文件或时间戳索引文件的大小达到broker 端参数 log.index.size.max.bytes，默认10M 追加的消息的偏移量与当前日志分段的偏移量之间的差值大于Integer.MAX_VALUE,即offset - baseOffset &gt; Integer.MAX_VALUE 对非当前活跃的日志分段而言，其对应的索引文件内容己经固定而不需要再写入索引项，所以会被设定为只读,而对当前活跃的日志分段C activeSegment ）而言，索引文件还会追加更多的索引项，所以被设定为可读写 在索引文件切分的时候， Kafka 会关闭当前正在写入的索引文件并置为只读模式，同时以可读写的模式创建新的索引文件 Kafka 在创建索引文件的时候会为其预分配log.index.size.max.bytes 大小的空间，注意这一点与日志分段文件不同，只有当索引文件进行切分的时候， Kafka 才会把该索引文件裁剪到实际的数据大小。也就是说，与当前活跃的日志分段对应的索引文件的大小固定为log.index.size.max.bytes，而其余日志分段对应的索引文件的大小为实际的占用空间。 偏移量索引 偏移量索引项的格式如下图，每个索引项占用8 个字节，分为两个部分。 relativeOffset：相对偏移量(4B)，表示消息相对于baseOffset 的偏移量，当前索引文件的文件名即为baseOffset 的值。 position：物理地址(4B)，也就是消息在日志分段文件中对应的物理位置 消息的偏移量(offset)占用8 个字节，也称绝对偏移量。索引项中没有直接使用绝对偏移量而改为只占用4 个字节的相对偏移量CrelativeOffset =offset - baseOffset，这样可以减小索引文件占用的空间。 举个例子， 一个日志分段的baseOffset 为32 ，则文件名是00000000000000000032.log，offset 为35 的消息在索引文件中的relativeOffset 的值为35 - 32=3 如果我们要查找偏移量为23 的消息，那么应该怎么做呢？ 首先通过二分法在偏移量索引文件中找到不大于23 的最大索引项，即［ 22 , 656 ］， 然后从日志分段文件中的物理位置656 开始顺序查找偏移量为23 的消息。 那么用户又是如何查找日志分段的呢？ 并不是顺序查找，而是使用跳跃表的结构。Kafka的每个日志对象中使用了 ConcurrentSkipListMap来保存各个日志分段，每个日志分段的baseOffset 作为key,这样可以根据指定偏移量快速定位到消息所在的日志分段 Kafka 强制要求索引文件大小必须是索引项大小的整数倍，对偏移量索引文件而言，必须为8 的整数倍 时间戳索引 每个索引项占用12 个字节，分为两个部分 timestamp ： 当前日志分段最大的时间戳。 relativeOffset ：时间戳所对应的消息的相对偏移量 时间戳索引文件中包含若干时间戳索引项， 每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的timestamp ，否则不予追加 与偏移量索引文件相似，时间戳索引文件大小必须是索引项大小(12B)的整数倍 会在偏移量索引文件和时间戳索引文件中分别增加一个偏移量索引项和时间戳索引项。两个文件增加索引项的操作是同时进行的，但并不意味着偏移量索引中的relativeOffset 和时间戳索引项中的relativeOffset 是同一个值 如果一个失败了怎么办？ 为什么会出现不是同一个值的情况 如果要查找指定时间戳 targetTimeStamp = 1526384718288 开始的消息，首先是找到不小于 指定时间戳的日志分段。这里就无法使用跳跃表来快速定位到相应的日志分段了， 需要分以下 几个步骤来完成。 步骤1 ： 将 targetTimeStamp 和每个日志分段中的最大时间戳 largestTimeStamp 逐一对比，直到找到不小于targetTimeStamp 的 largestTimeStamp 所对应的日志分段。日志分段中的 largestTimeStamp 的计算是先查询该日志分段所对应的时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于0，则取其值，否则取该日志分段的最近修改时间。 步骤2 ： 找到相应的日志分段之后，在时间戳索引文件中使用二分查找算法查找到不大于targetTimeStamp 的最大索引项，即［152638478283, 28］，如此便找到了一个相对偏移量28 。 步骤3 ： 在偏移量索引文件中使用二分算法查找到不大于28 的最大索引项，即［26, 838 ] 步骤4 ：从步骤1中找到日志分段文件中的838 的物理位置开始查找不小于targetTimeStamp的消息 日志清理 Kafka 提供了两种日志清理策略 日志删除(Log Retention)：按照一定的保留策略直接删除不符合条件的日志分段 日志压缩(Log Compaction)：针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本 通过broker端参数 log.cleanup.policy 来设置日志清理策略 “delete”：即采用日志删除的清理策略 “compact”: 即采用日志压缩的清理策略 “delete,compact”：同时迟滞日志删除和日志压缩两种策略 日志清理 在Kafka的日志管理器中有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过 broker 端参数 log.retention.check.interval.ms配置，默认5 分钟。当前日志分段的保留策略有3 种： 基于时间的保留策略 基于日志大小的保留策略 基于日志起始偏移量的保留策略 基于时间 日志删除任务 检查日志文件中是否有保留时间超过设定的阀值(retentionMs)来寻找可删除的日志分段文件集合(deletableSegments)，retentionMs 可以通过broker端参数log.retention.hours、log.retention.minutes 和log.retention.ms 来配置，默认7 天 查找过期的日志分段文件，并不是简单地根据日志分段的最近修改时间lastModifiedTime来计算的， 而是通过日志分段对应的时间戳索引文件，找出最后一条索引项(如果不大于0，则取最近修改时间lastModifiedTime) 如果所有的日志分段都己过期， 但该日志文件中还要有一个日志分段用于接收消息的写入，即必须要保证有一个活跃的日志分段acti veSegment ，在此种情况下，会先切分出一个新的日志分段作为activeSegment ， 然后执行删除操作 删除日志分段步骤 首先会从Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。 然后将日志分段所对应的所有文件添加上 .deleted 的后缀（当然也包括对应的索引文件） 。 最后交由一个以 delete-file 命名的延迟任务来删除这些以 .deleted为后缀的文件，这个任务的延迟执行时间可以通过 file.delete.delay.ms 参数默认1 分钟 基于日志大小 日志删除任务会检查当前日志的大小是否超过设定的阔值(retentionSize)来寻找可删除的日志分段的文件集合(deletableSegments)，retentionSize 可以通过broker 端参数log.retention.bytes 来配置，默认值为 -1，表示无穷大 注意 log.retention.bytes 配置的是 Log 中所有日志文件的总大小，而不是单个日志分段（确切地说应该为.log 日志文件）的大小。单个日志分段的大小由broker 端参数 log.segment.bytes 来限制，默认值为1GB 基于日志大小的保留策略与基于时间的保留策略类似 基于日志起始偏移量 基于日志起始偏移量的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量 baseOffset 是否小于等于logStartOffset，若是，则可以删除此日志分段。 如图，假设 logStartOffset 等于25，日志分段 1 的起始偏移量为0，日志分段2 的起始偏移量为11，日志分 段3 的起始偏移量为23 ，通过如下动作收集可删除的日志分段的文件集合deletableSegments : 从头开始遍历每个日志分段，日志分段 1 的下一个日志分段的起始偏移量为11 ，小于 logStartOffset 的大小，将日志分段 1 加入deletableSegments 。 日志分段2 的下一个日志偏移量的起始偏移量为23 ，也小于logStartOffset 的大小，将日志分段2 页加入deletableSegments 日志分段3 的下一个日志偏移量在 logStartOffset 的右侧，故从日志分段3 开始的所有日志分段都不会加deletableSegments logStartOffset是怎么来的？ 一般情况下：日志文件的起始偏移量 logStartOffset 等于第一个日志分段的baseOffset，但可以通过脚本或请求进行修改 日志压缩 如果只关心 key 对应的最新 value 值，则可以开启Kafka 的日志清理功能，Kafka 会定期将相同 key 的消息进行合井，只保留最新的value值 注意区分日志压缩与消息压缩 Log Compaction 执行前后，日志分段中的每条消息的偏移量和写入时的偏移量保持一致。Log Compaction 会生成新的日志分段文件，日志分段中每条消息的物理位置会重新按照新文件来组织 拉取状态是客户端保存的，这个时候如果进行了日志压缩，是否导致乱序？ 如何对日志文件中消息的Key进行筛选操作？ 每个日志清理线程都会使用 SkimpyOffsetMap的对象来构建key与offset的映射关系的哈希表 日志清理需要遍历两次日志文件 第一次：遍历把每个key的哈希值和最后出现的offset都保存在SkimpyOffsetMap中 第二次：检查每个消息的偏移量在Map中是否一样，否则就清理 墓碑消息是什么？ 执行日志压缩之后，日志分段的大小会比原来小，如何防止出现大量小文件？ 清理过程中并不对单个的日志分段进行单独清理，而是将日志文件中 offset 从 0 - firstUncleanableOffset的所有日志进行分组。每组中日志分段占用空间大小之和不超过 segmentSize( log.segment.bytes)，清理后生成一个新的日志分段 磁盘存储 在印象中，磁盘的速率要远低于内存，其实这要看我们怎么样使用磁盘。顺序写盘的速度不仅比随机写盘的速度快，而且也比随机写内存的速度快 Kafka在设计时候采用了文件追击的方式来写入消息，只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作 日志压缩是不是破坏了顺序写盘？ 页缓存 页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘I/O 的操作。其实是将磁盘中的数据缓存到内存中，将对磁盘的访问变为内存的访问 当进程准备读取磁盘上的文件时，操作系统会先查看待读取的数据所在的页(page)是否在页缓存(pagecache)中 如果存在(命中)则直接返回数据 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。 同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在， 则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性 Linux 操作系统中的vm.dirty background ratio 参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页， 一般设置为小于10 的值即可 Kafka 中大量使用了页缓存，这是Kafka 实现高吞吐的重要因素之一。 Kafka 中也提供了同步刷盘及间断性强制刷盘(fsync)的功能，这些功能可通过log.flush.interval.messages 、log.flush.int erval.ms 等参数来控制，但不建议使用，会严重影响性能，消息的可靠性应该由多副本机制保证 另外Linux会使用磁盘的一部分做为swap分区，非活跃进行调入swap分区，把进程空出来让活跃的进程使用，Kafka也应该尽量避免使用 vm.swappiness vm.swappiness 100 表示积极使用 0 表示任何时候都不要发生交换 磁盘I/O流程 磁盘IO四种场景如下： 用户调用IO操作接口，数据流为：应用程序buffer -&gt; C 库标准IObuffer -&gt; 文件系统页缓存 -&gt; 通过具体文件系统到磁盘 用户调用文件I/O，数据流为：应用程序buffer -&gt; 文件系统页缓存 -&gt; 通过具体文件系统到磁盘 用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘 用户使用类似dd工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘。 零拷贝 参考链接 《深入理解Kafka核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka3-主题与分区","slug":"MQ/MQ-Kafka3-主题与分区","date":"2022-01-06T12:43:02.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka3-主题与分区/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka3-%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA/","excerpt":"","text":"主题作为消息的归类，可以再细分为一个或多个分区，分区则可看作对消息的二次归类。 分区的划分不仅为Kafka 提供了可伸缩性、水平扩展的功能，还通过多副本机制提高数据可靠性 主题与分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段(LogSegment)，每个日志分段还可以细分为索引文件、日志存储文件和快照文件等 分区的管理 优先副本的选举 分区使用多副本机制来提升可靠性，但只有leader 副本对外提供读写服务，而follower 副本只负责在内部进行消息的同步 对同一个分区而言， 同一个broker 节点中不可能出现它的多个副本， 即Kafka 集群的一个broker 中最多只能有它的一个副本， 可以将leader 副本所在的broker 节点叫作分区的leader 节点，而follower副本所在的broker 节点叫作分区的follower 节点。 在创建主题的时候，该主题的分区及副本会尽可能均匀地分布到Kafka 集群的各个broker节点上，对应的leader 副本的分配也比较均匀。 将leader 副本所在的broker 节点叫作分区的leader 节点，而follower副本所在的broker 节点叫作分区的follower 节点。如果Kafka中leader副本过于集中在同一个节点上，集群会出现负载失衡的情况。 为此，Kafka 引入了优先副本(preferred replica) : 指在AR 集合列表中的第一个副本。也可以称之为preferred leader 。Kafka 要确保所有主题的优先副本在Kafka 集群中均匀分布，这样就保证了所有分区的leader 均衡分布。 所谓的优先副本的选举是指通过一定的方式促使优先副本选举为leader 副本，以此来促进集群的负载均衡， 这一行为也可以称为“分区平衡” 分区平衡并不意味着Kafka 集群的负载均衡！！！ 因为还要考虑集群中的分区分配是否均衡。每个分区的leader 副本的负载也是各不相同的 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的broker节点，计算每个broker 节点的分区不平衡率（ broker 中的不平衡率＝非优先副本的leader 个数／分区总数）是否超过leader .mbalance.per.broker.percentage 参数配置的比值，默认值为10% ，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。执行周期由参数leader.Imbalance.check .interval . seconds 控制，默认值为3 00 秒 分区重分配 当集群中的一个节点突然若机下线时， 如果节点上的分区是单副本的，这些分区就变得不可用了，在节点恢复前，相应的数据也就处于丢失状态； 如果节点上的分区是多副本的，位于这个节点上的 leader 副本的角色会转交到集群的其他follower副本中。 总而言之，这个节点上的分区副本都已经处于功能失效的状态， Kafka 并不会将这些失效的分区副本自动地迁移到集群中剩余的可用broker 节点上，如果放任不管，则不仅会影响整个集群的均衡负载，还会影响整体服务的可用性和可靠性 分区重分配的基本原理是 第一步：添加新副本(增加副本因子) 第二步：新副本将从分区的 leader 副本那里复制所有的数据。根据分区的大小不同，复制过程可能需要花一些时间，因为数据是通过网络复制到新副本上的。 第三步：在复制完成之后，控制器将旧副本从副本清单里移除(恢复为原先的副本因子数)注意在重分配的过程中要确保有足够的空间。 分区重分配的量如果太大必然会严重影响整体的性能，对副本间的复制流量加以限制来保证重分配期间整体服务不会受太大的影响，复制限流有两种实现方式： kafka-config. sh 脚本和kafka-reassign-partitions .sh 脚本 参考链接 《深入理解Kafka 核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka2-生产者与消费者","slug":"MQ/MQ-Kafka2-生产者与消费者","date":"2022-01-05T17:03:04.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka2-生产者与消费者/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka2-%E7%94%9F%E4%BA%A7%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E8%80%85/","excerpt":"","text":"生产者 如下图是生产者客户端的整体架构 整个生产者客户端由两个线程协调运行，这两个线程分别为 主线程：由 Producer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（ RecordAccumulator ，也称为消息收集器〉中 Sender 线程（发送线程）：负责从RecordAccumulator 中获取消息并将其发送到Kafka 中 RecordAccumulator 主要用来缓存消息以便批量发送，进而减少网络传输的资源消耗。RecordAccumulator 缓存的大小可通过客户端参数 buffer.memory 配置，默认32MB。 当生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer 的send()方法调用要么被阻塞，要么抛出异常，这个取决于参数 max.block.ms 的配置，此参数的默认值60 秒 重要的参数 acks(字符串类型)：用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的 acks = 1(默认)：生产者发送消息之后，只要分区的leader 副本成功写入消息，那么它就会收到来自服务端的成功响应。 如果消息写入leader 副本并返回成功响应给生产者，且在被其他fo llower 副本拉取之前leader 副本崩溃，那么此时消息还是会丢失，因为新选举的leader 副本中并没有这条对应的消息。 Acks = 0：生产者发送消息之后不需要等待任何服务端的响应 Acks = -1：生产者在消息发送之后，需要等待ISR 中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应 max.request.size：限制生产者客户端能发送的消息的最大值，默认值为1048576B ，即1MB retries 和retry. backoff.ms：retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作，retry. backoff.ms 则用来设置两次重试之间的间隔 compression.type：压缩方式 connections.max.idle.ms：指定在多久之后关闭闲置的连接，默认值是540000 ( ms ） ，即9 分钟 linger.ms：指定生产者发送ProducerBatch 之前等待更多消息（ ProducerRecord ）加入Producer Batch 的时间，默认值为0，生产者客户端会在ProducerBatch 被填满或等待时间超过 linger.ms 值时发迭出去 recvive.buffer.bytes：设置Socket接收消息的缓冲区(SO_RECBUF)的大小，默认值32KB send.buffer.bytes：设置Socket发送消息的缓冲区(SO_RECBUF)的大小，默认值128KB request.time.ms：配置Producer等待请求响应的最长时间，默认值为3000(ms) 消费者 消费者(Consumer)负责订阅Kafka 中的主题(Topic)，并从订阅的主题上拉取消息，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 主题中共有4 个分区(Partition) : PO 、Pl 、P2 、P3 。有两个消费组A和B 都订阅了这个主题，消费组A 中有4 个消费者(CO 、Cl 、C2 和C3)，消费组B 中有2个消费者(C4 和CS ） 。按照Kafka 默认的规则，最后的分配结果是消费组A 中的每一个消费者分配到l1个分区，消费组B 中的每一个消费者分配到2 个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。即每一个分区只能被一个消费组中的一个消费者所消费 假设目前某消费组内只有一个消费者C0 ，订阅了一个主题，这个主题包含7 个分区： PO 、Pl 、P2 、P3 、P4 、 PS 、P6 也就是说，这个消费者C0 订阅了7 个分区。消费组内又加入了一个新的消费者C1，按照既定的逻辑，需要将原来消费者C0 的部分分区分配给消费者C1消费， 彼此之间并无逻辑上的干扰 此时又加入了消费者C3，则按照上述规则继续分配。一昧地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况就会有消费者分配不到任何分区 通过以上方式 Kafka支持两种投递方式： 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布／订阅模式的应用 如何实现多副本的发布订阅与广播 每一个消费者只隶属于一个消费组 ！！！！！ 一个消费者可以订阅一个或多个主题 每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id 来配置，默认值为空宇符串 消息消费 Kafka 中的消费是基于拉模式的，Kafka 中的消息消费是一个不断轮询的过程 在默认的方式下，消费者每隔5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在拉取的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 在Kafka 消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用）。可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。 再平衡 再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，在再均衡发生期间，消费组内的消费者是无法读取消息的，即一段时间消费组会变得不可用。 另外，当一个分区被重新分配给另一个消费者时， 消费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作， 之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生 参考链接 《深入理解Kafka 核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka1-初识Kafka","slug":"MQ/MQ-Kafka1-初识Kafka","date":"2022-01-05T16:15:26.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka1-初识Kafka/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka1-%E5%88%9D%E8%AF%86Kafka/","excerpt":"","text":"Kafka一个采用Scala语言(0.9.0版本有java版本)开发的多分区、多副本且基于ZooKeeper协调的分布式消息系统，主要作用有： 消息系统：Kafka除了传统的消息中间件都有的系统解耦、流量削峰、异步通信的功能功能，还提供了消息顺序性消费保障和回溯消费的功能 存储系统：Kafka可以将消息持久化到磁盘。通过Kafka的消息持久化功能和多副本机制，可以将Kafka做长期的存储系统使用(数据可以&quot;永久&quot;保存、以及 主题日志压缩) 流式处理平台：Kafka不仅为流式处理框架提供可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、链接、变换和聚合等操作 基本概念 一个典型的Kafka系统架构 Producer：生产者，也就是消息发送方。负责创建消息，并将其投递给Kafka Broker: 服务代理节点。对于Kafka而言，Broker可以简单地看做一个独立的Kafka服务节点或Kafka服务实例。一个或多个Broker组成一个Kafka集群 Consumer：消费者，消息接收方。消费者连接到Kafka上并接收消息，进行相关的业务处理 Zookeeper：负责集群元数据的管理、控制器的选举等操作 主题与分区 主题(Topic)：主题是一个逻辑概念，Kafka的消息以主题为单位进行归类，生产者负责讲消息发送到特定的主题(发送到kafka集群中的每一条消息都要指定一个主题)，而消费者负责订阅主题并进行消费 分区(Partition): 每个主题可以分为多个分区，一个分区只属于单个主题，也叫主题分区(Tppic-Partition)。分区在存储层面可以看作是一个可追加的日志(Log)文件，消息被追加到分区日志文件的时候都会分配一个特定的偏移量(offset)。offset是消息在分区中的唯一标识，Kafka通过 offset 来保证消息在分区的顺序性。不过offset并不跨分区，也就是说，Kafka保证的是分区有序而不是主题有序 如下图，主题有四个分区，消息被顺序追加到每个分区日志文件的尾部 Kafka中的分区可以分布在不同的服务器(broker)上，也就是说，一个主题可以横跨多个broker。以此来提供比单个broker更强大的性能。 创建主题时候可以通过指定参数来设置主题的分区个数；也可以在创建完成之后去修改分区的数量，实现水平拓展 每一个消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区 问题1：分区规则是什么？？ TODO 分区容灾 Kafka分区引入了多副本(Replica)机制，同一个分区的不同副本中保存的是相同的消息(在同一时刻，副本之间并非完全相同)，副本之间是&quot;一主多从&quot;的关系，其中leader副本负责处理读写请求，follwer副本只负责与leader副本进行消息同步。 通过设置副本因子来指定分区的副本数量 follower 不支持读请求吗？ 生产者与消费者只与leader副本进行交互，follower副本只负责消息的同步 容灾概念： AR(Assigned Replicas): 分区中的所有副本统称 ISR(In-Sync Replicas): 所有与Leader副本保持一定程度同步的副本(包括leader副本在内)组成 OSR(Out-of-Sync Replicas): 与leader副本同步滞后过多的副本(不包括leader副本)组成 一定程度可以通过参数控制 leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除，如果OSR 集合中有follower 副本&quot;追上&quot;了leader 副本，那么leader 副本会把它从OSR 集合转移至ISR 集合 默认情况下，当leader副本出现故障，只有在ISR集合中的副本才有资格被选举为新的leader 以上选举规则也是可以修改的 问题1：如果所有副本都处于OSR怎么办 TODO 消息拉取 HW(High Watermark):高水位，标识一个特定的消息偏移量(offset)，消费者智能拉取到这个offset之前的消息 LEO(Log End Offset)的缩写，标识当前日志文件中下一条待写入消息的offset 上图代表一个日志文件，这个日志文件中有9条消息，第一条消息的offset(LogStartOffset)为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0-5之间的消息，而offset为6的消息对消费者而言是不可见的。 分区ISR集合中的每个副本都会维护自身的LEO，而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息 注意上图所示 HW 为5 ，LEO 为 9 消费者容灾 Consumer使用拉(Pull)模式从服务端拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时，可以根据之前保存的消费者位置重新拉取需要的消息进行消费，保证消息不会丢失 参考链接 《深入理解Kafka核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Redis","slug":"MQ/MQ-Redis","date":"2022-01-04T16:08:38.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2022/01/05/MQ/MQ-Redis/","link":"","permalink":"http://xboom.github.io/2022/01/05/MQ/MQ-Redis/","excerpt":"","text":"在进行消息队列预研的时候，发现Redis也能做为消息队列，看看Redis做为消息队列是如何实现的 Redis做为消息队列有三种方案 List Streams Pub/Sub List Redis的List是简单的字符串列表，底层由 quicklist 实现。 List消息队列原理 命令 用法 描述 LPUSH LPUSH key value [value …] 将一个或多个值value插入到列表key的表头，如果有多个value值，那么各个value值按从左到右的顺序依次插入到表头 RPUSH RPUSH key value [value …] 将一个或多个值value插入到列表key的表尾(最右边) LPOP LPOP key [count] 移除并返回列表key的头元素(count 指定出队列数目) BLPOP BLPOP key [key …] timeout 移除并获取列表的第一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 RPOP PROP key 移除并返回列表key的尾元素 BRPOP BRPOP key [key …] timeout 移除并获取列表的最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 BRPRPLPUSH BRPOPLPUSH source destination timeout 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它；如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 RPOPLPUSH RPOPLPUSH source destination 命令RPOPLPUSH 在一个原子时间内，执行以下两个动作：将列表source中的最后一个元素(尾元素)弹出，并返回给客户端。将source弹出的元素插入到猎豹destination,做为destination列表的头元素 LLEN LLEN key 返回列表key的长度。如果key不存在，则key被解释为一个空列表，返回0。如果key不是列表类型，返回一个错误 LRANGE LRANGE key start stop 返回列表key中指定区间内的元素，区间以偏移量 start 和 stop 指定 使用命令组合即可实现消息的出队入队 LPUSH、RPOP 左进右出 RPUSH、LPOP 右进左出 通过LPUSH、RPOP这样的方式，会存在一个性能风险点： 消费者要即使的处理数据，类似要在消费端添加类似 while(true) 的逻辑，不停的调用RPOP或LPOP命令，这样就会给消费者程序带来不必要的性能损失，于是 --&gt; Redis 提供了BLPOP、BRPOP这样阻塞式读取的命令(带B-Bloking的都是阻塞式)，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据(节省不必要要的CPU开销) LPUSH、BRPOP左进右阻塞出 RPUSH、BLPOP右进左阻塞出 因为 Redis 单线程的特点，所以在消费数据时，同一个消息不会同时被多个 consumer 消费掉，但是需要我们考虑消费不成功的情况 可靠队列模式 List 队列中的消息一经发送出去，便从队列里删除。如果由于网络原因消费者没有收到消息，或者消费者在处理这条消息的过程中崩溃了，就再也无法还原出这条消息。究其原因，就是缺少消息确认机制。 为了保证消息的可靠性，消息队列都会有完善的消息确认机制(Acknowledge)，即消费者向队列报告消息已收到或已处理的机制 RPOPLPUSH、BRPOPLPUSH (阻塞)从一个 list 中获取消息的同时把这条消息复制到另一个 list 里(可以当做备份)，而且这个过程是原子的 数据标识从一个 List 取出后放入另一个 List，业务操作安全执行完成后，再去删除 List 中的数据，如果有问题的话，很好回滚 延时消息 通过 zset 来实现延时消息队列，原理就是将消息加到 zset 结构后，将要被消费的时间戳设置为对应的 score 即可，只要业务数据不会是重复数据就可以 Pub/Sub 消息模型包含 点对点：Point-to-Point(P2P) 发布订阅：Publish/Subscribe(Pub/Sub) List 实现方式就是点对点模式，Redis的发布订阅模式(消息多播)就是真正的Redis MQ &quot;发布/订阅&quot;模式包含两种角色，分别是发布者和订阅者。订阅者可以订阅一个或者多个频道(channel)，而发布者可以向指定的频道(channel)发送消息，所有订阅此频道的订阅者都会收到此消息 Redis 通过 PUBLISH 、 SUBSCRIBE 等命令实现了订阅与发布模式， 这个功能提供两种信息机制 订阅/发布到频道 订阅/发布到模式 频道可以先理解为是个 Redis 的 key 值，而模式则是一个类似正则匹配的 Key，只是个可以匹配给定模式的频道。这样就不需要显式的去订阅多个名称了，可以通过模式订阅这种方式，一次性关注多个频道 Pub/Sub常用命令 命令 用法 描述 PSUBSCRIBE PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道 PUBSUB PUBSUB subcommand [argument [argument …]] 查看订阅与发布系统状态 PUBLISH PUBLISH channel message 将信息发送到指定的频道 PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]] 退订所有给定模式的频道 SUBSCRIBE SUBSCRIBE channel [channel …] 订阅给定的一个或多个频道的信息 UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]] 指退订给定的频道 频道 如上创建一个生产者和两个消费者，消费者1 subscribe channel1 channel2，消费者2subscribe channel1。当生产者使用命令PUBBLISH channel message 向 隧道channel1发送消息时，两个消费者都能。向隧道channel2发送消息时，只有消费者1能够收到消息 其中消费者每次都可以收到3个参数的消息 消息的种类 频道的名称 实际的消息 模式 订阅符合给定模式的频道，命令是 PSUBSCRIBE 如上创建一个生产者和两个消费者，一个使用 SUBSCRIBE channel1，另外一个使用 PSUBSCRIBE chann*,当生产者使用PUBLISH channel1 msg3,两个消费者都能收到消息 PSUBSCRIBE 更像是支持匹配模式的消费者 Redis 发布订阅 (pub/sub) 有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。而且也没有 Ack 机制来保证数据的可靠性，假设一个消费者都没有，那消息就直接被丢弃了。 Streams Redis 5.0 版本新增了一个更强大的数据结构——Stream。它提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 像是个仅追加内容的消息链表，把所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容。而且消息是持久化的 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 Streams 是 Redis 专门为消息队列设计的数据类型，所以提供了丰富的消息队列操作命令 Stream常用命令 描述 命令 添加消息到末尾，保证有序，可以自动生成唯一ID XADD key ID field value [field value …] 对流进行修剪，限制长度 XTRIM key MAXLEN [~] count 删除消息 XDEL key ID [ID …] 获取流包含的元素数量，即消息长度 XLEN key 获取消息列表，会自动过滤已经删除的消息 XRANGE key start end [COUNT count] 以阻塞或非阻塞方式获取消息列表 XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key …] id [id …] 创建消费者组 XGROUP [CREATE key groupname id-or-] [DESTROY key groupname] [DELCONSUMER key groupname consumername] 读取消费者组中的消息 XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key …] ID [ID …] 将消息标记为&quot;已处理&quot; XACK key group ID [ID …] 为消费者组设置新的最后递送消息ID XGROUP SETID [CREATE key groupname id-or-] [DESTROY key groupname] 删除消费者 XGROUP DELCONSUMER [CREATE key groupname id-or-] [DESTROY key groupname] 删除消费者组 XGROUP DESTROY [CREATE key groupname id-or-] [DESTROY key groupname] [DEL 显示待处理消息的相关信息 XPENDING key group [start end count] [consumer] 查看流和消费者组的相关信息 XINFO [CONSUMERS key groupname] [GROUPS key] [STREAM key] [HELP] 打印流信息 XINFO STREAM [CONSUMERS key groupname] [GROUPS key] [STREAM key] [HELP] * 号表示服务器自动生成 ID，后面顺序跟着一堆 key/value 消息ID 必须要比上个 ID 大 -表示最小值 , + 表示最大值,也可以指定最大消息ID，或最小消息ID，配合 -、+ 使用 独立消费 xread 以阻塞或非阻塞方式获取消息列表，指定BLOCK选项即表示阻塞，超过时间0ms(意味用不超时) 阻塞的从尾部读取流，开启新的客户端xadd后发现这里就读到了,block 0 表示永久阻塞 没有给流 mystream 传入一个常规的 ID，而是传入了一个特殊的 ID $ $ 意思是: XREAD 应该使用流 streamtest 已经存储的最大 ID 作为最后一个 ID 当然，也可以指定任意有效的 ID。 而且， XREAD 的阻塞形式还可以同时监听多个 Strem，只需要指定多个键名即可 1127.0.0.1:6379&gt; xread block 0 streams mystream yourstream $ $ 多个客户端监听相同的stream，那么它们都会收到消息！！！，如果想多个客户端监听同一个流怎么办呢？便是创建消费者组 创建消费者组 上述 xread 虽然分发到 N 个客户端，如果想要做的不是向许多客户端提供相同的消息流，而是从同一流向许多客户端提供不同的消息子集。比如下图这样，三个消费者按轮训的方式去消费一个 Stream Redis Stream 借鉴了很多 Kafka 的设计。 Consumer Group：有了消费组的概念，每个消费组状态独立，互不影响，一个消费组可以有多个消费者 last_delivered_id ：每个消费组会有个游标 last_delivered_id 在数组之上往前移动，表示当前消费组已经消费到哪条消息了 pending_ids ：消费者的状态变量，作用是维护消费者的未确认的 id。pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack。如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少。这个 pending_ids 变量在 Redis 官方被称之为 PEL，也就是 Pending Entries List，这是一个很核心的数据结构，它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理。 Stream 不像 Kafak 那样有分区的概念，如果想实现类似分区的功能，就要在客户端使用一定的策略将消息写到不同的 Stream。 xgroup create：创建消费者组 xgreadgroup：读取消费组中的消息 xack：ack 掉指定消息 按消费组消费 Stream 提供了 xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息 ID。它同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除。 参考链接 https://stor.51cto.com/art/202101/640335.htm http://xiaorui.cc/archives/5285","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Docker入门1-Dockerfile","slug":"Docker/Docker入门1-Dockerfile","date":"2021-11-16T15:36:01.000Z","updated":"2021-11-16T15:36:31.000Z","comments":true,"path":"2021/11/16/Docker/Docker入门1-Dockerfile/","link":"","permalink":"http://xboom.github.io/2021/11/16/Docker/Docker%E5%85%A5%E9%97%A81-Dockerfile/","excerpt":"","text":"Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明 指令详解 FROM: 定制的镜像都是基于 FROM 的镜像，即基础镜像 RUN：用于执行后面跟着的命令行命令 12345678#shell 格式RUN &lt;命令行命令&gt;# &lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。#exec 格式RUN [\"可执行文件\", \"参数1\", \"参数2\"]# 例如：# RUN [\"./test.php\", \"dev\", \"offline\"] 等价于 RUN ./test.php dev offline COPY: 复制指令，从上下文目录中复制文件或者目录到容器里指定路径 123456789#格式COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;... &lt;目标路径&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"]#[--chown=&lt;user&gt;:&lt;group&gt;]：可选参数，用户改变复制到容器内文件的拥有者和属组。#&lt;源路径&gt;：源文件或者源目录，可以是通配符表达式，例如：#&lt;目标路径&gt;：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。COPY hom* /mydir/COPY hom?.txt /mydir/ ADD：ADD 指令和 COPY 的使用格类似（同样需求下，官方推荐使用 COPY） ADD 的优点：在执行 &lt;源文件&gt; 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 &lt;目标路径&gt;。 ADD 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。 CMD: 为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。类似于 RUN 指令，但二者运行的时间点不同: CMD 在docker run 时运行 RUN 是在 docker build 注意： 如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。 CMD 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖。 123CMD &lt;shell 命令&gt; CMD [\"&lt;可执行文件或命令&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] CMD [\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数 推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh ENTRYPOINT: 类似于 CMD 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序 优点：在执行 docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。 注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。 1ENTRYPOINT [\"&lt;executeable&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] 可以搭配 CMD 命令使用：一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参 1234FROM nginxENTRYPOINT [\"nginx\", \"-c\"] # 定参CMD [\"/etc/nginx/nginx.conf\"] # 变参 不传参 123$ docker run nginx:test#容器内会默认运行以下命令，启动主进程。#nginx -c /etc/nginx/nginx.conf 传参 123docker run nginx:test -c /etc/nginx/new.conf#容器内会默认运行以下命令，启动主进程(/etc/nginx/new.conf:假设容器内已有此文件)nginx -c /etc/nginx/new.conf ENV: 环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量 1234567ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...#例如ENV NODE_VERSION 7.2.0RUN curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\" \\ &amp;&amp; curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" ARG: 构建参数，与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量 1ARG &lt;参数名&gt;[=&lt;默认值&gt;] VOLUME: 定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。 避免重要的数据，因容器重启而丢失，这是非常致命的。 避免容器不断变大 12VOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; 在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点 EXPOSE: 仅仅只是声明端口 帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。 在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] WORKDIR: 用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在 1WORKDIR &lt;工作目录路径&gt; USER: 指定执行后续命令的用户和用户组，只是切换后续命令执行的用户(用户和用户组必须提前已经存在） 1USER &lt;用户名&gt;[:&lt;用户组&gt;] HEALTHCHECK: 指定某个程序或者指令来监控 docker 容器服务的运行状态 1234HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令HEALTHCHECK [选项] CMD &lt;命令&gt; : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。 ONBUILD: 用于延迟构建命令的执行。 简单的说，就是 Dockerfile 里用 ONBUILD 指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的 Dockerfile 使用了之前构建的镜像 FROM test-build ，这时执行新镜像的 Dockerfile 构建时候，会执行 test-build 的 Dockerfile 里的 ONBUILD 指定的命令 1ONBUILD &lt;其它指令&gt; LABEL: LABEL 指令用来给镜像添加一些元数据（metadata），以键值对的形式，语法格式如下： 123LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...#比如可以添加镜像的作者LABEL org.opencontainers.image.authors=\"runoob\" 构建镜像 12FROM nginxRUN echo '这是一个本地构建的nginx镜像' &gt; /usr/share/nginx/html/index.html docker build -t nginx:v3 . 已当前文件为上下文，构建nginx:v3镜像 注意 Dockerfile 的指令每执行一次都会在 docker 上新建一层 1234FROM centosRUN yum -y install wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN tar -xvf redis.tar.gz 以上执行会创建 3 层镜像，可简化为以下格式, 以 &amp;&amp; 符号连接命令，只会创建 1 层镜像。 1234FROM centosRUN yum -y install wget \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ &amp;&amp; tar -xvf redis.tar.gz 参考链接 https://www.runoob.com/docker/docker-dockerfile.html","categories":[{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/tags/Docker/"}]},{"title":"Redis入门11-发布与订阅","slug":"Redis/Redis入门11-发布与订阅","date":"2021-11-05T15:11:44.000Z","updated":"2023-03-19T16:25:28.861Z","comments":true,"path":"2021/11/05/Redis/Redis入门11-发布与订阅/","link":"","permalink":"http://xboom.github.io/2021/11/05/Redis/Redis%E5%85%A5%E9%97%A811-%E5%8F%91%E5%B8%83%E4%B8%8E%E8%AE%A2%E9%98%85/","excerpt":"","text":"Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息 带着问题看世界： 它的基本用法是什么 它的基本原理是什么 它跟其他消息中间件有什么区别，为什么很少听到用它 它的使用场景是哪些 基本功能 Redis有两种发布/订阅模式： 基于频道(Channel)的发布/订阅 基于模式(pattern)的发布/订阅 基于频道的发布/订阅 消息订阅者通过 subscribe channel [channe ...] 订阅频道 执行命令客户端会进入订阅状态，处于此状态下客户端不能使用除subscribe、unsubscribe、psubscribe和punsubscribe这四个属于&quot;发布/订阅&quot;之外的命令，否则会报错。 进入订阅状态后客户端可能收到3种类型的回复。每种类型的回复都包含3个值，第一个值是消息的类型，根据消类型的不同，第二个和第三个参数的含义可能不同。消息类型的取值可能是以下3个: subscribe。表示订阅成功的反馈信息。第二个值是订阅成功的频道名称，第三个是当前客户端订阅的频道数量。 12345127.0.0.1:6379&gt; subscribe channel:1Reading messages... (press Ctrl-C to quit)1) \"subscribe\" # 消息类型2) \"channel:1\" # 消息隧道3) (integer) 1 # 当前客户端订阅的频道数量 message。表示接收到的消息，第二个值表示产生消息的频道名称，第三个值是消息的内容。 12345678#发送端127.0.0.1:6379&gt; publish channel:1 hello(integer) 1#接收端1) \"message\" #消息类型表示接受到的消息2) \"channel:1\" #表示产生消息的频道3) \"hello\" #表示消息的内容 unsubscribe。表示成功取消订阅某个频道。第二个值是对应的频道名称，第三个值是当前客户端订阅的频道数量，当此值为0时客户端会退出订阅状态，之后就可以执行其他非&quot;发布/订阅&quot;模式的命令了。 1234127.0.0.1:6379&gt; unsubscribe channel:11) \"unsubscribe\" #表示成功取消订阅2) \"channel:1\" #表示取消订阅频道3) (integer) 0 #表示当前客户端订阅的频道数量 ​ 实际执行 subscribe 后 执行 Ctrl-C 就退出去，并不需要执行 unsubscribe，我想这个在实际代码执行的时候才用的到 当没有人订阅时候，消息发送失败 12127.0.0.1:6379&gt; publish channel:1 hello(integer) 0 基于模式的发布/订阅 如果有某个/某些模式和这个频道匹配的话，那么所有订阅这个/这些频道的客户端也同样会收到信息 **通配符中?表示1个占位符，表示任意个占位符(包括0)，?表示1个以上占位符 消息订阅 1234567891011127.0.0.1:6379&gt; psubscribe c? b* d?*Reading messages... (press Ctrl-C to quit)1) \"psubscribe\"2) \"c?\"3) (integer) 11) \"psubscribe\"2) \"b*\"3) (integer) 21) \"psubscribe\"2) \"d?*\"3) (integer) 3 消息发送结果 123456789101112131415161718127.0.0.1:6379&gt; publish c m1(integer) 0127.0.0.1:6379&gt; publish c1 m1(integer) 1127.0.0.1:6379&gt; publish c11 m1(integer) 0127.0.0.1:6379&gt; publish b m1(integer) 1127.0.0.1:6379&gt; publish b1 m1(integer) 1127.0.0.1:6379&gt; publish b11 m1(integer) 1127.0.0.1:6379&gt; publish d m1(integer) 0127.0.0.1:6379&gt; publish d1 m1(integer) 1127.0.0.1:6379&gt; publish d11 m1(integer) 1 注意： 使用psubscribe命令可重复订阅同一个频道，如客户端执行了psubscribe c? c?*。这时向c1发布消息客户端会接受到两条消息，而同时publish命令的返回值是2而不是1。同样的，如果有另一个客户端执行了subscribe c1和psubscribe c?*的话，向c1发送一条消息该客户顿也会受到两条消息(但是是两种类型:message和pmessage)，同时publish命令也返回2. punsubscribe命令可以退订指定的规则，用法是: punsubscribe [pattern [pattern ...]],如果没有参数则会退订所有规则。 使用punsubscribe只能退订通过psubscribe命令订阅的规则，不会影响直接通过subscribe命令订阅的频道；同样unsubscribe命令也不会影响通过psubscribe命令订阅的规则。 punsubscribe命令退订某个规则时不会将其中的通配符展开，而是进行严格的字符串匹配，所以punsubscribe * 无法退订c*规则，而是必须使用punsubscribe c*才可以退订。(它们是相互独立的) 实现原理 频道的是通过(pubsub_channels)字典实现的，这个字典就用于保存订阅频道的信息：字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端 订阅：当执行 SUBSCRIBE 命令，首先判断频道是否存在，没有则新建。然后将客户端添加到频道的对应值的链表中，如果订阅多个频道就添加到多个频道中 发布：当执行 PUBLISH 命令，首先根据 channel 定位到字典的键， 然后将信息发送给字典值链表中的所有客户端 退订：使用 UNSUBSCRIBE 命令退订指定的频道，从 pubsub_channels 字典的给定频道中，删除关于当前客户端的信息 模式的底层原理是链表 每当执行 PSUBSCRIBE 的时候,程序就创建一个包含客户端信息和被订阅模式的 pubsubPattern 结构， 并将该结构添加到 pubsub_patterns 链表中，相同的模式下存储多个客户端 订阅：如果一个执行 PSUBSCRIBE 就会在链表中建立一个节点(注意相同的模式也是不同的节点)，通过遍历整个链表可以检查所有被订阅的模式 发布：当执行 PUBLISH 命令，遍历链表定位到模式对应的客户端 退订：使用 PUNSUBSCRIBE 命令可以退订指定的模式 所以退订规则的时候是严格的字符串匹配，不会因为 退订 tweet.* 而将 tweet.shop.* 退订掉 问题1：它跟其他中间件有什么区别 订阅的消费者需要一直执行，阻塞获取消息，如果断开会表示退订。当出现网络波动或者一个消费者挂掉之后，消息就会直接被丢弃掉，Redis本身不会存储消息 不能重复消费 问题2：它的应用场景是什么 如果不在意消息的丢失，需要一个轻量级的消息通信方式，Redis发布与订阅模式是一个很好的选择 参考链接 https://pdai.tech/md/db/nosql-redis/db-redis-x-pub-sub.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门12-Lua脚本","slug":"Redis/Redis入门12-Lua脚本","date":"2021-11-05T15:11:44.000Z","updated":"2023-03-19T16:25:35.808Z","comments":true,"path":"2021/11/05/Redis/Redis入门12-Lua脚本/","link":"","permalink":"http://xboom.github.io/2021/11/05/Redis/Redis%E5%85%A5%E9%97%A812-Lua%E8%84%9A%E6%9C%AC/","excerpt":"","text":"写了一个lua脚本用于，记录一下 IP请求每分钟500次 每个用户最近10min只能发10次 每个用户最近1小时只能发20次 每个用户最近1天只能发30次 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879-- 设置限制-- 参数说明：-- key：限制的关键字，可以是用户ID或者IP地址-- period：时间段，单位为秒，可以是 600(10分钟)、3600(1小时)、86400(1天)-- limit：时间段内允许的最大请求次数-- ip_limit：时间段内IP地址允许的最大请求次数-- expire：超时时间，单位为秒-- 参数校验函数，校验传入的参数是否合法-- 参数校验方法local function validateParams(period, limit, ip_limit, expire) if not (tonumber(period) == 600 or tonumber(period) == 3600 or tonumber(period) == 86400) then return false end if not (tonumber(limit) and tonumber(ip_limit) and tonumber(expire)) then return false end return trueendlocal function setLimit(key, period, limit, ip_limit, expire) -- 校验参数 if not validateParams(period, limit, ip_limit, expire) then return 1 end local now = redis.call(\"TIME\")[1] local user_key = \"user:\" .. key .. \":\" .. period local ip_key = \"ip:\" .. key .. \":\" .. period -- 获取用户和IP的访问次数以及超时时间 local user_count = tonumber(redis.call(\"GET\", user_key) or \"0\") local ip_count = tonumber(redis.call(\"GET\", ip_key) or \"0\") local user_expire = tonumber(redis.call(\"PTTL\", user_key) or \"-1\") local ip_expire = tonumber(redis.call(\"PTTL\", ip_key) or \"-1\") -- 如果用户或IP的超时时间小于0，则将其重置为 expire if user_expire &lt; 0 then redis.call(\"SETEX\", user_key, expire, \"0\") end if ip_expire &lt; 0 then redis.call(\"SETEX\", ip_key, expire, \"0\") end -- 如果用户或IP的访问次数超过了限制，返回 0 表示限制 if user_count &gt;= limit or ip_count &gt;= ip_limit then return 0 end -- 计算时间段的开始和结束时间 local period_start = now - (now % period) local period_end = period_start + period -- 使用事务操作，将用户和IP的访问次数增加 1，并设置超时时间 redis.call(\"MULTI\") redis.call(\"INCR\", user_key) redis.call(\"EXPIREAT\", user_key, period_end) redis.call(\"INCR\", ip_key) redis.call(\"EXPIREAT\", ip_key, period_end) -- 获取用户和IP的访问次数，如果任意一个访问次数超过了限制，则返回 0 表示限制 local result = redis.call(\"EXEC\") if tonumber(result[1]) &gt; limit or tonumber(result[3]) &gt; ip_limit then return 0 end -- 计算用户和IP的访问次数总和，如果总和超过了每分钟限制，则设置用户和IP的超时时间为60秒 local total_count = user_count + ip_count + 1 if total_count &gt; 500 then redis.call(\"EXPIRE\", user_key, 60) redis.call(\"EXPIRE\", ip_key, 60) end -- 返回 1 表示不限制 return 1end-- 增加同步模式redis.replicate_commands()-- 调用 setLimit 函数return setLimit(KEYS[1], ARGV[1], ARGV[2], ARGV[3], ARGV[4]) 注意这里的 redis.replicate_commands() 目的从同步命令变更为同步内容，防止出现命令在主从设备不一致的情况(获取当前时间，当命令从主同步到从后由于时间差导致不一致)","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Protocol-01-HTTP2","slug":"Protocol/Protocol-01-HTTP2","date":"2021-11-01T23:25:03.000Z","updated":"2023-09-08T15:37:33.065Z","comments":true,"path":"2021/11/02/Protocol/Protocol-01-HTTP2/","link":"","permalink":"http://xboom.github.io/2021/11/02/Protocol/Protocol-01-HTTP2/","excerpt":"","text":"学习gRPC的过程中，发现gRPC是基于HTTP/2实现的，什么是HTTP2? HTTP/3又是因为什么而被推出 HTTP/1.1 随着网络的发展，单个页面为了显示和渲染需要的资源越来越多，这其中存在着一些问题 TCP连接数限制 对于同一个域名，浏览器最多只能同时创建 6~8 个 TCP 连接 (不同浏览器不一样)。为了解决数量限制，出现了 域名分片 技术，其实就是资源分域，将资源放在不同域名下 (比如二级子域名下)，这样就可以针对不同域名创建连接并请求，以一种讨巧的方式突破限制，但是滥用此技术也会造成很多问题，比如每个 TCP 连接本身需要经过 DNS 查询、三步握手、慢启动等，还占用额外的 CPU 和内存，对于服务器来说过多连接也容易造成网络拥挤、交通阻塞等，对于移动端来说问题更明显 在图中可以看到新建了六个 TCP 连接，每次新建连接 DNS 解析需要时间(几 ms 到几百 ms 不等)、TCP 慢启动也需要时间、TLS 握手又要时间，而且后续请求都要等待队列调度 队头阻塞(Head-Of-Line Blocking) 每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。 针对队头阻塞,有以下办法来解决: HTTP 管线化(HTTP pipelining)是将多个请求（request）整批提交的技术，而在发送过程中不需先等待服务器的回应。缺点是：第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化 将同一页面的资源分散到不同域名下，提升连接上限。对于同一个域名，Chrome默认允许同时建立 6 个 TCP持久连接，使用持久连接时。虽然能公用一个TCP管道，但一个管道同一时刻只能处理一个请求，在当前请求没有结束之前，其他的请求只能处于阻塞状态。另外如果在同一个域名下同时有10个请求发生，那么其中4个请求会进入排队等待状态，直至进行中的请求完成。 雪碧图：合并多张小图为一张大图,再用JavaScript或者CSS将小图重新“切割”出来的技术。 内联(Inlining)是另外一种防止发送很多小图请求的技巧，将图片的原始数据嵌入在CSS文件里面的URL里，减少网络请求次数 使用 quic 协议，由于使用的UDP协议，所以可以避免因为TCP自身机制而产生的对头阻塞问题 使用 SCTP 流控制传输协议 无状态特性–带来的巨大HTTP头部 报文Header一般会携带&quot;User Agent&quot;“Cookie”&quot;Accept&quot;等许多固定的头字段，存在大量重复的字段值，增加了传输的成本 明文传输–带来的不安全性 HTTP/1.1在传输数据时，所有传输的内容都是明文 不支持服务器推送消息 HTTP/2 HTTP/2由两个规范（Specification）组成： Hypertext Transfer Protocol version 2 - RFC7540 HPACK - Header Compression for HTTP/2 - RFC7541 报文解析 执行命令：curl --http2 -v nghttp2.org/robots.txt nghttp2.org/humans.txt 分析详情见：gRPC入门2-gRPC交互 HTTP优点 二进制传输 HTTP/2 采用二进制格式传输数据，而非HTTP/1.x 里纯文本形式的报文，二进制协议解析起来更高效。HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。 它把TCP协议的部分特性挪到了应用层，把原来的&quot;Header+Body&quot;的消息&quot;打散&quot;为数个小片的二进制&quot;帧&quot;(Frame),用&quot;HEADERS&quot;帧存放头数据、“DATA&quot;帧存放实体数据。HTTP/2数据分帧后&quot;Header+Body&quot;的报文结构就完全消失了，协议看到的只是一个个的&quot;碎片” HTTP/2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装 Header压缩 HTTP/2并没有使用传统的压缩算法，而是开发了专门的&quot;HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还采用哈夫曼编码来压缩整数和字符串，可以达到50%~90%的高压缩率。 具体来说: 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键-值对，对于相同的数据，不再通过每次请求和响应发送； 首部表在HTTP/2的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键-值对要么被追加到当前表的末尾，要么替换表中之前的值 例如下图中的两个请求， 请求一发送了所有的头部字段，第二个请求则只需要发送差异数据，这样可以减少冗余数据，降低开销 多路复用 在 HTTP/2 中引入了多路复用的技术。多路复用很好的解决了浏览器限制同一个域名下的请求数量的问题，同时也接更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度 在 HTTP/2 中，有了二进制分帧之后，HTTP /2 不再依赖 TCP 链接去实现多流并行了，在 HTTP/2中, 同域名下所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流。 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。 这一特性，使性能有了极大提升： 同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应,这样整个页面资源的下载过程只需要一次慢启动，同时也避免了多个TCP连接竞争带宽所带来的问题。 并行交错地发送多个请求/响应，请求/响应之间互不影响。 在HTTP/2中，每个请求都可以带一个31bit的优先值，0表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。 Server Push HTTP2还在一定程度上改变了传统的“请求-应答”工作模式，服务器不再是完全被动地响应请求，也可以新建“流”主动向客户端发送消息。比如，在浏览器刚请求HTML的时候就提前把可能会用到的JS、CSS文件发给客户端，减少等待的延迟，这被称为&quot;服务器推送&quot;（ Server Push，也叫 Cache push） 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，换句话说，服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行(怎么确认的？) 安全性 HTTP/2可以使用明文传输数据，不强制使用加密通信，不过格式还是二进制，只是不需要解密。 但由于HTTPS已是大势所趋，主流浏览器Chrome、Firefox等都公开宣布只支持加密的HTTP/2，所以“事实上”的HTTP/2是加密的。也就是说，互联网上通常所能见到的HTTP/2都是使用&quot;https”协议名，跑在TLS上面。HTTP/2协议定义了两个字符串标识符：“h2&quot;表示加密的HTTP/2，“h2c”表示明文的HTTP/2 应用层的重置连接 对于 HTTP/1 来说，是通过设置 tcp segment 里的 reset flag 来通知对端关闭连接的。这种方式会直接断开连接，下次再发请求就必须重新建立连接。HTTP/2 引入 RST_STREAM 类型的 frame，可以在不断开连接的前提下取消某个 request 的 stream，表现更好 请求优先级设置 HTTP/2 里的每个 stream 都可以设置依赖 (Dependency) 和权重，可以按依赖树分配优先级，解决了关键请求被阻塞的问题 流量控制 每个 http2 流都拥有自己的公示的流量窗口，它可以限制另一端发送数据。对于每个流来说，两端都必须告诉对方自己还有足够的空间来处理新的数据，而在该窗口被扩大前，另一端只被允许发送这么多数据 帧 - Frame 所有帧都是一个固定的 9 字节头部 (payload 之前) 跟一个指定长度的负载 (payload) Length 代表整个 frame 的长度，用一个 24 位无符号整数表示。除非接收者在 SETTINGS_MAX_FRAME_SIZE 设置了更大的值 (大小可以是 2^14(16384) 字节到 2^24-1(16777215) 字节之间的任意值)，否则数据长度不应超过 2^14(16384) 字节。头部的 9 字节不算在这个长度里 Type 定义 frame 的类型，用 8 bits 表示。帧类型决定了帧主体的格式和语义，如果 type 为 unknown 应该忽略或抛弃。 Flags 是为帧类型相关而预留的布尔标识。标识对于不同的帧类型赋予了不同的语义。如果该标识对于某种帧类型没有定义语义，则它必须被忽略且发送的时候应该赋值为 (0x0) R 是一个保留的比特位。这个比特的语义没有定义，发送时它必须被设置为 (0x0), 接收时需要忽略。 Stream Identifier 用作流控制，用 31 位无符号整数表示。客户端建立的 sid 必须为奇数，服务端建立的 sid 必须为偶数，值 (0x0) 保留给与整个连接相关联的帧 (连接控制消息)，而不是单个流 Frame Payload 是主体内容，由帧类型决定 共分为十种类型的帧: HEADERS: 报头帧 (type=0x1)，用来打开一个流或者携带一个首部块片段 DATA: 数据帧 (type=0x0)，装填主体信息，可以用一个或多个 DATA 帧来返回一个请求的响应主体 PRIORITY: 优先级帧 (type=0x2)，指定发送者建议的流优先级，可以在任何流状态下发送 PRIORITY 帧，包括空闲 (idle) 和关闭 (closed) 的流 RST_STREAM: 流终止帧 (type=0x3)，用来请求取消一个流，或者表示发生了一个错误，payload 带有一个 32 位无符号整数的错误码 (Error Codes)，不能在处于空闲 (idle) 状态的流上发送 RST_STREAM 帧 SETTINGS: 设置帧 (type=0x4)，设置此 连接 的参数，作用于整个连接 PUSH_PROMISE: 推送帧 (type=0x5)，服务端推送，客户端可以返回一个 RST_STREAM 帧来选择拒绝推送的流 PING: PING 帧 (type=0x6)，判断一个空闲的连接是否仍然可用，也可以测量最小往返时间 (RTT) GOAWAY: GOWAY 帧 (type=0x7)，用于发起关闭连接的请求，或者警示严重错误。GOAWAY 会停止接收新流，并且关闭连接前会处理完先前建立的流 WINDOW_UPDATE: 窗口更新帧 (type=0x8)，用于执行流量控制功能，可以作用在单独某个流上 (指定具体 Stream Identifier) 也可以作用整个连接 (Stream Identifier 为 0x0)，只有 DATA 帧受流量控制影响。初始化流量窗口后，发送多少负载，流量窗口就减少多少，如果流量窗口不足就无法发送，WINDOW_UPDATE 帧可以增加流量窗口大小 CONTINUATION: 延续帧 (type=0x9)，用于继续传送首部块片段序列，见 首部的压缩与解压缩 HTTP/2缺点 HTTP/2的缺点是底层支撑的 TCP 协议造成的。HTTP/2的缺点主要有以下几点： TCP 以及 TCP+TLS建立连接的延时 HTTP/2都是使用TCP协议来传输的，而如果使用HTTPS的话，还需要使用TLS协议进行安全传输，而使用TLS也需要一个握手过程，这样就需要有两个握手延迟过程： ① 在建立TCP连接的时候，需要和服务器进行三次握手来确认连接成功，也就是说需要在消耗完1.5个RTT之后才能进行数据传输。 ② 进行TLS连接，TLS有两个版本——TLS1.2和TLS1.3，每个版本建立连接所花的时间不同，大致是需要1~2个RTT。 总之，在传输数据之前，我们需要花掉 3～4 个 RTT。 TCP的队头阻塞并没有彻底解决 在HTTP/2中，多个请求是跑在一个TCP管道中的。但当出现了丢包时，HTTP/2 的表现反倒不如 HTTP/1 了。因为TCP为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，HTTP/2出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该TCP连接中的所有请求。而对于 HTTP/1.1 来说，可以开启多个 TCP 连接，出现这种情况反到只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据。 HTTP3 因为HTTP/2的问题，HTTP/3诞生，一个基于 UDP 协议的“QUIC”协议，让HTTP跑在QUIC上而不是TCP上。而这个“HTTP over QUIC”就是HTTP协议的下一个大版本。 实现了类似TCP的流量控制、传输可靠性的功能 实现了快速握手功能 集成了TLS加密功能 多路复用，彻底解决TCP中队头阻塞的问题 参考链接 https://blog.csdn.net/howgod/article/details/102597450?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link https://blog.wangriyu.wang/2018/05-HTTP2.html","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://xboom.github.io/tags/HTTP/"}]},{"title":"Redis入门10-事务","slug":"Redis/Redis入门10-事务","date":"2021-10-25T13:03:44.000Z","updated":"2023-03-18T08:56:17.990Z","comments":true,"path":"2021/10/25/Redis/Redis入门10-事务/","link":"","permalink":"http://xboom.github.io/2021/10/25/Redis/Redis%E5%85%A5%E9%97%A810-%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"Redis 事务的本质就是一次性、顺序性、排他性的执行一个队列中的一系列命令 Redis命令如下： MULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。 EXEC：执行事务中的所有操作命令。 DISCARD：取消事务，放弃执行事务块中的所有命令。 WATCH：监视一个或多个key，如果事务在执行前，key(或多个key)被其他命令修改，则事务被中断，不执行事务中的任何命令。 UNWATCH：取消WATCH对所有key的监视 基本使用 情况一：标准事务的执行 1234567891011121314127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 11QUEUED127.0.0.1:6379(TX)&gt; set k2 22QUEUED127.0.0.1:6379(TX)&gt; exec1) OK2) OK127.0.0.1:6379&gt; get k1\"11\"127.0.0.1:6379&gt; get k2\"22\"127.0.0.1:6379&gt; 情况二：事务取消 123456789101112127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k3 33QUEUED127.0.0.1:6379(TX)&gt; set k4 44QUEUED127.0.0.1:6379(TX)&gt; discardOK127.0.0.1:6379&gt; get k3(nil)127.0.0.1:6379&gt; get k4(nil) 情况三：事务异常(语法异常) 开启事务后，修改k1值为111，修改k2的值为222，但是语法错误，导致提交的时候都保留原值 123456789101112127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 111QUEUED127.0.0.1:6379(TX)&gt; sets k2 222(error) ERR unknown command 'sets', with args beginning with: 'k2' '222' 127.0.0.1:6379(TX)&gt; exec(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; get k1\"11\"127.0.0.1:6379&gt; get k2\"22\" 情况四：事务异常(运行时异常) 开启事务后，修改k1值为1111，修改k2的值为2222，但是类型错误，都能提交，执行时只有一个成功一个失败 12345678910111213127.0.0.1:6379&gt; multi OK127.0.0.1:6379(TX)&gt; set k1 1111QUEUED127.0.0.1:6379(TX)&gt; lpush k2 2222QUEUED127.0.0.1:6379(TX)&gt; exec1) OK2) (error) WRONGTYPE Operation against a key holding the wrong kind of value127.0.0.1:6379&gt; get k1\"1111\"127.0.0.1:6379&gt; get k2\"22\" 也就是说，当事务提交后，并不能保证所有的请求都能成功，可能出现一部分成功，一部分失败 WATCH WATCH 命令可以为 Redis 事务提供 check-and-set(CAS)行为 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回nil-reply来表示事务已经失败 假设需要原子性地为某个值进行增 1 操作(假设 INCR 不存在)，开两个窗口： 另一个在执行过程中(exec 提交事务前执行修改操作)修改key 1set k5 3 一个执行 Watch 并执行事务 1234567891011121314127.0.0.1:6379&gt; watch k5OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k5 2QUEUED127.0.0.1:6379(TX)&gt; set k6 1QUEUED127.0.0.1:6379(TX)&gt; exec(nil)127.0.0.1:6379&gt; get k5\"3\"127.0.0.1:6379&gt; get k6(nil) 可以看出得出的结果是 3，另外需要注意的是 k6 的值也设置失败，也就是事务所有请求都失败了(而且测试发现全部失败与命令在事务的第几条没有关系 ) 参考链接 https://pdai.tech/md/db/nosql-redis/db-redis-x-trans.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门9-缓存","slug":"Redis/Redis入门9-缓存","date":"2021-10-20T11:04:44.000Z","updated":"2023-03-26T11:54:13.505Z","comments":true,"path":"2021/10/20/Redis/Redis入门9-缓存/","link":"","permalink":"http://xboom.github.io/2021/10/20/Redis/Redis%E5%85%A5%E9%97%A89-%E7%BC%93%E5%AD%98/","excerpt":"","text":"问题背景：当前需要实现一个硬件准入的功能，通过获取数据库中设备过期记录，判断设备是否能够正常使用 这里除了之前学到的 自适应熔断 自适应降载/速率限制 也需要用到缓存，对于缓存的异常一直弄混了，直到看到一种解释： 缓存雪崩：大量的key都没有了，请求像雪崩一样打到数据库上 缓存击穿：Redis这个盾被刺了一个洞(单个key没有了，热点Key)，这个Key的请求像剑一样刺破盾刺向数据库 缓存穿透：Redis这个盾没有防住(这个Key缓存没有)，这个Key的请求像剑一样穿过盾刺向数据库 缓存雪崩 当某一个时刻出现大规模的缓存失效的情况，那么就会导致大量的请求直接打在数据库上面，导致数据库压力巨大，那么可能的情况就是 大量的Key采用的相同的过期时间 Redis宕机 解决方案： 解决大量Key同时过期，在原有的失效时间上加上一个随机值，比如1-5min避免相同过期时间导致雪崩 按照实际情况限制请求，使用限速机制保障请求不会大面积打到数据库上(熔断与降载不能应对突发情况) 提升数据库的容灾能力，通过分库分表、读写分离的策略 提升Redis集群，提高Redis的容灾能力 热点数据永不过期 缓存击穿 缓存雪崩是大规模的key失效，而缓存击穿是一个热点的Key，有大并发集中对其进行访问，突然间这个Key失效了，导致大并发全部打在数据库上，导致数据库压力剧增。这种现象就叫做缓存击穿 解决方案： 解决 redis 热点 Key 过期的问题，设置成不过期。 如何判断一个 Key 是否是热点，可以通过 Redis 的 MONITOR 命令实时监控 Redis 服务器接收到的命令，并将其输出到控制台或日志文件中。热点key一般是人为指定的(如果谁请求多谁就是热点key不过期也是有问题的)，所以不需要实时性 使用互斥锁。如果缓存失效的情况，只有拿到锁才可以查询数据库，降低了在同一时刻打在数据库上的请求，防止数据库打死。 注意是缓存失效的情况下才去获取互斥锁 共享调用 缓存穿透 发送的请求传进来的key是不存在，导致大量请求直接打到了数据库上 解决方案： 解决Key不在缓存的问题，把无效的Key也存到缓存中，缓存空值 如果Key每次都不一样，是不是就失效了。这样就需要用到布隆过滤器 大量的Key都存储在缓存中，可能造成缓存污染，需要设置合理的缓存时间 解决Key不存在的判断问题，将数据通过布隆过滤器缓起来，在查缓存之前查询布隆过滤器，如果Key不存在，某个 key 不存在，那么就一定不存在。如果某个Key存在，那么有可能存在 适用于数据量小的问题，如果数据量大，那么内存中会有大量的缓存数据 如果使用Redis中的BitMap呢，那么每次都需要查询两遍Redis 注意数据更新的情况 如果数据删除，就需要重新构建布隆过滤器 需要实时同步到布隆过滤器，否则可能会误杀 共享调用 关联缓存场景 缓存预热：在系统上线前，提前将数据加载到缓存中 Redis及以上版本可以通过 redis-cli --latency-history 来查看 Redis 服务器的实时响应时间。在这个命令的输出结果中，可以看到每秒钟 Redis 接收到的请求数量，以及每个请求的响应时间。通过监控这些指标 缓存淘汰 Redis共支持八种淘汰策略，分别是noeviction、volatile-random、volatile-ttl、volatile-lru、volatile-lfu、allkeys-lru、allkeys-random 和 allkeys-lfu 策略。 怎么理解呢？主要看分三类看： 不淘汰 noeviction(v4.0后默认的)：一旦缓存写满，不再为写请求提供服务 对设置了过期时间的数据中进行淘汰 随机：volatile-random，在设置了过期时间的键值对中，随机删除 ttl：volatile-ttl，越早过期的数据越优先选择 lru：volatile-lru，最近最少使用的原则 lfu：volatile-lfu，先根据最近最少使用淘汰，如果访问次数相同，再根据访问时间更久的数据进行淘汰 全部数据进行淘汰 随机：allkeys-random lru：allkeys-lru lfu：allkeys-lfu 参考链接 https://blog.csdn.net/zeb_perfect/article/details/54135506 https://zhuanlan.zhihu.com/p/346651831 https://www.zhihu.com/question/329377564 https://redis.io/commands/monitor/","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门8-事件","slug":"Redis/Redis入门8-事件","date":"2021-10-15T11:04:44.000Z","updated":"2023-03-25T14:36:26.692Z","comments":true,"path":"2021/10/15/Redis/Redis入门8-事件/","link":"","permalink":"http://xboom.github.io/2021/10/15/Redis/Redis%E5%85%A5%E9%97%A88-%E4%BA%8B%E4%BB%B6/","excerpt":"","text":"Redis服务器是一个事件驱动程序，服务器需要处理以下两类事件： 文件事件(file event)：Redis 服务器通过套接字与客户端(或其他Redis服务器)进行连接、而文件事件就是服务器对套接字操作的抽象。服务器通过监听并处理这些事件来完成一些列网络通信操作 时间事件(time event)：Redis 服务器中的一些操作(比如 serverCron函数) 需要在给定的时间点执行，而时间事件就是服务器对这类定时操作的抽象 文件事件 Redis 基于 Reactor 模式开发了自己的网络事件处理器：这个处理器被称为文件处理器(file event handler) 文件事件处理器使用 I/O多路复用(multiplexing)程序来同时坚挺多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器 当被监听的套接字准备号执行链接应答、读取、写入、关闭等操作时，与操作相对应的文件事件就会产生，这时文件时间处理器就会调用套接字之前关联号的事件处理器来处理这些事件 文件事件处理器的构成 文件事件处理器由四部分组成，分别是：套接字、I/O多路复用程序、文件事件分派器(dispatcher)、事件处理器 I/O多路复用程序负责监听多个套接字、并向文件事件分派器传送那些产生了事件的套接字。 尽快多个文件事件可能会并发地出现。但I/O多路复用程序会将所有产生的事件的套接字多放到一个队列里面，通过这个队列，有序、同步、每次一个套接字的方式向文件事件分派器传送套接字，当上一个套接字产生的事件被处理完毕之后(该套接字为事件锁关联的事件处理器执行完毕)，I/O多路复用程序才会继续向文件事件分派器传送下一个套接字 这些处理器其实就是一个个的函数，一次Redis客户端与服务器进行连接并发送命令的过程就是 客户端向服务端发起建立 socket 连接的请求，那么监听套接字将产生 AE_READABLE 事件，触发连接应答处理器执行。处理器会对客户端的连接请求 进行应答，然后创建客户端套接字，以及客户端状态，并将客户端套接字的 AE_READABLE 事件与命令请求处理器关联。 客户端建立连接后，向服务器发送命令，那么客户端套接字将产生 AE_READABLE 事件，触发命令请求处理器执行，处理器读取客户端命令，然后传递给相关程序去执行。 执行命令获得相应的命令回复，为了将命令回复传递给客户端，服务器将客户端套接字的 AE_WRITEABLE 事件与命令回复处理器关联。当客户端试图读取命令回复时，客户端套接字产生 AE_WRITEABLE 事件，触发命令回复处理器将命令回复全部写入到套接字中。 I/O多路复用程序的实现 Redis 的I/O多路复用程序的所有功能都是通过包装常见的select、epoll、evport和kqueue这些I/O多路复用函数库来实现的，每个I/O多路复用函数库在Redis源码中读对应一个单独的文件，比如ae_select.c、ae_epoll.c、ae_kqueue.c。诸如此类 为每个I/O多路复用函数库都实现了相同的api，所以I/O多路复用程序底层实现是可以互换的 Redis 线程不会阻塞在某一个特定的监听或已连接套接字上，Redis 可以同时和多个客户端连接并处理请求，从而提升并发性。 select/epoll 一旦监测到 FD 上有请求到达时，就会触发相应的事件。这些事件会被放进一个事件队列，Redis 单线程对该事件队列不断进行处理。这样一来，Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时，Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。 类似于 大家去医院一起挂号，登记，然后一起排队等待医生的处理 时间事件 服务器将所有时间事件都放在一个无序链表(不按when属性的大小排序)中，每当时间时间执行器运行时，它就遍历整个链表，查找所有已达到的时间事件，并调用相应的事件处理器 时间事件应用实例 redis.c/serverCron 函数，主要工作有 更新服务器的各类统计信息，比如时间、内存占用、数据库占用情况 清理数据库中的过期键值对 关闭和清理连接失效的客户端 尝试执行AOF或RDB持久化操作 如果服务器是主服务器，那么对从服务器进行定期同步 如果处理集群模式，对集群进行定期同步和连接测试 事件的调度与执行 服务器同时存在文件事件和时间时间两种事件类型，所以服务器必须同时这两种事件进行调度， 决定何时应该处理文件时间，何时又应该处理时间时间 需要花多长时间来处理事件 事件的调度和执行由 ae.c/aeProcessEvents 函数负责 12345678void aeMain(aeEventLoop *eventLoop) &#123; eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) &#123; aeProcessEvents(eventLoop, AE_ALL_EVENTS| AE_CALL_BEFORE_SLEEP| AE_CALL_AFTER_SLEEP); &#125;&#125; 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门7-集群","slug":"Redis/Redis入门7-集群","date":"2021-10-15T11:04:44.000Z","updated":"2023-03-26T11:50:36.908Z","comments":true,"path":"2021/10/15/Redis/Redis入门7-集群/","link":"","permalink":"http://xboom.github.io/2021/10/15/Redis/Redis%E5%85%A5%E9%97%A87-%E9%9B%86%E7%BE%A4/","excerpt":"","text":"Redis 集群是一种分布式的 Redis 系统，通过数据分片和负载均衡来提高 Redis 的性能和可用性。下面介绍几种常见的 Redis 集群方案： 主从模式：Redis 主从复制是一种基于主从架构的数据复制方式，其中一个 Redis 服务器作为主节点，实现读写分离 Redis Sentinel：Redis 官方提供的高可用方案，可以监控 Redis 节点的状态，并在节点故障时进行自动切换。Redis Sentinel 可以实现主从复制、故障转移和自动故障检测等功能，同时也支持多个 Sentinel 节点之间的信息同步和状态转移。 Redis Cluster：Redis 官方提供的 Redis 集群方案，支持横向扩展，自动进行数据分片和负载均衡，同时也提供故障转移和容错功能。Redis Cluster 通过 Gossip 协议实现节点间的信息传递和状态同步，同时也支持节点间的复制和持久化。 其他方案：携程的Codis、推特的Twemproxy以及第三方集群方案，例如 Redisson、Redission 和 JedisCluster 等支持多种 Redis 客户端协议，并提供了 web 界面管理工具。 主从模式 redis单节点虽然有通过RDB和AOF持久化机制能将数据持久化到硬盘上，但数据是存储在一台服务器上的，如果服务器出现硬盘故障等问题，会导致数据不可用，而且读写无法分离，读写都在同一台服务器上，请求量大时会出现I/O瓶颈，为了避免 单点故障 和 读写不分离，Redis 提供了 复制(replication) 功能实现master数据库中的数据更新后，会自动将更新的数据同步到其他slave数据库上 主从结构特点：一个master可以有多个salve节点；salve节点可以有slave节点，从节点是级联结构 优缺点： 优点: 主从结构具有读写分离，提高效率、数据备份，提供多个副本等优点。 不足: 不具备自动容错和恢复功能，主节点故障，集群则无法进行工作，可用性比较低，从节点升主节点需要人工手动干预。 当主数据库崩溃时，需要手动切换从数据库成为主数据库: 在从数据库中使用SLAVE NO ONE命令将从数据库提升成主数据继续服务。 启动之前崩溃的主数据库，然后使用SLAVEOF命令将其设置成新的主数据库的从数据库，即可同步数据。 主从复制原理 为了实现主从复制，那么就会存在以下几个问题 如何确定主从关系的 如何进行全量同步与增量同步的 同步过程中新的写命令是如何处理的 全量复制 全量复制分为三个阶段 第一阶段：从库发送 psync 命令，表示要进行数据同步。psync 命令包含了主库的 runID 和复制进度 offset 两个参数。 runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设为 ？ offset，此时设为 -1，表示第一次复制。 主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。 第二阶段：主库同步数据给从库 主库执行 bgsave 命令，生成 RDB 文件，接着将文件发给从库。 从库收到 RDB 文件后，先清空当前数据库，然后加载 RDB 文件。 注意： 清空当前数据库是为了避免有旧数据 主库同步数据给从库过程中，主库不会阻塞，过程中新产生的写命令会存储在主库 replication buffer内存中 第三阶段：主库会把第二阶段执行过程中新收到的写命令，再发送给从库 如果在同步 replication buffer 过程中，又有新的写命令怎么办？ 同步过程中网络断了，如果不进行全量同步，那么怎么知道数据同步到哪里了？ 增量同步 每次都进行全量同步的陈本很高，所以就添加的增量同步的逻辑 这里有两个缓存， replication buffer 在上一节也是在全量同步过程中用于记录 同步过程中的新写命令 repl_backlog_buffer：为了能找到主从差异数据而设计的环形缓冲区，从而避免全量复制带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量复制，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量复制的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。 replication buffer：Redis和客户端通信也好，和从库通信也好，每个client 都会分配一个 replication buffer进行数据交互，所有数据交互都是通过这个buffer进行的。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，专门用来传播用户的写命令到从库，保证主从数据一致。 **主从模式主服务器必须开启持久化！**因为从库可能会因为主库崩溃没有数据而在同步过程中将从库的数据全部删掉 哨兵模式 主从模式异常情况下，需要人工干预，哨兵模式巧妙解决。哨兵模式核心还是主从复制，只不过在相对于主从模式在主节点宕机导致不可写的情况下，多了一个竞选机制：从所有的从节点竞选出新的主节点。竞选机制的实现，是依赖于在系统中启动一个sentinel进程。 哨兵本身也有单点故障的问题，所以在一个一主多从的Redis系统中，可以使用多个哨兵进行监控，哨兵不仅会监控主数据库和从数据库，哨兵之间也会相互监控。每一个哨兵都是一个独立的进程，作为进程，它会独立运行 哨兵的作用: 监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。 自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 配置提供者（Configuration provider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。 通知（Notification）：哨兵可以将故障转移的结果发送给客户端 哨兵的原理 哨兵在启动进程时，会读取配置文件的内容，通过如下的配置找出需要监控的主数据库： 1234sentinel monitor master-name ip port quorum#master-name是主数据库的名字#ip和port 是当前主数据库地址和端口号#quorum表示在执行故障切换操作前，需要多少哨兵节点同意。 之所以只需要连接主节点，是因为通过主节点的info命令，获取从节点信息，从而和从节点也建立连接，同时也能通过主节点的info信息知道新增从节点的信息。 一个哨兵节点可以监控多个主节点，但是并不提倡这么做，因为当哨兵节点崩溃时，同时有多个集群切换会发生故障。哨兵启动后，会与主数据库建立两条连接。 订阅主数据库_sentinel_:hello频道以获取同样监控该数据库的哨兵节点信息 定期向主数据库发送info命令，获取主数据库本身的信息。 跟主数据库建立连接后会定时执行以下三个操作： 每隔10s向master和 slave发送info命令。作用是获取当前数据库信息，比如发现新增从节点时，会建立连接，并加入到监控列表中，当主从数据库的角色发生变化进行信息更新。 每隔2s向主数据里和从数据库的_sentinel_:hello频道发送自己的信息。作用是将自己的监控数据和哨兵分享。每个哨兵会订阅数据库的_sentinel:hello频道，当其他哨兵收到消息后，会判断该哨兵是不是新的哨兵，如果是则将其加入哨兵列表，并建立连接。 每隔1s向所有主从节点和所有哨兵节点发送ping命令，作用是监控节点是否存活。 主客观下线 主观下线：任何一个哨兵都是可以监控探测，并作出Redis节点下线的判断； 客观下线：有哨兵集群共同决定Redis节点是否下线； 哨兵节点发送ping命令时，当超过一定时间(down-after-millisecond)后，如果节点未回复，则哨兵认为主观下线。主观下线表示当前哨兵认为该节点已经下面，如果该节点为主数据库，哨兵会进一步判断是够需要对其进行故障切换，这时候就要发送命令(SENTINEL is-master-down-by-addr)询问其他哨兵节点是否认为该主节点是主观下线，当达到指定数量(quorum)时，哨兵就会认为是客观下线。 当主节点客观下线时就需要进行主从切换，主从切换的步骤为： 选出领头哨兵(Raft)，需要同时满足以下两个条件 第一，拿到半数以上的赞成票； 第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值 过滤出不健康的(下线或者不响应哨兵ping响应) 领头哨兵所有的slave选出优先级高的从数据库。优先级可以通过slave-priority选项设置。 如果优先级相同，则从复制的命令偏移量越大（即复制同步数据越多，数据越新），越优先。 如果以上条件都一样，则选择run ID较小的从数据库。 选出一个从数据库后，哨兵发送slave no one命令升级为主数据库，发送slaveof命令将其他从节点的主数据库设置为新的主数据库。 故障转移 转移前 将slave-1脱离原从节点（PS: 5.0 中应该是replicaof no one)，升级主节点， 将从节点slave-2指向新的主节点 通知客户端主节点已更换 将原主节点（oldMaster）变成从节点，指向新的主节点 转移后 哨兵模式优缺点 优点 哨兵模式是基于主从模式的，解决可主从模式中master故障不可以自动切换故障的问题。 缺点 是一种中心化的集群实现方案：始终只有一个Redis主机来接收和处理写请求，写操作受单机瓶颈影响。 集群里所有节点保存的都是全量数据，浪费内存空间，没有真正实现分布式存储。数据量过大时，主从同步严重影响master的性能。 Redis主机宕机后，哨兵模式正在投票选举的情况之外，因为投票选举结束之前，谁也不知道主机和从机是谁，此时Redis也会开启保护机制，禁止写操作，直到选举出了新的Redis主机。 Redis Cluster 主从模式或哨兵模式每个节点存储的数据都是全量的数据，数据量过大时，就需要对存储的数据进行分片后存储到多个redis实例上。此时就要用到Redis Sharding技术。 Redis Sharding 客户端分片 是通过一致性hash对Key的访问转发到不同的Redis实例中，查询数据时把返回结果汇集 优点：分片逻辑都在客户端不依赖第三方分布式中间件，服务端的Redis实例彼此独立，相互无关联。每个实例都能拓展 缺点：增加或减少Redis实例，需要手工调整分片的程序，不同客户端程序都需要公用一套Redis集群 代理分片 代理分片即是通过中间件的形式，将请求转发到代理，由代理对多个Redis统一管理，例如推特的Twemproxy、豌豆荚的Codis 优点：客户端无序关心存储、分片逻辑，减少客户端与Redis实例连接数(所以代理也需要可拓展，防止单点故障) 缺点：请求转发一次会产生性能消耗 Cluster集群模式 redis在3.0上加入了 Cluster 集群模式，实现了 Redis 的分布式存储，也就是说每台 Redis 节点上存储不同的数据。cluster模式为了解决单机Redis容量有限的问题，将数据按一定的规则分配到多台机器，内存/QPS不受限于单机，可受益于分布式集群高扩展性。 Redis Cluster是一种服务器Sharding技术(分片和路由都是在服务端实现)，采用多主多从，每一个分区都是由一个Redis主机和多个从机组成，片区和片区之间是相互平行的。Redis Cluster集群采用了P2P的模式，完全去中心化。 官方推荐，集群部署至少要 3 台以上的master节点，3 主 3 从六个节点的模式。Redis Cluster集群具有如下几个特点： 集群完全去中心化，采用多主多从；所有的redis节点彼此互联(PING-PONG机制)，内部使用二进制协议优化传输速度和带宽。 客户端与 Redis 节点直连，不需要中间代理层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。 每一个分区都是由一个Redis主机和多个从机组成，分片和分片之间是相互平行的。 每一个master节点负责维护一部分槽，以及槽所映射的键值数据；集群中每个节点都有全量的槽信息，通过槽每个node都知道具体数据存储到哪个node上。 在 Redis Cluster 中，客户端通过连接任意一个节点，该节点将根据哈希槽的方式将请求转发到相应的节点上进行处理。在 Redis Cluster 中，有以下几种请求 对于 key 的读写请求：如果请求的 key 所对应的槽在当前节点上，该节点将直接处理该请求；如果请求的 key 所对应的槽在其他节点上，当前节点将将该请求转发到相应的节点上进行处理。 对于非 key 的读写请求（例如 incr、decr 等命令）：这类请求不涉及具体的 key，因此可以在任意一个节点上进行处理。 在 Redis Cluster 中，数据同步通过 Gossip 协议实现，每个节点都会周期性地向其他节点发送消息，以获取最新的集群信息和数据同步状态。当某个节点出现故障时，其他节点会立即进行检测和处理，将该节点负责的哈希槽自动迁移至其他节点上，以保证数据的高可用性和一致性 redis cluster主要是针对海量数据+高并发+高可用的场景，海量数据，如果你的数据量很大，那么建议就用redis cluster，数据量不是很大时，使用sentinel就够了。redis cluster的性能和高可用性均优于哨兵模式。 Redis Cluster采用虚拟哈希槽分区而非一致性hash算法，预先分配一些卡槽，所有的键根据哈希函数映射到这些槽内，每一个分区内的master节点负责维护一部分槽以及槽所映射的键值数据。 keys hash tags Hash tags提供了一种途径，用来将多个(相关的)key分配到相同的hash slot中。这时Redis Cluster中实现multi-key操作的基础。 hash tag规则如下，如果满足如下规则，{和}之间的字符将用来计算HASH_SLOT，以保证这样的key保存在同一个slot中。 key包含一个{字符 并且 如果在这个{的右面有一个}字符 并且 如果在{和}之间存在至少一个字符 例如： {user1000}.following和{user1000}.followers这两个key会被hash到相同的hash slot中，因为只有user1000会被用来计算hash slot值。 foo{}{bar}这个key不会启用hash tag因为第一个{和}之间没有字符。 foozap{bar}这个key中的**{**bar部分会被用来计算hash slot foo{bar}{zap}这个key中的bar会被用来计算计算hash slot，而zap不会 请求重定向 Redis cluster采用去中心化的架构，集群的主节点各自负责一部分槽，客户端如何确定key到底会映射到哪个节点上呢？即请求重定向 在cluster模式下，节点对请求的处理过程如下： 检查当前key是否存在当前NODE？ 通过crc16(key)/16384计算出slot 查询负责该slot负责的节点，得到节点指针 该指针与自身节点比较 若slot不是由自身负责，则返回MOVED重定向 若slot由自身负责，且key在slot中，则返回该key对应结果 若key不存在此slot中，检查该slot是否正在迁出(MIGRATING）？ 若key正在迁出，返回ASK错误重定向客户端到迁移的目的服务器上 若Slot未迁出，检查Slot是否导入中？ 若Slot导入中且有ASKING标记，则直接操作 否则返回MOVED重定向 这个过程中有两点需要具体理解下： MOVED重定向 和 ASK重定向。 ASK重定向 ASK重定向发生与集群伸缩时候，会要求访问目标 smart客户端 为了降低上述两种重定向复杂性，提供了smart客户端(HedisCluster)来降低复杂度，其实就是客户端内部缓存(key-&gt;slot-&gt;node)的映射关系 Redis Cluster 不建议使用发布订阅！会将每条publish数据在所有节点之间广播一次 总结 Redis 主从复制是一种基于主从架构的数据复制方式，其中一个 Redis 服务器作为主节点，负责数据写入和数据同步，其他 Redis 服务器作为从节点，负责数据的读取和数据同步。Redis 主从复制提供了数据备份和负载均衡的功能，适用于单节点读写比较频繁的场景。 Redis Cluster 是一种基于分布式架构的集群方案，其中多个 Redis 节点通过哈希槽的方式将数据分散到不同的节点中进行存储和访问。Redis Cluster 提供了数据的自动分片和负载均衡的功能，可以动态地增加或删除节点，并支持故障恢复和数据同步。Redis Cluster 适用于大规模的数据存储和高并发访问的场景。 主从模式和 Redis Cluster 之间的区别主要有以下几个方面： 数据复制方式不同：主从模式是一种基于主从架构的数据复制方式，而 Redis Cluster 是一种基于分布式架构的数据存储方案。 数据分片方式不同：主从模式中，主节点负责数据的写入和同步，从节点负责数据的读取和同步，而 Redis Cluster 中，数据根据哈希槽的方式分散到多个节点中存储和访问。 故障处理方式不同：主从模式中，当主节点出现故障时，需要手动切换到从节点上进行故障处理，而 Redis Cluster 中，当某个节点出现故障时，其他节点可以自动接管该节点负责的哈希槽。 扩展性不同：主从模式中，数据量增加时，需要增加更多的从节点进行负载均衡，而 Redis Cluster 中，可以动态地增加或删除节点，以适应不同的数据量和访问量。 总的来说，主从模式适用于单节点读写比较频繁的场景，Redis Cluster 适用于大规模的数据存储和高并发访问的场景。在选择 Redis 集群方案时，需要根据业务场景和实际需求进行评估和选择。 参考链接 https://z.itpub.net/article/detail/1AB0F39894B59F6F40975DA6AD2A7A81","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门6-AOF","slug":"Redis/Redis入门6-AOF","date":"2021-10-10T12:42:44.000Z","updated":"2023-03-26T09:16:01.722Z","comments":true,"path":"2021/10/10/Redis/Redis入门6-AOF/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A86-AOF/","excerpt":"","text":"除了RDB持久功能之外，Redis还提供了AOF(Append Only File)持久化功能。 RDB 持久化通过保存数据库中的键值对来记录数据库状态 AOF持久化是通过保存Redis 服务器锁执行的写命令来记录数据库状态 被写入AOF文件的命令是以Redis的命令请求协议格式保存的，Redis的命令请求协议是纯文本格式，可直接打开AOF文件，观察内容 AOF 持久化的实现 AOF持久化功能的实现可以分为命令的追加(append)、文件写入、文件同步(sync)三个步骤 命令追加 当AOF持久化功能处于打开状态时，服务器在执行一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾，为什么要在写命令之后再去写日志 优点： 避免额外的检查开销：Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。 缺点： AOF磁盘占用更大:由于存储的是命令，所以会占用更多的磁盘 AOF重放日志耗时 为什么 mysql 是先写日志再执行命令，而Redis 是先执行命令而后写日志？ 答：是由于两者的应用场景不一致。 MySQL是关系型数据库，为了保证数据的一致性和可靠性。在执行命令之前先将命令写入日志 redo log中，等到命令执行成功后再将数据写入磁盘中。这样即使在执行命令时出现异常或者系统崩溃，也可以通过redo log进行数据恢复。 Redis是一种内存型数据库，数据持久化属于低频要求操作。在执行修改操作时，Redis会将命令写入AOF缓存区中，等到缓存区积累到一定的数据量或者经过一定的时间后，才将数据写入磁盘中，避免频繁的磁盘写入，提高Redis的性能。Redis在命令持久化上不需要像MySQL那样保证绝对的数据一致性和可靠性，可以采用先执行命令后写入日志的方式 写会策略 这里的同步是操作系统在write 写入文件时,通常将数据保存在一个内存缓冲区，然后将缓冲区中的数据写入到磁盘的过程 Redis的服务器进程就是一个事件循环(loop)，这个循环中的文件事件负责接收客户端的命令请求以及回复，时间事件则负责执行像serverCron函数这样需要定时运行的函数 服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以每次结束一个事件循环之前，都会调用 flushAppendOnlyFile 函数，考虑是否将aof_buf缓冲区中的内容写入和保存到AOF文件里面 1234567891011def eventLoop(): while True: # 处理文件时间，接收命令请求以及发送命令回复 # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中 processFileEvents() # 处理时间事件 processTimeEvents() # 考虑是否将 aof_buf 中的内容写入和保存到 AOF 文件里面 flushAppendOnlyFile() 通过服务器配置的 appendfsync选型的值来决定AOF持久化行为 Appendfsync 写回策略 效率 安全 always 将aof_buf缓冲区中的所有内容写入并同步到AOF文件 最慢 最高 everysec 将aof_buf缓冲区的所有内容写入到AOF文件，如果上次同步AOF文件的时间距离现在超过1秒钟，那么子进程对AOF文件进行同步，并且这个同步操作是由一个线程专门负责执行的 适中 适中 no 将aof_buf缓冲区的中的所有内容写入到AOF文件，但并不对AOF文件进行同步，何时同步由操作系统自己决定 最快 最低 为了提高文件的写入效率，在现代操作系统中，用户调用write函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面，等到缓冲区的空间被填满，或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面 导致当计算机发生停机的时候，内存缓冲区的写入数据可能会丢失。为此，系统提供了 fsync 和 fdatashync 两个同步函数，强制操作系统立即将缓冲区中的数据写入到硬盘 AOF文件的载入与数据还原 由于AOF 文件里包含了重建数据库状态所需的所有写命令，所以服务器只要读入并重新执行一遍AOF文件里面保存的写命令 Redis 读取AOF文件并还原数据库状态的详细步骤如下： 创建一个不带网络链接的伪客户端(fake client)：因为 Redis 的命令智能在客户端上下文中执行，而载入AOF文件时锁使用的命令直接来源于AOF 文件而不是网络连接，所以服务器使用一个伪客户端来执行AOF文件保存的写命令 从AOF文件中分析并读取出一条写命令 使用伪客户端执行被读出的谢命令 一直执行步骤2和步骤3，直到AOF文件中的所有写命令都被处理完毕为止 AOF 重写 AOF持久化是通过保存被执行的写命令来记录数据库状态，随着服务器运行时间的流逝，AOF文件中的内容会越来越多。为了解决AOF文件体积膨胀的问题，Redis提供的AOF文件重写(rewrite)功能 有两个配置项控制AOF重写的触发： auto-aof-rewrite-min-size:表示运行AOF重写时文件的最小大小，默认为64MB。 auto-aof-rewrite-percentage:这个值的计算方式是，当前aof文件大小和上一次重写后aof文件大小的差值，再除以上一次重写后aof文件大小。也就是当前aof文件比上一次重写后aof文件的增量大小，和上一次重写后aof文件大小的比值。 重写的原理 Redis 将生成新AOF文件替换旧AOF文件的功能命令为 “AOF文件重写”。但实际上，AOF文件重写并不需要对现有的AOF文件进行任何读取、分析或写入操作，是通过读取服务器当前的数据库状态来实现的 例如： 保存当前list键(含有6个value)的状态，须在AOF文件中写六条命令，如果服务器想尽量少的命令来记录list键的状态，最简单高效的办法不是去读取和分析现有AOF文件，而是直接从数据库中读取键list的值，用 一条 RPUSH key value... 来代替保存在AOF文件中的六条命令 在实际执行过程中，为了避免执行命令时造成客户端输入缓冲区溢出，会检查键锁包含的元素个数，如果数据超过 redis.h/REDIS_AOF_REWRITE_ITEMS_PRE_CMD 的值，那么重写程序将使用多条命令来记录键的值，而不单单使用一条命令 AOF后台重写 AOF重写程序 aof_rewrite 函数可以很好地完成创建一个新AOF文件的任务，但这个函数进行了大量的写入操作，所以调用这个函数的线程将会被长时间阻塞。将其放到子进程里执行可以达到两个目的： 子进程进行AOF重写期间，服务器进程(父进程)可以继续处理命令请求 子进程带有服务器进程的数据副本，使用子进程而不是线程，避免使用锁的情况下，保证数据安全性 子进程在处理AOF重写期间，服务器还需要继续处理命令请求，可能导致新的数据库状态与AOF文件所保存的数据库状态不一致 如图：新的AOF文件只保存了k1一个键的数据，而服务器数据库现在却有k1、k2、k3、k4 四个键 为了解决这种数据不一致的问题，Redis服务器设置了一个AOF重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用 当Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区 步骤如下： 执行客户端发来的命令 将执行后的写命令追加到AOF缓冲区 将执行后的写命令追加到AOF重写缓冲区 这样可以保证： AOF缓冲区的内容讲定期被写入和同步到AOF文件，对现有AOF文件的处理工作正常进行 从创建子进程开始，服务器执行的所有写命令都会被记录到AOF重写缓冲区里面 当子进程完成AOF重写工作之后，它会向父进程发送一个信号，父进程在接到该信号之后，会调用一个信号处理函数，并执行以下工作： 将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保存的数据库状态将和服务器当前的数据库状态一致 对新的AOF文件进行改名，原子地(atomic)覆盖现有的AOF文件，完成新旧两个AOF文件的替换 这个信号处理函数执行完毕之后，父进程就可以像往常一样接受命令请求 整个AOF后台重写过程，只有信号处理函数执行时会对服务器进程(父进程)造成阻塞，在其他时候，AOF后台重写都不会阻塞父进程 主线程fork出子进程的是如何复制内存数据的 fork采用操作系统提供的写时复制**(copy on write)**机制，避免一次性拷贝大量内存数据给子进程造成阻塞。fork子进程时，子进程时会拷贝父进程的页表，即虚实映射关系（虚拟内存和物理内存的映射索引表），而不会拷贝物理内存。这个拷贝会消耗大量cpu资源，并且拷贝完成前会阻塞主线程，阻塞时间取决于内存中的数据量，数据量越大，则内存页表越大。拷贝完成后，父子进程使用相同的内存地址空间。 主进程可以有数据写入，这时候就会拷贝物理内存中的数据。如下图（进程1看做是主进程，进程2看做是子进程）： 在主进程有数据写入时，而这个数据刚好在页c中，操作系统会创建这个页面的副本（页c的副本），即拷贝当前页的物理数据，将其映射到主进程中，而子进程还是使用原来的的页c 在重写日志整个过程时，主线程有哪些地方会被阻塞？ fork子进程时，需要拷贝虚拟页表，会对主线程阻塞。 主进程有bigkey写入时，操作系统会创建页面的副本，并拷贝原有的数据，会对主线程阻塞。 子进程重写日志完成后，主进程追加aof重写缓冲区时可能会对主线程阻塞。 为什么AOF重写不复用原AOF日志？ 父子进程写同一个文件会产生竞争问题，影响父进程的性能。 如果AOF重写过程中失败了，相当于污染了原本的AOF文件，无法做恢复数据使用。 持久化配置 1234567891011121314151617181920212223242526# appendonly参数开启AOF持久化appendonly no# AOF持久化的文件名，默认是appendonly.aofappendfilename \"appendonly.aof\"# AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的dir ./# 同步策略# appendfsync alwaysappendfsync everysec# appendfsync no# aof重写期间是否同步no-appendfsync-on-rewrite no# 重写触发配置auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 加载aof出错如何处理aof-load-truncated yes# 文件重写策略aof-rewrite-incremental-fsync yes 混合模式 Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。 这样，快照不用很频繁地执行，避免了频繁 fork 对主线程的影响。AOF 日志也只用记录两次快照间的操作，降低了出现文件过大的风险，减少重写开销。 如下图所示，T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。 总结 AOF文件通过保存所有修改数据库的写命令请求来记录服务器的数据库状态 AOF文件中国暖的所有命令都以Redis命令请求协议的格式保存 命令请求会先保存到AOF缓冲区，之后再定期写入并同步到AOF文件 appendfsync选项的不同值对AOF持久化功能的安全性以及Redis服务器的性能有很大的影响 服务器只要载入并重新执行保存在AOF文件中的命令，就可以还原数据库状态 AOF重写可以产生一个新的AOF文件，新的AOF文件和原有的AOF文件所保存的数据库状态一样。但体积更小 AOF重写是有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无需对现有AOF文件进行任何读入、分析或写入操作 执行BGREWRITEAOF命令时，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，是的新旧两个AOF文件所保存的数据库状态一致。最后通过新的AOF替换旧的AOF文件，以此来完成AOF文件重写操作 参考文献 《Redis 设计与实现》 https://pdai.tech/md/db/nosql-redis/db-redis-x-rdb-aof.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门5-RDB","slug":"Redis/Redis入门5-RDB","date":"2021-10-10T11:11:18.000Z","updated":"2023-03-26T09:21:05.816Z","comments":true,"path":"2021/10/10/Redis/Redis入门5-RDB/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A85-RDB/","excerpt":"","text":"为了防止数据丢失，Redis提供了两种持久化方案：RDB与AOF RDB(Redis DataBase)：将当前数据库状态生成快照并保存到磁盘的过程 Redis 是一个键值对数据库服务器，服务器中通常包括含着任意个非空数据库，而每个非空数据库中又可以包含任意个键值对。将服务器中的非空数据库以及它们的键值对统称为数据库状态。下图展示包含三个非空数据库的Redis服务器。这三个数据库以及数据库中的键值对就是该服务器的数据库状态 Redis提供了RDB持久化功能，将某个时间点的数据库状态保存到一个RDB文件中。RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态 触发方式 Redis的触发方式分为两种，手动触发与自动触发 手动触发也有两种 SAVE 命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不处理任何命令请求 BGSAVE 命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程(父进程)继续处理命令请求 自动触发，在下面4种情况下会自动触发 redis.conf中配置save m n，即在m秒内有n次修改时，自动触发bgsave生成rdb文件； 主从复制时，从节点要从主节点进行全量复制时也会触发bgsave操作，生成当时的快照发送到从节点； 执行debug reload命令重新加载redis时也会触发bgsave操作； 默认情况下执行shutdown命令时，如果没有开启aof持久化，那么也会触发bgsave操作 具体流程如下： redis客户端执行bgsave命令或者自动触发bgsave命令； 主进程判断当前是否已经存在正在执行的子进程，如果存在，那么主进程直接返回； 如果不存在正在执行的子进程，那么就fork一个新的子进程进行持久化数据，fork过程是阻塞的，fork操作完成后主进程即可执行其他操作； 子进程先将数据写入到临时的rdb文件中，待快照数据写入完成后再原子替换旧的rdb文件； 同时发送信号给主进程，通知主进程rdb持久化完成，主进程更新相关的统计信息(info Persitence下的rdb_*相关选项) 如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据 注意事项： 同一时间只有一个子进程在执行rdb操作 生成成功新的rdb文件之后再替换旧的文件 fork完之后就通知主进程进行操作，那么这个时候新的数据怎么办?。这段时间的数据变化会存在在另一个区域，执行完之后才会同步到原来的内存区域 其他配置 1234567891011121314# 文件名称dbfilename dump.rdb# 文件保存路径dir /home/work/app/redis/data/# 如果持久化出错，主进程是否停止写入(影响正常写入，主要方便运维人员)stop-writes-on-bgsave-error yes# 是否压缩(LZF)rdbcompression yes# 导入时是否检查rdbchecksum yes RDB保存与载入 创建RDB文件实际工作有 rdb.c/rdbSave 函数完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667int rdbSave(char *filename, rdbSaveInfo *rsi) &#123; char tmpfile[256]; char cwd[MAXPATHLEN]; /* Current working dir path for error messages. */ FILE *fp = NULL; rio rdb; int error = 0; snprintf(tmpfile,256,\"temp-%d.rdb\", (int) getpid()); fp = fopen(tmpfile,\"w\"); if (!fp) &#123; char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \"Failed opening the RDB file %s (in server root dir %s) \" \"for saving: %s\", filename, cwdp ? cwdp : \"unknown\", strerror(errno)); return C_ERR; &#125; rioInitWithFile(&amp;rdb,fp); startSaving(RDBFLAGS_NONE); if (server.rdb_save_incremental_fsync) rioSetAutoSync(&amp;rdb,REDIS_AUTOSYNC_BYTES); if (rdbSaveRio(&amp;rdb,&amp;error,RDBFLAGS_NONE,rsi) == C_ERR) &#123; errno = error; goto werr; &#125; /* Make sure data will not remain on the OS's output buffers */ if (fflush(fp)) goto werr; if (fsync(fileno(fp))) goto werr; if (fclose(fp)) &#123; fp = NULL; goto werr; &#125; fp = NULL; /* Use RENAME to make sure the DB file is changed atomically only * if the generate DB file is ok. */ if (rename(tmpfile,filename) == -1) &#123; char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \"Error moving temp DB file %s on the final \" \"destination %s (in server root dir %s): %s\", tmpfile, filename, cwdp ? cwdp : \"unknown\", strerror(errno)); unlink(tmpfile); stopSaving(0); return C_ERR; &#125; serverLog(LL_NOTICE,\"DB saved on disk\"); server.dirty = 0; server.lastsave = time(NULL); server.lastbgsave_status = C_OK; stopSaving(1); return C_OK;werr: serverLog(LL_WARNING,\"Write error saving DB on disk: %s\", strerror(errno)); if (fp) fclose(fp); unlink(tmpfile); stopSaving(0); return C_ERR;&#125; RDB文件的载入工作是在服务器启动时自动执行的，只要Redis服务器在启动时检测到RDB文件存在，就会自动载入RDB文件 因为AOF文件的更新频率通常比RDB文件的更新频率高，所以： 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态 只有在AOF持久性功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态 载入RDB文件 实际工作由 rdb.c/rdbLoad函数完成 由于使用BGSAVE命令是由子进程执行的，Redis服务器仍然可以继续处理客户端的命令请求。但在BGSAVE命令执行期间，服务器处理SAVE、BGSAVE、GBREWRITEAOF 三个命令的方式会和平时有所不同 在 BGSAVE 期间，客户端发送的 SAVE 和 BGSAVE 命令会被服务器拒绝，避免父进程(服务器进程)和子进程同时执行两个rdbSave调用，防止发生竞争条件 BGREWRITEAOF 和 BGSAVE 两个命令不能同时执行： 如果BGSAVE 命令正在执行，那么客户端发送的 BGREWRITEAOF 命令 会被延迟到 BGSAVE 命令执行完毕之后执行 如果 BGREWIRTEAOF 命令正在执行，那么客户端发送的 BGSAVE 命令会被服务器拒绝 虽然 BGREWRITEAOF 和 BGSAVE 两个命令的实际工作都是由子进程执行，不让同时执行考虑到都同时执行大量的磁盘写入操作 RDB文件载入时的服务器会一致处于阻塞状态，直到载入工作完成为止 自动间隔性保存 saveparams 属性是一个数组，数组中的每个元素都是一个saveparam结构。通过配置save选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令 除了 saveparams 数组之外，服务器状态还维持着一个dirty计数器，以及一个lastsave属性 dirty 计数器记录距离上一次成功执行 SAVE 命令或者 BGSAVE 命令之后，服务器对数据库状态(服务器中的所有数据库) 进行了多少次修改(包括写入、删除、更新操作) lastsave 属性 是一个 UNIX 时间戳，记录了服务器上一次成功执行 SAVE 命令或 BGSAVE 命令的时间 12345678910//向服务器提供以下配置，只要满足以下三个任意条件， BGSAVE 命令就会被执行// 以下也是save 的默认条件save 900 1 save 300 10save 60 10000/*1. 服务器在900秒之内，对数据库进行了至少1次修改2. 服务器在300秒之内，对数据库进行了至少10次修改3. 服务器在60秒之内，对数据库进行了至少10000次修改*/ Redis的服务器周期性操作函数 serverCron 默认每隔 100ms 就会执行一次，其中一项工作就是检查 save 选洗那个锁设置的保存条件是否满足，满足则执行 BGSAVE 命令 当时间来到1378271101(1378270800 + 300 = 1378271100)，服务器将自动执行一次 BGSAVE 命令，假设BGSAVE 在执行5s之后完成，那么服务器状态将更新，其中 dirty 计数器已经被重置为0，而 lastsave 属性也被更新为 1378271106 以上就是Redis 服务器根据 save 选项 锁设置的保存条件，自动执行 BGSAVE 命令，进行剑歌行数据保存的实现原理 RDB文件结构 一个完整的RDB文件所包含的各个部分 RDB文件最开头是REDIS 部分，这个部分的长度是5字节，保存着 “REDIS” 五个字符，通过这五个字符，在载入文件时快速判断是否是RDB文件 db_version 长度为4字节，它的值是一个字符串表示的整数，这个整数记录了RDB文件的版本号，比如&quot;0006&quot;表示RDB文件的版本为第六版本 database 部分包含着0个或任意多个数据库，以及各个数据库中的键值对数据 如果服务器的数据库状态为空(所有数据库都是空的)，那么这个部分也为空，长度为0字节 EOF 常量长度为1字节，标志着RDB文件的正文内容的结束，当读入程序遇到这个值的时候，它直到所有数据库的所有键值载入完毕 check_sum 是一个8字节长的无符号整数，保存着一个校验和，这个校验和是通过对 REDIS、db_version、databases、EOF四个部分计算出来的 一个RDB文件的databases部分可以保存任意多个非空数据库 每个非空数据在RDB文件中都保存为 SELECTDB、db_number、key_value_pairs三个部分 SELECTDB 常量长度为1字节，当读入程序遇到这个值，直到接下来要读入的是一个数据库号码 db_number 保存着一个数据库号码，根据号码大量的不同，这个部分的长度可以是1字节、2字节或者5字节。当程序读入db_number部分之后，服务器会调用SELECT命令，根据读入的数据库号码进行数据库切换 db_number 长度不固定，怎么判断db_number已经读完了？ key_value_pairs 部分保存了数据库中的所有键值对数(如果键值对带有过期时间，那么过期时间也会和键值对保存在一起) 不带过期时间的键值对在RDB文件中由 TYPE、key、value 三部分组成，TYPE 记录了value的类型，长度为1字节 Key 总是一个字符串常量，长度不固定 根据 TYPE 类型不同，保存的内容长度不同，value 的结构和长度也会有所不同 带有过期时间的RDB文件结构 EXPIRETIME_MS: 常量的长度为1字节，告知接下来要读入的将是一个以毫秒为单位的过期时间 ms 是一个8字节长的带符号整数，记录一个以毫秒为单位的 UNIX 时间戳，即键值对的过期时间 总结 RDB文件用于保存和还原Redis服务器所有数据库中的所有键值对数据 SAVE 命令由服务器进程执行会阻塞服务器，BGSAVE 由子进程执行保存操作，不会阻塞服务器 RDB文件是一个经过压缩的二进制文件，由多个部分组成 对于不同类型的键值对，RDB文件会使用不同的方式保存它们 优点 RDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景； Redis加载RDB文件恢复数据要远远快于AOF方式； 缺点 RDB方式实时性不够，无法做到秒级的持久化； 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高； RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全； 版本兼容RDB文件问题； 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门4-数据库","slug":"Redis/Redis入门4-数据库","date":"2021-10-10T08:45:01.000Z","updated":"2023-03-17T13:54:45.453Z","comments":true,"path":"2021/10/10/Redis/Redis入门4-数据库/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A84-%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"服务器中的数据库 Redis服务器将所有数据库都保存在redis.h/redisServer 结构的 db数组 中，每个redisDb结构代表一个数据库 1234567891011121314151617181920212223//Redis服务结构struct redisServer &#123; //... //一个数组，保存着服务器中所有的数据库 redisDb *db; //初始化时候 根据dbnum 决定创建多少个数据库 int dbnum; //...&#125;//单个数据库结构typedef struct redisDb &#123; dict *dict; /* The keyspace for this DB */ dict *expires; /* Timeout of keys with a timeout set */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP)*/ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */ unsigned long expires_cursor; /* Cursor of the active expire cycle. */ list *defrag_later; /* List of key names to attempt to defrag one by one, gradually. */&#125; redisDb; 默认情况下，会创建 dbnum=16 个数据库 每个Redis客户端都有自己的目标数据库，默认情况下，Redis客户端的目标数据库为0号数据库 不同的客户端是怎么选定目标数据库的？不同数据库之间又不会进行数据同步，都用同一个数据库其他不就没有用了 可以通过 SELECT N 来切换目标数据库 数据库健空间 redisDb结构的dict 字典保存了数据数据库中的所有键值对，称这个字典为键空间(key space) 键空间的键也就是数据库的键，每个键都是一个字符串对象 键空间的值也就是数据库的值，每个值可以是字符串对象、列表对象、哈希表对象、集合对象和有序集合对洗那个中的任意一种Redis对象 因为键空间是一个字段，所以所有针对数据库的实际都是对键空间字段进行操作来实现的 Redis 键空间是怎么解决Hash冲突的 除了读写还有一些维护操作 读取一个值之后，服务器会更新键的LRU(最近一次使用)时间，可以用来计算键的闲置时间。使用 Object idletime key 来查看键的空闲时间 如果服务器读取键已经过期，则会先删除这个键再进行其他操作 如果客户端使用WATCH 命令监视某个键，当键被修改，服务器会将键标记为脏(ditry)，从而让事务直到这个键被修改 服务器每次修改一个键之后，都会对脏(dirty)键计数器的值增1，这个计数器会触发服务器的持久化以及复制操作 如果服务器开启了数据库通知功能，那么对键修改后，会根据配置发送相应的数据库通知 键的生存时间与过期时间 redisDb结构的expires字典保存了数据库中所有键的过期时间，称为字典的过期字典 过期字典的键是一个指针，指针指向键空间中国暖的某个键对象(即是某个数据库键) 过期字典的值是一个long long 类型的整数，这个整数保存了键所有指向数据库键的过期时间–一个毫秒精度的unix 时间戳 过期键的判断 通过过期字典，程序可以通过以下步骤检查一个给定键是否过期 检查给顶键是否存在与过期字典；如果存在，那么取得键的过期时间 检查当前UNIX时间戳是否大于键的过期时间 过期键删除策略 数据键的过期时间都保存在过期字典中，如果一个键过期了，那么它是什么时候被删除的，可能存在三种不同的删除策略 定时删除：在设置键的过期时间的同时，创建一个定时器(timer)，让定时器在键的过期时间来临时，立即执行对键的删除操作 惰性删除：放任键过期不管，但每次从键空间中获取键时，都检查取得的键是否过期。过期删除，否则返回该键 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定 定时删除 定时删除策略对内存是最友好的：通过使用定时器，定时删除策略可以保证过期键会尽可能快的删除，并释放过期键锁占用的内存 缺点： 对CPU时间是最不友好的：在过期键比较多的情况下，删除过期键可能会占用相当一部分CPU时间，在内存不紧张但是CPU时间非常紧张的情况下，将CPU时间用在删除和当前任务无关的过期键上。五一对服务器的响应时间和吞吐量造成影响 创建一个定时器需要用到Redis服务器中的时间事件，而当前时间事件的实现方式–无序链表，查找一个事件的时间复杂度为O(N)–并不能高效地处理大量时间事件 惰性删除 惰性删除对CPU时间是最友好的：程序只会在取出键时才对键进行过期检查 缺点：对内存是最不友好的，如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期键不被删除，它锁占用的内存就不会释放 定期删除 上面的定时删除和惰性删除在单一使用时都有明显的缺陷： 定时删除占用太多CPU时间，影响服务器的响应时间和吞吐量 惰性删除浪费太多内存，有内存泄露的危险 定期删除策略是前两种策略的一种整合和折中：每隔一段时间执行一次删除过期键的操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响 问题是：如何设置定期删除的时间间隔，以及删除操作的执行时长 如果删除太频繁或者执行时间太长，定期删除策略就会退化成定时删除策略，以至于将CPU过多的浪费在删除过期键上 如果删除操作执行的太少或时间太短，定期删除策略又会和惰性删除策略一样，出现浪费内存的情况 Redis 的过期删除策略 Redis 实际使用的是 惰性删除和定期删除两种策略 惰性删除策略：所有读写的Redis命令在执行前都调用 expireIfNeeded 函数对键进行检查，如果键过期，函数将键从数据库删除 1234567891011121314151617181920212223int expireIfNeeded(redisDb *db, robj *key) &#123; if (!keyIsExpired(db,key)) return 0; /* If we are running in the context of a slave, instead of * evicting the expired key from the database, we return ASAP: * the slave key expiration is controlled by the master that will * send us synthesized DEL operations for expired keys. * * Still we try to return the right information to the caller, * that is, 0 if we think the key should be still valid, 1 if * we think the key is expired at this time. */ if (server.masterhost != NULL) return 1; /* If clients are paused, we keep the current dataset constant, * but return to the client what we believe is the right state. Typically, * at the end of the pause we will properly expire the key OR we will * have failed over and the new primary will send us the expire. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1;&#125; 注意上面的 server.masterhost != NULL 如果不为空，说明当前服务器为备机。那么即使当前键是过期的，也仅仅返回后状态1，而不会进行下面的删除操作 定时删除策略：每当Redis的服务器周期性操作 serverCron 函数执行时，activeExpireCycle 函数就会被调用，在规定时间内，分多次遍历服务器中各个数据库，从数据库的 expires 字典中随机检查一部分键的过期时间，并删除其中的过期键 函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查，并删除其中的过期键 全局变量 current_db记录当前 activeExpireCycle函数检查的进度，并在下一次 activeExpireCycle 函数调用时，接着上一次的进度进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206void activeExpireCycle(int type) &#123; /* Adjust the running parameters according to the configured expire * effort. The default effort is 1, and the maximum configurable effort * is 10. */ unsigned long effort = server.active_expire_effort-1, /* Rescale from 0 to 9. */ config_keys_per_loop = ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP + ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP/4*effort, config_cycle_fast_duration = ACTIVE_EXPIRE_CYCLE_FAST_DURATION + ACTIVE_EXPIRE_CYCLE_FAST_DURATION/4*effort, config_cycle_slow_time_perc = ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC + 2*effort, config_cycle_acceptable_stale = ACTIVE_EXPIRE_CYCLE_ACCEPTABLE_STALE- effort; /* This function has some global state in order to continue the work * incrementally across calls. */ static unsigned int current_db = 0; /* Next DB to test. */ static int timelimit_exit = 0; /* Time limit hit in previous call? */ static long long last_fast_cycle = 0; /* When last fast cycle ran. */ int j, iteration = 0; int dbs_per_call = CRON_DBS_PER_CALL; long long start = ustime(), timelimit, elapsed; /* When clients are paused the dataset should be static not just from the * POV of clients not being able to write, but also from the POV of * expires and evictions of keys not being performed. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return; if (type == ACTIVE_EXPIRE_CYCLE_FAST) &#123; /* Don't start a fast cycle if the previous cycle did not exit * for time limit, unless the percentage of estimated stale keys is * too high. Also never repeat a fast cycle for the same period * as the fast cycle total duration itself. */ if (!timelimit_exit &amp;&amp; server.stat_expired_stale_perc &lt; config_cycle_acceptable_stale) return; if (start &lt; last_fast_cycle + (long long)config_cycle_fast_duration*2) return; last_fast_cycle = start; &#125; /* We usually should test CRON_DBS_PER_CALL per iteration, with * two exceptions: * * 1) Don't test more DBs than we have. * 2) If last time we hit the time limit, we want to scan all DBs * in this iteration, as there is work to do in some DB and we don't want * expired keys to use memory for too much time. */ if (dbs_per_call &gt; server.dbnum || timelimit_exit) dbs_per_call = server.dbnum; /* We can use at max 'config_cycle_slow_time_perc' percentage of CPU * time per iteration. Since this function gets called with a frequency of * server.hz times per second, the following is the max amount of * microseconds we can spend in this function. */ timelimit = config_cycle_slow_time_perc*1000000/server.hz/100; timelimit_exit = 0; if (timelimit &lt;= 0) timelimit = 1; if (type == ACTIVE_EXPIRE_CYCLE_FAST) timelimit = config_cycle_fast_duration; /* in microseconds. */ /* Accumulate some global stats as we expire keys, to have some idea * about the number of keys that are already logically expired, but still * existing inside the database. */ long total_sampled = 0; long total_expired = 0; for (j = 0; j &lt; dbs_per_call &amp;&amp; timelimit_exit == 0; j++) &#123; /* Expired and checked in a single loop. */ unsigned long expired, sampled; redisDb *db = server.db+(current_db % server.dbnum); /* Increment the DB now so we are sure if we run out of time * in the current DB we'll restart from the next. This allows to * distribute the time evenly across DBs. */ current_db++; /* Continue to expire if at the end of the cycle there are still * a big percentage of keys to expire, compared to the number of keys * we scanned. The percentage, stored in config_cycle_acceptable_stale * is not fixed, but depends on the Redis configured \"expire effort\". */ do &#123; unsigned long num, slots; long long now, ttl_sum; int ttl_samples; iteration++; /* If there is nothing to expire try next DB ASAP. */ if ((num = dictSize(db-&gt;expires)) == 0) &#123; db-&gt;avg_ttl = 0; break; &#125; slots = dictSlots(db-&gt;expires); now = mstime(); /* When there are less than 1% filled slots, sampling the key * space is expensive, so stop here waiting for better times... * The dictionary will be resized asap. */ if (slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (num*100/slots &lt; 1)) break; /* The main collection cycle. Sample random keys among keys * with an expire set, checking for expired ones. */ expired = 0; sampled = 0; ttl_sum = 0; ttl_samples = 0; if (num &gt; config_keys_per_loop) num = config_keys_per_loop; /* Here we access the low level representation of the hash table * for speed concerns: this makes this code coupled with dict.c, * but it hardly changed in ten years. * * Note that certain places of the hash table may be empty, * so we want also a stop condition about the number of * buckets that we scanned. However scanning for free buckets * is very fast: we are in the cache line scanning a sequential * array of NULL pointers, so we can scan a lot more buckets * than keys in the same time. */ long max_buckets = num*20; long checked_buckets = 0; while (sampled &lt; num &amp;&amp; checked_buckets &lt; max_buckets) &#123; for (int table = 0; table &lt; 2; table++) &#123; if (table == 1 &amp;&amp; !dictIsRehashing(db-&gt;expires)) break; unsigned long idx = db-&gt;expires_cursor; idx &amp;= db-&gt;expires-&gt;ht[table].sizemask; dictEntry *de = db-&gt;expires-&gt;ht[table].table[idx]; long long ttl; /* Scan the current bucket of the current table. */ checked_buckets++; while(de) &#123; /* Get the next entry now since this entry may get * deleted. */ dictEntry *e = de; de = de-&gt;next; ttl = dictGetSignedIntegerVal(e)-now; if (activeExpireCycleTryExpire(db,e,now)) expired++; if (ttl &gt; 0) &#123; /* We want the average TTL of keys yet * not expired. */ ttl_sum += ttl; ttl_samples++; &#125; sampled++; &#125; &#125; db-&gt;expires_cursor++; &#125; total_expired += expired; total_sampled += sampled; /* Update the average TTL stats for this database. */ if (ttl_samples) &#123; long long avg_ttl = ttl_sum/ttl_samples; /* Do a simple running average with a few samples. * We just use the current estimate with a weight of 2% * and the previous estimate with a weight of 98%. */ if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl; db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50); &#125; /* We can't block forever here even if there are many keys to * expire. So after a given amount of milliseconds return to the * caller waiting for the other active expire cycle. */ if ((iteration &amp; 0xf) == 0) &#123; /* check once every 16 iterations. */ elapsed = ustime()-start; if (elapsed &gt; timelimit) &#123; timelimit_exit = 1; server.stat_expired_time_cap_reached_count++; break; &#125; &#125; /* We don't repeat the cycle for the current database if there are * an acceptable amount of stale keys (logically expired but yet * not reclaimed). */ &#125; while (sampled == 0 || (expired*100/sampled) &gt; config_cycle_acceptable_stale); &#125; elapsed = ustime()-start; server.stat_expire_cycle_time_used += elapsed; latencyAddSampleIfNeeded(\"expire-cycle\",elapsed/1000); /* Update our estimate of keys existing but yet to be expired. * Running average with this sample accounting for 5%. */ double current_perc; if (total_sampled) &#123; current_perc = (double)total_expired/total_sampled; &#125; else current_perc = 0; server.stat_expired_stale_perc = (current_perc*0.05)+ (server.stat_expired_stale_perc*0.95);&#125; AOF、RDB和复制功能对过期键的处理 生成RDB文件：执行 SAVE 或 BGSAVE 创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中 载入RDB文件：在启动Redis服务器是，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入 如果服务器是以主服务器模式运行，那么载入RDB文件时，程序会对文件中保存键进行检查，过期键将被忽略。 如果服务器是已从服务器模式运行，那么载入RDB文件时，所有键不论是否过期，都将被载入到数据库中。因为主从服务器在进行数据同步时候，从服务器的数据库就会被清空，过期键对载入RDB文件的从服务器不会造成影响 AOF文件写入：当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但还没有被惰性或定期删除，AOF文件不会产生任何变化。如果被删除策略删除后，程序会向AOF文件追加AOF文件一个DEL命令，来显示的记录该键被删除 AOF重写：在执行AOF重写过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中 复制：当服务器运行在复制模式下，从服务器的过期键删除动作由主服务器控制 主服务器删除一个过期键之后，会限制地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键 从服务器在执行客户端发送的读命令式，即使碰到过期键页不会将过期键删除，而是继续将处理未过期的键一样来处理过期键 从服务器只有在街道主服务器发来的DEL命令之后，才会删除过期键 12345678910111213141516171819202122232425262728293031robj *lookupKeyReadWithFlags(redisDb *db, robj *key, int flags) &#123; robj *val; if (expireIfNeeded(db,key) == 1) &#123; /* If we are in the context of a master, expireIfNeeded() returns 1 * when the key is no longer valid, so we can return NULL ASAP. */ if (server.masterhost == NULL) goto keymiss; //如果备机是只读模式，那么返回的就是空值 if (server.current_client &amp;&amp; server.current_client != server.master &amp;&amp; server.current_client-&gt;cmd &amp;&amp; server.current_client-&gt;cmd-&gt;flags &amp; CMD_READONLY) &#123; goto keymiss; &#125; &#125; val = lookupKey(db,key,flags); if (val == NULL) goto keymiss; server.stat_keyspace_hits++; return val;keymiss: if (!(flags &amp; LOOKUP_NONOTIFY)) &#123; notifyKeyspaceEvent(NOTIFY_KEY_MISS, \"keymiss\", key, db-&gt;id); &#125; server.stat_keyspace_misses++; return NULL;&#125; 如果从服务器是只读模式，那么返回的就是空值 否则即使键值已经过期，但是从服务器仍然能够返回该值 总结 Redis服务器所有数据库 都保存在 redisServer.db数组中，而数据库的数量则由 redisServer.dbnum属性保存 客户端通过修改目标数据库指针，让它指向redisServer.db数组中的不同元素来切换不同的数据库 数据库主要有dict 和 expires 两个字段构成，其中 dict字典负责保存键值对，而 expires 字典则负责保存键的过期时间 数据库的键总是一个字符串对象，而值则可以是任意一种Redis对象类型 expires 字典的键指向数据库中的某个键，而值则记录了数据库键的过期时间，过期时间是一个以毫秒为单位的UNIX时间戳 Redis 使用惰性删除和定期删除两种策略来删除过期的键 当一个过期键被删除之后，服务器会追加一条DEL命令到现在AOF文件的末尾，显示地删除过期键。从服务器即使发现过期键也不会主动删除它，而是等待主节点发来DEL命令 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门3-结构对象","slug":"Redis/Redis入门3-结构对象","date":"2021-09-12T10:34:16.000Z","updated":"2023-03-25T12:16:25.185Z","comments":true,"path":"2021/09/12/Redis/Redis入门3-结构对象/","link":"","permalink":"http://xboom.github.io/2021/09/12/Redis/Redis%E5%85%A5%E9%97%A83-%E7%BB%93%E6%9E%84%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Redis 底层数据结构，比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合，等等。 Redis 并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象，每种对象都用到了至少一种前面所介绍的数据结构。 其中最新的版本中数据结构又进行了优化： Redis 的对象系统还实现了基于引用计数技术的内存回收机制：当程序不再使用某个对象的时候，对象所占用的内存就会被自动释放； Redis 还通过引用计数技术实现了对象共享机制，这一机制可以在适当的条件下，通过让多个数据库键共享同一个对象来节约内存。 最后，Redis 的对象带有访问时间记录信息，该信息可以用于计算数据库键的空转时长，在服务器启用了 maxmemory 功能的情况下，空转时长较大的那些键可能会优先被服务器删除。 对象的类型与编码 Redis 使用对象来表示数据库中的键和值，每当在 Redis 的数据库中新创建一个键值对时，至少会创建两个对象，一个对象用作键值对的键（键对象），另一个对象用作键值对的值（值对象）。 举个例子，以下 SET 命令在数据库中创建了一个新的键值对，其中键值对的键是一个包含了字符串值 &quot;msg&quot; 的对象，而键值对的值则是一个包含了字符串值 &quot;hello world&quot; 的对象： 12redis&gt; SET msg \"hello world\"OK Redis 中的每个对象都由一个 redisObject 结构表示，该结构中和保存数据有关的三个属性分别是 type 属性、 encoding 属性和 ptr 属性： 1234567891011121314typedef struct redisObject &#123; // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 指向底层实现数据结构的指针 void *ptr; // ... &#125; robj; 类型 对象的 type 属性记录了对象的类型，这个属性的值可以是表中列出的常量的其中一个。 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 对于 Redis 数据库保存的键值对来说，键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中一种，因此： 当我们称呼一个数据库键为“字符串键”时，我们指的是“这个数据库键所对应的值为字符串对象”； 当我们称呼一个键为“列表键”时，我们指的是“这个数据库键所对应的值为列表对象”， 诸如此类。 TYPE 命令的实现也与此类似，对一个数据库键执行 TYPE 命令时，返回的结果为数据库键对应的值对象的类型，而不是键对象的类型： 12345678910111213141516171819202122232425262728293031323334# 键为字符串对象，值为字符串对象redis&gt; SET msg \"hello world\"OK redis&gt; TYPE msgstring # 键为字符串对象，值为列表对象redis&gt; RPUSH numbers 1 3 5(integer) 6 redis&gt; TYPE numberslist # 键为字符串对象，值为哈希对象redis&gt; HMSET profile name Tome age 25 career ProgrammerOK redis&gt; TYPE profilehash # 键为字符串对象，值为集合对象redis&gt; SADD fruits apple banana cherry(integer) 3 redis&gt; TYPE fruitsset # 键为字符串对象，值为有序集合对象redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 redis&gt; TYPE pricezset TYPE 命令在面对不同类型的值对象时所产生的输出: 对象 对象 type 属性的值 TYPE 命令的输出 字符串对象 REDIS_STRING &quot;string&quot; 列表对象 REDIS_LIST &quot;list&quot; 哈希对象 REDIS_HASH &quot;hash&quot; 集合对象 REDIS_SET &quot;set&quot; 有序集合对象 REDIS_ZSET &quot;zset&quot; 编码和底层实现 对象的 ptr 指针指向对象的底层实现数据结构，而这些数据结构由对象的 encoding 属性决定。 encoding 属性记录了对象所使用的编码，即这个对象使用了什么数据结构作为对象的底层实现，这个属性的值以下常量的其中一个。 编码常量 编码所对应的底层数据结构 REDIS_ENCODING_INT long 类型的整数 REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 每种类型的对象都至少使用了两种不同的编码，每种类型的对象可以使用的编码: REDIS_STRING REDIS_ENCODING_INT 使用整数值实现的字符串对象。 REDIS_STRING REDIS_ENCODING_EMBSTR 使用 embstr 编码的简单动态字符串实现的字符串对象。 REDIS_STRING REDIS_ENCODING_RAW 使用简单动态字符串实现的字符串对象。 REDIS_LIST REDIS_ENCODING_ZIPLIST 使用压缩列表实现的列表对象。 REDIS_LIST REDIS_ENCODING_LINKEDLIST 使用双端链表实现的列表对象。 REDIS_HASH REDIS_ENCODING_ZIPLIST 使用压缩列表实现的哈希对象。 REDIS_HASH REDIS_ENCODING_HT 使用字典实现的哈希对象。 REDIS_SET REDIS_ENCODING_INTSET 使用整数集合实现的集合对象。 REDIS_SET REDIS_ENCODING_HT 使用字典实现的集合对象。 REDIS_ZSET REDIS_ENCODING_ZIPLIST 使用压缩列表实现的有序集合对象。 REDIS_ZSET REDIS_ENCODING_SKIPLIST 使用跳跃表和字典实现的有序集合对象。 使用 OBJECT ENCODING 命令可以查看一个数据库键的值对象的编码： 1234567891011121314151617181920212223redis&gt; SET msg \"hello wrold\"OK redis&gt; OBJECT ENCODING msg\"embstr\" redis&gt; SET story \"long long long long long long ago ...\"OK redis&gt; OBJECT ENCODING story\"raw\" redis&gt; SADD numbers 1 3 5(integer) 3 redis&gt; OBJECT ENCODING numbers\"intset\" redis&gt; SADD numbers \"seven\"(integer) 1 redis&gt; OBJECT ENCODING numbers\"hashtable\" OBJECT ENCODING 对不同编码的输出 对象所使用的底层数据结构 编码常量 OBJECT ENCODING 命令输出 整数 REDIS_ENCODING_INT &quot;int&quot; embstr 编码的简单动态字符串（SDS） REDIS_ENCODING_EMBSTR &quot;embstr&quot; 简单动态字符串 REDIS_ENCODING_RAW &quot;raw&quot; 字典 REDIS_ENCODING_HT &quot;hashtable&quot; 双端链表 REDIS_ENCODING_LINKEDLIST &quot;linkedlist&quot; 压缩列表 REDIS_ENCODING_ZIPLIST &quot;ziplist&quot; 整数集合 REDIS_ENCODING_INTSET &quot;intset&quot; 跳跃表和字典 REDIS_ENCODING_SKIPLIST &quot;skiplist&quot; 字符串对象 字符串对象的编码可以是 int 、 raw 或者 embstr 。 如果一个字符串对象保存的是整数值，并且这个整数值可以用 long 类型来表示，那么字符串对象会将整数值保存在字符串对象结构的 ptr 属性里面（将 void* 转换成 long ），并将字符串对象的编码设置为 int 。 举个例子，如果执行以下 SET 命令，那么服务器将创建一个如图 8-1 所示的 int 编码的字符串对象作为 number 键的值： 12345redis&gt; SET number 10086OK redis&gt; OBJECT ENCODING number\"int\" 如果字符串对象保存的是一个字符串值，并且这个字符串值的长度大于 39 字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为 raw 。 举个例子，如果我们执行以下命令，那么服务器将创建一个如图 8-2 所示的 raw 编码的字符串对象作为 story 键的值： 12345678redis&gt; SET story \"Long, long, long ago there lived a king ...\"OK redis&gt; STRLEN story(integer) 43 redis&gt; OBJECT ENCODING story\"raw\" 如果字符串对象保存的是一个字符串值，且这个字符串值的长度小于等于 39 字节，字符串对象将使用 embstr 编码的方式来保存这个字符串值。 embstr 编码是专门用于保存短字符串的一种优化编码方式，这种编码和 raw 编码一样，都使用 redisObject 结构和 sdshdr 结构来表示字符串对象，但 raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构，而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含 redisObject 和 sdshdr 两个结构，如图 8-3 所示 embstr 编码的字符串对象在执行命令时，产生的效果和 raw 编码的字符串对象执行命令时产生的效果是相同的，但使用 embstr 编码的字符串对象来保存短字符串值有以下好处： embstr 编码将创建字符串对象所需的内存分配次数从 raw 编码的两次降低为一次。 释放 embstr 编码的字符串对象只需要调用一次内存释放函数，而释放 raw 编码的字符串对象需要调用两次内存释放函数。 因为 embstr 编码的字符串对象的所有数据都保存在一块连续的内存里面，所以这种编码的字符串对象比起 raw 编码的字符串对象能够更好地利用缓存带来的优势。作为例子，以下命令创建了一个 embstr 编码的字符串对象作为 msg 键的值，值对象的样子如图 8-4 所示： 12345redis&gt; SET msg \"hello\"OK redis&gt; OBJECT ENCODING msg\"embstr\" 最后要说的是，可以用 long double 类型表示的浮点数在 Redis 中也是作为字符串值来保存的：如果我们要保存一个浮点数到字符串对象里面，那么程序会先将这个浮点数转换成字符串值，然后再保存起转换所得的字符串值。 举个例子，执行以下代码将创建一个包含 3.14 的字符串表示 &quot;3.14&quot; 的字符串对象： 12345redis&gt; SET pi 3.14OK redis&gt; OBJECT ENCODING pi\"embstr\" 在有需要的时候，程序会将保存在字符串对象里面的字符串值转换回浮点数值，执行某些操作，然后再将执行操作所得的浮点数值转换回字符串值，并继续保存在字符串对象里面。 举个例子，如果我们执行以下代码的话： 12345redis&gt; INCRBYFLOAT pi 2.0\"5.14\" redis&gt; OBJECT ENCODING pi\"embstr\" 那么程序首先会取出字符串对象里面保存的字符串值 &quot;3.14&quot; ，将它转换回浮点数值 3.14 ，然后把 3.14 和 2.0 相加得出的值 5.14 转换成字符串 &quot;5.14&quot; ，并将这个 &quot;5.14&quot; 保存到字符串对象里面。 总结并列出了字符串对象保存各种不同类型的值所使用的编码方式 值 编码 可以用 long 类型保存的整数。 int 可以用 long double 类型保存的浮点数。 embstr 或者 raw 字符串值，或者因为长度太大而没办法用 long 类型表示的整数，又或者因为长度太大而没办法用 long double 类型表示的浮点数。 embstr 或者 raw 编码的转换 int 编码的字符串对象和 embstr 编码的字符串对象在条件满足的情况下，会被转换为 raw 编码的字符串对象。 对于 int 编码的字符串对象来说，如果我们向对象执行了一些命令，使得这个对象保存的不再是整数值，而是一个字符串值，那么字符串对象的编码将从 int 变为 raw 。 在下面的示例中，我们通过 APPEND 命令，向一个保存整数值的字符串对象追加了一个字符串值，因为追加操作只能对字符串值执行，所以程序会先将之前保存的整数值 10086 转换为字符串值 &quot;10086&quot; ，然后再执行追加操作，操作的执行结果就是一个 raw 编码的、保存了字符串值的字符串对象： 1234567891011121314redis&gt; SET number 10086OK redis&gt; OBJECT ENCODING number\"int\" redis&gt; APPEND number \" is a good number!\"(integer) 23 redis&gt; GET number\"10086 is a good number!\" redis&gt; OBJECT ENCODING number\"raw\" 另外，因为 Redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序（只有 int 编码的字符串对象和 raw 编码的字符串对象有这些程序），所以 embstr 编码的字符串对象实际上是只读的：当我们对 embstr 编码的字符串对象执行任何修改命令时，程序会先将对象的编码从 embstr 转换成 raw ，然后再执行修改命令；因为这个原因，embstr 编码的字符串对象在执行修改命令之后，总会变成一个 raw 编码的字符串对象。 以下代码展示了一个 embstr 编码的字符串对象在执行 APPEND 命令之后，对象的编码从 embstr 变为 raw 的例子： 1234567891011redis&gt; SET msg \"hello world\"OK redis&gt; OBJECT ENCODING msg\"embstr\" redis&gt; APPEND msg \" again!\"(integer) 18 redis&gt; OBJECT ENCODING msg\"raw\" 字符串命令的实现 因为字符串键的值为字符串对象，所以用于字符串键的所有命令都是针对字符串对象来构建的， 列举部分字符串命令，以及这些命令在不同编码的字符串对象下的实现方法。 命令 int 编码的实现方法 embstr 编码的实现方法 raw 编码的实现方法 SET 使用 int 编码保存值。 使用 embstr 编码保存值。 使用 raw 编码保存值。 GET 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，然后向客户端返回这个字符串值。 直接向客户端返回字符串值。 直接向客户端返回字符串值。 APPEND 将对象转换成 raw 编码，然后按 raw编码的方式执行此操作。 将对象转换成 raw 编码，然后按 raw编码的方式执行此操作。 调用 sdscatlen 函数，将给定字符串追加到现有字符串的末尾。 INCRBYFLOAT 取出整数值并将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。 取出字符串值并尝试将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。如果字符串值不能被转换成浮点数，那么向客户端返回一个错误。 取出字符串值并尝试将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。如果字符串值不能被转换成浮点数，那么向客户端返回一个错误。 INCRBY 对整数值进行加法计算，得出的计算结果会作为整数被保存起来。 embstr 编码不能执行此命令，向客户端返回一个错误。 raw 编码不能执行此命令，向客户端返回一个错误。 DECRBY 对整数值进行减法计算，得出的计算结果会作为整数被保存起来。 embstr 编码不能执行此命令，向客户端返回一个错误。 raw 编码不能执行此命令，向客户端返回一个错误。 STRLEN 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，计算并返回这个字符串值的长度。 调用 sdslen 函数，返回字符串的长度。 调用 sdslen 函数，返回字符串的长度。 SETRANGE 将对象转换成 raw 编码，然后按 raw编码的方式执行此命令。 将对象转换成 raw 编码，然后按 raw编码的方式执行此命令。 将字符串特定索引上的值设置为给定的字符。 GETRANGE 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，然后取出并返回字符串指定索引上的字符。 直接取出并返回字符串指定索引上的字符。 直接取出并返回字符串指定索引上的字符。 列表对象 列表对象的编码可以是 ziplist 或者 linkedlist 。 ziplist 编码的列表对象使用压缩列表作为底层实现，每个压缩列表节点（entry）保存了一个列表元素。 举个例子，如果执行以下 RPUSH 命令，那么服务器将创建一个列表对象作为 numbers 键的值： 12redis&gt; RPUSH numbers 1 \"three\" 5(integer) 3 如果 numbers 键的值对象使用的是 ziplist 编码，这个这个值对象将会是图 8-5 所展示的样子。 另一方面，linkedlist 编码的列表对象使用双端链表作为底层实现，每个双端链表节点（node）都保存了一个字符串对象，而每个字符串对象都保存了一个列表元素。 举个例子，如果前面所说的 numbers 键创建的列表对象使用的不是 ziplist 编码，而是 linkedlist 编码，那么 numbers 键的值对象将是图 8-6 所示的样子 注意 linkedlist 编码的列表对象在底层的双端链表结构中包含了多个字符串对象，这种嵌套字符串对象的行为在稍后介绍的哈希对象、集合对象和有序集合对象中都会出现，字符串对象是 Redis 五种类型的对象中唯一一种会被其他四种类型对象嵌套的对象。 为了简化字符串对象的表示，在图 8-6 使用了一个带有 StringObject 字样的格子来表示一个字符串对象，而 StringObject 字样下面的是字符串对象所保存的值。 比如说，图 8-7 代表的就是一个包含了字符串值 &quot;three&quot; 的字符串对象，它是 8-8 的简化表示。 编码的转换 当列表对象可以同时满足以下两个条件时，列表对象使用 ziplist 编码： 列表对象保存的所有字符串元素的长度都小于 64 字节； 列表对象保存的元素数量小于 512 个；不能满足这两个条件的列表对象需要使用 linkedlist 编码。 以上两个条件的上限值是可以修改的，具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明 对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在压缩列表里的所有列表元素都会被转移并保存到双端链表里面，对象的编码也会从 ziplist 变为 linkedlist 。 会转换回来吗？ 以下代码展示了列表对象因为保存了长度太大的元素而进行编码转换的情况： 1234567891011121314# 所有元素的长度都小于 64 字节redis&gt; RPUSH blah \"hello\" \"world\" \"again\"(integer) 3 redis&gt; OBJECT ENCODING blah\"ziplist\" # 将一个 65 字节长的元素推入列表对象中redis&gt; RPUSH blah \"wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\"(integer) 4 # 编码已改变redis&gt; OBJECT ENCODING blah\"linkedlist\" 除此之外，以下代码展示了列表对象因为保存的元素数量过多而进行编码转换的情况： 1234567891011121314151617# 列表对象包含 512 个元素redis&gt; EVAL \"for i=1,512 do redis.call('RPUSH', KEYS[1], i) end\" 1 \"integers\"(nil) redis&gt; LLEN integers(integer) 512 redis&gt; OBJECT ENCODING integers\"ziplist\" # 再向列表对象推入一个新元素，使得对象保存的元素数量达到 513 个redis&gt; RPUSH integers 513(integer) 513 # 编码已改变redis&gt; OBJECT ENCODING integers\"linkedlist\" 列表命令的实现 因为列表键的值为列表对象，所以用于列表键的所有命令都是针对列表对象来构建的，表 8-8 列出了其中一部分列表键命令，以及这些命令在不同编码的列表对象下的实现方法 命令 ziplist 编码的实现方法 linkedlist 编码的实现方法 LPUSH 调用 ziplistPush 函数，将新元素推入到压缩列表的表头。 调用 listAddNodeHead 函数，将新元素推入到双端链表的表头。 RPUSH 调用 ziplistPush 函数，将新元素推入到压缩列表的表尾。 调用 listAddNodeTail 函数，将新元素推入到双端链表的表尾。 LPOP 调用 ziplistIndex 函数定位压缩列表的表头节点，在向用户返回节点所保存的元素之后，调用 ziplistDelete 函数删除表头节点。 调用 listFirst 函数定位双端链表的表头节点，在向用户返回节点所保存的元素之后，调用 listDelNode 函数删除表头节点。 RPOP 调用 ziplistIndex 函数定位压缩列表的表尾节点，在向用户返回节点所保存的元素之后，调用 ziplistDelete 函数删除表尾节点。 调用 listLast 函数定位双端链表的表尾节点，在向用户返回节点所保存的元素之后，调用 listDelNode 函数删除表尾节点。 LINDEX 调用 ziplistIndex 函数定位压缩列表中的指定节点，然后返回节点所保存的元素。 调用 listIndex 函数定位双端链表中的指定节点，然后返回节点所保存的元素。 LLEN 调用 ziplistLen 函数返回压缩列表的长度。 调用 listLength 函数返回双端链表的长度。 LINSERT 插入新节点到压缩列表的表头或者表尾时，使用 ziplistPush 函数；插入新节点到压缩列表的其他位置时，使用 ziplistInsert 函数。 调用 listInsertNode 函数，将新节点插入到双端链表的指定位置。 LREM 遍历压缩列表节点，并调用 ziplistDelete 函数删除包含了给定元素的节点。 遍历双端链表节点，并调用 listDelNode 函数删除包含了给定元素的节点。 LTRIM 调用 ziplistDeleteRange 函数，删除压缩列表中所有不在指定索引范围内的节点。 遍历双端链表节点，并调用 listDelNode 函数删除链表中所有不在指定索引范围内的节点。 LSET 调用 ziplistDelete 函数，先删除压缩列表指定索引上的现有节点，然后调用 ziplistInsert 函数，将一个包含给定元素的新节点插入到相同索引上面。 调用 listIndex 函数，定位到双端链表指定索引上的节点，然后通过赋值操作更新节点的值。 哈希对象 哈希对象的编码可以是 ziplist 或者 hashtable 。 ziplist 编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此： 保存了同一键值对的两个节点总是紧挨在一起，保存键的节点在前，保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向，而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 举个例子，如果我们执行以下 HSET 命令，那么服务器将创建一个列表对象作为 profile 键的值： 12345678redis&gt; HSET profile name \"Tom\"(integer) 1 redis&gt; HSET profile age 25(integer) 1 redis&gt; HSET profile career \"Programmer\"(integer) 1 如果 profile 键的值对象使用 ziplist 编码，那么这个值对象将会是图 8-9 所示的样子，其中对象所使用的压缩列表如图 8-10 所示 另一方面，hashtable 编码的哈希对象使用字典作为底层实现，哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象，对象中保存了键值对的键； 字典的每个值都是一个字符串对象，对象中保存了键值对的值。 举个例子，如果前面 profile 键创建的不是 ziplist 编码的哈希对象，而是 hashtable 编码的哈希对象，那么这个哈希对象应该会是图 8-11 所示的样子。 编码转换 当哈希对象可以同时满足以下两个条件时，哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； 不能满足这两个条件的哈希对象需要使用 hashtable 编码 注意： 两个条件的上限值可以修改，具体请看配置文件中 hash-max-ziplist-value 选项和 hash-max-ziplist-entries 选项的说明 对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在压缩列表里的所有键值对都会被转移并保存到字典里面，对象的编码也会从 ziplist 变为 hashtable 。 以下代码展示了哈希对象因为键值对的键长度太大而引起编码转换的情况： 1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET book name \"Mastering C++ in 21 days\"(integer) 1 redis&gt; OBJECT ENCODING book\"ziplist\" # 向哈希对象添加一个新的键值对，键的长度为 66 字节redis&gt; HSET book long_long_long_long_long_long_long_long_long_long_long_description \"content\"(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING book\"hashtable\" 除了键的长度太大会引起编码转换之外，值的长度太大也会引起编码转换，以下代码展示了这种情况的一个示例： 1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET blah greeting \"hello world\"(integer) 1 redis&gt; OBJECT ENCODING blah\"ziplist\" # 向哈希对象添加一个新的键值对，值的长度为 68 字节redis&gt; HSET blah story \"many string ... many string ... many string ... many string ... many\"(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING blah\"hashtable\" 最后，以下代码展示了哈希对象因为包含的键值对数量过多而引起编码转换的情况： 1234567891011121314151617181920# 创建一个包含 512 个键值对的哈希对象redis&gt; EVAL \"for i=1, 512 do redis.call('HSET', KEYS[1], i, i) end\" 1 \"numbers\"(nil) redis&gt; HLEN numbers(integer) 512 redis&gt; OBJECT ENCODING numbers\"ziplist\" # 再向哈希对象添加一个新的键值对，使得键值对的数量变成 513 个redis&gt; HMSET numbers \"key\" \"value\"OK redis&gt; HLEN numbers(integer) 513 # 编码改变redis&gt; OBJECT ENCODING numbers\"hashtable\" 哈希命令的实现 因为哈希键的值为哈希对象，所以用于哈希键的所有命令都是针对哈希对象来构建的，下表列出了其中一部分哈希键命令，以及这些命令在不同编码的哈希对象下的实现方法。 命令 ziplist 编码实现方法 hashtable 编码的实现方法 HSET 首先调用 ziplistPush 函数，将键推入到压缩列表的表尾，然后再次调用 ziplistPush 函数，将值推入到压缩列表的表尾。 调用 dictAdd 函数，将新节点添加到字典里面。 HGET 首先调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后调用 ziplistNext 函数，将指针移动到键节点旁边的值节点，最后返回值节点。 调用 dictFind 函数，在字典中查找给定键，然后调用 dictGetVal 函数，返回该键所对应的值。 HEXISTS 调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，如果找到的话说明键值对存在，没找到的话就说明键值对不存在。 调用 dictFind 函数，在字典中查找给定键，如果找到的话说明键值对存在，没找到的话就说明键值对不存在。 HDEL 调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后将相应的键节点、以及键节点旁边的值节点都删除掉。 调用 dictDelete 函数，将指定键所对应的键值对从字典中删除掉。 HLEN 调用 ziplistLen 函数，取得压缩列表包含节点的总数量，将这个数量除以 2 ，得出的结果就是压缩列表保存的键值对的数量。 调用 dictSize 函数，返回字典包含的键值对数量，这个数量就是哈希对象包含的键值对数量。 HGETALL 遍历整个压缩列表，用 ziplistGet函数返回所有键和值（都是节点）。 遍历整个字典，用 dictGetKey 函数返回字典的键，用 dictGetVal 函数返回字典的值。 集合对象 集合对象的编码可以是 intset 或者 hashtable 。 intset 编码的集合对象使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合里面。 举个例子，以下代码将创建一个如图 8-12 所示的 intset 编码集合对象： 12redis&gt; SADD numbers 1 3 5(integer) 3 另一方面，hashtable 编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为 NULL 。 举个例子，以下代码将创建一个如图 8-13 所示的 hashtable 编码集合对象： 12redis&gt; SADD fruits \"apple\" \"banana\" \"cherry\"(integer) 3 编码转换 当集合对象可以同时满足以下两个条件时，对象使用 intset 编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过 512 个； 不能满足这两个条件的集合对象需要使用 hashtable 编码。 注意 第二个条件的上限值是可以修改的，具体请看配置文件中关于 set-max-intset-entries 选项的说明。 对于使用 intset 编码的集合对象来说，当使用 intset 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在整数集合中的所有元素都会被转移并保存到字典里面，并且对象的编码也会从 intset 变为 hashtable 。 举个例子，以下代码创建了一个只包含整数元素的集合对象，该对象的编码为 intset ： 12345redis&gt; SADD numbers 1 3 5(integer) 3 redis&gt; OBJECT ENCODING numbers\"intset\" 不过，只要我们向这个只包含整数元素的集合对象添加一个字符串元素，集合对象的编码转移操作就会被执行： 1234redis&gt; SADD numbers \"seven\"(integer) 1 redis&gt; OBJECT ENCODING numbers\"hashtable\" 除此之外，如果我们创建一个包含 512 个整数元素的集合对象，那么对象的编码应该会是 intset ： 123456redis&gt; EVAL \"for i=1, 512 do redis.call('SADD', KEYS[1], i) end\" 1 integers(nil) redis&gt; SCARD integers(integer) 512 redis&gt; OBJECT ENCODING integers\"intset\" 但是，只要我们再向集合添加一个新的整数元素，使得这个集合的元素数量变成 513 ，那么对象的编码转换操作就会被执行 12345678redis&gt; SADD integers 10086(integer) 1 redis&gt; SCARD integers(integer) 513 redis&gt; OBJECT ENCODING integers\"hashtable\" 集合命令的实现 因为集合键的值为集合对象，所以用于集合键的所有命令都是针对集合对象来构建的，表 8-10 列出了其中一部分集合键命令，以及这些命令在不同编码的集合对象下的实现方法。 命令 intset 编码的实现方法 hashtable 编码的实现方法 SADD 调用 intsetAdd 函数，将所有新元素添加到整数集合里面。 调用 dictAdd ，以新元素为键， NULL 为值，将键值对添加到字典里面。 SCARD 调用 intsetLen 函数，返回整数集合所包含的元素数量，这个数量就是集合对象所包含的元素数量。 调用 dictSize 函数，返回字典所包含的键值对数量，这个数量就是集合对象所包含的元素数量。 SISMEMBER 调用 intsetFind 函数，在整数集合中查找给定的元素，如果找到了说明元素存在于集合，没找到则说明元素不存在于集合。 调用 dictFind 函数，在字典的键中查找给定的元素，如果找到了说明元素存在于集合，没找到则说明元素不存在于集合。 SMEMBERS 遍历整个整数集合，使用 intsetGet 函数返回集合元素。 遍历整个字典，使用 dictGetKey 函数返回字典的键作为集合元素。 SRANDMEMBER 调用 intsetRandom 函数，从整数集合中随机返回一个元素。 调用 dictGetRandomKey 函数，从字典中随机返回一个字典键。 SPOP 调用 intsetRandom 函数，从整数集合中随机取出一个元素，在将这个随机元素返回给客户端之后，调用 intsetRemove 函数，将随机元素从整数集合中删除掉。 调用 dictGetRandomKey 函数，从字典中随机取出一个字典键，在将这个随机字典键的值返回给客户端之后，调用 dictDelete 函数，从字典中删除随机字典键所对应的键值对。 SREM 调用 intsetRemove 函数，从整数集合中删除所有给定的元素。 调用 dictDelete 函数，从字典中删除所有键为给定元素的键值对 有序集合对象 有序集合的编码可以是 ziplist 或者 skiplist 。 ziplist 编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。 压缩列表内的集合元素按分值从小到大进行排序，分值较小的元素被放置在靠近表头的方向，而分值较大的元素则被放置在靠近表尾的方向。 举个例子，如果我们执行以下 ZADD 命令，那么服务器将创建一个有序集合对象作为 price 键的值： 12redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 如果 price 键的值对象使用的是 ziplist 编码，那么这个值对象将会是图 8-14 所示的样子，而对象所使用的压缩列表则会是 8-15 所示的样子。 skiplist 编码的有序集合对象使用 zset 结构作为底层实现，一个 zset 结构同时包含一个字典和一个跳跃表： 1234567typedef struct zset &#123; zskiplist *zsl; dict *dict; &#125; zset; zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素：跳跃表节点的 object 属性保存了元素的成员，而跳跃表节点的 score 属性则保存了元素的分值。通过这个跳跃表，程序可以对有序集合进行范围型操作，比如 ZRANK 、 ZRANGE 等命令就是基于跳跃表 API 来实现的。 除此之外，zset 结构中的 dict 字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，而字典的值则保存了元素的分值。通过这个字典，程序可以用 O(1) 复杂度查找给定成员的分值，ZSCORE 命令就是根据这一特性实现的，而很多其他有序集合命令都在实现的内部用到了这一特性。 有序集合每个元素的成员都是一个字符串对象，而每个元素的分值都是一个 double 类型的浮点数。值得一提的是，虽然 zset 结构同时使用跳跃表和字典来保存有序集合元素，但这两种数据结构都会通过指针来共享相同元素的成员和分值，所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值，也不会因此而浪费额外的内存。 为什么有序集合需要同时使用跳跃表和字典来实现？ 在理论上来说，有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现，但无论单独使用字典还是跳跃表，在性能上对比起同时使用字典和跳跃表都会有所降低。 举个例子，如果我们只使用字典来实现有序集合，那么虽然以 O(1) 复杂度查找成员的分值这一特性会被保留，但是，因为字典以无序的方式来保存集合元素，所以每次在执行范围型操作 ——比如 ZRANK 、 ZRANGE 等命令时，程序都需要对字典保存的所有元素进行排序，完成这种排序需要至少 O(N \\log N) 时间复杂度，以及额外的 O(N) 内存空间（因为要创建一个数组来保存排序后的元素）。 另一方面，如果我们只使用跳跃表来实现有序集合，那么跳跃表执行范围型操作的所有优点都会被保留，但因为没有了字典，所以根据成员查找分值这一操作的复杂度将从 O(1) 上升为 O(\\log N) 。 因为以上原因，为了让有序集合的查找和范围型操作都尽可能快地执行，Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。 举个例子，如果前面 price 键创建的不是 ziplist 编码的有序集合对象，而是 skiplist 编码的有序集合对象，那么这个有序集合对象将会是图 8-16 所示的样子，而对象所使用的 zset 结构将会是图 8-17 所示的样子。 注意 为了展示方便，图 8-17 在字典和跳跃表中重复展示了各个元素的成员和分值，但在实际中，字典和跳跃表会共享元素的成员和分值，所以并不会造成任何数据重复，也不会因此而浪费任何内存 编码的转换 当有序集合对象可以同时满足以下两个条件时，对象使用 ziplist 编码： 有序集合保存的元素数量小于 128 个； 有序集合保存的所有元素成员的长度都小于 64 字节； 不能满足以上两个条件的有序集合对象将使用 skiplist 编码。 注意 以上两条件的上限值可修改，具体请看配置文件中 zset-max-ziplist-entries 选项和 zset-max-ziplist-value 选项的说明。 对于使用 ziplist 编码的有序集合对象来说，当使用 ziplist 编码所需的两个条件中的任意一个不能被满足时，程序就会执行编码转换操作，将原本储存在压缩列表里面的所有集合元素转移到 zset 结构里面，并将对象的编码从 ziplist 改为 skiplist 。 以下代码展示了有序集合对象因为包含了过多元素而引发编码转换的情况： 123456789101112131415161718192021# 对象包含了 128 个元素redis&gt; EVAL \"for i=1, 128 do redis.call('ZADD', KEYS[1], i, i) end\" 1 numbers(nil) redis&gt; ZCARD numbers(integer) 128 redis&gt; OBJECT ENCODING numbers\"ziplist\" # 再添加一个新元素redis&gt; ZADD numbers 3.14 pi(integer) 1 # 对象包含的元素数量变为 129 个redis&gt; ZCARD numbers(integer) 129 # 编码已改变redis&gt; OBJECT ENCODING numbers\"skiplist\" 以下代码则展示了有序集合对象因为元素的成员过长而引发编码转换的情况： 1234567891011121314# 向有序集合添加一个成员只有三字节长的元素redis&gt; ZADD blah 1.0 www(integer) 1 redis&gt; OBJECT ENCODING blah\"ziplist\" # 向有序集合添加一个成员为 66 字节长的元素redis&gt; ZADD blah 2.0 oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING blah\"skiplist\" 有序集合命令的实现 因为有序集合键的值为有序集合对象，所以用于有序集合键的所有命令都是针对有序集合对象来构建的，表 8-11 列出了其中一部分有序集合键命令，以及这些命令在不同编码的有序集合对象下的实现方法。 命令 ziplist 编码的实现方法 zset 编码的实现方法 ZADD 调用 ziplistInsert 函数，将成员和分值作为两个节点分别插入到压缩列表。 先调用 zslInsert 函数，将新元素添加到跳跃表，然后调用 dictAdd 函数，将新元素关联到字典。 ZCARD 调用 ziplistLen 函数，获得压缩列表包含节点的数量，将这个数量除以 2 得出集合元素的数量。 访问跳跃表数据结构的 length 属性，直接返回集合元素的数量。 ZCOUNT 遍历压缩列表，统计分值在给定范围内的节点的数量。 遍历跳跃表，统计分值在给定范围内的节点的数量。 ZRANGE 从表头向表尾遍历压缩列表，返回给定索引范围内的所有元素。 从表头向表尾遍历跳跃表，返回给定索引范围内的所有元素。 ZREVRANGE 从表尾向表头遍历压缩列表，返回给定索引范围内的所有元素。 从表尾向表头遍历跳跃表，返回给定索引范围内的所有元素。 ZRANK 从表头向表尾遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 从表头向表尾遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 ZREVRANK 从表尾向表头遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 从表尾向表头遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 ZREM 遍历压缩列表，删除所有包含给定成员的节点，以及被删除成员节点旁边的分值节点。 遍历跳跃表，删除所有包含了给定成员的跳跃表节点。并在字典中解除被删除元素的成员和分值的关联。 ZSCORE 遍历压缩列表，查找包含了给定成员的节点，然后取出成员节点旁边的分值节点保存的元素分值。 直接从字典中取出给定成员的分值。 类型检查和命令多态 Redis 中用于操作键的命令基本上可以分为两种类型。 其中一种命令可以对任何类型的键执行，比如说 DEL 命令、 EXPIRE 命令、 RENAME 命令、 TYPE 命令、 OBJECT 命令，等等。 举个例子，以下代码就展示了使用 DEL 命令来删除三种不同类型的键： 1234567891011121314151617181920# 字符串键redis&gt; SET msg \"hello\"OK # 列表键redis&gt; RPUSH numbers 1 2 3(integer) 3 # 集合键redis&gt; SADD fruits apple banana cherry(integer) 3 redis&gt; DEL msg(integer) 1 redis&gt; DEL numbers(integer) 1 redis&gt; DEL fruits(integer) 1 而另一种命令只能对特定类型的键执行，比如说： SET 、 GET 、 APPEND 、 STRLEN 等命令只能对字符串键执行； HDEL 、 HSET 、 HGET 、 HLEN 等命令只能对哈希键执行； RPUSH 、 LPOP 、 LINSERT 、 LLEN 等命令只能对列表键执行； SADD 、 SPOP 、 SINTER 、 SCARD 等命令只能对集合键执行； ZADD 、 ZCARD 、 ZRANK 、 ZSCORE 等命令只能对有序集合键执行； 举个例子，可以用 SET 命令创建一个字符串键，然后用 GET 命令和 APPEND 命令操作这个键，但如果我们试图对这个字符串键执行只有列表键才能执行的 LLEN 命令，那么 Redis 将向我们返回一个类型错误： 1234567891011121314redis&gt; SET msg \"hello world\"OK redis&gt; GET msg\"hello world\" redis&gt; APPEND msg \" again!\"(integer) 18 redis&gt; GET msg\"hello world again!\" redis&gt; LLEN msg(error) WRONGTYPE Operation against a key holding the wrong kind of value 类型检查的实现 从上面发生类型错误的代码示例可以看出，为了确保只有指定类型的键可以执行某些特定的命令，在执行一个类型特定的命令之前，Redis 会先检查输入键的类型是否正确，然后再决定是否执行给定的命令。 类型特定命令所进行的类型检查是通过 redisObject 结构的 type 属性来实现的： 在执行一个类型特定命令之前，服务器会先检查输入数据库键的值对象是否为执行命令所需的类型，如果是的话，服务器就对键执行指定的命令； 否则，服务器将拒绝执行命令，并向客户端返回一个类型错误。 举个例子，对于 LLEN 命令来说： 在执行 LLEN 命令之前，服务器会先检查输入数据库键的值对象是否为列表类型，也即是，检查值对象 redisObject 结构 type 属性的值是否为 REDIS_LIST ，如果是的话，服务器就对键执行 LLEN 命令； 否则的话，服务器就拒绝执行命令并向客户端返回一个类型错误； 多态命令的实现 Redis 除了会根据值对象的类型来判断键是否能够执行指定命令之外，还会根据值对象编码方式，选择正确的命令实现代码来执行命令。 举个例子，在前面介绍列表对象的编码时我们说过，列表对象有 ziplist 和 linkedlist 两种编码可用，其中前者使用压缩列表 API 来实现列表命令，而后者则使用双端链表 API 来实现列表命令。 现在，考虑这样一个情况，如果我们对一个键执行 LLEN 命令，那么服务器除了要确保执行命令的是列表键之外，还需要根据键的值对象所使用的编码来选择正确的 LLEN 命令实现： 如果列表对象的编码为 ziplist ，那么说明列表对象的实现为压缩列表，程序将使用 ziplistLen 函数来返回列表的长度； 如果列表对象的编码为 linkedlist ，那么说明列表对象的实现为双端链表，程序将使用 listLength 函数来返回双端链表的长度； 借用面向对象方面的术语来说，我们可以认为 LLEN 命令是多态（polymorphism）的：只要执行 LLEN 命令的是列表键，那么无论值对象使用的是 ziplist 编码还是 linkedlist 编码，命令都可以正常执行。 图 8-19 展示了 LLEN 命令从类型检查到根据编码选择实现函数的整个执行过程，其他类型特定命令的执行过程也是类似的。 实际上，我们可以将 DEL 、 EXPIRE 、 TYPE 等命令也称为多态命令，因为无论输入的键是什么类型，这些命令都可以正确地执行。 DEL 、 EXPIRE 等命令和 LLEN 等命令的区别在于，前者是基于类型的多态 —— 一个命令可以同时用于处理多种不同类型的键，而后者是基于编码的多态 —— 一个命令可以同时用于处理多种不同编码 内存回收 因为 C 语言并不具备自动的内存回收功能，所以 Redis 在自己的对象系统中构建了一个引用计数（reference counting）技术实现的内存回收机制，通过这一机制，程序可以通过跟踪对象的引用计数信息，在适当的时候自动释放对象并进行内存回收。 每个对象的引用计数信息由 redisObject 结构的 refcount 属性记录： 12345678910typedef struct redisObject &#123; // ... // 引用计数 int refcount; // ... &#125; robj; 对象的引用计数信息会随着对象的使用状态而不断变化： 在创建一个新对象时，引用计数的值会被初始化为 1 ； 当对象被一个新程序使用时，它的引用计数值会被增一； 当对象不再被一个程序使用时，它的引用计数值会被减一； 当对象的引用计数值变为 0 时，对象所占用的内存会被释放。 函数 作用 incrRefCount 将对象的引用计数值增一。 decrRefCount 将对象的引用计数值减一，当对象的引用计数值等于 0 时，释放对象。 resetRefCount 将对象的引用计数值设置为 0 ，但并不释放对象，这个函数通常在需要重新设置对象的引用计数值时使用。 对象的整个生命周期可以划分为创建对象、操作对象、释放对象三个阶段。 作为例子，以下代码展示了一个字符串对象从创建到释放的整个过程： 12345678// 创建一个字符串对象 s ，对象的引用计数为 1robj *s = createStringObject(...) // 对象 s 执行各种操作 ... // 将对象 s 的引用计数减一，使得对象的引用计数变为 0// 导致对象 s 被释放decrRefCount(s) 谁来负责查看数量以及触发回收呢？ 对象共享 除了用于实现引用计数内存回收机制之外，对象的引用计数属性还带有对象共享的作用。 举个例子，假设键 A 创建了一个包含整数值 100 的字符串对象作为值对象，如图 8-20 所示。 如果这时键 B 也要创建一个同样保存了整数值 100 的字符串对象作为值对象，那么服务器有以下两种做法： 为键 B 新创建一个包含整数值 100 的字符串对象； 让键 A 和键 B 共享同一个字符串对象；以上两种方法很明显是第二种方法更节约内存。 在 Redis 中，让多个键共享同一个值对象需要执行以下两个步骤： 将数据库键的值指针指向一个现有的值对象； 将被共享的值对象的引用计数增一。举个例子，图 8-21 就展示了包含整数值 100 的字符串对象同时被键 A 和键 B 共享之后的样子，可以看到，除了对象的引用计数从之前的 1 变成了 2 之外，其他属性都没有变化。 它是怎么知道有相同的值的？ 共享对象机制对于节约内存非常有帮助，数据库中保存的相同值对象越多，对象共享机制就能节约越多的内存。 比如说，假设数据库中保存了整数值 100 的键不只有键 A 和键 B 两个，而是有一百个，那么服务器只需要用一个字符串对象的内存就可以保存原本需要使用一百个字符串对象的内存才能保存的数据。 目前来说，Redis 会在初始化服务器时，创建一万个字符串对象，这些对象包含了从 0 到 9999 的所有整数值，当服务器需要用到值为 0 到 9999 的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象。 注意：创建共享字符串对象的数量可以通过修改 redis.h/REDIS_SHARED_INTEGERS 常量来修改。 举个例子，如果我们创建一个值为 100 的键 A ，并使用 OBJECT REFCOUNT 命令查看键 A 的值对象的引用计数，我们会发现值对象的引用计数为 2 ： 12345redis&gt; SET A 100OK redis&gt; OBJECT REFCOUNT A(integer) 2 引用这个值对象的两个程序分别是持有这个值对象的服务器程序，以及共享这个值对象的键 A ，如图 8-22 所示。 如果这时再创建一个值为 100 的键 B ，那么键 B 也会指向包含整数值 100 的共享对象，使得共享对象的引用计数值变为 3 ： 12345678redis&gt; SET B 100OK redis&gt; OBJECT REFCOUNT A(integer) 3 redis&gt; OBJECT REFCOUNT B(integer) 3 另外，这些共享对象不单单只有字符串键可以使用，那些在数据结构中嵌套了字符串对象的对象（linkedlist 编码的列表对象、 hashtable 编码的哈希对象、 hashtable 编码的集合对象、以及 zset 编码的有序集合对象）都可以使用这些共享对象 为什么 Redis 不共享包含字符串的对象？ 当服务器考虑将一个共享对象设置为键的值对象时，程序需要先检查给定的共享对象和键想创建的目标对象是否完全相同，只有在共享对象和目标对象完全相同的情况下，程序才会将共享对象用作键的值对象，而一个共享对象保存的值越复杂，验证共享对象和目标对象是否相同所需的复杂度就会越高，消耗的 CPU 时间也会越多： 如果共享对象是保存整数值的字符串对象，那么验证操作的复杂度为 O(1) ； 如果共享对象是保存字符串值的字符串对象，那么验证操作的复杂度为 O(N) ； 如果共享对象是包含了多个值（或者对象的）对象，比如列表对象或者哈希对象，那么验证操作的复杂度将会是 O(N^2) 。 因此，尽管共享更复杂的对象可以节约更多的内存，但受到 CPU 时间的限制，Redis 只对包含整数值的字符串对象进行共享。 对象的空转时长 除了前面介绍过的 type 、 encoding 、 ptr 和 refcount 四个属性之外，redisObject 结构包含的最后一个属性为 lru 属性，该属性记录了对象最后一次被命令程序访问的时间： 123456789typedef struct redisObject &#123; // ... unsigned lru:22; // ... &#125; robj; OBJECT IDLETIME 命令可以打印出给定键的空转时长，这一空转时长就是通过将当前时间减去键的值对象的 lru 时间计算得出的： 123456789101112131415161718redis&gt; SET msg \"hello world\"OK # 等待一小段时间redis&gt; OBJECT IDLETIME msg(integer) 20 # 等待一阵子redis&gt; OBJECT IDLETIME msg(integer) 180 # 访问 msg 键的值redis&gt; GET msg\"hello world\" # 键处于活跃状态，空转时长为 0redis&gt; OBJECT IDLETIME msg(integer) 0 注意: OBJECT IDLETIME 命令的实现是特殊的，这个命令在访问键的值对象时，不会修改值对象的 lru 属性。 除了可以被 OBJECT IDLETIME 命令打印出来之外，键的空转时长还有另外一项作用：如果服务器打开了 maxmemory 选项，并且服务器用于回收内存的算法为 volatile-lru 或者 allkeys-lru ，那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存。 配置文件的 maxmemory 选项和 maxmemory-policy 选项的说明介绍了关于这方面的更多信息 总结 Redis 数据库中的每个键值对的键和值都是一个对象。 Redis 共有字符串、列表、哈希、集合、有序集合五种类型的对象，每种类型的对象至少都有两种或以上的编码方式，不同的编码可以在不同的使用场景上优化对象的使用效率。 服务器在执行某些命令之前，会先检查给定键的类型能否执行指定的命令，而检查一个键的类型就是检查键的值对象的类型。 Redis 的对象系统带有引用计数实现的内存回收机制，当一个对象不再被使用时，该对象所占用的内存就会被自动释放。 Redis 会共享值为 0 到 9999 的字符串对象。 对象会记录自己的最后一次被访问的时间，这个时间可以用于计算对象的空转时间 参考链接 https://www.bookstack.cn/read/redisbook/433f95a1e435a4da.md https://www.cnblogs.com/remcarpediem/p/11755860.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门2-底层结构","slug":"Redis/Redis入门2-底层结构","date":"2021-09-12T09:24:54.000Z","updated":"2023-06-02T11:18:46.711Z","comments":true,"path":"2021/09/12/Redis/Redis入门2-底层结构/","link":"","permalink":"http://xboom.github.io/2021/09/12/Redis/Redis%E5%85%A5%E9%97%A82-%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/","excerpt":"","text":"前面描述了底层数据结构，与之对应的底层数据结构实现： 简单动态字符串 Redis 自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将 SDS 用作 Redis 的默认字符串表示。SDS是一种可以被修改的字符串 当执行命令： 12redis&gt; SET msg \"hello world\"OK Redis 将在数据库中创建了一个新的键值对，其中： 键值对的键是一个字符串对象，对象的底层实现是一个保存着字符串 &quot;msg&quot; 的 SDS 。 键值对的值也是一个字符串对象，对象的底层实现是一个保存着字符串 &quot;hello world&quot; 的 SDS 。 除了用来保存数据库中的字符串值之外，AOF 模块中的 AOF 缓冲区，以及客户端状态中的输入缓冲区，都是由 SDS 实现的 SDS定义 每个 sds.h/sdshdr 结构表示一个 SDS 值： 123456789struct sdshdr &#123; // 记录 buf 数组中已使用字节的数量 // 等于 SDS 所保存字符串的长度 int len; // 记录 buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[]; &#125;; free 属性的值为 0 ，表示这个 SDS 没有分配任何未使用空间。 len 属性的值为 5 ，表示这个 SDS 保存了一个五字节长的字符串。 buf 属性是一个 char 类型的数组，数组的前五个字节分别保存了 'R' 、 'e' 、 'd' 、 'i' 、 's' 五个字符，而最后一个字节则保存了空字符 '\\0' 。 SDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，这个空字符对于 SDS 的使用者来说是完全透明的。 遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。 举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 s ，那么我们可以直接使用 stdio.h/printf 函数，通过执行以下语句： 这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 &quot;Redis&quot; 。 这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 buf 数组分配了五字节未使用空间，所以它的 free 属性的值为 5（图中使用五个空格来表示五字节的未使用空间） 其中 保存空字符的 1 字节空间不计算在 SDS 的 len 和 free 属性里面 SDS与C字符串的区别 C 语言使用长度为 N+1 的字符数组来表示长度为 N 的字符串，并且字符数组的最后一个元素总是空字符 '\\0' 。 比如说，图 2-3 就展示了一个值为 &quot;Redis&quot; 的 C 字符串 C 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，接下来将说明 SDS 比 C 字符串更适用于 Redis 的原因 常数复杂度获取字符串长度 因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。 举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程 和 C 字符串不同，因为 SDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。 举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 len 属性，就可以立即知道 SDS 的长度为 5 字节： 又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 len 属性，就可以立即知道 SDS 的长度为 11 字节。 设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。 通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，即使对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 杜绝缓冲区溢出 除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出 (buffer overflow) 举个例子，&lt;string.h&gt;/strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾： 1char *strcat(char *dest, const char *src); 因为 C 字符串不记录自身的长度，所以 strcat 假定用户在执行这个函数时，已经为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。 举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 s1 和 s2 ，其中 s1 保存了字符串 &quot;Redis&quot; ，而 s2 则保存了字符串 &quot;MongoDB&quot; ，如图 2-7 所示 如果一个程序员决定通过执行： 1strcat(s1, \" Cluster\"); 将 s1 的内容修改为 &quot;Redis Cluster&quot; ，但粗心的他却忘了在执行 strcat 之前为 s1 分配足够的空间，那么在 strcat 函数执行之后，s1 的数据将溢出到 s2 所在的空间中，导致 s2 保存的内容被意外地修改，如图 2-8 所示 与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。 举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 sdscat 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，sdscat 会先检查给定 SDS 的空间是否足够，如果不够的话，sdscat 就会先扩展 SDS 的空间，然后才执行拼接操作 比如说，如果我们执行： 1sdscat(s, \" Cluster\"); 其中 SDS 值 s 如图 2-9 所示，那么 sdscat 将在执行拼接操作之前检查 s 的长度是否足够，在发现 s 目前的空间不足以拼接 &quot; Cluster&quot; 之后，sdscat 就会先扩展 s 的空间，然后才执行拼接 &quot; Cluster&quot; 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。 注意图 2-10 所示的 SDS ：sdscat 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 13 字节的未使用空间，并且拼接之后的字符串也正好是 13 字节长，分配的空间 SDS 的空间分配策略有关 减少修改带来的内存重分配次数 正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 N 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 N+1 个字符长的数组（额外的一个字符空间用于保存空字符）。 因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作： 如果是增长字符串的操作，比如拼接操作(append)，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。 如果是缩短字符串的操作，比如截断操作(trim)，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。 问题1：如果是分配，那么每次分配多少呢? 问题2：如果是释放不再使用的空间，那么什么时候释放呢，或者使用什么策略？ 举个例子，如果我们持有一个值为 &quot;Redis&quot; 的 C 字符串 s ，那么为了将 s 的值改为 &quot;Redis Cluster&quot; ，在执行： 1strcat(s, \" Cluster\"); 之前，我们需要先使用内存重分配操作，扩展 s 的空间。 之后，如果我们又打算将 s 的值从 &quot;Redis Cluster&quot; 改为 &quot;Redis Cluster Tutorial&quot; ，那么在执行： 1strcat(s, \" Tutorial\"); 之前，我们需要再次使用内存重分配扩展 s 的空间，诸如此类。 因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作： 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。 但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。 为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，buf 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 free 属性记录。 通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略 空间预分配 空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。 额外分配的未使用空间数量由以下公式决定： 如果对 SDS 进行修改之后，SDS 的长度（也即是 len 属性的值）将小于 1 MB ，那么程序分配和 len 属性同样大小的未使用空间，这时 SDS len 属性的值将和 free 属性的值相同。举个例子，如果进行修改之后，SDS 的 len 将变成 13 字节，那么程序也会分配 13 字节的未使用空间，SDS 的 buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 SDS 进行修改之后，SDS 的长度将大于等于 1 MB ，那么程序会分配 1 MB 的未使用空间。举个例子，如果进行修改之后，SDS 的 len 将变成 30 MB ，那么程序会分配 1 MB 的未使用空间，SDS 的 buf 数组的实际长度将为 30 MB + 1 MB + 1 byte 。 通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。 举个例子，对于图 2-11 所示的 SDS 值 s 来说，如果我们执行： 1sdscat(s, \" Cluster\"); 那么 sdscat 将执行一次内存重分配操作，将 SDS 的长度修改为 13 字节，并将 SDS 的未使用空间同样修改为 13 字节，如图 2-12 所示。 如果这时，我们再次对 s 执行： 1sdscat(s, \" Tutorial\"); 那么这次 sdscat 将不需要执行内存重分配：因为未使用空间里面的 13 字节足以保存 9 字节的 &quot; Tutorial&quot; ，执行 sdscat 之后的 SDS 如图 2-13 所示 在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。 通过这种预分配策略，SDS 将连续增长 N 次字符串所需的内存重分配次数从必定 N 次降低为最多 N 次。 惰性空间释放 惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来使用。 举个例子，sdstrim 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。 比如对于图 2-14 所示的 SDS 值 s 来说，执行： 1sdstrim(s, &quot;XY&quot;); &#x2F;&#x2F; 移除 SDS 字符串中的所有 &#39;X&#39; 和 &#39;Y&#39; 会将 SDS 修改成图 2-15 所示的样子 注意执行 sdstrim 之后的 SDS 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。 举个例子，如果现在对 s 执行： 1sdscat(s, &quot; Redis&quot;); 那么完成这次 sdscat 操作将不需要执行内存重分配：因为 SDS 里面预留的 8 字节空间已经足以拼接 6 个字节长的 &quot; Redis&quot; ，如图 2-16 所示。 通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。 什么时候或者有什么接口进行内存释放 二进制安全 C 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 有个问题：到底是怎么存进去并计算长度的 如果是先计算长度，然后再存进去，那么可能出现长度误判，导致整体长度不够的情况 如果是先存进去再计算长度(不可能，需要先分配空间) 原来它用的是二进制数据？ 举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 &quot;Redis&quot; ，而忽略之后的 &quot;Cluster&quot; 。 虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的(binary-safe): 所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。 将 SDS 的 buf 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。 比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示 通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据 兼容部分C函数 虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 buf 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 &lt;string.h&gt; 库定义的函数。 举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 sds ，那么我们就可以重用 &lt;string.h&gt;/strcasecmp 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串： 1strcasecmp(sds-&gt;buf, &quot;hello world&quot;); 这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。 与此类似，我们还可以将一个保存文本数据的 SDS 作为 strcat 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面： 1strcat(c_string, sds-&gt;buf); 这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。 通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 &lt;string.h&gt; 函数库，从而避免了不必要的代码重复。 总结 C 字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 &lt;string.h&gt; 库中的函数。 可以使用一部分 &lt;string.h&gt; 库中的函数。 接口实现：https://www.cnblogs.com/yinbiao/p/10740212.html 链表 链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。 除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer） 链表和链表节点的实现 每个链表节点使用一个 adlist.h/listNode 结构来表示： 12345678typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode; 多个 listNode 可以通过 prev 和 next 指针组成双端链表，如图 3-1 所示 虽然仅仅使用多个 listNode 结构就可以组成链表，但使用 adlist.h/list 来持有链表的话，操作起来会更方便 12345678910111213141516171819typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key);&#125; list; list 结构为链表提供了表头指针 head 、表尾指针 tail ，以及链表长度计数器 len ，而 dup 、 free 和 match 成员则是用于实现多态链表所需的类型特定函数： dup 函数用于复制链表节点所保存的值； free 函数用于释放链表节点所保存的值； match 函数则用于对比链表节点所保存的值和另一个输入值是否相等 图 3-2 是由一个 list 结构和三个 listNode 结构组成的链表： Redis 的链表实现的特性可以总结如下： 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ，对链表的访问以 NULL 为终点。 带表头指针和表尾指针：通过 list 结构的 head 指针和 tail 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器：程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。 多态：链表节点使用 void* 指针来保存节点值，并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。 总结 链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。 每个链表节点由一个 listNode 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ，所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。 字典 在字典中，一个键（key）可以和一个值（value）进行关联（或者说将键映射为值），这些关联的键和值就被称为键值对。 举个例子，当执行命令： 12redis&gt; SET msg \"hello world\"OK 在数据库中创建一个键为 &quot;msg&quot; ，值为 &quot;hello world&quot; 的键值对时，这个键值对就是保存在代表数据库的字典里面的。 除了用来表示数据库之外，字典还是哈希键的底层实现之一：当一个哈希键包含的键值对比较多，又或者键值对中的元素都是比较长的字符串时，Redis 就会使用字典作为哈希键的底层实现。 举个例子，website 是一个包含 10086 个键值对的哈希键，这个哈希键的键都是一些数据库的名字，而键的值就是数据库的主页网址： 123456789redis&gt; HLEN website(integer) 10086 redis&gt; HGETALL website1) \"Redis\"2) \"Redis.io\"3) \"MariaDB\"4) \"MariaDB.org\"5) \"MongoDB\"6) \"MongoDB.org\"# ... website 键的底层实现就是一个字典，字典中包含了 10086 个键值对： 其中一个键值对的键为 &quot;Redis&quot; ，值为 &quot;Redis.io&quot; 。 另一个键值对的键为 &quot;MariaDB&quot; ，值为 &quot;MariaDB.org&quot; ； 还有一个键值对的键为 &quot;MongoDB&quot; ，值为 &quot;MongoDB.org&quot; ； 除了用来实现数据库和哈希键之外，Redis 的不少功能也用到了字典，在后续的章节中会不断地看到字典在 Redis 中的各种不同应用 字典的实现 Redis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。 接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现 Hash表 Redis 字典所使用的哈希表由 dict.h/dictht 结构定义 12345678910111213141516typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; &#125; dictht; table 属性是一个数组，数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针，每个 dictEntry 结构保存着一个键值对。 size 属性记录了哈希表的大小，也即是 table 数组的大小，而 used 属性则记录了哈希表目前已有节点（键值对）的数量。 sizemask 属性的值总是等于 size - 1 ，这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。 图 4-1 展示了一个大小为 4 的空哈希表（没有包含任何键值对） 哈希表节点 哈希表节点使用 dictEntry 结构表示，每个 dictEntry 结构都保存着一个键值对： 12345678910111213141516typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next; &#125; dictEntry; key 属性保存着键值对中的键，而 v 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 uint64_t 整数，又或者是一个 int64_t 整数。 next 属性是指向另一个哈希表节点的指针，可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题 举个例子，图 4-2 就展示了如何通过 next 指针，将两个索引值相同的键 k1 和 k0 连接在一起。 字典 12345678910111213141516typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ &#125; dict; type 属性和 privdata 属性是针对不同类型的键值对，为创建多态字典而设置的： type 属性是一个指向 dictType 结构的指针，每个 dictType 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。 而 privdata 属性则保存了需要传给那些类型特定函数的可选参数 123456789101112131415161718192021typedef struct dictType &#123; // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj); &#125; dictType; ht 属性是一个包含两个项的数组，数组中的每个项都是一个 dictht 哈希表，一般情况下，字典只使用 ht[0] 哈希表，ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用。 除了 ht[1] 之外，另一个和 rehash 有关的属性就是 rehashidx ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 -1 。 图 4-3 展示了一个普通状态下（没有进行 rehash）的字典 Hash算法 当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 Redis 计算哈希值和索引值的方法如下： 123456# 使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key); # 使用哈希表的 sizemask 属性和哈希值，计算出索引值# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 k0 和 v0 添加到字典里面，那么程序会先使用语句： 1hash &#x3D; dict-&gt;type-&gt;hashFunction(k0); 计算键 k0 的哈希值。 假设计算得出的哈希值为 8 ，那么程序会继续使用语句： 1index &#x3D; hash &amp; dict-&gt;ht[0].sizemask &#x3D; 8 &amp; 3 &#x3D; 0; 计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。 MurmurHash 算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。 MurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：http://code.google.com/p/smhasher/ 。 解决键冲突 当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时，称这些键发生了冲突（collision）。 Redis 的哈希表使用链地址法（separate chaining）来解决键冲突：每个哈希表节点都有一个 next 指针，多个哈希表节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来，这就解决了键冲突的问题。 举个例子，假设程序要将键值对 k2 和 v2 添加到图 4-6 所示的哈希表里面，并且计算得出 k2 的索引值为 2 ，那么键 k1 和 k2 将产生冲突，而解决冲突的办法就是使用 next 指针将键 k2 和 k1 所在的节点连接起来，如图 4-7 所示。 因为 dictEntry 节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置（复杂度为 O(1)），排在其他已有节点的前面 链表使用的是双向链表所以可以直接添加到链表结尾，但是字典的链表中并只有一个指向next的指针，所以如果发生hash冲突，则直接添加到链表头部 rehash 随着操作的不断执行，哈希表保存的键值对会逐渐地增多或者减少，为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩。 负载因子是什么时候定义的，怎么去判断什么时候去rehash呢？ rehash过程中会停掉插入和获取吗，新来的值怎么办？ 查询的时候会同时查询 hash[0] 和 hash[1] 吗 扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成，Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的 ht[1] 哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及 ht[0] 当前包含的键值对数量（也即是 ht[0].used属性的值）： 如果执行的是扩展操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面：rehash 指的是重新计算键的哈希值和索引值，然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后（ht[0] 变为空表），释放 ht[0] ，将 ht[1] 设置为 ht[0] ，并在 ht[1] 新创建一个空白哈希表，为下一次 rehash 做准备。 举个例子，假设程序要对图 4-8 所示字典的 ht[0] 进行扩展操作，那么程序将执行以下步骤： ht[0].used 当前的值为 4 ，4 * 2 = 8 ，而 8 （2^3）恰好是第一个大于等于 4 的 2 的 n 次方，所以程序会将 ht[1] 哈希表的大小设置为 8 。图 4-9 展示了 ht[1] 在分配空间之后，字典的样子。 将 ht[0] 包含的四个键值对都 rehash 到 ht[1] ，如图 4-10 所示。 释放 ht[0] ，并将 ht[1] 设置为 ht[0] ，然后为 ht[1] 分配一个空白哈希表，如图 4-11 所示。 至此，对哈希表的扩展操作执行完毕，程序成功将哈希表的大小从原来的 4 改为了现在的 8 。 hash表的拓展和收缩 当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 5 ； 其中哈希表的负载因子可以通过公式： 1# 负载因子 = 哈希表已保存节点数量 / 哈希表大小load_factor = ht[0].used / ht[0].size 计算得出。比如说，这个哈希表的负载因子为： 12345# 对于一个大小为 `4` ，包含 `4` 个键值对的哈希表来说load_factor = 4 / 4 = 1# 对于一个大小为 `512` ，包含 `256` 个键值对的哈希表来说load_factor = 256 / 512 = 0.5 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行，服务器执行扩展操作所需的负载因子并不相同 因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中，Redis 需要创建当前服务器进程的子进程，而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率，所以在子进程存在期间，服务器会提高执行扩展操作所需的负载因子，从而尽可能地避免在子进程存在期间进行哈希表扩展操作，这可以避免不必要的内存写入操作，最大限度地节约内存 渐进式rehash 扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面，但rehash 动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的。原因在于，如果哈希表里保存大量键值对，要一次性将这些键值对全部 rehash 到 ht[1] 的话，庞大的计算量可能会导致服务器在一段时间内停止服务。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间，让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ，并将它的值设置为 0 ，表示 rehash 工作正式开始。 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ，当 rehash 工作完成之后，程序将 rehashidx 属性的值增一。 随着字典操作的不断执行，最终在某个时间点上，ht[0] 的所有键值对都会被 rehash 至 ht[1] ，这时程序将 rehashidx 属性的值设为 -1 ，表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式，将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 而带来的庞大计算量 同时在两个ht都存在数据，那么 增删改查 在哪个进行操作的，怎么判断哪个ht才是目的地 图 4-12 至图 4-17 展示了一次完整的渐进式 rehash 过程，注意观察在整个 rehash 过程中，字典的 rehashidx 属性是如何变化的 渐进式 rehash 执行期间的哈希表操作 因为在进行渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个哈希表，所以在渐进式 rehash 进行期间，字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行：比如说，要在字典里面查找一个键的话，程序会先在 ht[0] 里面进行查找，如果没找到的话，就会继续到 ht[1] 里面进行查找，诸如此类。 另外，在渐进式 rehash 执行期间，新添加到字典的键值对一律会被保存到 ht[1] 里面，而 ht[0] 则不再进行任何添加操作：这一措施保证了 ht[0] 包含的键值对数量会只减不增，并随着 rehash 操作的执行而最终变成空表 总结 字典被广泛用于实现 Redis 的各种功能，其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现，每个字典带有两个哈希表，一个用于平时使用，另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突，被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时，程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面，并且这个 rehash 过程并不是一次性地完成的，而是渐进式地完成的 跳跃表 跳跃表（skiplist）是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。 跳跃表支持平均 O(log N) 最坏 O(N) 复杂度的节点查找，还可以通过顺序性操作来批量处理节点 为什么使用跳表而不是平衡树 平衡树的操作比跳表更加的复杂 平衡树的插入与删除需要子树的调整，而跳表只需要删除相邻节点 平衡树每个节点包含2个指针(左右子树)，而跳表每个节点包含的指针数平均为1/(1-p)，p表示的是密度 查找单个key的平均时间复杂度都是O(logn) Redis 使用跳跃表作为有序集合键的底层实现之一：如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员（member）是比较长的字符串时，Redis 就会使用跳跃表来作为有序集合键的底层实现。 举个例子，fruit-price 是一个有序集合键，这个有序集合以水果名为成员，水果价钱为分值，保存了 130 款水果的价钱： 12345678910redis&gt; ZRANGE fruit-price 0 2 WITHSCORES1) \"banana\"2) \"5\"3) \"cherry\"4) \"6.5\"5) \"apple\"6) \"8\" redis&gt; ZCARD fruit-price(integer) 130 fruit-price 有序集合的所有数据都保存在一个跳跃表里面，其中每个跳跃表节点都保存了一款水果的价钱信息，所有水果按价钱的高低从低到高在跳跃表里面排序： 跳跃表的第一个元素的成员为 &quot;banana&quot; ，它的分值为 5 ； 跳跃表的第二个元素的成员为 &quot;cherry&quot; ，它的分值为 6.5 ； 跳跃表的第三个元素的成员为 &quot;apple&quot; ，它的分值为 8 ； 和链表、字典等数据结构被广泛地应用在 Redis 内部不同，Redis 只在两个地方用到了跳跃表，一个是实现有序集合键，另一个是在集群节点中用作内部数据结构，除此之外，跳跃表在 Redis 里面没有其他用途。 跳表的结构如下 Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针，等等 图 5-1 展示了一个跳跃表示例，位于图片最左边的是 zskiplist 结构，该结构包含以下属性： header ：指向跳跃表的表头节点 tail ：指向跳跃表的表尾节点 level ：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内） length ：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内） 位于 zskiplist 结构右方的是四个 zskiplistNode 结构，该结构包含以下属性： 层(level)：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 后退(backward)指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值(score)：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。 成员对象(obj)：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象。 注意表头节点和其他节点的构造是一样的：表头节点也有后退指针、分值和成员对象，不过表头节点的这些属性都不会被用到，所以图中省略了这些部分，只显示了表头节点的各个层。 跳表实现 跳跃表节点的实现由 redis.h/zskiplistNode 结构定义： 12345678910111213141516171819202122typedef struct zskiplistNode &#123; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[]; &#125; zskiplistNode; 层 跳跃表节点的 level 数组可以包含多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。 每次创建一个新跳跃表节点的时候，程序都根据幂次定律（power law，越大的数出现的概率越小）随机生成一个介于 1 和 32 之间的值作为 level 数组的大小，这个大小就是层的“高度”。 图 5-2 分别展示了三个高度为 1 层、 3 层和 5 层的节点，因为 C 语言的数组索引总是从 0 开始的，所以节点的第一层是 level[0] ，而第二层是 level[1] ，以此类推。 前进指针 每个层都有一个指向表尾方向的前进指针（level[i].forward 属性），用于从表头向表尾方向访问节点。 图 5-3 用虚线表示出了程序从表头向表尾方向，遍历跳跃表中所有节点的路径： 首先访问跳跃表的第一个节点（表头），然后从第四层的前进指针移动到表中的第二个节点。 在第二个节点时，程序沿着第二层的前进指针移动到表中的第三个节点。 在第三个节点时，程序同样沿着第二层的前进指针移动到表中的第四个节点。 当程序再次沿着第四个节点的前进指针移动时，它碰到一个 NULL ，程序知道这时已经到达了跳跃表的表尾，于是结束这次遍历 跨度 层的跨度（level[i].span 属性）用于记录两个节点之间的距离： 两个节点之间的跨度越大，它们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0 ，因为它们没有连向任何节点。 初看上去，很容易以为跨度和遍历操作有关，但实际上并不是这样 ——遍历操作只使用前进指针就可以完成了，跨度实际上是用来计算排位（rank）的：在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位。 举个例子，图 5-4 用虚线标记了在跳跃表中查找分值为 3.0 、成员对象为 o3 的节点时，沿途经历的层：查找的过程只经过了一个层，并且层的跨度为 3 ，所以目标节点在跳跃表中的排位为 3 。 再举个例子，图 5-5 用虚线标记了在跳跃表中查找分值为 2.0 、成员对象为 o2 的节点时，沿途经历的层：在查找节点的过程中，程序经过了两个跨度为 1 的节点，因此可以计算出，目标节点在跳跃表中的排位为 2 后退指针 节点的后退指针（backward 属性）用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。 图 5-6 用虚线展示了如果从表尾向表头遍历跳跃表中的所有节点：程序首先通过跳跃表的 tail 指针访问表尾节点，然后通过后退指针访问倒数第二个节点，之后再沿着后退指针访问倒数第三个节点，再之后遇到指向 NULL 的后退指针，于是访问结束 分值与成员 节点的分值（score 属性）是一个 double 类型的浮点数，跳跃表中的所有节点都按分值从小到大来排序。 节点的成员对象（obj 属性）是一个指针，它指向一个字符串对象，而字符串对象则保存着一个 SDS 值。 在同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却可以是相同的：分值相同的节点将按照成员对象在字典序中的大小来进行排序，成员对象较小的节点会排在前面（靠近表头的方向），而成员对象较大的节点则会排在后面（靠近表尾的方向）。 举个例子，在图 5-7 所示的跳跃表中，三个跳跃表节点都保存了相同的分值 10086.0 ，但保存成员对象 o1 的节点却排在保存成员对象 o2 和 o3 的节点之前，而保存成员对象 o2 的节点又排在保存成员对象 o3 的节点之前，由此可见，o1 、 o2 、 o3 三个成员对象在字典中的排序为 o1 &lt;= o2 &lt;= o3 。 跳跃表 虽然仅靠多个跳跃表节点就可以组成一个跳跃表，如图 5-8 所示。 但通过使用一个 zskiplist 结构来持有这些节点，程序可以更方便地对整个跳跃表进行处理，比如快速访问跳跃表的表头节点和表尾节点，又或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息，如图 5-9 所示。 zskiplist 结构的定义如下 123456789101112typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level; &#125; zskiplist; header 和 tail 指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的复杂度为 O(1) 。 通过使用 length 属性来记录节点的数量，程序可以在 O(1) 复杂度内返回跳跃表的长度。 level 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量，注意表头节点的层高并不计算在内。 跳跃更新 跳表的插入删除与普通的查找过程一样，但是出现了一个问题，他是如何维护这些索引的？不然就有可能出现两个索引节点之间的数据非常多！ 整体步骤如下： 查找并获取最底层链表中第一个小于等于num的位置插入 随机产生该节点的层数 它肯定有第1层的指针 如果节点在第i层有指针，那么他在第i+1层有指针的概率是p，那么就有 12345678static int random_level() &#123; int level = 1; while(rand() &lt; SKIPLIST_P_VAL &amp;&amp; level &lt; MAX_LEVEL) level++; return level;&#125;//SKIPLIST_P_VAL = 1/4//MAX_LEVEL = 32 总结 跳跃表是有序集合的底层实现之一，除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成，其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度），而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中，多个节点可以包含相同的分值，但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序，当分值相同时，节点按照成员对象的大小进行排序。 整数集合 整数集合（intset）是集合键的底层实现之一：当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集合键的底层实现。 举个例子，如果创建一个只包含五个元素的集合键，并且集合中的所有元素都是整数值，那么这个集合键的底层实现就会是整数集合： 12345redis&gt; SADD numbers 1 3 5 7 9(integer) 5 redis&gt; OBJECT ENCODING numbers\"intset\" 整数集合的实现 整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构，它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值，并且保证集合中不会出现重复元素。 每个 intset.h/intset 结构表示一个整数集合： 123456789101112typedef struct intset &#123; // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; &#125; intset; contents 数组是整数集合的底层实现：整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复项。 通过什么决定排序？ length 属性记录了整数集合包含的元素数量，也即是 contents 数组的长度。 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值 ——contents 数组的真正类型取决于 encoding 属性的值： 如果 encoding 属性的值为 INTSET_ENC_INT16 ，那么 contents 就是一个 int16_t 类型的数组，数组里的每个项都是一个 int16_t 类型的整数值（最小值为 -32,768 ，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ，那么 contents 就是一个 int32_t 类型的数组，数组里的每个项都是一个 int32_t 类型的整数值（最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ，那么 contents 就是一个 int64_t 类型的数组，数组里的每个项都是一个 int64_t 类型的整数值（最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ） 这是怎么实现的？ 图 6-1 展示了一个整数集合示例： encoding 属性的值为 INTSET_ENC_INT16 ，表示整数集合的底层实现为 int16_t 类型的数组，而集合保存的都是 int16_t 类型的整数值。 length 属性的值为 5 ，表示整数集合包含五个元素。 contents 数组按从小到大的顺序保存着集合中的五个元素。 因为每个集合元素都是 int16_t 类型的整数值，所以 contents 数组的大小等于 sizeof(int16_t) *5 = 16* 5 = 80 位。 图 6-2 展示了另一个整数集合示例： encoding 属性的值为 INTSET_ENC_INT64 ，表示整数集合的底层实现为 int64_t 类型的数组，而数组中保存的都是 int64_t 类型的整数值。 length 属性的值为 4 ，表示整数集合包含四个元素。 contents 数组按从小到大的顺序保存着集合中的四个元素。 因为每个集合元素都是 int64_t 类型的整数值，所以 contents 数组的大小为 sizeof(int64_t) *4 = 64* 4 = 256 位。 虽然 contents 数组保存的四个整数值中，只有 -2675256175807981027 是真正需要用 int64_t 类型来保存的，而其他的 1 、 3 、 5 三个值都可以用 int16_t 类型来保存，不过根据整数集合的升级规则，当向一个底层为 int16_t 数组的整数集合添加一个 int64_t 类型的整数值时，整数集合已有的所有元素都会被转换成 int64_t 类型，所以 contents 数组保存的四个整数值都是 int64_t 类型的，不仅仅是 -2675256175807981027 升级 每当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrade），然后才能将新元素添加到整数集合里面。 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。举个例子，假设现在有一个 INTSET_ENC_INT16 编码的整数集合，集合中包含三个 int16_t 类型的元素，如图 6-3 所示。 因为每个元素都占用 16 位空间，所以整数集合底层数组的大小为 3 * 16 = 48 位，图 6-4 展示了整数集合的三个元素在这 48 位里的位置 现在，假设我们要将类型为 int32_t 的整数值 65535 添加到整数集合里面，因为 65535 的类型 int32_t 比整数集合当前所有元素的类型都要长，所以在将 65535 添加到整数集合之前，程序需要先对整数集合进行升级。 升级首先要做的是，根据新类型的长度，以及集合元素的数量（包括要添加的新元素在内），对底层数组进行空间重分配。 整数集合目前有三个元素，再加上新元素 65535 ，整数集合需要分配四个元素的空间，因为每个 int32_t 整数值需要占用 32 位空间，所以在空间重分配之后，底层数组的大小将是 32 * 4 = 128 位，如图 6-5 所示。 虽然程序对底层数组进行了空间重分配，但数组原有的三个元素 1 、 2 、 3 仍然是 int16_t 类型，这些元素还保存在数组的前 48 位里面，所以程序接下来要做的就是将这三个元素转换成 int32_t 类型，并将转换后的元素放置到正确的位上面，而且在放置元素的过程中，需要维持底层数组的有序性质不变。 首先，因为元素 3 在 1 、 2 、 3 、 65535 四个元素中排名第三，所以它将被移动到 contents 数组的索引 2 位置上，也即是数组 64 位至 95 位的空间内，如图 6-6 所示。 接着，因为元素 2 在 1 、 2 、 3 、 65535 四个元素中排名第二，所以它将被移动到 contents 数组的索引 1 位置上，也即是数组的 32 位至 63 位的空间内，如图 6-7 所示。 之后，因为元素 1 在 1 、 2 、 3 、 65535 四个元素中排名第一，所以它将被移动到 contents 数组的索引 0 位置上，也即是数组的 0 位至 31 位的空间内，如图 6-8 所示 然后，因为元素 65535 在 1 、 2 、 3 、 65535 四个元素中排名第四，所以它将被添加到 contents 数组的索引 3 位置上，也即是数组的 96 位至 127 位的空间内，如图 6-9 所示 最后，程序将整数集合 encoding 属性的值从 INTSET_ENC_INT16 改为 INTSET_ENC_INT32 ，并将 length 属性的值从 3 改为 4 ，设置完成之后的整数集合如图 6-10 所示 因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为 O(N) 。 其他类型的升级操作，比如从 INTSET_ENC_INT16 编码升级为 INTSET_ENC_INT64 编码，或者从 INTSET_ENC_INT32 编码升级为 INTSET_ENC_INT64 编码，升级的过程都和上面展示的升级过程类似。 升级之后新元素的摆放位置 因为引发升级的新元素的长度总是比整数集合现有所有元素的长度都大，所以这个新元素的值要么就大于所有现有元素，要么就小于所有现有元素： 在新元素小于所有现有元素的情况下，新元素会被放置在底层数组的最开头（索引 0 ）； 在新元素大于所有现有元素的情况下，新元素会被放置在底层数组的最末尾（索引 length-1 ） 升级的好处 提升灵活度 一般只使用 int16_t 类型的数组来保存 int16_t 类型的值，只使用 int32_t 类型的数组来保存 int32_t 类型的值，诸如此类。 整数集合通过自动升级底层数组来适应新元素，可以随意地将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中，而不必担心出现类型错误，这种做法非常灵活。 节约内存 让一个数组可以同时保存 int16_t 、 int32_t 、 int64_t 三种类型的值，最简单的做法就是直接使用 int64_t 类型的数组作为整数集合的底层实现。即使添加到整数集合里面的都是 int16_t 类型或者 int32_t 类型的值，数组都需要使用 int64_t 类型的空间去保存它们，从而出现浪费内存的情况。 整数集合的做法既可以让集合能同时保存三种不同类型的值，又可以确保升级操作只会在有需要的时候进行，这可以尽量节省内存。 如果一直只向整数集合添加 int16_t 类型的值，那么整数集合的底层实现就会一直是 int16_t 类型的数组，只有在将 int32_t 类型或者 int64_t 类型的值添加到集合时，程序才会对数组进行升级 降级 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。 举个例子，对于图 6-11 所示的整数集合来说，即使我们将集合里唯一一个真正需要使用 int64_t 类型来保存的元素 4294967295 删除了，整数集合的编码仍然会维持 INTSET_ENC_INT64 ，底层数组也仍然会是 int64_t 类型的，如图 6-12 所示 总结 整数集合是集合键的底层实现之一。 整数集合的底层实现为数组，这个数组以有序、无重复的方式保存集合元素，在有需要时，程序会根据新添加元素的类型，改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性，并且尽可能地节约了内存。 整数集合只支持升级操作，不支持降级操作。 压缩列表 压缩列表（ziplist）是列表键和哈希键的底层实现之一。 当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做列表键的底层实现。 比如说，执行以下命令将创建一个压缩列表实现的列表键： 1234redis&gt; RPUSH lst 1 3 5 10086 \"hello\" \"world\"(integer) 6 redis&gt; OBJECT ENCODING lst\"ziplist\" 因为列表键里面包含的都是 1 、 3 、 5 、 10086 这样的小整数值，以及 &quot;hello&quot; 、 &quot;world&quot; 这样的短字符串。 另外，当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做哈希键的底层实现。 举个例子，执行以下命令将创建一个压缩列表实现的哈希键： 1234redis&gt; HMSET profile \"name\" \"Jack\" \"age\" 28 \"job\" \"Programmer\"OK redis&gt; OBJECT ENCODING profile\"ziplist\" 因为哈希键里面包含的所有键和值都是小整数值或者短字符串。 本章将对压缩列表的定义以及相关操作进行详细的介绍 压缩列表 压缩列表是 Redis 为了节约内存而开发的，由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。 一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值。 图 7-1 展示了压缩列表的各个组成部分，表 7-1 则记录了各个组成部分的类型、长度、以及用途。 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数：在对压缩列表进行内存重分配，或者计算 zlend 的位置时使用。 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节：通过这个偏移量，程序无须遍历整个压缩列表就可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量：当这个属性的值小于 UINT16_MAX （65535）时，这个属性的值就是压缩列表包含节点的数量；当这个值等于 UINT16_MAX 时，节点的真实数量需要遍历整个压缩列表才能计算得出。 entryX 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。 图 7-2 展示了一个压缩列表示例： 列表 zlbytes 属性的值为 0x50 （十进制 80），表示压缩列表的总长为 80 字节。 列表 zltail 属性的值为 0x3c （十进制 60），这表示如果我们有一个指向压缩列表起始地址的指针 p ，那么只要用指针 p 加上偏移量 60 ，就可以计算出表尾节点 entry3 的地址。 列表 zllen 属性的值为 0x3 （十进制 3），表示压缩列表包含三个节点 图 7-3 展示了另一个压缩列表示例： 列表 zlbytes 属性的值为 0xd2 （十进制 210），表示压缩列表的总长为 210 字节。 列表 zltail 属性的值为 0xb3 （十进制 179），这表示如果我们有一个指向压缩列表起始地址的指针 p ，那么只要用指针 p 加上偏移量 179 ，就可以计算出表尾节点 entry5 的地址。 列表 zllen 属性的值为 0x5 （十进制 5），表示压缩列表包含五个节点。 压缩列表节点构成 每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种： 长度小于等于 63 （2^{6}-1）字节的字节数组； 长度小于等于 16383 （2^{14}-1） 字节的字节数组； 长度小于等于 4294967295 （2^{32}-1）字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4 位长，介于 0 至 12 之间的无符号整数； 1 字节长的有符号整数； 3 字节长的有符号整数； int16_t 类型整数； int32_t 类型整数； int64_t 类型整数。 每个压缩列表节点都由 previous_entry_length 、 encoding 、 content 三个部分组成，如图 7-4 所示 previous_entry_length 节点的 previous_entry_length 属性以字节为单位，记录了压缩列表中前一个节点的长度。 previous_entry_length 属性的长度可以是 1 字节或者 5 字节： 如果前一节点的长度小于 254 字节，那么 previous_entry_length 属性的长度为 1 字节：前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节，那么 previous_entry_length 属性的长度为 5 字节：其中属性的第一字节会被设置为 0xFE （十进制值 254），而之后的四个字节则用于保存前一节点的长度。 图 7-5 展示了一个包含一字节长 previous_entry_length 属性的压缩列表节点，属性的值为 0x05 ，表示前一节点的长度为 5 字节 图 7-6 展示了一个包含五字节长 previous_entry_length 属性的压缩节点，属性的值为 0xFE00002766 ， 值的最高位字节 0xFE 表示这是一个五字节长的 previous_entry_length 属性， 之后的四字节 0x00002766 （十进制值 10086 ）才是前一节点的实际长度 因为节点的 previous_entry_length 属性记录了前一个节点的长度，所以程序可以通过指针运算，根据当前节点的起始地址来计算出前一个节点的起始地址。 举个例子，如果我们有一个指向当前节点起始地址的指针 c ，那么我们只要用指针 c 减去当前节点 previous_entry_length 属性的值，就可以得出一个指向前一个节点起始地址的指针 p ，如图 7-7 所示。 压缩列表的从表尾向表头遍历操作就是使用这一原理实现的：只要我们拥有了一个指向某个节点起始地址的指针，那么通过这个指针以及这个节点的 previous_entry_length 属性，程序就可以一直向前一个节点回溯，最终到达压缩列表的表头节点。 图 7-8 展示了一个从表尾节点向表头节点进行遍历的完整过程： 首先，我们拥有指向压缩列表表尾节点 entry4 起始地址的指针 p1（指向表尾节点的指针可以通过指向压缩列表起始地址的指针加上 zltail 属性的值得出）； 通过用 p1 减去 entry4 节点 previous_entry_length 属性的值，我们得到一个指向 entry4 前一节点 entry3 起始地址的指针 p2 ； 通过用 p2 减去 entry3 节点 previous_entry_length 属性的值，我们得到一个指向 entry3 前一节点 entry2 起始地址的指针 p3 ； 通过用 p3 减去 entry2 节点 previous_entry_length 属性的值，我们得到一个指向 entry2 前一节点 entry1 起始地址的指针 p4 ，entry1 为压缩列表的表头节点； 最终，我们从表尾节点向表头节点遍历了整个列表。 encoding 节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度： 一字节、两字节或者五字节长，值的最高位为 00 、 01 或者 10 的是字节数组编码：这种编码表示节点的 content 属性保存着字节数组，数组的长度由编码除去最高两位之后的其他位记录； 一字节长，值的最高位以 11 开头的是整数编码：这种编码表示节点的 content 属性保存着整数值，整数值的类型和长度由编码除去最高两位之后的其他位记录； 表 7-2 记录了所有可用的字节数组编码，而表 7-3 则记录了所有可用的整数编码。表格中的下划线 _ 表示留空，而 b 、 x 等变量则代表实际的二进制数据，为了方便阅读，多个字节之间用空格隔开。 表7-2: 编码 编码长度 content 属性保存的值 00bbbbbb 1 字节 长度小于等于 63 字节的字节数组。 01bbbbbb xxxxxxxx 2 字节 长度小于等于 16383 字节的字节数组。 10**__** aaaaaaaa bbbbbbbb cccccccc dddddddd 5 字节 长度小于等于 4294967295 的字节数组。 表7-3: 编码 编码长度 content 属性保存的值 11000000 1 字节 int16_t 类型的整数。 11010000 1 字节 int32_t 类型的整数。 11100000 1 字节 int64_t 类型的整数。 11110000 1 字节 24 位有符号整数。 11111110 1 字节 8 位有符号整数。 1111xxxx 1 字节 使用这一编码的节点没有相应的 content 属性，因为编码本身的 xxxx 四个位已经保存了一个介于 0 和 12 之间的值，所以它无须 content 属性。 content 节点的 content 属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的 encoding 属性决定。 图 7-9 展示了一个保存字节数组的节点示例： 编码的最高两位 00 表示节点保存的是一个字节数组； 编码的后六位 001011 记录了字节数组的长度 11 ； content 属性保存着节点的值 &quot;hello world&quot; 。 图 7-10 展示了一个保存整数值的节点示例： 编码 11000000 表示节点保存的是一个 int16_t 类型的整数值； content 属性保存着节点的值 10086 。 连锁更新 前面说过，每个节点的 previous_entry_length 属性都记录了前一个节点的长度： 如果前一节点的长度小于 254 字节，那么 previous_entry_length 属性需要用 1 字节长的空间来保存这个长度值。 如果前一节点的长度大于等于 254 字节，那么 previous_entry_length 属性需要用 5 字节长的空间来保存这个长度值。 考虑这样一种情况：在一个压缩列表中，有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ，如图 7-11 所示。 因为 e1 至 eN 的所有节点的长度都小于 254 字节，所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性，换句话说，e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。 如果将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点，那么 new 将成为 e1 的前置节点，如图 7-12 所示。 因为 e1 的 previous_entry_length 属性仅长 1 字节，它没办法保存新节点 new 的长度，所以程序将对压缩列表执行空间重分配操作，并将 e1 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 现在，麻烦的事情来了 ——e1 原本的长度介于 250 字节至 253 字节之间，在为 previous_entry_length 属性新增四个字节的空间之后，e1 的长度就变成了介于 254 字节至 257 字节之间，而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。 因此，为了让 e2 的 previous_entry_length 属性可以记录下 e1 的长度，程序需要再次对压缩列表执行空间重分配操作，并将 e2 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 正如扩展 e1 引发了对 e2 的扩展一样，扩展 e2 也会引发对 e3 的扩展，而扩展 e3 又会引发对 e4 的扩展……为了让每个节点的 previous_entry_length 属性都符合压缩列表对节点的要求，程序需要不断地对压缩列表执行空间重分配操作，直到 eN 为止。 Redis 将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update），图 7-13 展示了这一过程。 除了添加新节点可能会引发连锁更新之外，删除节点也可能会引发连锁更新。 考虑图 7-14 所示的压缩列表，如果 e1 至 eN 都是大小介于 250 字节至 253 字节的节点，big 节点的长度大于等于 254 字节（需要 5 字节的 previous_entry_length 来保存），而 small 节点的长度小于 254 字节（只需要 1 字节的 previous_entry_length 来保存），那么当我们将 small 节点从压缩列表中删除之后，为了让 e1 的 previous_entry_length 属性可以记录 big 节点的长度，程序将扩展 e1 的空间，并由此引发之后的连锁更新 因为连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配操作，而每次空间重分配的最坏复杂度为 O(N) ，所以连锁更新的最坏复杂度为 O(N^2) 。 要注意的是，尽管连锁更新的复杂度较高，但它真正造成性能问题的几率是很低的： 首先，压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点，连锁更新才有可能被引发，在实际中，这种情况并不多见； 其次，即使出现连锁更新，但只要被更新的节点数量不多，就不会对性能造成任何影响：比如说，对三五个节点进行连锁更新是绝对不会影响性能的； 因为以上原因，ziplistPush 等命令的平均复杂度仅为 O(N) ，在实际中，我们可以放心地使用这些函数，而不必担心连锁更新会影响压缩列表的性能。 总结 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表，或者从压缩列表中删除节点，可能会引发连锁更新操作，但这种操作出现的几率并不高。 参考链接 https://www.bookstack.cn/read/redisbook/ https://www.cnblogs.com/yinbiao/p/10740212.html https://www.jianshu.com/p/9d8296562806 https://blog.csdn.net/qq_42604176/article/details/110550937","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Go-11-基础类型","slug":"Go/Go-11-基础类型","date":"2021-09-12T05:15:43.000Z","updated":"2023-07-15T05:36:42.705Z","comments":true,"path":"2021/09/12/Go/Go-11-基础类型/","link":"","permalink":"http://xboom.github.io/2021/09/12/Go/Go-11-%E5%9F%BA%E7%A1%80%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"go源码下载地址 golang的数据类型和数据结构的底层实现 Go的类型系统 Golang的类型分为以下几类： 命名类型[named (defined) type]具有名称的类型：int,int64,float32,string,bool。这些GO预先声明好了 通过类型声明(type declaration)创建的所有类型都是命令类型 123var i int //named typetype myInt int //named typevar b bool //named type 一个命名类型一定和其他类型不同 未命名类型(unnamed type):数组，结构体，指针，函数，接口，切片,map,通道都是未命令类型 123[]string // unnamed typemap[string]string // unnamed type[10]int // unnamed type 虽然没有名字，但却有一个类型字面量(type literal)来描述它们由什么构成 基础类型 任何类型T都有基础类型 如果T 是预先声明类型：boolean, numeric, or string（布尔，数值，字符串）中的一个，或者是一个类型字面量(type literal)，他们对应的基础类型就是T自身。 T的基础类型就是T所引用的那个类型的类型声明(type declaration) 基本类型 在/src/runtime/type.go 对 type类型进行了定义 123456789101112131415161718type _type struct &#123; size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; string类型 在 /src/runtime/string.go 对string类型进行了声明 1234type stringStruct struct &#123; str unsafe.Pointer len int&#125; 参考链接 深入研究Go(golang) Type类型系统","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-03-Groutine","slug":"Go/Go-03-Groutine","date":"2021-09-08T16:08:34.000Z","updated":"2023-07-15T05:36:04.104Z","comments":true,"path":"2021/09/09/Go/Go-03-Groutine/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go-03-Groutine/","excerpt":"","text":"创建一个协程 1go func()//通过go关键字启动一个协程来运行函数 那么它到底干了什么 123456789101112131415func newproc(siz int32, fn *funcval) &#123; argp := add(unsafe.Pointer(&amp;fn), sys.PtrSize) gp := getg() pc := getcallerpc() systemstack(func() &#123; newg := newproc1(fn, argp, siz, gp, pc) _p_ := getg().m.p.ptr() runqput(_p_, newg, true) if mainStarted &#123; wakep() &#125; &#125;)&#125; 关键术语 并发：一个cpu上能同时执行多项任务，在很短时间内，cpu来回切换任务执行(在某段很短时间内执行程序a，然后又迅速得切换到程序b去执行)，有时间上的重叠（宏观上是同时的，微观仍是顺序执行）,这样看起来多个任务像是同时执行，这就是并发 并行：当系统有多个CPU时,每个CPU同一时刻都运行任务，互不抢占自己所在的CPU资源，同时进行，称为并行 进程：cpu在切换程序的时候，如果不保存上一个程序的状态（也就是context–上下文），直接切换下一个程序，就会丢失上一个程序的一系列状态，于是引入了进程这个概念，用以划分好程序运行时所需要的资源。因此进程就是一个程序运行时候的所需要的基本资源单位（也可以说是程序运行的一个实体） 线程：cpu切换多个进程的时候，会花费不少的时间，因为切换进程需要切换到内核态，而每次调度需要内核态都需要读取用户态的数据，进程一旦多起来，cpu调度会消耗一大堆资源，因此引入了线程的概念，线程本身几乎不占有资源，他们共享进程里的资源，内核调度起来不会那么像进程切换那么耗费资源 协程：协程拥有自己的寄存器上下文和栈。 协程与线程 goroutine与thread的不同 内存占用 一个 goroutine 的栈内存消耗为 2 KB(如果栈空间不够用，会自动进行扩容)。 一个 thread 则需要消耗 1 MB 栈内存，还需要一个被称为 “a guard page” 的区域(用于与其他thread隔离) 创建和销毀 goroutine的切换会消耗200ns(用户态，3个寄存器)，相当于2400-3600条指令 除了使用时需要陷入内核，线程切换会消耗1000-1500ns 1ns平均可执行12-18条指令 当 threads 切换时，需要保存各种寄存器，以便将来恢复： 16 general purpose registers: 通用寄存器 PC (Program Counter): 程序计数器 SP (Stack Pointer): 栈指针 segment registers: 段寄存器 16 XMM registers: FP coprocessor state 16 AVX registers all MSRs etc 而 goroutines 切换只需保存三个寄存器 Program Counter Stack Pointer BP：基址指针寄存器，常用于在访问内存时存放内存单元的偏移地址 Thread内存堆栈 创建一个 thread 为了尽量避免极端情况下操作系统线程栈的溢出，默认会为其分配一个较大的栈内存( 1 - 8 MB 栈内存，线程标准 POSIX Thread)，而且还需要一个被称为 guard page 的区域用于和其他 thread 的栈空间进行隔离。而栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险 调度模型 Go 程序的执行由两层组成：Go Program，Runtime(即用户程序和运行时) Go创建M个线程(CPU执行调度的单元，内核的task_struck)，之后创建N个goroutine会依附在M个线程上执行即M:N模型。当M指定了线程栈，则M.stack-&gt;G.stack，M的PC寄存器指向G提供的函数，然后执行 GMP Go的调度器内部有四个重要的结构：M，P，G，Sched M:M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息 G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。 P:代表一个Processor(Core 虚拟处理器)，它的主要用途就是用来执行goroutine的，维护了一个需要执行goroutine的队列(LRQ Local Run Queue) Sched:代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等 LRQ：本地运行队列，它属于每个处理器，以便管理分配给要执行的goroutines GRQ：全局运行队列，存在于未分配的goroutine中 下面四种情形下，Go scheduler有机会进行调度 使用关键字go: go创建一个新的goroutine, Go scheduler 会考虑调度 GC: 由于进行 GC 的 goroutine 也需要在 M 上运行，因此肯定会发生调度。当然，Go scheduler 还会做很多其他的调度，例如调度不涉及堆访问的 goroutine 来运行。GC 不管栈上的内存，只会回收堆上的内存 系统调用：当 goroutine 进行系统调用时，会阻塞 M，所以它会被调度走，同时一个新的 goroutine 会被调度上来 内存同步访问：atomic，mutex，channel 操作等会使 goroutine 阻塞，因此会被调度走。等条件满足后（例如其他 goroutine 解锁了）还会被调度上来继续运行 M:N 模型 Go runtime 会负责 goroutine 的生老病死，从创建到销毁。Runtime 会在程序启动的时候，创建 M 个线程(CPU 执行调度的单位)，之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行。这就是 M:N 模型： 在同一时刻，一个线程上只能跑一个 goroutine。当 goroutine 发生阻塞(例如向一个 channel 发送数据，被阻塞)时，runtime 会把当前 goroutine 调度走，让其他 goroutine 来执行。目的就是不让一个线程闲着，榨干 CPU 的每一滴油水 工作窃取 Go scheduler 的职责就是将所有处于 runnable 的 goroutines 均匀分布到在 P 上运行的 M。当一个 P 发现自己的 LRQ 已经没有 G 时，会从其他 P “偷” 一些 G 来运行。这被称为 Work-stealing Go scheduler 使用 M:N 模型，在任一时刻，M 个 goroutines（G） 要分配到 N 个内核线程（M），这些 M 跑在个数最多为 GOMAXPROCS 的逻辑处理器（P）上。每个 M 必须依附于一个 P，每个 P 在同一时刻只能运行一个 M。如果 P 上的 M 阻塞了，那它就需要其他的 M 来运行 P 的 LRQ 里的 goroutines Go scheduler 每一轮调度要做的工作就是找到处于 runnable 的 goroutines，并执行它。找的顺序如下： 12345678runtime.schedule() &#123; // only 1/61 of the time, check the global runnable queue for a G. // if not found, check the local queue. // if not found, // try to steal from other Ps. // if not, check the global runnable queue. // if not found, poll network.&#125; 找到一个可执行的 goroutine 后，就会一直执行下去，直到被阻塞 当 P2 上的一个 G 执行结束，就会去 LRQ 获取下一个 G 来执行。如果 LRQ 已经空了，就是说本地可运行队列已经没有 G 需要执行，并且这时 GRQ 也没有 G 了。这时，P2 会随机选择一个 P（称为 P1），P2 会从 P1 的 LRQ “偷”过来一半的 G 状态切换 GPM 共同成就 Go scheduler。G 需要在 M 上才能运行，M 依赖 P 提供的资源，P 则持有待运行的 G。 M 会从与它绑定的 P 的本地队列获取可运行的 G，也会从 network poller 里获取可运行的 G，还会从其他 P 偷 G G 的状态流转： 省略了一些垃圾回收的状态 P 的状态流转： 通常情况下（在程序运行时不调整 P 的个数），P 只会在上图中的四种状态下进行切换。 当程序刚开始运行进行初始化时，所有的 P 都处于 _Pgcstop 状态， 随着 P 的初始化（runtime.procresize），会被置于 _Pidle。 当 M 需要运行时，会 runtime.acquirep 来使 P 变成 Prunning 状态，并通过 runtime.releasep 来释放。 当 G 执行时需要进入系统调用，P 会被设置为 _Psyscall， 如果这个时候被系统监控抢夺（runtime.retake），则 P 会被重新修改为 _Pidle。 如果在程序运行中发生 GC，则 P 会被设置为 _Pgcstop， 并在 runtime.startTheWorld 时重新调整为 _Prunning M 的状态变化： M 只有自旋和非自旋两种状态。自旋的时候，会努力找工作；找不到的时候会进入非自旋状态，之后会休眠，直到有工作需要处理时，被其他工作线程唤醒，又进入自旋状态 调度器 Go调度程序不是抢占式调度程序，而是协作式调度程序。 成为协作调度程序意味着调度程序需要在代码的安全点明确定义用户空间事件，以制定调度决策。 以下是调度的关键点： 启动协程的关键字go 垃圾回收GC 系统调用 对于异步系统调用例如网络请求，将可能阻止的goroutine移到网络轮询器，让处理程序可以执行下一个 处理像文件IO同步请求，当前的G和M对将与G，P，M模型分开。 同时，将创建一台新机器，以保持原始的G，P，M模型正常工作，并且在系统调用完成时将收回块goroutine 地鼠(gopher)用小车运着一堆待加工的砖。M就可以看作图中的地鼠，P就是小车，G就是小车里装的砖。一图胜千言啊，弄清楚了它们三者的关系，下面我们就开始重点聊地鼠是如何在搬运砖块的 启动过程 runtime·schedinit 调度器初始化：主要是根据用户设置的GOMAXPROCS来创建一批小车§,不管设置多大，最多也只能创建256个小车§。这些小车§初始创建好后都是闲置状态，也就是还没开始使用，所以它们都放置在调度器结构(Sched)的pidle字段维护的链表中存储起来了，以备后续之需。 runtime.newproc 创建一个协程 runtime.main runtime·mstart 源码阅读 G 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990type g struct &#123; // goroutine 使用的栈 stack stack // offset known to runtime/cgo // 用于栈的扩张和收缩检查，抢占标志 stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink _defer *_defer // innermost defer //当前与g 绑定的 m m *m // current m; offset known to arm liblink // goroutine 的运行现场 sched gobuf syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc stktopsp uintptr // expected sp at top of stack, to check in traceback // wakeup 时传入的参数 param unsafe.Pointer atomicstatus uint32 stackLock uint32 // sigprof/scang lock; TODO: fold in to atomicstatus goid int64 schedlink guintptr // g 被阻塞之后的近似时间 waitsince int64 // approx time when the g become blocked // g 被阻塞的原因 waitreason waitReason // if status==Gwaiting // 抢占调度标志。这个为 true 时，stackguard0 等于 stackpreempt preempt bool // preemption signal, duplicates stackguard0 = stackpreempt preemptStop bool // transition to _Gpreempted on preemption; otherwise, just deschedule preemptShrink bool // shrink stack at synchronous safe point // asyncSafePoint is set if g is stopped at an asynchronous // safe point. This means there are frames on the stack // without precise pointer information. asyncSafePoint bool paniconfault bool // panic (instead of crash) on unexpected fault address gcscandone bool // g has scanned stack; protected by _Gscan bit in status throwsplit bool // must not split stack // activeStackChans indicates that there are unlocked channels // pointing into this goroutine's stack. If true, stack // copying needs to acquire channel locks to protect these // areas of the stack. activeStackChans bool // parkingOnChan indicates that the goroutine is about to // park on a chansend or chanrecv. Used to signal an unsafe point // for stack shrinking. It's a boolean value, but is updated atomically. parkingOnChan uint8 raceignore int8 // ignore race detection events sysblocktraced bool // StartTrace has emitted EvGoInSyscall about this goroutine tracking bool // whether we're tracking this G for sched latency statistics trackingSeq uint8 // used to decide whether to track this G runnableStamp int64 // timestamp of when the G last became runnable, only used when tracking runnableTime int64 // the amount of time spent runnable, cleared when running, only used when tracking // syscall 返回之后的 cputicks，用来做 tracing sysexitticks int64 // cputicks when syscall has returned (for tracing) traceseq uint64 // trace event sequencer tracelastp puintptr // last P emitted an event for this goroutine // 如果调用了 LockOsThread，那么这个 g 会绑定到某个 m 上 lockedm muintptr sig uint32 writebuf []byte sigcode0 uintptr sigcode1 uintptr sigpc uintptr // 创建该 goroutine 的语句的指令地址 gopc uintptr // pc of go statement that created this goroutine ancestors *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors) // goroutine 函数的指令地址 startpc uintptr // pc of goroutine function racectx uintptr waiting *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order cgoCtxt []uintptr // cgo traceback context labels unsafe.Pointer // profiler labels // time.Sleep 缓存的定时器 timer *timer // cached timer for time.Sleep selectDone uint32 // are we participating in a select and did someone win the race? // Per-G GC state // gcAssistBytes is this G's GC assist credit in terms of // bytes allocated. If this is positive, then the G has credit // to allocate gcAssistBytes bytes without assisting. If this // is negative, then the G must correct this by performing // scan work. We track this in bytes to make it fast to update // and check for debt in the malloc hot path. The assist ratio // determines how this corresponds to scan work debt. gcAssistBytes int64&#125; 其中g结构体关联了两个比较简单的结构体，stack 表示 goroutine 运行时的栈 123456789101112131415161718192021// 描述栈的数据结构，栈的范围：[lo, hi)type stack struct &#123; // 栈顶，低地址 lo uintptr // 栈低，高地址 hi uintptr&#125;type gobuf struct &#123; // 存储 rsp 寄存器的值 sp uintptr // 存储 rip 寄存器的值 pc uintptr // 指向 goroutine g guintptr ctxt unsafe.Pointer // this has to be a pointer so that gc scans it // 保存系统调用的返回值 ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer&#125; M 再来看 M，取 machine 的首字母，它代表一个工作线程，或者说系统线程。G 需要调度到 M 上才能运行，M 是真正工作的人。结构体 m 就是我们常说的 M，它保存了 M 自身使用的栈信息、当前正在 M 上执行的 G 信息、与之绑定的 P 信息…… 当 M 没有工作可做的时候，在它休眠前，会“自旋”地来找工作：检查全局队列，查看 network poller，试图执行 gc 任务，或者“偷”工作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// m 代表工作线程，保存了自身使用的栈信息type m struct &#123; // 记录工作线程（也就是内核线程）使用的栈信息。在执行调度代码时需要使用 // 执行用户 goroutine 代码时，使用用户 goroutine 自己的栈，因此调度时会发生栈的切换 g0 *g // goroutine with scheduling stack morebuf gobuf // gobuf arg to morestack divmod uint32 // div/mod denominator for arm - known to liblink // Fields not known to debuggers. procid uint64 // for debuggers, but offset not hard-coded gsignal *g // signal-handling g goSigStack gsignalStack // Go-allocated signal handling stack sigmask sigset // storage for saved signal mask tls [tlsSlots]uintptr // thread-local storage (for x86 extern register) mstartfn func() curg *g // current running goroutine caughtsig guintptr // goroutine running during fatal signal p puintptr // attached p for executing go code (nil if not executing go code) nextp puintptr oldp puintptr // the p that was attached before executing a syscall id int64 mallocing int32 throwing int32 preemptoff string // if != \"\", keep curg running on this m locks int32 dying int32 profilehz int32 spinning bool // m is out of work and is actively looking for work blocked bool // m is blocked on a note newSigstack bool // minit on C thread called sigaltstack printlock int8 incgo bool // m is executing a cgo call freeWait uint32 // if == 0, safe to free g0 and delete m (atomic) fastrand [2]uint32 needextram bool traceback uint8 ncgocall uint64 // number of cgo calls in total ncgo int32 // number of cgo calls currently in progress cgoCallersUse uint32 // if non-zero, cgoCallers in use temporarily cgoCallers *cgoCallers // cgo traceback if crashing in cgo call doesPark bool // non-P running threads: sysmon and newmHandoff never use .park park note alllink *m // on allm schedlink muintptr lockedg guintptr createstack [32]uintptr // stack that created this thread. lockedExt uint32 // tracking for external LockOSThread lockedInt uint32 // tracking for internal lockOSThread nextwaitm muintptr // next m waiting for lock waitunlockf func(*g, unsafe.Pointer) bool waitlock unsafe.Pointer waittraceev byte waittraceskip int startingtrace bool syscalltick uint32 freelink *m // on sched.freem // mFixup is used to synchronize OS related m state // (credentials etc) use mutex to access. To avoid deadlocks // an atomic.Load() of used being zero in mDoFixupFn() // guarantees fn is nil. mFixup struct &#123; lock mutex used uint32 fn func(bool) bool &#125; // these are here because they are too large to be on the stack // of low-level NOSPLIT functions. libcall libcall libcallpc uintptr // for cpu profiler libcallsp uintptr libcallg guintptr syscall libcall // stores syscall parameters on windows vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call) vdsoPC uintptr // PC for traceback while in VDSO call // preemptGen counts the number of completed preemption // signals. This is used to detect when a preemption is // requested, but fails. Accessed atomically. preemptGen uint32 // Whether this is a pending preemption signal on this M. // Accessed atomically. signalPending uint32 dlogPerM mOS // Up to 10 locks held by this m, maintained by the lock ranking code. locksHeldLen int locksHeld [10]heldLockInfo&#125; P 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140type p struct &#123; id int32 status uint32 // one of pidle/prunning/... link puintptr schedtick uint32 // incremented on every scheduler call syscalltick uint32 // incremented on every system call sysmontick sysmontick // last tick observed by sysmon m muintptr // back-link to associated m (nil if idle) mcache *mcache pcache pageCache raceprocctx uintptr deferpool [5][]*_defer // pool of available defer structs of different sizes (see panic.go) deferpoolbuf [5][32]*_defer // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen. goidcache uint64 goidcacheend uint64 // Queue of runnable goroutines. Accessed without lock. runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready'd by // the current G and should be run next instead of what's in // runq if there's time remaining in the running G's time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready'd // goroutines to the end of the run queue. // // Note that while other P's may atomically CAS this to zero, // only the owner P can CAS it to a valid G. runnext guintptr // Available G's (status == Gdead) gFree struct &#123; gList n int32 &#125; sudogcache []*sudog sudogbuf [128]*sudog // Cache of mspan objects from the heap. mspancache struct &#123; // We need an explicit length here because this field is used // in allocation codepaths where write barriers are not allowed, // and eliminating the write barrier/keeping it eliminated from // slice updates is tricky, moreso than just managing the length // ourselves. len int buf [128]*mspan &#125; tracebuf traceBufPtr // traceSweep indicates the sweep events should be traced. // This is used to defer the sweep start event until a span // has actually been swept. traceSweep bool // traceSwept and traceReclaimed track the number of bytes // swept and reclaimed by sweeping in the current sweep loop. traceSwept, traceReclaimed uintptr palloc persistentAlloc // per-P to avoid mutex _ uint32 // Alignment for atomic fields below // The when field of the first entry on the timer heap. // This is updated using atomic functions. // This is 0 if the timer heap is empty. timer0When uint64 // The earliest known nextwhen field of a timer with // timerModifiedEarlier status. Because the timer may have been // modified again, there need not be any timer with this value. // This is updated using atomic functions. // This is 0 if there are no timerModifiedEarlier timers. timerModifiedEarliest uint64 // Per-P GC state gcAssistTime int64 // Nanoseconds in assistAlloc gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic) // gcMarkWorkerMode is the mode for the next mark worker to run in. // That is, this is used to communicate with the worker goroutine // selected for immediate execution by // gcController.findRunnableGCWorker. When scheduling other goroutines, // this field must be set to gcMarkWorkerNotWorker. gcMarkWorkerMode gcMarkWorkerMode // gcMarkWorkerStartTime is the nanotime() at which the most recent // mark worker started. gcMarkWorkerStartTime int64 // gcw is this P's GC work buffer cache. The work buffer is // filled by write barriers, drained by mutator assists, and // disposed on certain GC state transitions. gcw gcWork // wbBuf is this P's GC write barrier buffer. // // TODO: Consider caching this in the running G. wbBuf wbBuf runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point // statsSeq is a counter indicating whether this P is currently // writing any stats. Its value is even when not, odd when it is. statsSeq uint32 // Lock for timers. We normally access the timers while running // on this P, but the scheduler can also do it from a different P. timersLock mutex // Actions to take at some time. This is used to implement the // standard library's time package. // Must hold timersLock to access. timers []*timer // Number of timers in P's heap. // Modified using atomic instructions. numTimers uint32 // Number of timerDeleted timers in P's heap. // Modified using atomic instructions. deletedTimers uint32 // Race context used while executing timer functions. timerRaceCtx uintptr // preempt is set to indicate that this P should be enter the // scheduler ASAP (regardless of what G is running on it). preempt bool // Padding is no longer needed. False sharing is now not a worry because p is large enough // that its size class is an integer multiple of the cache line size (for any of our architectures).&#125; Sched Go scheduler 在源码中的结构体为 schedt，保存调度器的状态信息、全局的可运行 G 队列等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112// 保存调度器的信息type schedt struct &#123; // accessed atomically. keep at top to ensure alignment on 32-bit systems. // 需以原子访问访问。 // 保持在 struct 顶部，以使其在 32 位系统上可以对齐 goidgen uint64 lastpoll uint64 // time of last network poll, 0 if currently polling pollUntil uint64 // time to which current poll is sleeping lock mutex // 由空闲的工作线程组成的链表 midle muintptr // idle m's waiting for work // 空闲的工作线程数量 nmidle int32 // number of idle m's waiting for work // 空闲的且被 lock 的 m 计数 nmidlelocked int32 // number of locked m's waiting for work // 已经创建的工作线程数量 mnext int64 // number of m's that have been created and next M ID // 表示最多所能创建的工作线程数量 maxmcount int32 // maximum number of m's allowed (or die) // goroutine 的数量，自动更新 nmsys int32 // number of system m's not counted for deadlock //累计释放的m数量 nmfreed int64 // cumulative number of freed m's // 由空闲的 p 结构体对象组成的链表 ngsys uint32 // number of system goroutines; updated atomically // 空闲的 p 结构体对象的数量 pidle puintptr // idle p's npidle uint32 nmspinning uint32 // See \"Worker thread parking/unparking\" comment in proc.go. // Global runnable queue. // 全局可运行的 G队列 runq gQueue runqsize int32 // disable controls selective disabling of the scheduler. // // Use schedEnableUser to control this. // // disable is protected by sched.lock. disable struct &#123; // user disables scheduling of user goroutines. user bool runnable gQueue // pending runnable Gs n int32 // length of runnable &#125; // Global cache of dead G's. // dead G 的全局缓存 // 已退出的 goroutine 对象，缓存下来 // 避免每次创建 goroutine 时都重新分配内存 gFree struct &#123; lock mutex stack gList // Gs with stacks noStack gList // Gs without stacks n int32 &#125; // Central cache of sudog structs. // sudog 结构的集中缓存 sudoglock mutex sudogcache *sudog // Central pool of available defer structs of different sizes. // 不同大小的可用的 defer struct 的集中缓存池 deferlock mutex deferpool [5]*_defer // freem is the list of m's waiting to be freed when their // m.exited is set. Linked through m.freelink. freem *m gcwaiting uint32 // gc is waiting to run stopwait int32 stopnote note sysmonwait uint32 sysmonnote note // While true, sysmon not ready for mFixup calls. // Accessed atomically. sysmonStarting uint32 // safepointFn should be called on each P at the next GC // safepoint if p.runSafePointFn is set. safePointFn func(*p) safePointWait int32 safePointNote note profilehz int32 // cpu profiling rate procresizetime int64 // nanotime() of last change to gomaxprocs totaltime int64 // ∫gomaxprocs dt up to procresizetime // sysmonlock protects sysmon's actions on the runtime. // // Acquire and hold this mutex to block sysmon from interacting // with the rest of the runtime. sysmonlock mutex _ uint32 // ensure timeToRun has 8-byte alignment // timeToRun is a distribution of scheduling latencies, defined // as the sum of time a G spends in the _Grunnable state before // it transitions to _Grunning. // // timeToRun is protected by sched.lock. timeToRun timeHistogram&#125; 还有一些重要的常量 1234567891011121314151617181920212223242526// 所有 g 的长度allglen uintptr// 保存所有的 gallgs []*g// 保存所有的 mallm *m// 保存所有的 p，_MaxGomaxprocs = 1024allp [_MaxGomaxprocs + 1]*p// p 的最大值，默认等于 ncpugomaxprocs int32// 程序启动时，会调用 osinit 函数获得此值ncpu int32// 调度器结构体对象，记录了调度器的工作状态sched schedt// 代表进程的主线程m0 m// m0 的 g0，即 m0.g0 = &amp;g0g0 g 参考文献： https://colobu.com/2017/05/04/golang-runtime-scheduler/ http://morsmachine.dk/go-scheduler https://www.zhihu.com/question/20862617 https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html https://www.bookstack.cn/read/go-internals/zh-05.1.md https://golang.design/go-questions/sched/goroutine-vs-thread/ https://learnku.com/articles/41728 https://www.codercto.com/a/38162.html https://www.cnblogs.com/flhs/p/12677335.html","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-06-内存管理","slug":"Go/Go-06-内存管理","date":"2021-09-08T16:08:22.000Z","updated":"2023-07-15T05:36:20.422Z","comments":true,"path":"2021/09/09/Go/Go-06-内存管理/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go-06-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"带着问题看世界 内存是如何管理的 如何根据指定的大小分配内存的 基础概念 Go在程序启动的时候，会先向操作系统申请一块内存(注意这时还只是一段虚拟的地址空间，并不会真正地分配内存)，切成小块后自己进行管理。申请到的内存块被分配了三个区域，在X64上分别是512MB，16GB，512GB大小。 arena区域就是所谓的堆区，Go动态分配的内存都是在这个区域，它把内存分割成8KB大小的页，一些页组合起来称为mspan。 bitmap区标识arena区域哪些地址保存了对象，并且用4bit标志位表示对象是否包含指针、GC标记信息。bitmap中一个byte大小的内存对应arena区域中4个指针大小(指针大小为 8B)的内存，所以bitmap区域的大小是512GB/(4*8B)=16GB 从上图其实还可以看到bitmap的高地址部分指向arena区域的低地址部分，也就是说bitmap的地址是由高地址向低地址增长的。 spans区域存放mspan(也就是一些arena分割的页组合起来的内存管理基本单元)的指针，每个指针对应一页，所以spans区域的大小就是512GB/8KB*8B=512MB。除以8KB是计算arena区域的页数，而最后乘以8是计算spans区域所有指针的大小。创建mspan的时候，按页填充对应的spans区域，在回收object时，根据地址很容易就能找到它所属的mspan 内存管理单元 mspan：Go中内存管理的基本单元，是由一片连续的8KB的页组成的大块内存。它是一个包含起始地址、mspan规格、页的数量等内容的双端链表。 每个mspan按照它自身的属性Size Class的大小分割成若干个object，每个object可存储一个对象。并且会使用一个位图来标记其尚未使用的object，属性Size Class决定object大小，而mspan只会分配给和object尺寸大小接近的对象，当然，对象的大小要小于object大小。还有一个概念：Span Class，它和Size Class的含义差不多，Size_Class = Span_Class / 2。 为什么乘以2，因为每个 Size Class有两个mspan，也就是有两个Span Class。其中一个分配给含有指针的对象，另一个分配给不含有指针的对象。如何寻找为对象寻span，寻找span的流程如下： 计算对象所需内存大小size。 根据size到size class映射，计算出所需的size class。 根据size class和对象是否包含指针计算出span class。 获取该span class指向的span。 如下图，mspan由一组连续的页组成，按照一定大小划分成object。 mspan的Size Class共有67种，每种mspan分割的object大小是8*2n的倍数，这个是写死在代码里的： 1234// path: /usr/local/go/src/runtime/sizeclasses.goconst _NumSizeClasses = 67var class_to_size = [_NumSizeClasses]uint16&#123;0, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536,1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768&#125; 数组里最大的数是32768，也就是32KB，超过此大小就是大对象；类型Size Class为0表示大对象，它实际上直接由堆内存分配，而小对象都要通过mspan来分配。 对于mspan来说，它的Size Class会决定它所能分到的页数，这也是写死在代码里的： 1234// path: /usr/local/go/src/runtime/sizeclasses.goconst _NumSizeClasses = 67var class_to_allocnpages = [_NumSizeClasses]uint8&#123;0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 2, 3, 4, 5, 6, 1, 7, 6, 5, 4, 3, 5, 7, 2, 9, 7, 5, 8, 3, 10, 7, 4&#125; 比如要申请一个object大小为32B的mspan的时候，在class_to_size里对应的索引是3，而索引3在class_to_allocnpages数组里对应的页数就是1。 mspan结构体定义 12345678910111213141516171819202122232425262728type mspan struct &#123; //链表前向指针，用于将span链接起来 next *mspan //链表前向指针，用于将span链接起来 prev *mspan // 起始地址，也即所管理页的地址 startAddr uintptr // 管理的页数 npages uintptr // 块个数，表示有多少个块可供分配 nelems uintptr //分配位图，每一位代表一个块是否已分配 allocBits *gcBits // 已分配块的个数 allocCount uint16 // class表中的class ID，和Size Classs相关 spanclass spanClass // class表中的对象大小，也即块大小 elemsize uintptr &#125; 将mspan放到更大的视角来看： 上图可以看到有两个S指向了同一个mspan，因为这两个S指向的P是同属一个mspan的。所以，通过arena上的地址可以快速找到指向它的S，通过S就能找到mspan，回忆一下前面我们说的mspan区域的每个指针对应一页。 假设最左边第一个mspan的Size Class等于10，根据前面的class_to_size数组，得出这个msapn分割的object大小是144B，算出可分配的对象个数是8KB/144B=56.89个，取整56个，所以会有一些内存浪费掉了，Go的源码里有所有Size Class的mspan浪费的内存的大小；再根据class_to_allocnpages数组，得到这个mspan只由1个page组成；假设这个mspan是分配给无指针对象的，那么spanClass等于20。 startAddr直接指向arena区域的某个位置，表示这个mspan的起始地址，allocBits指向一个位图，每位代表一个块是否被分配了对象；allocCount则表示总共已分配的对象个数。 这样，左起第一个mspan的各个字段参数就如下图所示： 内存管理元件 内存分配由内存分配器完成。分配器由3种组件构成：mcache, mcentral, mheap Span 是 Go 内存管理的基本单位，代码中为 mspan，由一组连续的 page 组成 mcache 与TCMalloc中的ThreadCache类似，mcache保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以无锁访问。 Go中是每个P拥有1个mcache，最多需要 GOMAXPROCS 个mcache就可以保证各线程对mcache的无锁访问 mcentral 与TCMalloc中的CentralCache不同的是CentralCache 是每个级别的Span有1个链表，mcache是每个级别的Span有2个链表。为什么有两个？ mheap 与TCMalloc中的PageHeap不同点的是 mheap把Span组织成了树结构，而不是链表，并且还是2棵树，然后把Span分配到heapArena进行管理，它包含地址映射和span是否包含指针等位图，这样做的主要原因是为了更高效的利用内存：分配、回收和再利用 Go的内存管理基本单位是span，每个span通过spanclass标识属于哪种规格的span，golang的span规格一共有67种，详细可查看 src/runtime/sizeclasses.go mcache mcache：每个工作线程都会绑定一个mcache，本地缓存可用的mspan资源，这样就可以直接给Goroutine分配，因为不存在多个Goroutine竞争的情况，所以不会消耗锁资源。 mcache的结构体定义： 12345type mcache struct &#123; alloc [numSpanClasses]*mspan&#125;numSpanClasses = _NumSizeClasses &lt;&lt; 1 mcache用Span Classes作为索引管理多个用于分配的mspan，它包含所有规格的mspan。它是_NumSizeClasses的2倍，也就是67*2=134 为什么有一个两倍的关系：为了加速之后内存回收的速度，数组里一半的mspan中分配的对象不包含指针，另一半则包含指针。而无指针的mspan在进行垃圾回收的时候是不需要扫描它是否引用了其他活跃对象的。 mcache在初始化的时候是没有任何mspan资源的，在使用过程中会动态地从mcentral申请，之后会缓存下来。当对象小于等于32KB大小时，使用mcache的相应规格的mspan进行分配 mcentral mcentral：为所有mcache提供切分好的mspan资源。每个central保存一种特定大小的全局mspan列表，包括已分配出去的和未分配出去的。 每个mcentral对应一种mspan，而mspan的种类导致它分割的object大小不同。当工作线程的mcache中没有合适（也就是特定大小的）的mspan时就会从mcentral获取。 mcentral被所有的工作线程共同享有，存在多个Goroutine竞争的情况，因此会消耗锁资源。结构体定义： 1234567891011121314151617//path: /usr/local/go/src/runtime/mcentral.gotype mcentral struct &#123; // 互斥锁 lock mutex // 规格 sizeclass int32 // 尚有空闲object的mspan链表 nonempty mSpanList // 没有空闲object的mspan链表，或者是已被mcache取走的msapn链表 empty mSpanList // 已累计分配的对象个数 nmalloc uint64 &#125; empty表示这条链表里的mspan都被分配了object，或者是已经被cache取走了的mspan，这个mspan就被那个工作线程独占了。而nonempty则表示有空闲对象的mspan列表。每个central结构体都在mheap中维护。 mcache从mcentral获取和归还mspan的流程： 获取 加锁； 从nonempty链表找到一个可用的mspan； 并将其从nonempty链表删除； 将取出的mspan加入到empty链表； 将mspan返回给工作线程； 解锁。 归还 加锁； 将mspan从empty链表删除； 将mspan加入到nonempty链表； 解锁。 mheap mheap：代表Go程序持有的所有堆空间，Go程序使用一个mheap的全局对象_mheap来管理堆内存。 当mcentral没有空闲的mspan时，会向mheap申请。 而mheap没有资源时，会向操作系统申请新内存。 mheap主要用于大对象的内存分配，以及管理未切割的mspan，用于给mcentral切割成小对象。 mheap中含有所有规格的mcentral，所以，当一个mcache从mcentral申请mspan时，只需要在独立的mcentral中使用锁，并不会影响申请其他规格的mspan。 mheap结构体定义： 12345678910111213141516171819202122232425//path: /usr/local/go/src/runtime/mheap.gotype mheap struct &#123; lock mutex // spans: 指向mspans区域，用于映射mspan和page的关系 spans []*mspan // 指向bitmap首地址，bitmap是从高地址向低地址增长的 bitmap uintptr // 指示arena区首地址 arena_start uintptr // 指示arena区已使用地址位置 arena_used uintptr // 指示arena区末地址 arena_end uintptr central [67*2]struct &#123; mcentral mcentral pad [sys.CacheLineSize - unsafe.Sizeof(mcentral&#123;&#125;)%sys.CacheLineSize]byte &#125;&#125; 内存分配流程 Go的内存分配器在分配对象时，根据对象的大小，分成三类：小对象（小于等于16B）、一般对象（大于16B，小于等于32KB）、大对象（大于32KB）。 大体上的分配流程： 大于 32KB 的对象，直接从mheap上分配； 小于等于 16B 的对象使用mcache的tiny分配器分配； (16B,32KB] 的对象，首先计算对象的规格大小，然后使用mcache中相应规格大小的mspan分配； 如果mcache没有相应规格大小的mspan，则向mcentral申请 如果mcentral没有相应规格大小的mspan，则向mheap申请 如果mheap中也没有合适大小的mspan，则向操作系统申请 地址空间 Go 语言的运行时构建了操作系统的内存管理抽象层，将运行时管理的地址空间分成以下四种状态 None: 内存没有被保留或者映射，是地址空间的默认状态 Reserved: 运行时持有该地址空间，但是访问该内存会导致错误 Prepared: 内存被保留，一般没有对应的物理内存访问该片内存的行为是未定义的可以快速转换到 Ready 状态 Ready: 可以被安全访问 不同状态之间的转换过程： 运行时中包含多个操作系统实现的状态转换方法，所有的实现都包含在以 mem_ 开头的文件中，本节将介绍 Linux 操作系统对上图中方法的实现： runtime.sysAlloc: 会从操作系统中获取一大块可用的内存空间，可能为几百 KB 或者几 MB； runtime.sysFree: 会在程序发生内存不足（Out-of Memory，OOM）时调用并无条件地返回内存； runtime.sysReserve 会保留操作系统中的一片内存区域，访问这片内存会触发异常； runtime.sysMap 保证内存区域可以快速转换至就绪状态； runtime.sysUsed 通知操作系统应用程序需要使用该内存区域，保证内存区域可以安全访问； runtime.sysUnused 通知操作系统虚拟内存对应的物理内存已经不再需要，可以重用物理内存； runtime.sysFault 将内存区域转换成保留状态，主要用于运行时的调试； 运行时使用 Linux 提供的 mmap、munmap 和 madvise 等系统调用实现了操作系统的内存管理抽象层 参考链接 https://qcrao.com/2019/03/13/graphic-go-memory-allocation/ https://www.infoq.cn/article/IEhRLwmmIM7-11RYaLHR https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-memory-allocator/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-04-Select","slug":"Go/Go-04-Select","date":"2021-09-08T16:08:10.000Z","updated":"2023-07-15T05:36:09.358Z","comments":true,"path":"2021/09/09/Go/Go-04-Select/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go-04-Select/","excerpt":"","text":"select 实例 实例1与实例2有什么不同？ 123456789101112131415161718192021222324252627282930313233func main()&#123; var count int for &#123; select &#123; case &lt;-time.Tick(time.Millisecond * 500): fmt.Println(\"hello\") count++ fmt.Println(\"count---&gt;\" , count) case &lt;-time.Tick(time.Millisecond * 499) : fmt.Println(\"world\") count++ fmt.Println(\"count---&gt;\" , count) &#125; &#125;&#125;func main()&#123; t1 := time.Tick(time.Second) t2 := time.Tick(time.Second) var count int for &#123; select &#123; case &lt;-t1: fmt.Println(\"hello\") count++ fmt.Println(\"count---&gt;\" , count) case &lt;-t2 : fmt.Println(\"world\") count++ fmt.Println(\"count---&gt;\" , count) &#125; &#125;&#125; 执行结果为： 实例1：只会输出含有world日志的case 实例2：交替执行t1与t2 select原理 select通过select.go实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314/* cas0 为*scase指针 指向[ncases]*scase的数组 order0 *uint16 ncases 记录case个数 返回选中第几个case以及是否收到值*/func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) &#123; //debug // NOTE: In order to maintain a lean stack size, the number of scases // is capped at 65536. cas1 := (*[1 &lt;&lt; 16]scase)(unsafe.Pointer(cas0)) order1 := (*[1 &lt;&lt; 17]uint16)(unsafe.Pointer(order0)) scases := cas1[:ncases:ncases] pollorder := order1[:ncases:ncases] lockorder := order1[ncases:][:ncases:ncases] //遍历case如果chan为空且不是default则置位空case对象 scase&#123;&#125; for i := range scases &#123; cas := &amp;scases[i] if cas.c == nil &amp;&amp; cas.kind != caseDefault &#123; *cas = scase&#123;&#125; &#125; &#125; // generate permuted order for i := 1; i &lt; ncases; i++ &#123; j := fastrandn(uint32(i + 1)) pollorder[i] = pollorder[j] pollorder[j] = uint16(i) &#125; // sort the cases by Hchan address to get the locking order. // simple heap sort, to guarantee n log n time and constant stack footprint. //根据chan的地址进行重新排序 for i := 0; i &lt; ncases; i++ &#123; j := i // Start with the pollorder to permute cases on the same channel. c := scases[pollorder[i]].c for j &gt; 0 &amp;&amp; scases[lockorder[(j-1)/2]].c.sortkey() &lt; c.sortkey() &#123; k := (j - 1) / 2 lockorder[j] = lockorder[k] j = k &#125; lockorder[j] = pollorder[i] &#125; for i := ncases - 1; i &gt;= 0; i-- &#123; o := lockorder[i] c := scases[o].c lockorder[i] = lockorder[0] j := 0 for &#123; k := j*2 + 1 if k &gt;= i &#123; break &#125; if k+1 &lt; i &amp;&amp; scases[lockorder[k]].c.sortkey() &lt; scases[lockorder[k+1]].c.sortkey() &#123; k++ &#125; if c.sortkey() &lt; scases[lockorder[k]].c.sortkey() &#123; lockorder[j] = lockorder[k] j = k continue &#125; break &#125; lockorder[j] = o &#125; // lock all the channels involved in the select sellock(scases, lockorder) var ( gp *g sg *sudog c *hchan k *scase sglist *sudog sgnext *sudog qp unsafe.Pointer nextp **sudog )loop: // pass 1 - look for something already waiting var dfli int var dfl *scase var casi int var cas *scase var recvOK bool for i := 0; i &lt; ncases; i++ &#123; casi = int(pollorder[i]) cas = &amp;scases[casi] c = cas.c switch cas.kind &#123; case caseNil: continue case caseRecv: sg = c.sendq.dequeue() if sg != nil &#123; goto recv &#125; if c.qcount &gt; 0 &#123; goto bufrecv &#125; if c.closed != 0 &#123; goto rclose &#125; case caseSend: if c.closed != 0 &#123; goto sclose &#125; sg = c.recvq.dequeue() if sg != nil &#123; goto send &#125; if c.qcount &lt; c.dataqsiz &#123; goto bufsend &#125; case caseDefault: dfli = casi dfl = cas &#125; &#125; if dfl != nil &#123; selunlock(scases, lockorder) casi = dfli cas = dfl goto retc &#125; // pass 2 - enqueue on all chans gp = getg() if gp.waiting != nil &#123; throw(\"gp.waiting != nil\") &#125; nextp = &amp;gp.waiting for _, casei := range lockorder &#123; casi = int(casei) cas = &amp;scases[casi] if cas.kind == caseNil &#123; continue &#125; c = cas.c sg := acquireSudog() sg.g = gp sg.isSelect = true // No stack splits between assigning elem and enqueuing // sg on gp.waiting where copystack can find it. sg.elem = cas.elem sg.releasetime = 0 if t0 != 0 &#123; sg.releasetime = -1 &#125; sg.c = c // Construct waiting list in lock order. *nextp = sg nextp = &amp;sg.waitlink switch cas.kind &#123; case caseRecv: c.recvq.enqueue(sg) case caseSend: c.sendq.enqueue(sg) &#125; &#125; // wait for someone to wake us up gp.param = nil // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(&amp;gp.parkingOnChan, 1) gopark(selparkcommit, nil, waitReasonSelect, traceEvGoBlockSelect, 1) gp.activeStackChans = false sellock(scases, lockorder) gp.selectDone = 0 sg = (*sudog)(gp.param) gp.param = nil // pass 3 - dequeue from unsuccessful chans // otherwise they stack up on quiet channels // record the successful case, if any. // We singly-linked up the SudoGs in lock order. casi = -1 cas = nil sglist = gp.waiting // Clear all elem before unlinking from gp.waiting. for sg1 := gp.waiting; sg1 != nil; sg1 = sg1.waitlink &#123; sg1.isSelect = false sg1.elem = nil sg1.c = nil &#125; gp.waiting = nil for _, casei := range lockorder &#123; k = &amp;scases[casei] if k.kind == caseNil &#123; continue &#125; if sglist.releasetime &gt; 0 &#123; k.releasetime = sglist.releasetime &#125; if sg == sglist &#123; // sg has already been dequeued by the G that woke us up. casi = int(casei) cas = k &#125; else &#123; c = k.c if k.kind == caseSend &#123; c.sendq.dequeueSudoG(sglist) &#125; else &#123; c.recvq.dequeueSudoG(sglist) &#125; &#125; sgnext = sglist.waitlink sglist.waitlink = nil releaseSudog(sglist) sglist = sgnext &#125; if cas == nil &#123; // We can wake up with gp.param == nil (so cas == nil) // when a channel involved in the select has been closed. // It is easiest to loop and re-run the operation; // we'll see that it's now closed. // Maybe some day we can signal the close explicitly, // but we'd have to distinguish close-on-reader from close-on-writer. // It's easiest not to duplicate the code and just recheck above. // We know that something closed, and things never un-close, // so we won't block again. goto loop &#125; c = cas.c if cas.kind == caseRecv &#123; recvOK = true &#125; selunlock(scases, lockorder) goto retcbufrecv: recvOK = true qp = chanbuf(c, c.recvx) if cas.elem != nil &#123; typedmemmove(c.elemtype, cas.elem, qp) &#125; typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.qcount-- selunlock(scases, lockorder) goto retcbufsend: // can send to buffer typedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ selunlock(scases, lockorder) goto retcrecv: // can receive from sleeping sender (sg) recv(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) recvOK = true goto retcrclose: // read at end of closed channel selunlock(scases, lockorder) recvOK = false if cas.elem != nil &#123; typedmemclr(c.elemtype, cas.elem) &#125; goto retcsend: // can send to a sleeping receiver (sg) send(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) if debugSelect &#123; print(\"syncsend: cas0=\", cas0, \" c=\", c, \"\\n\") &#125; goto retcretc: if cas.releasetime &gt; 0 &#123; blockevent(cas.releasetime-t0, 1) &#125; return casi, recvOKsclose: // send on closed channel selunlock(scases, lockorder) panic(plainError(\"send on closed channel\"))&#125; 流程图如下： 参考链接 https://blog.csdn.net/u011957758/article/details/82230316","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-12-Interface","slug":"Go/Go-12-Interface","date":"2021-09-05T10:36:41.000Z","updated":"2023-07-15T05:36:47.099Z","comments":true,"path":"2021/09/05/Go/Go-12-Interface/","link":"","permalink":"http://xboom.github.io/2021/09/05/Go/Go-12-Interface/","excerpt":"","text":"interface赋值问题 123456789101112131415161718192021package mainimport ( \"fmt\")type People interface &#123; Speak(string) string&#125;type Stduent struct&#123;&#125;func (stu *Stduent) Speak(think string) (talk string) &#123; if think == \"love\" &#123; talk = \"You are a good boy\" &#125; else &#123; talk = \"hi\" &#125; return&#125;func main() &#123; var peo People = Stduent&#123;&#125; think := \"love\" fmt.Println(peo.Speak(think))&#125; 多态的几个要素： 1、有interface接口，并且有接口定义的方法。 2、有子类去重写interface的接口。 3、有父类指针指向子类的具体对象 所以上述代码报错的地方在var peo People = Stduent{}这条语句， Student{}已经重写了父类People{}中的Speak(string) string方法，那么只需要用父类指针指向子类对象即可。 所以应该改成var peo People = &amp;Student{} 即可编译通过。（People为interface类型，就是指针类型） interface的内部构造(非空接口iface情况) 123456789101112131415161718192021package mainimport ( \"fmt\")type People interface &#123; Show()&#125;type Student struct&#123;&#125;func (stu *Student) Show() &#123;&#125;func live() People &#123; var stu *Student return stu&#125;func main() &#123; if live() == nil &#123; fmt.Println(\"AAAAAAA\") &#125; else &#123; fmt.Println(\"BBBBBBB\") &#125;&#125; 结果是: BBBBBB interface在使用的过程中，共有两种表现形式 一种为空接口(empty interface)，定义如下： 1var MyInterface interface&#123;&#125; 另一种为非空接口(non-empty interface), 定义如下： 1type MyInterface interface &#123;function()&#125; 这两种interface类型分别用两种struct表示，空接口为eface, 非空接口为iface. 空接口eface 空接口eface结构，由两个属性构成，一个是类型信息_type，一个是数据信息。其数据结构声明如下： 1234type eface struct &#123; //空接口 _type *_type //类型信息 data unsafe.Pointer //指向数据的指针(go语言中特殊的指针类型unsafe.Pointer类似于c语言中的void*)&#125; _type属性：是GO语言中所有类型的公共描述，Go语言几乎所有的数据结构都可以抽象成 _type，是所有类型的公共描述，**type负责决定data应该如何解释和操作，**type的结构代码如下: 12345678910111213type _type struct &#123; size uintptr //类型大小 ptrdata uintptr //前缀持有所有指针的内存大小 hash uint32 //数据hash值 tflag tflag align uint8 //对齐 fieldalign uint8 //嵌入结构体时的对齐 kind uint8 //kind 有些枚举值kind等于0是无效的 alg *typeAlg //函数指针数组，类型实现的所有方法 gcdata *byte str nameOff ptrToThis typeOff&#125; data属性: 表示指向具体的实例数据的指针，他是一个unsafe.Pointer类型，相当于一个C的万能指针void* 非空接口iface iface 表示 non-empty interface 的数据结构，非空接口初始化的过程就是初始化一个iface类型的结构，其中data的作用同eface的相同，这里不再多加描述。 1234type iface struct &#123; tab *itab data unsafe.Pointer&#125; iface结构中最重要的是itab结构（结构如下），每一个 itab 都占 32 字节的空间。itab可以理解为pair&lt;interface type, concrete type&gt; 。itab里面包含了interface的一些关键信息，比如method的具体实现。 12345678type itab struct &#123; inter *interfacetype // 接口自身的元信息 _type *_type // 具体类型的元信息 link *itab bad int32 hash int32 // _type里也有一个同样的hash，此处多放一个是为了方便运行接口断言 fun [1]uintptr // 函数指针，指向具体类型所实现的方法&#125; 其中值得注意的字段： interface type包含了一些关于interface本身的信息，比如package path，包含的method。这里的interfacetype是定义interface的一种抽象表示。 type表示具体化的类型，与eface的 type类型相同。 hash字段其实是对_type.hash的拷贝，它会在interface的实例化时，用于快速判断目标类型和接口中的类型是否一致。另，Go的interface的Duck-typing机制也是依赖这个字段来实现。 fun字段其实是一个动态大小的数组，虽然声明时是固定大小为1，但在使用时会直接通过fun指针获取其中的数据，并且不会检查数组的边界，所以该数组中保存的元素数量是不确定的 1234func live() People &#123; var stu *Student return stu &#125; stu是一个指向nil的空指针，但是最后return stu 会触发匿名变量 People = stu值拷贝动作，所以最后live()放回给上层的是一个People insterface{}类型，也就是一个iface struct{}类型。 stu为nil，只是iface中的data 为nil而已。 但是iface struct{}本身并不为nil. Interface 内部构造(空接口eface情况) 1234567891011func Foo(x interface&#123;&#125;) &#123; if x == nil &#123; fmt.Println(\"empty interface\") return &#125; fmt.Println(\"non-empty interface\")&#125;func main() &#123; var p *int = nil Foo(p)&#125; 结果为 “non-empty interface” 因为 Fool()的形参 x interface{} 是一个空接口类型eface struct{},在执行Foo(p)的时候，触发x interface{} = p语句的时候，此X的结构为 所以 x 结构体本身不为nil，而是data指针指向的p为nil Interface{}与*Interface{} 1234567891011121314type S struct &#123;&#125;func f(x interface&#123;&#125;) &#123;&#125;func g(x *interface&#123;&#125;) &#123;&#125;func main() &#123; s := S&#123;&#125; p := &amp;s f(s) //A g(s) //B f(p) //C g(p) //D&#125; 结果： 12345B、D两行错误B错误为： cannot use s (type S) as type *interface &#123;&#125; in argument to g: *interface &#123;&#125; is pointer to interface, not interfaceD错误为：cannot use p (type *S) as type *interface &#123;&#125; in argument to g: *interface &#123;&#125; is pointer to interface, not interface 看到这道题需要第一时间想到的是Golang是强类型语言，interface是所有golang类型的父类 函数中func f(x interface{})的interface{}可以支持传入golang的任何类型，包括指针，但是函数func g(x *interface{})只能接受*interface{} 参考链接 https://www.bookstack.cn/read/aceld-golang/4、interface.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-07-垃圾回收","slug":"Go/Go-07-垃圾回收","date":"2021-09-03T15:37:38.000Z","updated":"2023-07-15T05:36:24.976Z","comments":true,"path":"2021/09/03/Go/Go-07-垃圾回收/","link":"","permalink":"http://xboom.github.io/2021/09/03/Go/Go-07-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"","text":"垃圾回收 GC，全称 Garbage Collection，即垃圾回收，是一种自动内存管理的机制 通常，垃圾回收器的执行过程被划分为两个半独立的组件： 赋值器（Mutator）：这一名称本质上是在指代用户态的代码。因为对垃圾回收器而言，用户态的代码仅仅只是在修改对象之间的引用关系，也就是在对象图（对象之间引用关系的一个有向图）上进行操作。 回收器（Collector）：负责执行垃圾回收的代码 根对象在垃圾回收的术语中又叫做根集合，它是垃圾回收器在标记过程时最先检查的对象，包括： 全局变量：程序在编译期就能确定的那些存在于程序整个生命周期的变量。 执行栈：每个 goroutine 都包含自己的执行栈，这些执行栈上包含栈上的变量及指向分配的堆内存区块的指针。 寄存器：寄存器的值可能表示一个指针，参与计算的这些指针可能指向某些赋值器分配的堆内存区块。 所有的 GC 算法其存在形式可归结为追踪(Tracing)和引用计数(Reference Counting)这两种形式的混合运用 追踪式 GC:从根对象出发，根据对象之间的引用信息，一步步推进直到扫描完毕整个堆并确定需要保留的对象，从而回收所有可回收的对象。Go、 Java、V8 对 JavaScript 的实现等均为追踪式 GC。 引用计数式 GC:每个对象自身包含一个被引用的计数器，当计数器归零时自动得到回收。因为此方法缺陷较多，在追求高性能时通常不被应用。Python、Objective-C 等均为引用计数式 GC。 目前比较常见的 GC 实现方式包括： 追踪式，分为多种不同类型，例如： 标记清扫：从根对象出发，将确定存活的对象进行标记，并清扫可以回收的对象。 标记整理：为了解决内存碎片问题而提出，在标记过程中，将对象尽可能整理到一块连续的内存上。 增量式：将标记与清扫的过程分批执行，每次执行很小的部分，从而增量的推进垃圾回收，达到近似实时、几乎无停顿的目的。 增量整理：在增量式的基础上，增加对对象的整理过程。 分代式：将对象根据存活时间的长短进行分类，存活时间小于某个值的为年轻代，存活时间大于某个值的为老年代，永远不会参与回收的对象为永久代。并根据分代假设（如果一个对象存活时间不长则倾向于被回收，如果一个对象已经存活很长时间则倾向于存活更长时间）对对象进行回收。 引用计数：根据对象自身的引用计数来回收，当引用计数归零时立即回收。 Go 的 GC 目前使用的是无分代(对象没有代际之分)、不整理(回收过程中不对对象进行移动与整理)、并发(与用户代码并发执行)的三色标记清扫算法。原因在于： 对象整理的优势是解决内存碎片问题以及“允许”使用顺序内存分配器。但 Go 运行时的分配算法基于 tcmalloc，基本上没有碎片问题。 并且顺序内存分配器在多线程的场景下并不适用。Go 使用的是基于 tcmalloc 的现代内存分配算法，对对象进行整理不会带来实质性的性能提升。 分代 GC 依赖分代假设，即 GC 将主要的回收目标放在新创建的对象上（存活时间短，更倾向于被回收），而非频繁检查所有对象。但 Go 的编译器会通过逃逸分析将大部分新生对象存储在栈上（栈直接被回收），只有那些需要长期存在的对象才会被分配到需要进行垃圾回收的堆中。也就是说，分代 GC 回收的那些存活时间短的对象在 Go 中是直接被分配到栈上，当 goroutine 死亡后栈也会被直接回收，不需要 GC 的参与，进而分代假设并没有带来直接优势。并且 Go 的垃圾回收器与用户代码并发执行，使得 STW 的时间与对象的代际、对象的 size 没有关系。Go 团队更关注于如何更好地让 GC 与用户代码并发执行（使用适当的 CPU 来执行垃圾回收），而非减少停顿时间这一单一目标上。 三色标记法 理解三色标记法的关键是理解对象的三色抽象以及波面（wavefront）推进这两个概念。三色抽象只是一种描述追踪式回收器的方法，在实践中并没有实际含义，它的重要作用在于从逻辑上严密推导标记清理这种垃圾回收方法的正确性。也就是说，当我们谈及三色标记法时，通常指标记清扫的垃圾回收。 从垃圾回收器的视角来看，三色抽象规定了三种不同类型的对象，并用不同的颜色相称： 白色对象（可能死亡）：未被回收器访问到的对象。在回收开始阶段，所有对象均为白色，当回收结束后，白色对象均不可达。 灰色对象（波面）：已被回收器访问到的对象，但回收器需要对其中的一个或多个指针进行扫描，因为他们可能还指向白色对象。 黑色对象（确定存活）：已被回收器访问到的对象，其中所有字段都已被扫描，黑色对象中任何一个指针都不可能直接指向白色对象。 这样三种不变性所定义的回收过程其实是一个波面不断前进的过程，这个波面同时也是黑色对象和白色对象的边界，灰色对象就是这个波面。 当垃圾回收开始时，只有白色对象。随着标记过程开始进行时，灰色对象开始出现（着色），这时候波面便开始扩大。当一个对象的所有子节点均完成扫描时，会被着色为黑色。当整个堆遍历完成时，只剩下黑色和白色对象，这时的黑色对象为可达对象，即存活；而白色对象为不可达对象，即死亡。这个过程可以视为以灰色对象为波面，将黑色对象和白色对象分离，使波面不断向前推进，直到所有可达的灰色对象都变为黑色对象为止的过程 三色标记法 实际上就是通过三个阶段的标记来确定清楚的对象都有哪些. 第一步: 就是只要是新创建的对象,默认的颜色都是标记为“白色”. 这里面需要注意的是, 所谓“程序”, 则是一些对象的根节点集合。 第二步: 每次GC回收开始, 然后从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合，类似层序遍历 第三步: 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，灰色被遍历之后就放入黑色集合 第四步: 重复第三步, 直到灰色中无任何对象 第五步: 回收所有的白色标记表的对象. 也就是回收垃圾 以上就是三色并发标记法，Go是如何解决标记-清除(mark and sweep)算法中的卡顿(stw，stop the world)问题？ 没有STW的三色标记法 一定要依赖STW的。因为如果不暂停程序，程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性。例如 已经标记为灰色的对象2有指针指向白色的对象3 在还没有扫描到对象2，已经标记为黑色的对象4创建指针q，指向对象3 于此同时对象2将指针p移除，对象3就被挂载了已经扫描完成的黑色的对象4下面 正常执行逻辑，对象2和对象7 被标记为黑色，而对象3因为对象4不在被扫描，而等待被回收 对象3被无辜的清除掉了 当下列两个条件同时满足时, 就会出现对象丢失现象! 条件1: 一个白色对象被黑色对象引用**(白色被挂在黑色下)**，它的下游对象也会一并被清理掉 条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏**(灰色同时丢了该白色)** 为了防止这种现象的发生，最简单的方式就是STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是STW的过程有明显的资源浪费，对所有的用户程序都有很大影响，如何能在保证对象不丢失的情况下合理的尽可能的提高GC效率，减少STW时间呢？ 答案就是, 那么我们只要使用一个机制,来破坏上面的两个条件就可以了 屏蔽机制 让GC回收器,满足下面两种情况之一时,可保对象不丢失. 所以引出两种方式. 强-弱三色不变式 强三色不变式: 不存在黑色对象引用到白色对象的指针 弱三色不变式: 所有被黑色对象引用的白色对象都处于灰色保护状态 为了遵循上述的两个方式,Golang团队初步得到了如下具体的两种屏障方式“插入屏障”, “删除屏障”。 插入屏障 具体操作: 在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色) 满足: 强三色不变式. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色) 黑色对象的内存槽有两种位置, 栈和堆. 栈空间的特点是容量小,但是要求相应速度快,因为函数调用弹出频繁使用, 所以“插入屏障”机制,在栈空间的对象操作中不使用. 而仅仅使用在堆空间对象的操作中。 第一步：程序起初创建，全部标记为白色，将所有对象放入白色集合中 第二步：遍历Root Set(非递归形式，只遍历一次，得到灰色节点) 第三步：遍历Grey 灰色标记表。将可达的对象从白色标记为灰色遍历之后的灰色，标记为黑色 第四步：如果此刻外界向对象4添加对象8，对象1添加对象9。对象4在堆区触发插入屏蔽机制，对象1不触发 第五步：由于插入写屏障(黑色对象添加白色，将白色改为灰色)，对象8变为灰色，对象9依然时i白色 第六步：继续循环上述流程进行三色标记，直到没有灰色节点 但是如果栈不添加,当全部三色标记扫描之后,栈上有可能依然存在白色对象被引用的情况(如上图的对象9). 所以要对栈重新进行三色标记扫描, 但这次为了对象不丢失, 要对本次标记扫描启动STW暂停. 直到栈空间的三色标记结束 第七步：在准备回收白色前，重新遍历扫描一次栈空间。此时加STW暂停保护栈，防止外界干扰 第八步：在STW中，将栈中的对象一次三色标记，直到没有灰色标记 第八步：停止STW 第十步: 最后将栈和堆空间 扫描剩余的全部 白色节点清除. 这次STW大约的时间在10~100ms间 删除屏障 具体操作: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。 满足: 弱三色不变式. (保护灰色对象到白色对象的路径不会断) 第一步：程序起初创建，全部标记为白色，将所有对象放入白色集合 第二步：遍历Root Set(非递归形式，只遍历一次)，得到灰色节点 第三步：灰色对象1删除对象5，如果不触发删除写屏障，5-2-3路径与主链路断开，最后均会被清除 第四步：触发删除写屏障，被删除对象5，被标记为黑色 第五步：遍历Grey 灰色标记表，将可达的对象，从白色标记为灰色。遍历之后的灰色，标记为黑色 第六步：继续循环上述流程进行三色标记，直到没有灰色节点 第七步：这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉 第八步：清除白色 混合写屏障(hybrid write barrier)机制 插入写屏障和删除写屏障的短板： 插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活； 删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象。 Go V1.8版本引入了混合写屏障机制（hybrid write barrier），避免了对栈re-scan的过程，极大的减少了STW的时间。结合了两者的优点 具体操作: 1、GC开始将栈上的对象全部扫描并标记为黑色(之后不再进行第二次重复扫描，无需STW)， 2、GC期间，任何在栈上创建的新对象，均为黑色。 3、被删除的对象标记为灰色。 4、被添加的对象标记为灰色。 第一步：GC刚刚开始，都标记为白色 第二步：优先扫描栈对象，将可达对象全部标记为黑色 场景一 对象被一个堆对象删除引用，成为栈对象的下游 第一步：将对象7添加到对象1的下游，因为栈不启动写屏障，所以直接挂载下面 第二步：对象4删除对象7的引用关系，因为对象4是堆区，所以触发写屏障，标记被删除的对象7为灰色(删除即赋新值为nil) 参考文献 https://golang.design/go-questions/memgc/principal/ https://www.bookstack.cn/read/aceld-golang/5、Golang三色标记+混合写屏障GC模式全分析.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"微服务-12-自适应降载","slug":"Microservices/微服务-12-自适应降载","date":"2021-08-07T14:29:14.000Z","updated":"2023-08-10T04:19:22.022Z","comments":true,"path":"2021/08/07/Microservices/微服务-12-自适应降载/","link":"","permalink":"http://xboom.github.io/2021/08/07/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-12-%E8%87%AA%E9%80%82%E5%BA%94%E9%99%8D%E8%BD%BD/","excerpt":"","text":"问题背景 调用链路错综复杂，做为服务的提供者需要有一种保护自己的机制，防止调用方无脑调用压垮自己，保证自身服务的高可用。自适应降载能根据服务自身的系统负载动态判断是否需要降载，它的目标： 保证系统不被拖垮 在系统稳定的前提下，保持系统的吞吐量 问题：服务怎么知道自己需要降载？ 通过CPU负载与并发数判断往往存在较大波动，这种被称为毛刺的现象可能导致系统一致频繁的自动进行降载操作。所以如果能通过统计最近一段时间内的指标均值使均值更加平滑 实现原理 统计学上有一种算法：滑动平均（exponential moving average），用来估算变量的局部均值，使得变量的更新与历史一段时间的历史取值有关，无需记录所有的历史局部变量就可以实现平均值估算 变量 V 在 t 时刻记为 Vt，θt 为变量 V 在 t 时刻的取值，即在不使用滑动平均模型时 Vt=θt，在使用滑动平均模型后，Vt 的更新公式如下： Vt=β⋅Vt−1+(1−β)⋅θtVt=β⋅Vt−1+(1−β)⋅θt Vt=β⋅Vt−1+(1−β)⋅θt β = 0 时 Vt = θt β = 0.9 时,大致相当于过去 10 个 θt 值的平均 β = 0.99 时,大致相当于过去 100 个 θt 值的平均 而统计最近一段时间内的数据则可以使用 滑动窗口算法，接下来看看如何进行自适应降载判断 技术内幕 来看看 go-zero 的自适应降载的实现 代码：core/load/adaptiveshedder.go 使用案例 1234567891011121314151617181920212223242526272829func UnarySheddingInterceptor(shedder load.Shedder, metrics *stat.Metrics) grpc.UnaryServerInterceptor &#123; ensureSheddingStat() return func(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (val interface&#123;&#125;, err error) &#123; sheddingStat.IncrementTotal() var promise load.Promise // 检查是否被降载 promise, err = shedder.Allow() // 降载，记录相关日志与指标 if err != nil &#123; metrics.AddDrop() sheddingStat.IncrementDrop() return &#125; // 最后回调执行结果 defer func() &#123; // 执行失败 if err == context.DeadlineExceeded &#123; promise.Fail() // 执行成功 &#125; else &#123; sheddingStat.IncrementPass() promise.Pass() &#125; &#125;() // 执行业务方法 return handler(ctx, req) &#125;&#125; 仅需要调用 Allow 接口进行降载逻辑初始化 如果降载 shedder.Allow()，那么直接记录信息并返回 否则执行业务逻辑 成功则执行 promise.Pass 失败则执行 promise.Fail，业务调用错误这种算执行成功，这里使用业务超时(DeadlineExceeded)表示执行失败需要降载。 这里的promise变量就是上图中的返回结果句柄，用于将业务逻辑结果更新到降载器中 接口定义 12345678910111213141516171819202122232425262728293031323334353637383940414243444546const ( defaultBuckets = 50 //默认滑动窗口槽 defaultWindow = time.Second * 5 //默认滑动窗口大小 defaultCpuThreshold = 900 //CPU阈值 defaultMinRt = float64(time.Second / time.Millisecond) //最小速率 flyingBeta = 0.9 //平均请求 系数 coolOffDuration = time.Second //冷静时间)type ( //回到函数结果处理 Promise interface &#123; // 请求成功时回调此函数 Pass() // 请求失败时回调此函数 Fail() &#125; //降载接口 Shedder interface &#123; // 降载检查 // 1. 允许调用，需手动执行 Promise.accept()/reject()上报实际执行任务结构 // 2. 拒绝调用，将会直接返回err：服务过载错误 ErrServiceOverloaded Allow() (Promise, error) &#125; // ShedderOption lets caller customize the Shedder. ShedderOption func(opts *shedderOptions) shedderOptions struct &#123; window time.Duration buckets int cpuThreshold int64 &#125; adaptiveShedder struct &#123; cpuThreshold int64 //CPU阈值 windows int64 //滑动窗口大小 flying int64 //调度统计 avgFlying float64 //平均调度 avgFlyingLock syncx.SpinLock //调度锁 dropTime *syncx.AtomicDuration //降载时间 droppedRecently *syncx.AtomicBool //降载标识 passCounter *collection.RollingWindow //滑动窗口 通过统计 rtCounter *collection.RollingWindow //滑动窗口 速率统计 &#125;) 初始化 1234567891011121314151617181920//构建一个自适应熔断调度器func NewAdaptiveShedder(opts ...ShedderOption) Shedder &#123; if !enabled.True() &#123; return newNopShedder() &#125; //... 可选参数执行 bucketDuration := options.window / time.Duration(options.buckets) //滑动窗口槽的大小 return &amp;adaptiveShedder&#123; cpuThreshold: options.cpuThreshold, //CPU阈值 windows: int64(time.Second / bucketDuration), //1s滑动窗口个数 dropTime: syncx.NewAtomicDuration(), //熔断时间 droppedRecently: syncx.NewAtomicBool(), //最近是否熔断 passCounter: collection.NewRollingWindow(options.buckets, bucketDuration, collection.IgnoreCurrentBucket()), //滑动窗口通过统计 rtCounter: collection.NewRollingWindow(options.buckets, bucketDuration, collection.IgnoreCurrentBucket()), //滑动窗口速率统计 &#125;&#125; 逻辑判断 12345678910111213141516//降载入口func (as *adaptiveShedder) Allow() (Promise, error) &#123; if as.shouldDrop() &#123; as.dropTime.Set(timex.Now()) as.droppedRecently.Set(true) return nil, ErrServiceOverloaded &#125; as.addFlying(1) return &amp;promise&#123; start: timex.Now(), shedder: as, &#125;, nil&#125; 结果句柄操作 123456789101112131415type promise struct &#123; start time.Duration shedder *adaptiveShedder&#125;func (p *promise) Fail() &#123; p.shedder.addFlying(-1)&#125;func (p *promise) Pass() &#123; rt := float64(timex.Since(p.start)) / float64(time.Millisecond) //花费的毫秒数 p.shedder.addFlying(-1) p.shedder.rtCounter.Add(math.Ceil(rt)) p.shedder.passCounter.Add(1) &#125; 注意 其中的 p.shedder.addFlying(-1) 也就是说 flying 变量用于更新调度请求数量的 失败并不会记录到调度统计中，因为计算平均请求不需要失败 1234567891011121314func (as *adaptiveShedder) addFlying(delta int64) &#123; flying := atomic.AddInt64(&amp;as.flying, delta) //请求数量更新 // 当请求完成时更新 avgFlying。 // 这个策略使得 avgFlying 相对 flying 有一点延迟，并且更平滑。 // 当 flying 请求快速增加时，avgFlying 增加较慢，接受更多请求。 // 当 flying 请求快速下降时，avgFlying 下降较慢，接受较少的请求。 // 它使服务尽可能多地处理请求。 if delta &lt; 0 &#123; //当 &lt; 0 表示 请求完成，计算平均请求 as.avgFlyingLock.Lock() as.avgFlying = as.avgFlying*flyingBeta + float64(flying)*(1-flyingBeta) //滑动平均算法 as.avgFlyingLock.Unlock() &#125;&#125; CPU超过限制 这里的CPU也是经过定时统计得出的最近一段时间CPU负载，防止毛刺 123systemOverloadChecker = func(cpuThreshold int64) bool &#123; return stat.CpuUsage() &gt;= cpuThreshold&#125; 过载中判断 如果是正在过载中则，超过一段时间冷静期就恢复正常 过载标识/时间 是当初过载时候设置的 dropTime 与droppedRecently 123456789101112131415161718//过载中判断func (as *adaptiveShedder) stillHot() bool &#123; if !as.droppedRecently.True() &#123; //是否过热中 return false &#125; dropTime := as.dropTime.Load() //加载降载时间 if dropTime == 0 &#123; return false &#125; hot := timex.Since(dropTime) &lt; coolOffDuration //是否超过冷静期 if !hot &#123; as.droppedRecently.Set(false) //更新降载标识 &#125; return hot&#125; 过载判断 过载判断 的逻辑是 12//平均请求数大于 且 当前未完成请求的数量超过了最大请求数avgFlying &gt; maxFlight &amp;&amp; flying &gt; maxFlight 这个 最大并发数 又是怎样计算的呢？ 当前系统的最大并发数 = 窗口单位时间内的最大通过数量 * 窗口单位时间内的最小响应时间 12345678910111213141516171819202122232425262728293031323334353637383940func (as *adaptiveShedder) maxFlight() int64 &#123; // windows = buckets per second // maxQPS = maxPASS * windows // minRT = min average response time in milliseconds // maxQPS * minRT / milliseconds_per_second // as.maxPass()*as.windows - 每个桶最大的qps * 1s内包含桶的数量 // as.minRt()/1e3 - 窗口所有桶中最小的平均响应时间 / 1000ms这里是为了转换成秒 return int64(math.Max(1, float64(as.maxPass()*as.windows)*(as.minRt()/1e3)))&#125;//当前滑动窗口中的最大请求的统计func (as *adaptiveShedder) maxPass() int64 &#123; var result float64 = 1 as.passCounter.Reduce(func(b *collection.Bucket) &#123; if b.Sum &gt; result &#123; result = b.Sum &#125; &#125;) return int64(result)&#125;//当前滑动窗口中最小速率的统计func (as *adaptiveShedder) minRt() float64 &#123; result := defaultMinRt as.rtCounter.Reduce(func(b *collection.Bucket) &#123; if b.Count &lt;= 0 &#123; return &#125; avg := math.Round(b.Sum / float64(b.Count)) if avg &lt; result &#123; result = avg &#125; &#125;) return result&#125; 总结 自适应降载逻辑处理 当请求突然增大的时候，虽然没有达到服务能够承受的极限，也有可能出现降载。因为平均请求数量以及最大请求数量 都超过了最近一段时间能承载的最大水平 按照第一条逻辑，如果服务刚启动那会请求确实比较多，是不是就会出现降载了。不会，这里在计算 最大并发数的时候，给定了一个最小最大并发数 1 * defaultMinRt / milliseconds_per_second 。也就是并发数低于 1000的时候也不会触发降载 参考文档 https://talkgo.org/t/topic/3058 https://www.cnblogs.com/wuliytTaotao/p/9479958.html","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-11-自适应熔断","slug":"Microservices/微服务-11-自适应熔断","date":"2021-08-06T14:29:14.000Z","updated":"2023-08-10T04:19:16.084Z","comments":true,"path":"2021/08/06/Microservices/微服务-11-自适应熔断/","link":"","permalink":"http://xboom.github.io/2021/08/06/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-11-%E8%87%AA%E9%80%82%E5%BA%94%E7%86%94%E6%96%AD/","excerpt":"","text":"问题背景 使用负载均衡策略是一种避免超负载的处理方式，但服务的容量是有限的。部分服务还是会出现超载的情况，如果优雅的处理过载则对可靠的服务至关重要。 在高并发场景下，为了应对依赖服务过载，服务不可用等情况，提出了熔断、限流与降级方案。这里主要描述熔断的原理，这里存在几个问题： 都有哪些熔断的解决方案 熔断器的实现原理是什么 使用较多的熔断组件： hystrix circuit breaker(不再维护) hystrix-go resilience4j(推荐) sentinel(推荐) 熔断器原理 熔断器一般具有三个状态： 关闭: 默认状态，请求能被到达目标服务，同时统计在窗口时间成功和失败次数，如果达到错误率阈值将会进入断开状态。 断开: 此状态下将会直接返回错误，如果有 fallback 配置则直接调用 fallback 方法。 半断开: 进行断开状态会维护一个超时时间，到达超时时间开始进入 半断开 状态，尝试允许一部分请求正常通过并统计成功数量，如果请求正常则认为此时目标服务已恢复进入 关闭 状态，否则进入 断开 状态 基于熔断器的原理，通常熔断器主要关注以下参数： 错误比例阈值: 达到该阈值进入 断开 状态 断开状态超时时间: 超时后进入 半断开 状态 半断开状态允许请求数量 窗口时间大小 这里有更将详细可参考的参考以及算法说明 https://resilience4j.readme.io/docs/circuitbreaker https://sre.google/sre-book/handling-overload/ 由于go-zero 的熔断器是基于google文章实现，来看下基本算法 技术内幕 无论什么熔断器都得依靠指标统计来转换状态，而统计指标一般要求是最近的一段时间内的数据，所以通常采用一个 滑动时间窗口 数据结构 来存储统计数据。同时熔断器的状态也需要依靠指标统计来实现可观测性。 外部服务请求结果各式各样，所以需要提供一个自定义的判断方法，判断请求是否成功。熔断器需要实时收集此数据。 当外部服务被熔断时使用者往往需要自定义快速失败的逻辑，考虑提供自定义的 fallback() 功能。 接口定义 代码路径：core/breaker/breaker.go 12345678910111213141516171819202122232425262728293031323334353637383940// 250ms for bucket durationwindow = time.Second * 10buckets = 40k = 1.5protection = 5//判断断路器是否通过Acceptable func(err error) bool//Breaker 断路器 Breaker interface &#123; // 熔断器名称 Name() string //检查请求是否允许。调用成功则使用 Promise.Accept()，失败则调用 Promise.Reject()，否则表示不许云 Allow() (Promise, error) // 如果 Breaker 接受，Do 运行给定的请求。 // 如果 Breaker 拒绝请求，Do 立即返回错误。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 Do(req func() error) error // 如果 Breaker 接受，DoWithAcceptable 运行给定的请求。 // 如果 Breaker 拒绝请求，DoWithAcceptable 会立即返回错误。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 // 可接受的检查它是否是一个成功的调用，即使错误不是零。 DoWithAcceptable(req func() error, acceptable Acceptable) error // 如果 Breaker 接受，DoWithFallback 运行给定的请求。 // 如果 Breaker 拒绝请求，DoWithFallback 运行回退。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 DoWithFallback(req func() error, fallback func(err error) error) error // 如果 Breaker 接受，DoWithFallbackAcceptable 运行给定的请求。 // DoWithFallbackAcceptable 如果 Breaker 拒绝请求，则运行回退。 // 如果请求发生恐慌，Breaker 将其作为错误处理并再次引起同样的恐慌。 // 可接受的检查它是否是一个成功的调用，即使错误不是零。 DoWithFallbackAcceptable(req func() error, fallback func(err error) error, acceptable Acceptable) error &#125; 断路器 12345678910111213141516171819202122232425262728//断路器circuitBreaker struct &#123; name string throttle&#125;//断路器内部则通过两个接口实现throttle interface &#123; allow() (Promise, error) doReq(req func() error, fallback func(err error) error, acceptable Acceptable) error&#125;//自定义断路器对象type googleBreaker struct &#123; k float64 stat *collection.RollingWindow //使用滑动窗口统计最近一段时间数据 proba *mathx.Proba&#125;func newGoogleBreaker() *googleBreaker &#123; bucketDuration := time.Duration(int64(window) / int64(buckets)) st := collection.NewRollingWindow(buckets, bucketDuration) return &amp;googleBreaker&#123; stat: st, k: k, proba: mathx.NewProba(), &#125;&#125; 数据记录 其实是将成功或者失败记录到滑动窗口中 1234567func (b *googleBreaker) markSuccess() &#123; b.stat.Add(1)&#125;func (b *googleBreaker) markFailure() &#123; b.stat.Add(0)&#125; 请求操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//统计滑动窗口中所代表的最近一段时间的数据func (b *googleBreaker) history() (accepts, total int64) &#123; b.stat.Reduce(func(b *collection.Bucket) &#123; accepts += int64(b.Sum) total += b.Count &#125;) return&#125;//检查是否允许func (b *googleBreaker) accept() error &#123; accepts, total := b.history() weightedAccepts := b.k * float64(accepts) // https://landing.google.com/sre/sre-book/chapters/handling-overload/#eq2101 dropRatio := math.Max(0, (float64(total-protection)-weightedAccepts)/float64(total+1)) if dropRatio &lt;= 0 &#123; return nil &#125; if b.proba.TrueOnProba(dropRatio) &#123; return ErrServiceUnavailable &#125; return nil&#125;//返回断路器权限func (b *googleBreaker) allow() (internalPromise, error) &#123; if err := b.accept(); err != nil &#123; return nil, err &#125; return googlePromise&#123; b: b, &#125;, nil&#125;func (b *googleBreaker) doReq(req func() error, fallback func(err error) error, acceptable Acceptable) error &#123; if err := b.accept(); err != nil &#123; if fallback != nil &#123; return fallback(err) &#125; return err &#125; defer func() &#123; if e := recover(); e != nil &#123; b.markFailure() panic(e) &#125; &#125;() err := req() if acceptable(err) &#123; //结果的处理 b.markSuccess() &#125; else &#123; b.markFailure() &#125; return err&#125; 总结 通过滑动窗口进行最近一段数据(成功失败次数)的统计 是否断路则是通过指定的公式计算 失败率=总数−可接收误差−k∗成功/失败失败率 = 总数 - 可接收误差 - k * 成功 / 失败 失败率=总数−可接收误差−k∗成功/失败 参考文档 https://talkgo.org/t/topic/3035","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-10-数据库缓存","slug":"Microservices/微服务-10-数据库缓存","date":"2021-08-05T14:29:14.000Z","updated":"2023-08-10T04:18:25.002Z","comments":true,"path":"2021/08/05/Microservices/微服务-10-数据库缓存/","link":"","permalink":"http://xboom.github.io/2021/08/05/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-10-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%93%E5%AD%98/","excerpt":"","text":"问题背景 在 进程内缓存中说到的是直接在进程进行缓存的自管理，一般应用于业务生成的自定义数据(多副本情况下可能存在缓存一致性问题)。而随着业务量的增加，利用Redis构建专门的数据缓存，加快数据库访问。 由于数据库数据的特点，那么这里就有几个实现需要注意的点 数据一致性 缓存击穿、穿透、雪崩 缓存访问量、缓存命中率 实现原理 源码注释：core/stores/cache 构建缓存对象 根据缓存节点配置构建一个缓存对象用于业务层进行缓存处理 缓存支持通过多节点构建的缓存集群，也支持单个节点结构 多节点缓存集群通过一致性Hash进行访问，这里的多节点是指多个无关的缓存节点。而每个缓存节点可能都是一个Redis集群 共享调用防止缓存击穿 统计将环中命中，DB查询等情况上报给外部 12345//缓存集群对象cacheCluster struct &#123; dispatcher *hash.ConsistentHash errNotFound error&#125; 关于一致性Hash 在 《go-zero-一致性Hash》有说明 集群缓存操作 集群的缓存操作都是根据一致性Hash算法得出对应节点，然后演变成单节点的缓存操作 与单点不同的是，缓存集群批量删除Key,而Key有可能存在于多个节点上 当keys数量大于1的时候，首先会 for Hash(Key) 找到所有的缓存节点，然后再通过 for del 单次映射Hash节点或者删除Key并不会影响后续操作，而是通过 BatchError记录每一次错误 通过 make(map[interface{}][]string)保存节点与Key的关系 单点缓存操作 查询缓存 查询缓存的过程其实就是从Redis中获取数据的过程 这里有两点需要注意 当加载数据完毕，有一个将结果与空占位符比较的过程 value = &quot;*&quot;，是为了防止缓存穿透而故意设置的占位符。那么它是什么时候怎么插入的？ 将结果反序列如果失败，那么会去Redis删除这个 Key，表示存储的缓存异常 设置缓存 设置缓存是直接按照redis语法设置 k/v 与 expire 删除缓存 删除缓存时候，如果本身就是一个缓存集群，当对 keys进行批量删除的时候，需要依次删除每一个Key，而不是直接 del(keys...) 删除。 删除失败这里添加延时任务进行重试，但只会重试一次，失败后直接退出 获取缓存 获取缓存当缓存数据库没有的时候，就会直接从数据库加载并将数据保存到缓存数据库 当缓存中没有且数据库中也没有的时候，那么这个时候就会设置占位符(防止缓存穿透)，占位符的过期时间与普通的Key一致 关于查询数据库操作，这里仅仅是将结果实体传入，由数据层进行数据加载 当缓存没有的时候，是先查询数据库，然后更新缓存 技术内幕 代码：core/stores/cache/cachenode.go cacheNode 表示是单个缓存节点的对象 12345678910111213141516171819//cacheNode 表示单个缓存节点type cacheNode struct &#123; rds *redis.Redis //redis句柄 expiry time.Duration //缓存过期时间 notFoundExpiry time.Duration // barrier syncx.SingleFlight //共享调用 r *rand.Rand lock *sync.Mutex //原子锁 unstableExpiry mathx.Unstable stat *Stat //统计(单个节点的统计) errNotFound error&#125;const ( notFoundPlaceholder = \"*\" //避免缓存雪崩，这里加上随机过期时间随机值 [0.95, 1.05] * seconds expiryDeviation = 0.05) 这里几个过期分别有什么作用 expiry: notFoundExpiry: unstableExpiry: 使用共享调用 barrier减少缓存调用 rand.Rand 的随机数 新建缓存节点 123456789101112131415func NewNode(rds *redis.Redis, barrier syncx.SingleFlight, st *Stat, errNotFound error, opts ...Option) Cache &#123; o := newOptions(opts...) return cacheNode&#123; rds: rds, //redis句柄 expiry: o.Expiry, //过期时间 notFoundExpiry: o.NotFoundExpiry, //设置占位符过期时间 barrier: barrier, //共享调用 r: rand.New(rand.NewSource(time.Now().UnixNano())), lock: new(sync.Mutex), unstableExpiry: mathx.NewUnstable(expiryDeviation), //一定范围内过期时间 stat: st, //统计逻辑 errNotFound: errNotFound, //找不到缓存 &#125;&#125; 查询缓存 12345678910111213141516171819202122232425262728293031323334353637func (c cacheNode) doGetCache(ctx context.Context, key string, v interface&#123;&#125;) error &#123; c.stat.IncrementTotal() data, err := c.rds.GetCtx(ctx, key) if err != nil &#123; c.stat.IncrementMiss() return err &#125; if len(data) == 0 &#123; //数据为空 c.stat.IncrementMiss() return c.errNotFound &#125; c.stat.IncrementHit() if data == notFoundPlaceholder &#123; //防止缓存穿透 return errPlaceholder &#125; return c.processCache(ctx, key, data, v) //缓存处理&#125;func (c cacheNode) processCache(ctx context.Context, key, data string, v interface&#123;&#125;) error &#123; err := jsonx.Unmarshal([]byte(data), v) //反序列化 if err == nil &#123; return nil &#125; //... 日志记录 if _, e := c.rds.DelCtx(ctx, key); e != nil &#123; //删除缓存 logger.Errorf(\"delete invalid cache, node: %s, key: %s, value: %s, error: %v\", c.rds.Addr, key, data, e) &#125; // returns errNotFound to reload the value by the given queryFn return c.errNotFound&#125; 设置缓存 123456789func (c cacheNode) SetWithExpireCtx(ctx context.Context, key string, val interface&#123;&#125;, expire time.Duration) error &#123; data, err := jsonx.Marshal(val) //序列化 if err != nil &#123; return err &#125; return c.rds.SetexCtx(ctx, key, string(data), int(expire.Seconds()))&#125; 获取缓存 获取缓存直接加载加载redis，并根据加载结果进行不同的处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (c cacheNode) doTake(ctx context.Context, v interface&#123;&#125;, key string, query func(v interface&#123;&#125;) error, cacheVal func(v interface&#123;&#125;) error) error &#123; logger := logx.WithContext(ctx) val, fresh, err := c.barrier.DoEx(key, func() (interface&#123;&#125;, error) &#123; //共享调用加载key if err := c.doGetCache(ctx, key, v); err != nil &#123; if err == errPlaceholder &#123; //如果是占位符直接返回找不到 return nil, c.errNotFound &#125; else if err != c.errNotFound &#123; //如果是其他错误，直接返回而不要继续将错误蔓延到dbs //如果不这样，可能高并发导致redis奔溃之后，dbs也会跟着崩溃 return nil, err &#125; //数据库查询 if err = query(v); err == c.errNotFound &#123; //没有找到就设置占位符 if err = c.setCacheWithNotFound(ctx, key); err != nil &#123; logger.Error(err) &#125; return nil, c.errNotFound &#125; else if err != nil &#123; c.stat.IncrementDbFails() return nil, err &#125; if err = cacheVal(v); err != nil &#123; //缓存找到的缓存 logger.Error(err) &#125; &#125; return jsonx.Marshal(v) &#125;) if err != nil &#123; return err &#125; if fresh &#123; return nil &#125; //放在最后的原因是共享调用，这里共享调用不记录到总数中 c.stat.IncrementTotal() c.stat.IncrementHit() return jsonx.Unmarshal(val.([]byte), v) //反序列化&#125; 删除缓存 1234567891011121314151617181920func (c cacheNode) DelCtx(ctx context.Context, keys ...string) error &#123; if len(keys) == 0 &#123; return nil &#125; logger := logx.WithContext(ctx) if len(keys) &gt; 1 &amp;&amp; c.rds.Type == redis.ClusterType &#123; //如果是集群则for循环批量删除 for _, key := range keys &#123; if _, err := c.rds.DelCtx(ctx, key); err != nil &#123; logger.Errorf(\"failed to clear cache with key: %q, error: %v\", key, err) c.asyncRetryDelCache(key) //失败添加重试任务 &#125; &#125; &#125; else if _, err := c.rds.DelCtx(ctx, keys...); err != nil &#123; logger.Errorf(\"failed to clear cache with keys: %q, error: %v\", formatKeys(keys), err) c.asyncRetryDelCache(keys...) &#125; return nil&#125; 缓存统计 代码：core/stores/cache/cachestat.go 用于统计缓存情况 新建统计对象 12345678910111213141516171819202122232425262728293031// NewStat returns a Stat.func NewStat(name string) *Stat &#123; ret := &amp;Stat&#123; name: name, &#125; go ret.statLoop() return ret&#125;//statLoop 开启统计func (s *Stat) statLoop() &#123; ticker := time.NewTicker(statInterval) defer ticker.Stop() for range ticker.C &#123; total := atomic.SwapUint64(&amp;s.Total, 0) if total == 0 &#123; continue &#125; //计算缓存情况 hit := atomic.SwapUint64(&amp;s.Hit, 0) percent := 100 * float32(hit) / float32(total) //命中率 miss := atomic.SwapUint64(&amp;s.Miss, 0) //未命中 dbf := atomic.SwapUint64(&amp;s.DbFails, 0) //数据库调用事变 //发送统计信息 logx.Statf(\"dbcache(%s) - qpm: %d, hit_ratio: %.1f%%, hit: %d, miss: %d, db_fails: %d\", s.name, total, percent, hit, miss, dbf) &#125;&#125; 注意： 协程是一个常驻协程，缺少退出 SwapUint64的作用是：将新的值写入 addr，而返回addr中旧的值 缓存清理 当缓存删除失败，这里添加一个重试机制 初始化 1234567891011121314151617181920212223242526272829303132func init() &#123; var err error //时间轮 timingWheel, err = collection.NewTimingWheel(time.Second, timingWheelSlots, clean) logx.Must(err) //监听系统异常退出 proc.AddShutdownListener(func() &#123; timingWheel.Drain(clean) &#125;)&#125;//clean 时间轮操作func clean(key, value interface&#123;&#125;) &#123; taskRunner.Schedule(func() &#123; dt := value.(delayTask) err := dt.task() //执行任务 if err == nil &#123; return &#125; next, ok := nextDelay(dt.delay) //重复任务设置 if ok &#123; dt.delay = next timingWheel.SetTimer(key, dt, next) &#125; else &#123; msg := fmt.Sprintf(\"retried but failed to clear cache with keys: %q, error: %v\", formatKeys(dt.keys), err) logx.Error(msg) stat.Report(msg) //暂未实现 &#125; &#125;)&#125; 这里做了一个动态清理 1234567891011121314func nextDelay(delay time.Duration) (time.Duration, bool) &#123; switch delay &#123; case time.Second: return time.Second * 5, true case time.Second * 5: return time.Minute, true case time.Minute: return time.Minute * 5, true case time.Minute * 5: return time.Hour, true default: return 0, false &#125;&#125; 添加清理任务 12345678// AddCleanTask adds a clean task on given keys.func AddCleanTask(task func() error, keys ...string) &#123; timingWheel.SetTimer(stringx.Randn(taskKeyLen), delayTask&#123; delay: time.Second, task: task, keys: keys, &#125;, time.Second)&#125; 总结 采用共享调用的方式防止缓存击穿 采用占位符方式缓存床头 设置范围过期时间防止缓存雪崩 增加重试删除机制(时间轮) 参考链接 https://talkgo.org/t/topic/1716 https://talkgo.org/t/topic/1505","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-09-进程内缓存","slug":"Microservices/微服务-09-进程内缓存","date":"2021-08-04T14:29:14.000Z","updated":"2023-08-10T04:17:57.787Z","comments":true,"path":"2021/08/04/Microservices/微服务-09-进程内缓存/","link":"","permalink":"http://xboom.github.io/2021/08/04/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-09-%E8%BF%9B%E7%A8%8B%E5%86%85%E7%BC%93%E5%AD%98/","excerpt":"","text":"问题背景 缓存，目的是减少繁重的IO操作，增加系统并发能力。之前做的内存方案 同一类数据，设计一个双链表，然后添加一个定时器，动态删除过期内存。 实现一个简单的LRU缓存淘汰算法。 设计内进程内缓存主要需要考虑的是 可拓展性(缓存不同类型的数据) 过期处理，怎样才能友好的删除过期数据，不能无限存储 定时删除，不断循环所有key。缺点：需要遍历所有key，空耗CPU 惰性删除，访问的时候判断该键是否。缺点：如果不在访问，那么就会一直存在 如果保障进程内缓存一致性 单节点通知：一个节点完成修改，通知其他节点(类似raft，但raft有主)。缺点：时效性、可靠性 MQ订阅通知：一个节点完成修改，通知其他节点进行修改。缺点：缓存维护更复杂 定时更新：单个节点定时器，定时拉去新数据，更新内存数据。缺点：时效性 实现原理 这里看看go-zero是如何实现进程缓存设计的 从图中可以看出 缓存被存放在map LRU(使用双向链表+Map实现)用于对缓存的过期删除 limit 进行缓存数量限制 定时逻辑则是通过时间轮进行管理 还包含一个统计逻辑，进行缓存命中情况的上报 接下来看看它们是如何运转起来的 更新缓存 设置缓存要做三件事情 更新缓存 将Key到LRU管理中 添加或更新定时器 问题1：图中还有一个获取过期时间的过程，设置缓存的时候不是应该已经设置了过期时间了吗，为什么这个地方还要再次获取？ 其实这里获取的与过期时间近似的时间，目的是防止缓存雪崩 deviation:表示一个正负差值 base:给定的过期时间 v=(1+deviation−2∗deviation∗random)∗basev = (1 + deviation - 2 * deviation * random) * base v=(1+deviation−2∗deviation∗random)∗base 问题2：原理图中还有一个设置limit的过程，为什么在添加的过程中没有体现？ 其实这是LRU内部实现的时候使用的，当LRU双向链表长度大于limit的时候，会直接删除尾部节点 查询缓存 当缓存存在的时候，需要更新LRU。这里还添加了一个命中的逻辑 获取缓存 获取缓存与查询缓存不同的是，如果缓存中不存在，那么需要从DB获取数据并插入到缓存中 注意： 使用共享调用防止缓存穿透 查询DB操作其实是自定义操作(所以缓存不关心数据来源) 当缓存不存在，启动共享调用逻辑的时候它再次查询了一次缓存。因为DB操作是IO操作，而查询缓存是 O(1)的map内存操作 删除缓存 删除分为定时删除以及主动删除，当未更新定时器的时候，那么到期就会主动删除 K/V 如果是主动删除，则直接删除缓存，LRU 和 定时器 技术内幕 代码路径：core/collection/cache.go 123456789101112131415// 自定义缓存可选参数CacheOption func(cache *Cache)// 进程内缓存对象Cache struct &#123; name string //缓存名称，默认为 defaultCacheName=\"proc\" lock sync.Mutex //map锁 data map[string]interface&#123;&#125; //存储缓存 expire time.Duration //过期时间 timingWheel *TimingWheel //时间轮 lruCache lru //LRU 缓存淘汰机制 barrier syncx.SingleFlight //共享调用 unstableExpiry mathx.Unstable //随机过期时间差值(防止雪崩) stats *cacheStat //缓存统计&#125; 初始化缓存 1234567891011121314151617181920212223242526272829303132333435func NewCache(expire time.Duration, opts ...CacheOption) (*Cache, error) &#123; cache := &amp;Cache&#123; data: make(map[string]interface&#123;&#125;), expire: expire, lruCache: emptyLruCache, barrier: syncx.NewSingleFlight(), //共享调用 unstableExpiry: mathx.NewUnstable(expiryDeviation), &#125; //自定义操作包含 limit设置、缓存过期时间范围 expiryDeviation for _, opt := range opts &#123; opt(cache) &#125; if len(cache.name) == 0 &#123; cache.name = defaultCacheName //默认名称可以忽略 &#125; cache.stats = newCacheStat(cache.name, cache.size) //时间轮已经订好了1s时间间隔 timingWheel, err := NewTimingWheel(time.Second, slots, func(k, v interface&#123;&#125;) &#123; key, ok := k.(string) if !ok &#123; return &#125; cache.Del(key) //到期删除缓存 &#125;) if err != nil &#123; return nil, err &#125; cache.timingWheel = timingWheel return cache, nil&#125; 查询缓存 查询缓存很简单 1234567891011121314151617181920212223// Get returns the item with the given key from c.func (c *Cache) Get(key string) (interface&#123;&#125;, bool) &#123; value, ok := c.doGet(key) if ok &#123; c.stats.IncrementHit() &#125; else &#123; c.stats.IncrementMiss() &#125; return value, ok&#125;func (c *Cache) doGet(key string) (interface&#123;&#125;, bool) &#123; c.lock.Lock() defer c.lock.Unlock() value, ok := c.data[key] if ok &#123; c.lruCache.add(key) &#125; return value, ok&#125; 设置缓存 1234567891011121314151617181920//Set 设置缓存，默认使用缓存过期时间func (c *Cache) Set(key string, value interface&#123;&#125;) &#123; c.SetWithExpire(key, value, c.expire) &#125;// SetWithExpire 设置缓存 自定义过期时间func (c *Cache) SetWithExpire(key string, value interface&#123;&#125;, expire time.Duration) &#123; c.lock.Lock() _, ok := c.data[key] c.data[key] = value c.lruCache.add(key) c.lock.Unlock() expiry := c.unstableExpiry.AroundDuration(expire) //获取过期时间 if ok &#123; c.timingWheel.MoveTimer(key, expiry) &#125; else &#123; c.timingWheel.SetTimer(key, value, expiry) &#125;&#125; 获取缓存 123456789101112131415161718192021222324252627282930313233343536func (c *Cache) Take(key string, fetch func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; if val, ok := c.doGet(key); ok &#123; //查询缓存 c.stats.IncrementHit() return val, nil &#125; var fresh bool val, err := c.barrier.Do(key, func() (interface&#123;&#125;, error) &#123; // because O(1) on map search in memory, and fetch is an IO query // so we do double check, cache might be taken by another call // 再进行一次缓存操作 if val, ok := c.doGet(key); ok &#123; return val, nil &#125; v, e := fetch() if e != nil &#123; return nil, e &#125; fresh = true c.Set(key, v) return v, nil &#125;) if err != nil &#123; return nil, err &#125; if fresh &#123; c.stats.IncrementMiss() return val, nil &#125; // got the result from previous ongoing query c.stats.IncrementHit() return val, nil&#125; 删除缓存 1234567func (c *Cache) Del(key string) &#123; c.lock.Lock() delete(c.data, key) //删除k/v c.lruCache.remove(key) //删除LRU c.lock.Unlock() c.timingWheel.RemoveTimer(key) //删除定时器&#125; 总结 设置缓存过期时间的时候，设置一个范围时间防止缓存雪崩 使用共享调用去请求数据防止缓存击穿 通过LRU进行缓存淘汰，当多副本存在可能存在缓存一致性问题 统计逻辑用于上报，并没有一个开关(可能存在性能问题) 参考链接 https://talkgo.org/t/topic/2263","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-08-服务管理","slug":"Microservices/微服务-08-服务管理","date":"2021-08-03T14:29:14.000Z","updated":"2023-08-10T04:17:26.424Z","comments":true,"path":"2021/08/03/Microservices/微服务-08-服务管理/","link":"","permalink":"http://xboom.github.io/2021/08/03/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-08-%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/","excerpt":"","text":"问题背景 之前云平台是基于MQ实现的通信机制，后来需要将通信方式切换为Grpc但又不想修改老的接口和自定义逻辑。所以就需要修改原本的库。因为内部包含了配置加载、MQ适配、Grpc功能、服务注册发现、链路追踪、自定义功能(限速、大包处理、调试、过滤、统计、存储…)。这些子服务是相互独立且可选(不启用也可以)，部分存在依赖关系。所以就想着使用服务管理的方式管理这些小功能，并按照期望的方式进行启动运行 实现方案 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package mainimport \"context\"type ServerNum uint8const ( DemoA ServerNum = iota DemoB MaxServerNum)var serverList = []Server&#123; DemoA: &amp;Server1&#123;&#125;, DemoB: &amp;Server2&#123;&#125;,&#125;type Server interface &#123; Start(ctx context.Context) (Server, error) //服务启动 ID() int //返回服务标志，用于循序启动 Close() error //关闭服务 IsOpen() bool //是否开启&#125;//开启服务func Start(ctx context.Context) &#123; for _, v := range serverList &#123; v.Start(ctx) &#125;&#125;//服务管理type Manager struct &#123;&#125;//服务启动入口func (m *Manager)Start(ctx context.Context, s Server) error &#123; if s == nil || !s.IsOpen() &#123; //未设置对象或未开启则直接退出 return nil &#125; v, err := s.Start(ctx) if err != nil &#123; panic(err) &#125; key := v.ID() //构建k/v进行服务句柄存储 //TODO 这里使用 封装的context return nil&#125;func (m *Manager)Stop(ctx context.Context) &#123; for i := MaxServerNum - 1; i &gt;=0; i++ &#123; v := serverList[i] if v != nil &#123; v.Close() &#125; &#125;&#125; 单个服务内的子模块是否相互依赖的，所以这里使用切片进行存储，关闭时按照启动顺序的反方向 构建 IsOpen 方便根据配置加载服务需要的模块 服务句柄的存储暂时保存，需要使用服务句柄也许直接使用大写暴露会更好，或者使用不需要句柄的外部函数 注意服务的常驻等待(例如HTTP服务就是Listen之后不做后续处理，这个时候就需要使用协程拉起)，而所有服务加载完成之后，需要有一个常驻等待逻辑 12345678func Loop() &#123; exit := make(chan os.Signal, 1) signal.Notify(exit, syscall.SIGINT, syscall.SIGTERM) select &#123; case sig := &lt;- exit: log.Infof(context.Background(), \"recv signal %s\", sig.String()) &#125;&#125; 技术内幕 代码： core/service/servicegroup.go 1234567891011import \"github.com/zeromicro/go-zero/core/service\"// more codefunc main() &#123; group := service.NewServiceGroup() //初始化管理对象 defer group.Stop() //停止 group.Add(Morning&#123;&#125;) //添加服务morning group.Add(Evening&#123;&#125;) //添加服务evening group.Start() //服务开启&#125; 数据结构 123456789101112131415161718192021type ( Starter interface &#123; //开启接口 Start() &#125; Stopper interface &#123; //停止接口 Stop() &#125; //服务接口，开启与停止 Service interface &#123; Starter Stopper &#125; //服务批量管理接口 ServiceGroup struct &#123; services []Service //切片保存服务 stopOnce func() //停止一次接口 &#125;) 初始化对象 123456// NewServiceGroup returns a ServiceGroup.func NewServiceGroup() *ServiceGroup &#123; sg := new(ServiceGroup) sg.stopOnce = syncx.Once(sg.doStop) //停止逻辑使用sync.Once 处理 return sg&#125; 增加对象 12345//Add 服务管理中添加自定义服务func (sg *ServiceGroup) Add(service Service) &#123; // push front, stop with reverse order. sg.services = append([]Service&#123;service&#125;, sg.services...)&#125; 开启服务 1234567891011121314151617181920212223//Start 服务管理启动func (sg *ServiceGroup) Start() &#123; proc.AddShutdownListener(func() &#123; //将服务添加到退出监听中 log.Println(\"Shutting down...\") sg.stopOnce() &#125;) sg.doStart()&#125;//doStart 开始func (sg *ServiceGroup) doStart() &#123; routineGroup := threading.NewRoutineGroup() for i := range sg.services &#123; service := sg.services[i] routineGroup.RunSafe(func() &#123; //使用每个服务使用协程开启(包含panic处理) service.Start() &#125;) &#125; routineGroup.Wait() //等待退出&#125; 使用协程池进行每个服务的启动，所以每个服务启动的顺序是不一定的 使用RunSafe进行服务启动，所以一个服务panic，另外的服务也能启动 服务关闭 1234567891011// Stop stops the ServiceGroup.func (sg *ServiceGroup) Stop() &#123;NewCache sg.stopOnce()&#125;//停止服务管理(服务初始化的时候已经设置好了)func (sg *ServiceGroup) doStop() &#123; for _, service := range sg.services &#123; service.Stop() &#125;&#125; 仅有开启服务 12345678910111213141516171819202122232425262728293031323334// WithStart wraps a start func as a Service.func WithStart(start func()) Service &#123; return startOnlyService&#123; start: start, &#125;&#125;// WithStarter wraps a Starter as a Service.func WithStarter(start Starter) Service &#123; return starterOnlyService&#123; Starter: start, &#125;&#125;type ( stopper struct&#123;&#125; startOnlyService struct &#123; start func() stopper &#125; starterOnlyService struct &#123; Starter stopper &#125;)func (s stopper) Stop() &#123;&#125;func (s startOnlyService) Start() &#123; s.start()&#125; 总结 由于子服务无法保证启动顺序，所以各服务之间不能有相互依赖或关联 使用 stopOnce 防止出现多次调用stop的情况 使用 proc.AddShutdownListener做退出监听 优化： 如果能控制服务启动和调用顺序是否会更好(一个服务内的多个子模块必然存在先后启动关系)，那么停止服务的时候也注意先后关系 如果使用协程池拉起服务，那么单个服务的异常无法让所有子服务都退出。缺少一个同步退出机制 使用 startOnlyService是否有点多余，在服务停止中不做任何处理，那么退出时也能正常退出 参考文档 https://mp.weixin.qq.com/s/G6WG_-C6d-raoRmH4hBjoQ","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-07-时间轮","slug":"Microservices/微服务-07-时间轮","date":"2021-08-02T14:29:14.000Z","updated":"2023-08-10T04:16:56.255Z","comments":true,"path":"2021/08/02/Microservices/微服务-07-时间轮/","link":"","permalink":"http://xboom.github.io/2021/08/02/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-07-%E6%97%B6%E9%97%B4%E8%BD%AE/","excerpt":"","text":"问题背景 一个系统中存在着大量的延迟/定时任务： 在一个间隔时间之后做某事: 例如在最后一次消息发送的5分钟之后, 断开连接 在一个间隔时间之后不停的做某事: 例如每隔5分钟之后去发送心跳，检测连接是否正常 如果每个任务都使用自己的调度器来管理任务声明周期的话，浪费CPU的资源而且很低效，比如： 在定时器的数量增长到百万级之后, 基于最小堆实现的定时器的性能会显著降低 客户端会定时发送心跳以此来确保连接的可用性。导致每个连接都需要新建一些协程去维护 解决方案 延迟操作，通常可以采用两个方案： Timer：定时器维护一个优先队列，到时间点执行，然后把需要执行的 task 存储在 map 中 collection 中的 timingWheel ，维护一个存放任务组的数组，每一个槽都维护一个存储 task 的双向链表。开始执行时，计时器每隔指定时间执行一个槽里面的 tasks 时间轮是一种高效来利用线程资源来进行批量化调度的一种调度模型。把大批量的调度任务全部都绑定到同一个的调度器上面，使用这一个调度器来进行所有任务的管理(manager)，触发(trigger)以及运行(runnable) 实现原理 使用一个 类Map 的结构进行时间轮而构建，其中： 定时器的间隔就是从第n个槽进入到第n+1个槽的时间 每个槽中的任务通过双向链表进行存储，定时器到达槽的位置之后，并发处理槽的任务(指定时间相同) 问题1：新建的任务如何添加到对应的位置 如上图，槽位(numSlots)为12的时间轮，时间轮的定时间隔(interval)为1s，当前正在执行槽(tickedPos)为1位置的任务。 当添加一个延时时间(delay)为5s的任务的时候，那么： 需要等待的间隔数step：step = delay / interval 为 5。 需要放入的槽的位置position:position = (step + tickedPos) % numSlots 为 6。 问题2：新建任务超过槽的数目怎么办？ 如上图，当添加一个延时时间(delay)为18s的时候，根据上面的计算公司 需要等待的间隔数step = delay / interval = 18 需要放入的槽的位置 postion = (step + tickedPos) % numSlots = 6。 这个时候就出现一个问题，延迟5s的任务和延迟18s的任务会一起在5s后执行，这个时候就出现了多层环的概念 当第一层环的时间无法满足任务的延时的时候，可以将任务放置到第n层环上。时间轮还是会按照环的顺序进行执行 环的位置circle: circle = (steps - 1) / numSlots 所以延时5s的任务它的环 circle = (5 - 1) / 12 = 0，延时18s的任务它的环 circle = (18 -1)/12 = 1 随着时间的推移第一层环完成之后再去执行第二层环 问题3：不同环的任务是否都需要构建一个新的环来保存任务 不需要，不同环但是槽相同的任务放入到同一个双向链表中，当执行该槽任务的时候，只需要判断当前的任务if circle == 0，否则就不是当前环的任务，将circle -= 1 这样也有一个缺点：就是在遍历槽中任务的时候，虽然不是相同时间执行，但是槽中所有任务都需要遍历。不过一个微服务中定时任务也不会太多，所以缺点基本可以忽略 问题4：定时任务时间轮都是是如何处理的 根据任务类型可以分为延时任务与定时任务。根据任务执行一次可以分为执行一次和重复执行。这里有两点需要解决 不同类型任务的开始时间是怎么计算开始时间的 需要重复的任务是如何存储的 延时任务 执行一次：根据延时时间计算在时间轮中的位置，定时触发即可 重复执行：每次执行任务完成，判断任务是否重复，然后重新计算任务位置重新插入即可 定时任务： 执行一次： 问题5：如果一个时间间隔内时间任务处理不过来怎么办 通过下一节技术内幕来查看处理逻辑。不过可以预想到的是每个槽都会拉起协程来进行任务处理，如果是顺序执行有可能导致部分任务超出执行时间，所以每个任务都会使用一个协程处理，那么这里就可以考虑协程池以及对象池了。 问题6：如果一个任务的延时时间小于时间轮的时间片间隔，那么任务何时执行 这里有两个解决方案，go-zero使用第一种方案： 由于小于延时时间小于时间间隔，那么就认为任务不需要等到下一个时间轮的时间片执行，而是立即执行 利用多层环的原理处理更细粒度的任务。这样就不能重用circle的概念，而是需要 sub circle子环概念 circle用来解决延时时间超过当前环的问题 sub circle用来解决更细粒度的时间片需求。缺点就是逻辑结构更复杂，每个sub circle都需要有一个更细的时间粒度 技术内幕 代码路径：core/collection/timingwheel.go，前一节就是 go-zero 中时间轮的时间原理 时间轮对象 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// A TimingWheel is a timing wheel object to schedule tasks.TimingWheel struct &#123; interval time.Duration //时间划分刻度 ticker timex.Ticker slots []*list.List //数组内为双向链表指针 timers *SafeMap // tickedPos int //环的位置 numSlots int //时间槽 execute Execute //时间点执行任务的方法 //以下为不同任务的隧道 setChannel chan timingEntry //设置任务隧道 moveChannel chan baseEntry //移动任务隧道 removeChannel chan interface&#123;&#125; //删除任务隧道 drainChannel chan func(key, value interface&#123;&#125;) //并发执行任务隧道 stopChannel chan lang.PlaceholderType //时间轮停止通知隧道&#125;//Execute 执行任务的方法Execute func(key, value interface&#123;&#125;)//timingEntry timingEntry struct &#123; //时间轮实体对象 baseEntry value interface&#123;&#125; circle int diff int removed bool //是否删除&#125;//baseEntry 基础属性baseEntry struct &#123; delay time.Duration key interface&#123;&#125;&#125;//positionEntry 位置对象positionEntry struct &#123; pos int item *timingEntry&#125;//timingTask 时间任务timingTask struct &#123; key interface&#123;&#125; value interface&#123;&#125;&#125; 其中： 每个槽中因为是链表，所以并没有数量限制 时间轮实体timingEntry中有两个值需要关注 diff 和 removed 时间轮结构中 timers 作用往下看 timers 使用的 SafeMap 不仅仅是为了处理并发安全，还有就是原生map内存泄露问题的临时替代品(详见《GoZero-SafeMap》) 初始化时间轮 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// NewTimingWheel 初始化并返回一个时间轮func NewTimingWheel(interval time.Duration, numSlots int, execute Execute) (*TimingWheel, error) &#123; if interval &lt;= 0 || numSlots &lt;= 0 || execute == nil &#123; return nil, fmt.Errorf(\"interval: %v, slots: %d, execute: %p\", interval, numSlots, execute) &#125; return newTimingWheelWithClock(interval, numSlots, execute, timex.NewTicker(interval))&#125;//newTimingWheelWithClock 初始化时间轮//interval：时间划分刻度//numSlots：时间槽//execute：时间点执行函数func newTimingWheelWithClock(interval time.Duration, numSlots int, execute Execute, ticker timex.Ticker) (*TimingWheel, error) &#123; tw := &amp;TimingWheel&#123; interval: interval, // 单个时间格时间间隔 ticker: ticker, // 定时器，做时间推动，以interval为单位推进 slots: make([]*list.List, numSlots), // 时间轮 timers: NewSafeMap(), // 存储task&#123;key, value&#125;的map [执行execute所需要的参数] tickedPos: numSlots - 1, // at previous virtual circle execute: execute, // 执行函数 numSlots: numSlots, // 初始化 slots num setChannel: make(chan timingEntry), // 以下几个channel是做task传递的 moveChannel: make(chan baseEntry), removeChannel: make(chan interface&#123;&#125;), drainChannel: make(chan func(key, value interface&#123;&#125;)), stopChannel: make(chan lang.PlaceholderType), &#125; tw.initSlots() //使用list.New()初始化每个槽，构建双向链表保存任务 go tw.run() //开启协程，使用channel来做task时间任务接收与处理 return tw, nil&#125;//根据隧道类型等待不同的任务执行func (tw *TimingWheel) run() &#123; for &#123; select &#123; case &lt;-tw.ticker.Chan(): tw.onTick() case task := &lt;-tw.setChannel: //收到设置任务 tw.setTask(&amp;task) case key := &lt;-tw.removeChannel: //删除隧道 tw.removeTask(key) case task := &lt;-tw.moveChannel: //重复任务隧道 tw.moveTask(task) case fn := &lt;-tw.drainChannel: //并发任务 tw.drainAll(fn) case &lt;-tw.stopChannel: //停止隧道 tw.ticker.Stop() return &#125; &#125;&#125; 在时间轮拉起协程进行隧道的监听与处理，这里需要注意的是： 除了时间轮的定时器隧道，其他隧道都可以通过外部接口将消息传入处理 其中moveChannel和drainChannel需要解释： moveChannel：更新任务延时时间 drainChannel：使用自定义函数并发执行所有任务 添加任务 12345678910111213141516171819202122232425262728293031323334353637// SetTimer 设置过期时间func (tw *TimingWheel) SetTimer(key, value interface&#123;&#125;, delay time.Duration) error &#123; if delay &lt;= 0 || key == nil &#123; return ErrArgument &#125; select &#123; case tw.setChannel &lt;- timingEntry&#123; //向设置隧道中添加一个基础的任务对象 baseEntry: baseEntry&#123; delay: delay, key: key, &#125;, value: value, &#125;: return nil //设置完直接结束 case &lt;-tw.stopChannel: //当设置过程中收到退出通知则直接退出 return ErrClosed &#125;&#125;//setTask 时间轮收到设置任务任务之后进行任务添加func (tw *TimingWheel) setTask(task *timingEntry) &#123; if task.delay &lt; tw.interval &#123; //如果任务延迟时间小于时间轮的刻度，那么延迟时间等于刻度 task.delay = tw.interval &#125; if val, ok := tw.timers.Get(task.key); ok &#123; //如果已经存在这个任务，更新任务 entry := val.(*positionEntry) entry.item.value = task.value tw.moveTask(task.baseEntry) &#125; else &#123; pos, circle := tw.getPositionAndCircle(task.delay) //根据延时获取任务位置 task.circle = circle //设置任务所在的环 tw.slots[pos].PushBack(task) //将任务添加到对应槽中 tw.setTimerPosition(pos, task) //更新任务最新位置 &#125;&#125; 注意： 在添加任务的时候这里的 value 类型是 interface 类型，而不是指定的任务对象 注意这里使用 setTimerPosition 又缓存任务，它的作用往下看 如 实现原理 中介绍的那样计算任务所在的槽以及环 1234567891011121314//getPositionAndCircle 获取位置与环//@param: d 任务延时时间//@return:// pos 任务所在槽// circle 任务环func (tw *TimingWheel) getPositionAndCircle(d time.Duration) (pos, circle int) &#123; //如果将任务的延迟时间按照时间轮刻度划分，那么 steps 就是在第几个刻度内 steps := int(d / tw.interval) // 时间轮的相对位置 + 延迟相对时间刻度的位置 % 槽的数量 = 任务在时间轮中的位置 pos = (tw.tickedPos + steps) % tw.numSlots circle = (steps - 1) / tw.numSlots return&#125; 设置任务的位置 12345678910111213//setTimerPosition 设置任务位置func (tw *TimingWheel) setTimerPosition(pos int, task *timingEntry) &#123; if val, ok := tw.timers.Get(task.key); ok &#123; //如果任务缓存已经存在，那么更新槽的位置 timer := val.(*positionEntry) timer.item = task timer.pos = pos &#125; else &#123; tw.timers.Set(task.key, &amp;positionEntry&#123; //如果任务缓存不存在，则设置任务缓存 pos: pos, item: task, &#125;) &#125;&#125; 所以 时间轮中 timers其实是通过key保存任务，并把完成任务和位置都保存 更新任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//MoveTimer移动任务func (tw *TimingWheel) MoveTimer(key interface&#123;&#125;, delay time.Duration) error &#123; if delay &lt;= 0 || key == nil &#123; return ErrArgument &#125; select &#123; case tw.moveChannel &lt;- baseEntry&#123; delay: delay, key: key, &#125;: return nil case &lt;-tw.stopChannel: return ErrClosed &#125;&#125;//moveTask 更新任务执行位置func (tw *TimingWheel) moveTask(task baseEntry) &#123; val, ok := tw.timers.Get(task.key) //通过key获取任务(因为是移动，所以当任务不存在，那么直接结束。) if !ok &#123; return &#125; timer := val.(*positionEntry) if task.delay &lt; tw.interval &#123; //如果任务的delay小于时间片，那么立即执行 threading.GoSafe(func() &#123; //拉起协程进行执行 tw.execute(timer.item.key, timer.item.value) &#125;) return &#125; pos, circle := tw.getPositionAndCircle(task.delay) //计算任务位置 if pos &gt;= timer.pos &#123; timer.item.circle = circle timer.item.diff = pos - timer.pos //计算任务位置与缓存中任务位置的不同 &#125; else if circle &gt; 0 &#123; //不是当前环的任务，那么 circl - 1 circle-- timer.item.circle = circle timer.item.diff = tw.numSlots + pos - timer.pos //不同 加上一个环的槽数，判断位置是否相同 &#125; else &#123; timer.item.removed = true newItem := &amp;timingEntry&#123; baseEntry: task, value: timer.item.value, &#125; tw.slots[pos].PushBack(newItem) tw.setTimerPosition(pos, newItem) &#125;&#125; 注意： 移动任务MoveTimer(key interface{}, delay time.Duration) error {... }并没有参数 Value，用来移动更新已有的任务 执行任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//定时器触发任务func (tw *TimingWheel) onTick() &#123; //获取需要执行任务的槽位置 tw.tickedPos = (tw.tickedPos + 1) % tw.numSlots l := tw.slots[tw.tickedPos] //获取槽的对应链表 tw.scanAndRunTasks(l)&#125;func (tw *TimingWheel) scanAndRunTasks(l *list.List) &#123; var tasks []timingTask for e := l.Front(); e != nil; &#123; //遍历链表 task := e.Value.(*timingEntry) if task.removed &#123; //如果任务状态为删除，则从任务链表中删除任务 next := e.Next() l.Remove(e) e = next continue &#125; else if task.circle &gt; 0 &#123; //如果任务所在的环大于0 task.circle-- e = e.Next() continue &#125; else if task.diff &gt; 0 &#123; //如果任务位置发生了变化 next := e.Next() l.Remove(e) //先删掉任务 // (tw.tickedPos+task.diff)%tw.numSlots // cannot be the same value of tw.tickedPos pos := (tw.tickedPos + task.diff) % tw.numSlots //根据任务最新的位置计算位置重新放入任务 tw.slots[pos].PushBack(task) tw.setTimerPosition(pos, task) task.diff = 0 e = next continue &#125; tasks = append(tasks, timingTask&#123; key: task.key, value: task.value, &#125;) next := e.Next() l.Remove(e) tw.timers.Del(task.key) e = next &#125; tw.runTasks(tasks)&#125; 具体执行 12345678910111213func (tw *TimingWheel) runTasks(tasks []timingTask) &#123; if len(tasks) == 0 &#123; return &#125; go func() &#123; for i := range tasks &#123; //拉起一个协程，不断执行任务(多任务非并发) threading.RunSafe(func() &#123; tw.execute(tasks[i].key, tasks[i].value) &#125;) &#125; &#125;()&#125; 注意：每个任务都使用一个协程进行执行，这里使用的是 go-zero封装的协程池，这个go 会等待所有任务执行完毕才会结束 删除任务 12345678910111213141516171819202122232425// RemoveTimer removes the task with the given key.func (tw *TimingWheel) RemoveTimer(key interface&#123;&#125;) error &#123; if key == nil &#123; return ErrArgument &#125; select &#123; case tw.removeChannel &lt;- key: return nil case &lt;-tw.stopChannel: return ErrClosed &#125;&#125;//removeTaskfunc (tw *TimingWheel) removeTask(key interface&#123;&#125;) &#123; val, ok := tw.timers.Get(key) //任务不存在 if !ok &#123; return &#125; timer := val.(*positionEntry) timer.item.removed = true //增加一个状态表示已经被删除 tw.timers.Del(key) &#125; 排空任务 123456789101112131415161718192021222324252627//Drain 使用 fn 排空任务func (tw *TimingWheel) Drain(fn func(key, value interface&#123;&#125;)) error &#123; select &#123; case tw.drainChannel &lt;- fn: return nil case &lt;-tw.stopChannel: return ErrClosed &#125;&#125;//drainAll 排空任务func (tw *TimingWheel) drainAll(fn func(key, value interface&#123;&#125;)) &#123; runner := threading.NewTaskRunner(drainWorkers) for _, slot := range tw.slots &#123; for e := slot.Front(); e != nil; &#123; task := e.Value.(*timingEntry) next := e.Next() slot.Remove(e) e = next if !task.removed &#123; runner.Schedule(func() &#123; fn(task.key, task.value) &#125;) &#125; &#125; &#125;&#125; 排空任务并不是直接删除，而是提供一个自定义函数接口用于处理剩下的任务，有利于安全退出。但需要注意的是每个任务都会拉一个协程进行处理，也就是不能立即执行 时间轮的 Stop 停止时间轮 1234// Stop stops tw. No more actions after stopping a TimingWheel.func (tw *TimingWheel) Stop() &#123; close(tw.stopChannel)&#125; 总结 task从 优先队列 O(nlog(n)) 降到 双向链表 O(1)，而执行task也只要轮询一个时间点的tasks O(N)，不需要像优先队列，放入和删除元素 O(nlog(n))。 时间轮中的多层环是一种虚拟概念，用来记录超出范围(nusSlots * interval)的任务 使用chan接收外部接口调用的好处是并发安全，当然内部逻辑实现还是需要注意map安全 补充： 可以添加一个在协程池中讲到的时间轮状态，保证不会有新的任务在退出的时候加进来 可以使用协程池和内存池的节省内存空间，但如果协程池处理不过来使后续任务阻塞，可能会导致时间轮功能异常 分布式系统中系统的定时调用则需要使用分布式定时器，这在另外的章节中学习 技术应用 go-zero用于缓存的定时删除 12345678910111213timingWheel, err := NewTimingWheel(time.Second, slots, func(k, v interface&#123;&#125;) &#123; key, ok := k.(string) if !ok &#123; return &#125; cache.Del(key) //到达则删除缓存&#125;)if err != nil &#123; return nil, err&#125;cache.timingWheel = timingWheel 参考链接 https://www.ericcai.fun/detail/16 https://juejin.cn/post/6844904110399946766 https://xiaorui.cc/archives/6160 https://zhuanlan.zhihu.com/p/264826698 https://lk668.github.io/2021/04/05/2021-04-05-手把手教你如何用golang实现一个timewheel/ https://go-zero.dev/cn/docs/blog/principle/timing-wheel","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-06-协程池","slug":"Microservices/微服务-06-协程池","date":"2021-08-01T14:29:14.000Z","updated":"2023-08-10T04:16:33.581Z","comments":true,"path":"2021/08/01/Microservices/微服务-06-协程池/","link":"","permalink":"http://xboom.github.io/2021/08/01/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-06-%E5%8D%8F%E7%A8%8B%E6%B1%A0/","excerpt":"","text":"问题背景 需要协程池吗？ 虽然协程非常轻量级的，一般用不上协程池。协程池的作用 无休止地创建大量goroutine，势必会因为对大量go 程的创建、调度和销毁带来性能损耗。 为了解决这个问题，可以引入协程池 解决方案 协程池需要什么？ 协程如何重用、任务如何执行 协程池支持自定义协程池大小 如果当前任务数量超过协程池大小，那么当前任务需要等待，等待时间支持超时退出 协程支持自定义退出 异常捕获，防止因为单个协程的异常处理导致整个协程池无法使用 协程池大致的逻辑如下图所示： 方案1 这是一个网上能搜到的 “100行实现一个协程池”， 第一步：定义一个任务 1234type Task struct &#123; Handler func(v ...interface&#123;&#125;) Params []interface&#123;&#125;&#125; 其实是将需要执行的协程方法，使用结构体封装起来 第二步：定义一个协程池 12345678910111213type Pool struct &#123; capacity uint64 //容量 runningWorkers uint64 //正在运行的协程树 status int64 //状态(防止在退出过程中，仍然在新建协程) chTask chan *Task //任务队列 PanicHandler func(interface&#123;&#125;)//panic处理函数 sync.Mutex //原子锁,用于保证runningWorkers原子数据变化&#125;const ( RUNNING = 1 STOPED = 0) 第三步：利用协程池启动一个协程执行任务 12345678910111213141516171819202122232425262728293031323334353637func (p *Pool) run() &#123; p.incRunning() //增加正在运行的协程数目 go func() &#123; defer func() &#123; p.decRunning() //结束任务 if r := recover(); r != nil &#123; if p.PanicHandler != nil &#123; p.PanicHandler(r) &#125; else &#123; log.Printf(\"Worker panic: %s\\n\", r) &#125; &#125; p.checkWorker() // check worker avoid no worker running &#125;() for &#123; select &#123; case task, ok := &lt;-p.chTask: if !ok &#123; return &#125; task.Handler(task.Params...) //任务处理 &#125; &#125; &#125;()&#125;//如果任务数量大于0，但是这个时候协程都退出，则再次构建一个协程func (p *Pool) checkWorker() &#123; p.Lock() defer p.Unlock() if p.runningWorkers == 0 &amp;&amp; len(p.chTask) &gt; 0 &#123; p.run() &#125;&#125; 协程池的协程不是常驻协程吗，为什么会出现协程数量为0，但是任务大于0的情况呢？ 答：工作协程可能因为 panic 都退出了，那么这个时候就需要有一个重新拉起协程去执行任务 第四步：生产任务 1234567891011121314151617181920func (p *Pool) Put(task *Task) error &#123; p.Lock() defer p.Unlock() if p.status == STOPED &#123; return ErrPoolAlreadyClosed &#125; // run worker if p.GetRunningWorkers() &lt; p.GetCap() &#123; p.run() &#125; // send task if p.status == RUNNING &#123; p.chTask &lt;- task &#125; return nil&#125; 为什么在存放任务的时候，会多一个协程池的判断呢？ 答：可能会出现协程池结束关闭的情况，如果这个时候又有新的任务，那就又会创建新的协程去执行 最后：关闭协程池 12345678910111213// Close close pool gracefulfunc (p *Pool) Close() &#123; if !p.setStatus(STOPED) &#123; // stop put task return &#125; for len(p.chTask) &gt; 0 &#123; // 等待所有的任务都被消费 time.Sleep(1e6) // 防止等待任务清空 cpu 负载突然变大, 这里小睡一下 &#125; close(p.chTask)&#125; 总结： 这里添加了协程池的状态，防止退出时候的任务增加 为什么在退出的时候，如果任务大于0，那么需要 sleep 一下？ 异常捕获之后再次检查是否有协程在执行任务，没有则添加一个协程 使用无缓冲channel进行任务执行，可能会出现添加任务阻塞的情况 任务是否可以添加一个运行超时时间，防止单个任务死锁？ 方案2 字节跳动开源的协程池，仓库地址：https://github.com/bytedance/gopkg/tree/develop/util/gopool 使用 生产者-消费者模式 设计协程池 第一步：协程池 具有的功能 12345678type Pool interface &#123; Name() string //协程池名称 SetCap(cap int32) //协程池容量 Go(f func()) //使用协程执行 f CtxGo(ctx context.Context, f func()) //使用协程执行f并支持参数 ctx SetPanicHandler(f func(context.Context, interface&#123;&#125;)) //设置协程处理函数 WorkerCount() int32 //返回正在运行的协程数量&#125; 第二步：协程池的结构 1234567891011type pool struct &#123; name string //协程池名称 cap int32 //协程池容量 config *Config //协程池配置 taskHead *task //任务头部 taskTail *task //任务尾部 taskLock sync.Mutex //任务原子锁(竞争) taskCount int32 //任务数量 workerCount int32 //正在运行的协程数量 panicHandler func(context.Context, interface&#123;&#125;) //Panic处理逻辑&#125; 第三步：看看 任务 task 的定义 123456789101112type task struct &#123; ctx context.Context f func() //执行函数 next *task //指向下一个任务的指针&#125;type taskList struct &#123; //使用双向链表将任务连接起来 sync.Mutex taskHead *task taskTail *task&#125; 第四步：查看协程池是怎么运行的 1234567891011121314151617181920212223242526272829var taskPool sync.Pool //对象池func (p *pool) CtxGo(ctx context.Context, f func()) &#123; t := taskPool.Get().(*task) //从对象池获取任务对象 t.ctx = ctx t.f = f p.taskLock.Lock() //获取任务写锁 if p.taskHead == nil &#123; //如果任务链表为空则新建，否则插入链表尾部 p.taskHead = t p.taskTail = t &#125; else &#123; p.taskTail.next = t p.taskTail = t &#125; p.taskLock.Unlock() //释放任务写锁 atomic.AddInt32(&amp;p.taskCount, 1) //增加任务数量 /* 1. 如果任务数量大于配置的数量 &amp;&amp; 正在执行的协程数量小于协程池容量，说明任务太多，还有空闲的协程 那么就开启一个新的协程处理 如果任务数量小于配置的数量 &amp;&amp; 正在执行的协程数量小于协程池容量。说明任务还不多，就让当前协程顺序执行 2. 正在执行的协程为0 */ if (atomic.LoadInt32(&amp;p.taskCount) &gt;= p.config.ScaleThreshold &amp;&amp; p.WorkerCount() &lt; atomic.LoadInt32(&amp;p.cap)) || p.WorkerCount() == 0 &#123; p.incWorkerCount() w := workerPool.Get().(*worker) w.pool = p w.run() &#125;&#125; 这里额外定义了一个 workPool，其实是消费者池 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354var workerPool sync.Pooltype worker struct &#123; //消费者其实是一个协程池 pool *pool&#125;func (w *worker) run() &#123; go func() &#123; for &#123; //这是一个常驻协程 var t *task w.pool.taskLock.Lock() //获取任务写锁 if w.pool.taskHead != nil &#123; //获取任务并将任务数量-1 t = w.pool.taskHead w.pool.taskHead = w.pool.taskHead.next atomic.AddInt32(&amp;w.pool.taskCount, -1) &#125; if t == nil &#123; //如果没有任务，那么worker销毁 // if there's no task to do, exit w.close() w.pool.taskLock.Unlock() w.Recycle() return &#125; w.pool.taskLock.Unlock() //释放任务写锁 func() &#123; defer func() &#123; if r := recover(); r != nil &#123; //异常处理 if w.pool.panicHandler != nil &#123; w.pool.panicHandler(t.ctx, r) &#125; else &#123; msg := fmt.Sprintf(\"GOPOOL: panic in pool: %s: %v: %s\", w.pool.name, r, debug.Stack()) logger.CtxErrorf(t.ctx, msg) &#125; &#125; &#125;() t.f() //执行任务 f &#125;() t.Recycle() //任务结束后，会收work &#125; &#125;()&#125;func (w *worker) close() &#123; //减少worker数目 w.pool.decWorkerCount()&#125;func (w *worker) zero() &#123; //释放缓存池 w.pool = nil&#125;func (w *worker) Recycle() &#123; //释放worker(存入缓存池) w.zero() workerPool.Put(w)&#125; 最后，它还定义了一个 poolMap 用于根据名称注册与使用 多个协程池 注意： 使用双向链表存储任务，表示它理论上支持无限个任务。后面的任务可能存在长时间等待的情况 使用任务上限的好处是，不是每来一个任务都开启一个协程，而是任务超过一定数量而又空闲的协程才开启新的协程去执行 技术内幕 go-zero是如何实现协程池的，代码路径：core/threading 第一步，定义了recover逻辑，用于 panic 之后的清理操作 12345678910// core/rescue/recover.gofunc Recover(cleanups ...func()) &#123; for _, cleanup := range cleanups &#123; cleanup() &#125; if p := recover(); p != nil &#123; //logx.ErrorStack(p) &#125;&#125; 第二步，定义了一个安全运行goroutine的方案 GoSafe，包含处理panic逻辑 123456789func GoSafe(fn func()) &#123; go RunSafe(fn)&#125;func RunSafe(fn func()) &#123; defer rescue.Recover() fn()&#125; TaskRunner： 使用 limitChan 协程池 执行协程 123456789101112131415161718192021222324// TaskRunner 用于并发控制协程数量type TaskRunner struct &#123; limitChan chan lang.PlaceholderType&#125;// 创建 TaskRunner 对象func NewTaskRunner(concurrency int) *TaskRunner &#123; return &amp;TaskRunner&#123; limitChan: make(chan lang.PlaceholderType, concurrency), &#125;&#125;// 在 任务并发控制下执行 taskfunc (rp *TaskRunner) Schedule(task func()) &#123; rp.limitChan &lt;- lang.Placeholder //limitChan 类似一个并发锁 go func() &#123; defer rescue.Recover(func() &#123; &lt;-rp.limitChan &#125;) task() &#125;()&#125; 注意：当limitChan满那么任务执行会出现超时，缺乏超时逻辑 WorkerGroup：使用 wokers 并发执行任务 job 123456789101112131415161718192021type WorkerGroup struct &#123; job func() workers int&#125;// NewWorkerGroup returns a WorkerGroup with given job and workers.func NewWorkerGroup(job func(), workers int) WorkerGroup &#123; return WorkerGroup&#123; job: job, workers: workers, &#125;&#125;// Start starts a WorkerGroup.func (wg WorkerGroup) Start() &#123; group := NewRoutineGroup() for i := 0; i &lt; wg.workers; i++ &#123; group.RunSafe(wg.job) &#125; group.Wait()&#125; RoutineGroup: 多协程等待 123456789101112131415161718192021222324252627282930313233// RoutineGroup 多协程等待type RoutineGroup struct &#123; waitGroup sync.WaitGroup&#125;func NewRoutineGroup() *RoutineGroup &#123; return new(RoutineGroup)&#125;// 不要引用外部参数，可能被其他协程修改func (g *RoutineGroup) Run(fn func()) &#123; g.waitGroup.Add(1) go func() &#123; defer g.waitGroup.Done() fn() &#125;()&#125;//不要引用外部参数，可能被其他协程修改func (g *RoutineGroup) RunSafe(fn func()) &#123; g.waitGroup.Add(1) GoSafe(func() &#123; defer g.waitGroup.Done() fn() &#125;)&#125;// 等待所有协程结束func (g *RoutineGroup) Wait() &#123; g.waitGroup.Wait()&#125; 所以 go-zero的threading 并不是真正的协程池，仅仅是提供多种并发执行 goroutine的方法 总结 所以从目前来看，实现一个协程池都有哪些值得学习的地方呢？ 将需要使用临时协程执行的函数已任务的形式 任务 -- 协程池(Pool) -- 工人执行 协程池是有容量限制的，有了容量就有正在运行的协程数 协程池有状态防止在退出的时候仍然进行任务构建与执行 协程池有异常捕获机制，保证单个异常不会影响整个协程池 任务已任务合集的形式存在，让消费者并发消费 有了异常捕获与任务合集，为了防止工人都发生异常，而还有任务没有执行，则需要有工人唤起机制 可以使用本地缓存池进行工人的重复利用 任务合集缓隧道还是双链表、每个任务都构建一个协程还是单个任务多任务执行的选择 下一节，将学习另外一个协程池 ants 的实现方式 参考文档 https://go-zero.dev/cn/docs/goctl/installation/","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-05-MapReduce","slug":"Microservices/微服务-05-MapReduce","date":"2021-07-31T14:29:14.000Z","updated":"2023-08-10T04:19:40.195Z","comments":true,"path":"2021/07/31/Microservices/微服务-05-MapReduce/","link":"","permalink":"http://xboom.github.io/2021/07/31/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-05-MapReduce/","excerpt":"","text":"什么是MapReduce? MapReduce是Google提出了一个软件架构，用于大规模数据集的并行运算。 MapReduce通过把对数据集的大规模操作分发给网络上的每个节点实现可靠性；每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。 go-zero的MapReduce则借鉴其中的思想，接下来一起看下go-zero是如何应用这一思想的 问题背景 在微服务中开发中，如果多个服务串行依赖的话那么整个API的耗时将会大大增加。通过什么手段来优化？ 传输层面通过MQ的解耦特性来降低API的耗时 MQ通信效率没有grpc高(消息通过MQ服务器进行中转) 业务层面通过Go语言的WaitGroup工具来进行并发控制 自行封装Add与Done 实际业务场景中： 如果接口的多个依赖有一个出错，则期望能立即返回且不必等待所有依赖都执行完毕。已经完成的接口调用也应该回滚 多个依赖可能有部分依赖之间也存在着相互依赖，或者上下关系 go-zero的主要应用场景为：需要从不同的rpc服务中获取相应属性组装成复杂对象，比如要查询商品详情： 商品服务-查询商品属性 库存服务-查询库存属性 价格服务-查询价格属性 营销服务-查询营销属性 如果是串行调用的话响应时间会随着 rpc 调用次数呈线性增长，简单场景下使用 WaitGroup 也能够满足需求，但如果要对 rpc 调用返回的数据进行校验、数据加工转换、数据汇总呢？ go-zero通过mapreduce来处理这种对输入数据进行处理最后输出清洗数据的问题。是一种经典的模式：生产者消费者模式。将数据处理分为三个阶段： 数据生产 generate(查询，必选) 数据加工 mapper(加工，可选) 数据聚合 reducer(聚合，可选) 利用协程处理以及管道通信，实现数据的加速处理 应用场景 场景1 对数据批处理，比如对一批用户id，效验每个用户的合法性并且效验过程中有一个出错就认为效验失败，返回的结果为效验合法的用户id 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mapreduceimport ( \"errors\" \"log\" \"github.com/zeromicro/go-zero/core/mr\")func Run(uids []int) ([]int, error) &#123; r, err := mr.MapReduce(func(source chan&lt;- interface&#123;&#125;) &#123; for _, uid := range uids &#123; source &lt;- uid &#125; &#125;, func(item interface&#123;&#125;, writer mr.Writer, cancel func(error)) &#123; uid := item.(int) ok, err := check(uid) if err != nil &#123; //如果校验逻辑有问题，这里执行cancel整个校验过程停止 cancel(err) &#125; if ok &#123; //如果校验失败，那么不返回该uid writer.Write(uid) &#125; &#125;, func(pipe &lt;-chan interface&#123;&#125;, writer mr.Writer, cancel func(error)) &#123; var uids []int for p := range pipe &#123; uids = append(uids, p.(int)) &#125; writer.Write(uids) &#125;) if err != nil &#123; log.Printf(\"check error: %v\", err) return nil, err &#125; return r.([]int), nil&#125;func check(uid int) (bool, error) &#123; // do something check user legal if uid == 0 &#123; return false, errors.New(\"uid wrong\") &#125; return true, nil&#125; 其实是利用N个协程等待数据生产者的数据传输然后转交给聚合逻辑处理 场景2 某些功能的结果往往需要依赖多个服务，比如商品详情的结果往往会依赖用户服务、库存服务、订单服务等等，一般被依赖的服务都是以rpc的形式对外提供，为了降低依赖的耗时我们往往需要对依赖做并行处理 1234567891011121314151617181920func productDetail(uid, pid int64) (*ProductDetail, error) &#123; var pd ProductDetail err := mr.Finish(func() (err error) &#123; pd.User, err = userRpc.User(uid) return &#125;, func() (err error) &#123; pd.Store, err = storeRpc.Store(pid) return &#125;, func() (err error) &#123; pd.Order, err = orderRpc.Order(pid) return &#125;) if err != nil &#123; log.Printf(\"product detail error: %v\", err) return nil, err &#125; return &amp;pd, nil&#125; 技术内幕 源码目录：core/mr/mapreduce.go 其中利用到的对外函数有 12345678910111213141516171819202122// MapReduce 包含数据生产、数据处理以及数据聚合阶段并返回结果func MapReduce(generate GenerateFunc, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123;&#125;// MapReduceChan 包含数据生产、数据处理以及数据聚合阶段并返回结果。其中利用chan代替数据生产func MapReduceChan(source &lt;-chan interface&#123;&#125;, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; panicChan := &amp;onceChan&#123;channel: make(chan interface&#123;&#125;)&#125; return mapReduceWithPanicChan(source, panicChan, mapper, reducer, opts...)&#125; // ForEach 只包含数据生产和数据处理阶段，但没有任何输出func ForEach(generate GenerateFunc, mapper ForEachFunc, opts ...Option) &#123;&#125;// FinishVoid 并行运行 fnsfunc FinishVoid(fns ...func()) &#123; // Finish 并行运行 fns，在任何错误时取消func Finish(fns ...func() error) error &#123;&#125; //WithWorkers 定义一个 mapreduce 有几个协程func WithWorkers(workers int) Option &#123;&#125; 数据生产阶段 首先定义 buildSource使用协程进行数据生产 12345678910111213141516//buildSource 使用协程执行generate 并将协程的参数一个非缓冲的channel返回。如果generate发生panic，则将错误写入 onceChanfunc buildSource(generate GenerateFunc, panicChan *onceChan) chan interface&#123;&#125; &#123; source := make(chan interface&#123;&#125;) go func() &#123; defer func() &#123; if r := recover(); r != nil &#123; panicChan.write(r) &#125; close(source) &#125;() generate(source) //返回的非缓冲隧道也是数据生产的入口 &#125;() return source&#125; 其中的 onceChan则是一个非阻塞会缓冲的channel，当channel中还有数据没有处理完，则直接返回 123456789101112type onceChan struct &#123; channel chan interface&#123;&#125; wrote int32&#125;func (oc *onceChan) write(val interface&#123;&#125;) &#123; if atomic.AddInt32(&amp;oc.wrote, 1) &gt; 1 &#123; return &#125; oc.channel &lt;- val&#125; 数据处理阶段 接着利用mapReduceWithPanicChan进行数据处理mapper和数据聚合reducer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func MapReduce(generate GenerateFunc, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; panicChan := &amp;onceChan&#123;channel: make(chan interface&#123;&#125;)&#125; source := buildSource(generate, panicChan) //使用协程执行generate并返回数据生产无缓冲隧道source return mapReduceWithPanicChan(source, panicChan, mapper, reducer, opts...) //将隧道和处理函数传入&#125;//source就是数据来源隧道func mapReduceWithPanicChan(source &lt;-chan interface&#123;&#125;, panicChan *onceChan, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; options := buildOptions(opts...) // output is used to write the final result output := make(chan interface&#123;&#125;) //...defer //collector 用于从mapper收集数据，消费者是数据聚合 collector := make(chan interface&#123;&#125;, options.workers) //done表示结束，所有的mappers和reducer都要结束 done := make(chan lang.PlaceholderType) writer := newGuardedWriter(options.ctx, output, done) var closeOnce sync.Once // use atomic.Value to avoid data race //... go func() &#123; //起一个协程进行 数据聚合 //panic.wirte() reducer(collector, writer, cancel) &#125;() go executeMappers(mapperContext&#123; //进行 数据处理 ctx: options.ctx, mapper: func(item interface&#123;&#125;, w Writer) &#123; mapper(item, w, cancel) &#125;, source: source, panicChan: panicChan, collector: collector, doneChan: done, workers: options.workers, &#125;) select &#123; //等待结果 case &lt;-options.ctx.Done(): cancel(context.DeadlineExceeded) return nil, context.DeadlineExceeded case v := &lt;-panicChan.channel: panic(v) case v, ok := &lt;-output: if err := retErr.Load(); err != nil &#123; return nil, err &#125; else if ok &#123; return v, nil &#125; else &#123; return nil, ErrReduceNoOutput &#125; &#125;&#125; 其中数据处理阶段又定义了一个协程进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142func executeMappers(mCtx mapperContext) &#123; var wg sync.WaitGroup defer func() &#123; wg.Wait() close(mCtx.collector) drain(mCtx.source) &#125;() var failed int32 pool := make(chan lang.PlaceholderType, mCtx.workers) //协程池 writer := newGuardedWriter(mCtx.ctx, mCtx.collector, mCtx.doneChan) for atomic.LoadInt32(&amp;failed) == 0 &#123; select &#123; case &lt;-mCtx.ctx.Done(): return case &lt;-mCtx.doneChan: return case pool &lt;- lang.Placeholder: //这里是定义的N个works的chan，也就是会在下面创建n个协程 item, ok := &lt;-mCtx.source if !ok &#123; //如果来源关闭，那么将pool数据释放 &lt;-pool return &#125; wg.Add(1) go func() &#123; defer func() &#123; if r := recover(); r != nil &#123; atomic.AddInt32(&amp;failed, 1) mCtx.panicChan.write(r) &#125; wg.Done() &lt;-pool &#125;() //item:生产的数据 //writer: 数据处理对象 mCtx.mapper(item, writer) //执行map &#125;() &#125; &#125;&#125; 数据聚合阶段 最后来看一下是数据处理 1234567891011121314type guardedWriter struct &#123; //写入接口 ctx context.Context //保证超时退出 channel chan&lt;- interface&#123;&#125; //接收数据，这里传输的就是 长度为 N 的 collect channel done &lt;-chan lang.PlaceholderType //主动结束退出&#125;func newGuardedWriter(ctx context.Context, channel chan&lt;- interface&#123;&#125;, done &lt;-chan lang.PlaceholderType) guardedWriter &#123; return guardedWriter&#123; ctx: ctx, channel: channel, done: done, &#125;&#125; Finish Finish逻辑只进行并发处理，其实内部是将执行函数做为数据生产的生产的数据，然后又数据处理逻辑进行处理 12345678910111213141516171819202122232425262728func Finish(fns ...func() error) error &#123; if len(fns) == 0 &#123; //n个外部调用 return nil &#125; return MapReduceVoid(func(source chan&lt;- interface&#123;&#125;) &#123; for _, fn := range fns &#123; source &lt;- fn //数据生产者将函数传入 &#125; &#125;, func(item interface&#123;&#125;, writer Writer, cancel func(error)) &#123; fn := item.(func() error) if err := fn(); err != nil &#123; //数据处理逻辑执行函数 cancel(err) //这里并没有写入，所以第三个函数其实并没有执行 &#125; &#125;, func(pipe &lt;-chan interface&#123;&#125;, cancel func(error)) &#123; &#125;, WithWorkers(len(fns)))&#125;//底层还是执行的MapReduce逻辑func MapReduceVoid(generate GenerateFunc, mapper MapperFunc, reducer VoidReducerFunc, opts ...Option) error &#123; _, err := MapReduce(generate, mapper, func(input &lt;-chan interface&#123;&#125;, writer Writer, cancel func(error)) &#123; reducer(input, cancel) &#125;, opts...) if errors.Is(err, ErrReduceNoOutput) &#123; return nil &#125; return err&#125; 总结： 适用于并发无顺序依赖的并发调用，如果是多个调用具有前后依赖关系，依然需要有先后调用顺序(废话) 也不存在回滚的操作，内部只是将不在等待处理结果直接退出 select 参考链接 https://talkgo.org/t/topic/1452 https://zh.wikipedia.org/wiki/MapReduce","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-04-共享调用","slug":"Microservices/微服务-04-共享调用","date":"2021-07-30T14:29:14.000Z","updated":"2023-08-10T04:15:45.487Z","comments":true,"path":"2021/07/30/Microservices/微服务-04-共享调用/","link":"","permalink":"http://xboom.github.io/2021/07/30/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-04-%E5%85%B1%E4%BA%AB%E8%B0%83%E7%94%A8/","excerpt":"","text":"问题背景 并发场景下，可能会有多个线程（协程）同时请求同一份资源，如果每个请求都要走一遍资源的请求过程，除了比较低效之外，还会对资源服务造成并发的压力。 例如： 缓存失效的同时多个请求同时到达某服务请求相同资源，这些请求会继续访问DB做查询，会引起数据库压力瞬间增大。而使用 SharedCalls 可以使得同时多个请求只需要发起一次拿结果的调用，其他请求&quot;坐享其成&quot;。有效减少了资源服务的并发压力，可以有效防止缓存击穿 云平台服务众多，使用grpc通信的时候不期望每个服务之间都建立链接，而只在向对端发送消息的时候，才在服务之间建立通信。老的逻辑是当开始建立连接的时候会将创建改为正在创建链接，后续消息会因为正在建立链接会直接返回错误或阻塞等待结果(自行实现)，而使用SharedCalls可以短时间内等待链接建立然后继续发送消息 演示代码 1234567891011121314151617181920212223242526func main() &#123; const round = 5 var wg sync.WaitGroup barrier := syncx.NewSharedCalls() wg.Add(round) for i := 0; i &lt; round; i++ &#123; // 多个线程同时执行 go func() &#123; defer wg.Done() // 可以看到，多个线程在同一个 key 上去请求资源，获取资源的实际函数只会被调用一次 val, err := barrier.Do(\"once\", func() (interface&#123;&#125;, error) &#123; // sleep 1秒，为了让多个线程同时取 once 这个 key 上的数据 time.Sleep(time.Second) // 生成了一个随机的 id return stringx.RandId(), nil &#125;) if err != nil &#123; fmt.Println(err) &#125; else &#123; fmt.Println(val) &#125; &#125;() &#125; wg.Wait()&#125; 技术内幕 文件目录：core/syncx/singleflight.go SingleFlight 通过为并发的请求根据相同的key提供相同的结果 一共提供了 Do 与 DoEx 两种接口 123456789101112131415var SingleFlight interface &#123; Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) DoEx(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, bool, error)&#125;var call struct &#123; //代指一次调用 wg sync.WaitGroup //用于等待call结束 val interface&#123;&#125; err error&#125;var flightGroup struct &#123; calls map[string]*call lock sync.Mutex&#125; 首先查看两个基础函数 123456789101112131415161718192021222324252627282930func (g *flightGroup) createCall(key string) (c *call, done bool) &#123; // 先申请加锁 g.lock.Lock() if c, ok := g.calls[key]; ok &#123; //如果key存在，那么等待 // 拿到 call 以后，释放锁，此处 call 可能还没有实际数据，只是一个空的内存占位 g.lock.Unlock() //调用 wg.Wait，判断是否有其他 goroutine 正在申请资源，如果阻塞，说明有其他 goroutine 正在获取资源 c.wg.Wait() //等待相同的call的结束 // 当 wg.Wait 不再阻塞，表示资源获取已经结束，可以直接返回结果 return c, true &#125; c = new(call) //创建一个新的call c.wg.Add(1) //并为这个call添加一个的等待 g.calls[key] = c g.lock.Unlock() return c, false&#125;func (g *flightGroup) makeCall(c *call, key string, fn func() (interface&#123;&#125;, error)) &#123; defer func() &#123; g.lock.Lock() delete(g.calls, key) //删除 g.lock.Unlock() c.wg.Done() //结束call &#125;() c.val, c.err = fn() //执行函数并返回结果&#125; 再看实现逻辑 1234567891011121314151617181920212223242526func (g *flightGroup) Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; c, done := g.createCall(key) //根据key定义一个call if done &#123; //如果是等待结束了，且等待结束了，则返回Call的value return c.val, c.err &#125; //如果是新的，那么就首次执行call，并返回结果 g.makeCall(c, key, fn) return c.val, c.err&#125;func (g *flightGroup) DoEx(key string, fn func() (interface&#123;&#125;, error)) (val interface&#123;&#125;, fresh bool, err error) &#123; c, done := g.createCall(key) if done &#123; //等待结束 return c.val, false, c.err &#125; g.makeCall(c, key, fn) return c.val, true, c.err //新的结束&#125;// NewSingleFlight returns a SingleFlight.func NewSingleFlight() SingleFlight &#123; return &amp;flightGroup&#123; calls: make(map[string]*call), &#125;&#125; DoEx 相较于 Do 中增加了一个 bool 类型的返回值，表示返回的值是共享的还是首次拿到的 参考链接 https://talkgo.org/t/topic/968","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"MySql-05-事务","slug":"MySql/MySql-05-事务","date":"2021-07-30T03:20:52.000Z","updated":"2023-06-28T16:00:28.935Z","comments":true,"path":"2021/07/30/MySql/MySql-05-事务/","link":"","permalink":"http://xboom.github.io/2021/07/30/MySql/MySql-05-%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"概述 InnoDB存储引擎中的事务完全符合ACID特性： 原子性(atomicity)：整个事务是不可分割的工作单位，任何一个SQL语句执行失败，已经执行成功SQL语句也必须撤销，数据库状态退回到执行事务前的状态。默认情况下一条SQL就是一个单独事务，事务是自动提交的。只有显式的使用start transaction开启一个事务，才能将一个代码块放在事务中执行 一致性(consistency)：一致性事务指数据库从一个状态转变为下一种一致的状态。事务开始前和事务结束后，数据库完整性并没有被破坏。如事务执行后数据库唯一约束被破坏，则自动撤销事务，返回初始化状态。 隔离性(isolation)：事务的隔离性要求事务提交前对其他事务不可见 持久性(durability)：事务一旦提交，其结果就是永久性的。即使数据库崩崩溃而恢复，提交的数据也不会丢失 事务的分类： 隐式事务：事务没有明显的开始和结束的标记。MySql的每一条DML(增删改)语句都是一个单独的事务，每条语句DML执行完毕自动提交事务。MySql默认自动提交事务 SHOW VARIABLES LIKE 'autocommit'; 显示事务：事务具有明显的开启和结束的标记，前提是必须设置自动提交功能为禁用 开启事务 - 执行SQL语句 - 成功 - 提交事务 开启事务 - 执行SQL语句 - 失败 - 回滚事务 还能设置回滚点： 12savepoint &lt;回滚点名字&gt;rollback to &lt;回滚点名字&gt; 以 update test set name = 'test' where id=2;为例事务的执行流程 事务开始 申请锁资源，对id=2这行数据上排他锁 将需要修改的data pages读取到innodb_buffer_cache 记录id=2的数据到undo log 记录id=2修改后的数据到redo log buffer 将buffer cache中id=2得name改为test commit，触发二阶段提交2pc 事务结束 WAL(write ahead logging):针对数据文件的修改，必须遵循日志先行原则。也即是将数据持久化到磁盘之前必须确保redo log落盘。 二阶段提交(2pc two phase commit): 首先，redo log prepare，redo持久化到磁盘(redo group commit)，并将回滚段置为prepared状态，此时binlog不做操作 然后写入 binlog 最后redo log commit，innodb释放锁，释放回滚段，设置undo log提交状态，binlog持久化到磁盘，然后存储引擎层提交 主要是保证redo log事务写入顺序和binlog 事务顺序一致(通过事务id保证一致)。 隔离级别 并发可能产生的问题： 脏读 dirty read: 读到其他事务未提交的数据(针对其他事务未提交的操作) 不可重复读 non-repeatable read: 前后读取的记录内容不一致(针对其他事务修改或删除的操作) 幻读 phantom read: 前后读取的记录数量不一致(针对其他事务新增的操作) 针对解决上述问题，将事务的隔离级别分为(由低到高)，假设两个事务按照下面的流程执行，初始记录 name = A 时刻 事务A 事务B 1 begin; 2 begin; 3 select name from user where id = 1; 4 select name from user where id = 1; 5 update user set name=‘B’ where id = 1 6 select name from user where id = 1;(N1) 7 commit; 8 select name from user where id = 1;(N2) 9 commit; 10 select name from user where id = 1;(N3) 读未提交: 一个事务未提交，改动能被另一个事务看到 =&gt; 都执行。执行结果，N1 = B、N2 = B、N3 = B 读提交: 一个事务提交了，改动才能被另一个事务看到 =&gt; 没有赃读。执行结果，N1 = A、N2 = B、N3 = B 可重复读(默认级别): 一个事务提交了，改动也不能被另一个事务看到 =&gt; 只有幻读。执行结果，N1 = A、N2 = A、N3 = B 为什么N1 与 N2 都是A，事务在执行期间看到的数据前后必须一致 串行化 serializable: 后访问的事务必须等待前一个事务执行完成才能访问 =&gt; 都解决。事务A的执行结果，N1 = A、N2 = A、N3 = B 事务的隔离级别规定了一个事务中所做的修改，在事务内和事务间的可见性。较低级别的隔离通常可以执行更高的并发，系统开销也更低 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED (读取未提交) 可能发生 可能发生 可能发生 READ-COMMITTED (读已提交) 解决 可能发生 可能发生 REPEATABLE-READ (可重复读) 解决 解决 可能发生 SERIALIZABLE (可串行化) 解决 解决 解决 修改隔离级别语句： 123456set session transaction isolation level read uncommitted; #读未提交set session transaction isolation level read committed; #读已提交set session transaction isolation level repeatable read; #可重复读set session transaction isolation level serializable; #可串行化set autocommit &#x3D; 0; #取消自动提交select @@tx_isolation; #查询隔离级别 事务的基础 数据库的隔离性包括(ACID)，由不同的功能实现 Atomicity 原子性: 使用 undo log 实现回滚 Consistency 一致性: 通过原子性、持久性、隔离性实现数据一致性 Lsolation 隔离性: 使用锁以及MVCC 实现读写分离、读读并行、读写并行 Durability 持久性: 通过redo log 恢复，和在并发环境下的隔离做到一致性 redo log 称为重做日志，是物理日志，记录页的物理修改操作，用来恢复提交事务修改的页操作 undo log 称为回滚日志，是逻辑日志，根据每行记录进行记录。用来回滚行记录到某个特定版本 Redo Log redo log 也做重做日志，日志文件由两部分组成： 重做日志缓冲 redo log buffer 重做日志文件 redo log file 1234567start transaction;select balance from bank where name=\"zhangsan\";#生成 重做日志 balance=600 update bank set balance = balance - 400; # 生成 重做日志 amount=400 update finance set amount = amount + 400;commit; 为了提升性能不会把每次的修改都实时同步到磁盘，而是会先存到Boffer Pool(缓冲池)里头，把这个当作缓存来用。然后使用后台线程去做缓冲池和磁盘之间的同步，如果还没有来得及同步就宕机怎么办？ 通过Force Log at Commit机制实现事务持久性，即当事务提交(COMMIT)时，必须先将该事务的所有日志写入到重做日志进行持久化，待事务的COMMIT操作完成才算完成 为了确保每次日志都写入重做日志，在每次将重做日志缓冲写入重做日志文件后，InnoDB存储引擎还需要调用一次fsync操作。由于重做日志文件打开并没有使用O_DIRECT选项，因此重做日志缓冲先写入文件系统缓存，为了确保重做日志写入磁盘，必须进行一次fsync操作 系统重启之后在读取redo log恢复最新数据 通过 innodb_flush_log_at_trx_commit 来控制充足哦日志刷新到磁盘的策略 1：事务提交时必须调用一次fsync操作(默认) 0：事务提交时不进行写入重做日志，而是通过master thread 每1秒进行一次重做日志文件的fsync操作 2： 事务提交时将重做日志写入重做日志文件，但仅仅写入文件系统缓存，不进行fsync操作 与二进制日志 binlog的区别是： redo log 是在InnoDB存储引擎层产生的，而二进制文件是服务层产生的 binlog记录的是逻辑日志，记录的是对应的SQL日志。而redo log是物理日志，记录的是对每个页的修改 存入磁盘的时间点不一样 binlog 只在事务提交完成后进行一次写入 redo log在事务进行中不断地被写入，表现为日志并不是随事务提交的顺序进行写入的 总结：redo log是用来恢复数据的 用于保障，已提交事务的持久化特性 undo log undo 也叫做回滚日志，为了回滚需要将之前的操作都记录下来 每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log 当发生系统错误或执行回滚的时候使用undo log undo log 记录事务修改之前版本的数据信息，因此假如由于系统错误或者rollback操作而回滚的话可以根据undo log的信息来进行回滚到没被修改前的状态 (1) 如果在回滚日志里有新增数据记录，则生成删除该条的语句 (2) 如果在回滚日志里有删除数据记录，则生成生成该条的语句 (3) 如果在回滚日志里有修改数据记录，则生成修改到原先数据的语句 总结：undo log是用来回滚数据的用于保障 未提交事务的原子性 MVCC MVCC是通过在每行记录的后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存了行的过期时间，存储的不是实际的时间值而是系统版本号，主要实现思想是通过数据多版本来做到读写分离。从而实现不加锁读进而做到读写并行 MVCC 在mysql中的实现依赖的是undo log 与 read view undo log: unlog 中就某行数据的多个版本数据 read view: 用来判断当前版本数据的可行性 原子性的实现 一个事务必须被视为不可分割的最小工作单位，一个事务中的所有操作要么全部成功提交，要么全部失败回滚，对于一个事务来说不可能只执行其中的部分操作，这就是事务的原子性，通过上述undo log 即可以实现 持久性的实现 事务一旦提交，其所作做的修改会永久保存到数据库中，此时即使系统崩溃修改的数据也不会丢失 InnoDB提供了缓冲池(Buffer Pool)，Buffer Pool中包含了磁盘数据页的映射，可以当做缓存来使用 读数据：会首先从缓冲池中读取，如果缓冲池中没有，则从磁盘读取在放入缓冲池 写数据：会首先写入缓冲池，缓冲池中的数据会定期同步到磁盘中 如果数据已提交，但在缓冲池里还未来得及磁盘持久化，需要一种机制保存已提交事务的数据，为恢复数据使用。redo log 即可解决这个问题，既然redo log也需要存储，也涉及磁盘IO为啥还用它？ redo log 的存储是顺序存储，而缓存同步是随机操作 缓存同步是以数据页为单位的，每次传输的数据大小大于redo log 隔离性的实现 读未提交 读未提交可以理解为没有隔离 串行话(读写锁) 串行话读的时候加共享锁，其他事务可以并发读但不能写。写的时候加排它锁，其他事务不能并发写也不能并发读 可重复读 MVCC:多版本并发控制，通过undo log版本链和read-view实现事务隔离 以可重复读为例 每条记录在更新时除了记录一条变更记录到redo log中，还会记录一条变更相反的回滚操作记录在undo log中 例如一个值从1被按顺序改成了2、3、4，在回滚日志里就会有如下记录： 当前值是4，但查询这条记录的时候，不同时刻启动的事务有不同的read-view 在视图 A、B、C 里面，这一个记录的值分别是 1、2、4 对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到 即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图(也叫快照)，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现 日志什么时候删除？ 日志怎么存储 长事务导致undolog一致存在不被删除，什么是长事务 什么是视图、MVCC 对于一个视图(快照)来说，它能够读到那些版本数据，要遵循以下规则： 当前事务内的更新，可以读到； 版本未提交，不能读到； 版本已提交，但是却在快照创建后提交的，不能读到； 版本已提交，且是在快照创建前提交的，可以读到； 可重复读和读提交的区别在于：在快照的创建上，可重复读仅在事务开始是创建一次，而读提交每次执行语句的时候都要重新创建一次 什么时候删除回滚日志undo-log 当没有比回滚日志更早的读视图（读视图在事务开启时创建）的时候，这个数据不会再有谁驱使它回滚了，这个回滚日志也就失去了用武之地，可以删除了 ibdata文件是共享表空间数据文件。 5.7版本支持单独配置undo log的路径和表空间文件。 为什么回滚到清理后，文件还是不会变小？这个“清理”的意思是 “逻辑上这些文件位置可以复用”，但是并没有删除文件，也没有把文件变小。那到底什么时候删除呢？ 不同的事务拥有不同的Read view，如果一个事务长时间没有提交，意味着会存在很多老的事务视图，同时也保存了大量回滚日志，占用磁盘空间，且在mysql5.5之前，回滚日志和字典都保存在ibdata文件中，即使长事务被提交了，回滚段被清理，文件也不会变小。同时长事务还长期占用锁资源，降低并发效率 幻读 并发写问题的解决方式就是行锁，而解决幻读用的也是锁，叫做间隙锁，MySQL 把行锁和间隙锁合并在一起，解决了并发写和幻读的问题，这个锁叫做 Next-Key锁。 假设现在表中有两条记录，并且 age 字段已经添加了索引，两条记录 age 的值分别为 10 和 30 如图所示，分成了3 个区间，(负无穷,10]、(10,30]、(30,正无穷]，在这3个区间是可以加间隙锁的。 之后，我用下面的两个事务演示一下加锁过程 在事务A提交之前，事务B的插入操作只能等待，这就是间隙锁起得作用。当事务A执行update user set name='风筝2号’ where age = 10; 的时候，由于条件 where age = 10 ，数据库不仅在 age =10 的行上添加了行锁，而且在这条记录的两边，也就是(负无穷,10]、(10,30]这两个区间加了间隙锁，从而导致事务B插入操作无法完成，只能等待事务A提交。不仅插入 age = 10 的记录需要等待事务A提交，age&lt;10、10&lt;age&lt;30 的记录页无法完成，而大于等于30的记录则不受影响，这足以解决幻读问题了。 这是有索引的情况，如果 age 不是索引列，那么数据库会为整个表加上间隙锁。所以，如果是没有索引的话，不管 age 是否大于等于30，都要等待事务A提交才可以成功插入 参考链接 https://time.geekbang.org/column/article/68963 https://blog.csdn.net/youanyyou/article/details/108722263 https://blog.csdn.net/kongliand/article/details/107953656 《MySQL技术内幕》","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"微服务-03-缓存一致性","slug":"Microservices/微服务-03-缓存一致性","date":"2021-07-29T14:29:14.000Z","updated":"2023-08-10T04:15:27.665Z","comments":true,"path":"2021/07/29/Microservices/微服务-03-缓存一致性/","link":"","permalink":"http://xboom.github.io/2021/07/29/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-03-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/","excerpt":"","text":"问题背景 一致性有很多种 强一致性：保证写入后立即可以读取 弱一致性：在系统写入后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态 最终一致性：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态 缓存可以提升性能、缓解数据库压力，使用缓存也会导致数据不一致性的问题 缓存系统的数据一致性通常包括持久化层和缓存层的一致性、以及多级缓存之间的一致性，这里讨论是前者。持久化层和缓存层的一致性问题也通常被称为双写一致性问题。 实现原理 引入 Cache 之后，延迟或程序失败等都会导致缓存和实际存储层数据不一致，下面几种模式减少不一致风险 Cache-Aside Pattern，即旁路缓存模式 Read-Through/Write-Through，读写穿透模式 Write behind，异步缓存写入模式 Cache-Aside 读模式 当缓存命中则直接返回，否则从数据库读取数据并更新缓存 写模式 首先更新数据库，然后删除缓存 问题1：为什么是删除缓存，而不是更新缓存 如果缓存需要通过大量的计算(联表查询更新)，那么更新缓存会是一笔不小的开销 另外如果写操作比较多，可能存在刚更新的缓存还没有读取就又要更新的情况(称为缓存扰动)，所以此模式适用于读多写少的模式 等到读请求未命中再去更新，符合懒加载思路 并发更新可能导致缓存落后与数据库，读请求读到的仍然是旧缓存 问题2：为什么是先更新数据库，而不是先删除缓存 数据库查询请求往往比更新请求更快，可能这种异常更容易出现 Read/Write Through 读模式 当缓存命中则直接返回，否则从数据库读取数据并更新缓存 Read/Write Through模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过抽象缓存层完成的 写模式 Write Through模式在发生Cache Miss的时候，只会在读请求中更新缓存。 写请求在发生Cache Miss的时候不会更新缓存，而是直接写入数据库； 如果命中缓存则先更新缓存，由缓存自己再将数据写回到数据库中 注意这个时候如果命中缓存，是先更新缓存的。也就说和 Cache-Aside一样存在并发场景下的一致性问题 这个策略的核心原则：用户只与缓存打交道，由缓存组件和DB通信，写入或者读取数据。在一些本地进程缓存组件可以考虑这种策略 Write-Through 存在的缺陷：写数据时缓存和数据库同步，但是我们知道这两块存储介质的速度差几个数量级，对写入性能是有很大影响。那我们是否异步更新数据库 Write behind Write behind 跟有相似的地方，都是由Cache Provider来负责缓存和数据库的读写。它两又有个很大的不同：Read/Write Through是同步更新缓存和数据的，Write Behind则是只更新缓存，不直接更新数据库，通过批量异步的方式来更新数据库 缓存和数据库的一致性不强，对一致性要求高的系统要谨慎使用。但是它适合频繁写的场景，MySQL的InnoDB Buffer Pool机制就使用到这种模式 延时双删 延时双删主要用于 Redis主从节点的场景，延时的原因是，mysql 和 redis 主从节点数据不是实时同步的，同步数据需要时间。 服务节点删除 redis 主库数据 服务节点修改 mysql 主库数据 服务节点使得当前业务处理 等待一段时间，等 redis 和 mysql 主从节点数据同步成功。 服务节点从 redis 主库删除数据。 当前或其它服务节点读取 redis 从库数据，发现 redis 从库没有数据，从 mysql 从库读取数据，并写入 redis 主库 注意： 延时双删，有等待环节，如果系统要求低延时，这种场景就不合适了。 延时双删，不适合“秒杀”这种频繁修改数据和要求数据强一致的场景。 延时双删，延时时间是一个预估值，不能确保 mysql 和 redis 数据在这个时间段内都实时同步或持久化成功了 重试保障 方案1：服务自行订阅删除缓存消息 更新数据库数据； 缓存因为种种问题删除失败； 将需要删除的key发送至消息队列； 自己消费消息，获得需要删除的key； 继续重试删除操作，直到成功 方案2：利用第三方服务删除缓存 更新数据库数据； 数据库会将操作信息写入binlog日志当中； 订阅程序提取出所需要的数据以及key； 另起一段非业务代码，获得该信息； 尝试删除缓存操作，发现删除失败； 将这些信息发送至消息队列； 重新从消息队列中获得该数据，重试操作 注意： 删除缓存也可能存储缓存击穿的问题 在 GoZero8-数据库缓存中中使用共享调用的方式(类似自旋锁)进行数据查询 使用方案1进行消息订阅的时候可能出现消息队列也失败的情况 强一致性肯定会有性能影响(比如 raft协议需要等待超过半数节点做出响应)，另外强一致性的异常处理 技术内幕 来看看 rockscache 如何解决缓存一致性的， 地址：https://github.com/dtm-labs/rockscache The First Redis Cache Library To Ensure Eventual Consistency And Strong Consistency With DB. 变量定义 1234567891011121314151617181920212223//rockscache client 可选参数type Options struct &#123; //标记删除key的延时删除时间 默认10s Delay time.Duration //EmptyExpire 是空结果的过期时间。默认为 60 秒 EmptyExpire time.Duration // LockExpire 是更新缓存时分配的锁的过期时间。默认为 3s LockExpire time.Duration //锁失败后的重试等待时间 100ms LockSleep time.Duration // 等待副本数量 WaitReplicas int // 副本等待超时时间 默认300ms WaitReplicasTimeout time.Duration //随机过期时间，0.1的偏移(缓存雪崩) RandomExpireAdjustment float64 // 标识缓存禁止读，默认关闭。用于缓存宕机时候的降级 DisableCacheRead bool // 标识缓存删除，默认关闭。用于缓存宕机时候的降级 DisableCacheDelete bool // 强一致性，默认关闭 StrongConsistency bool&#125; lua脚本 使用脚本进行redis操作，lua的好处是一次性执行，执行过程其他脚本或命令无法执行(注意不确定参数)。 这里使用hash进行数据存储，同时保存 key/value 与 key/lock 12345678910111213141516171819202122232425262728293031func (c *Client) luaGet(key string, owner string) ([]interface&#123;&#125;, error) &#123; res, err := callLua(c.rdb.Context(), c.rdb, ` -- luaGet local v = redis.call('HGET', KEYS[1], 'value') //获取值 local lu = redis.call('HGET', KEYS[1], 'lockUtil') //获取过期时间 if lu ~= false and tonumber(lu) &lt; tonumber(ARGV[1]) or lu == false and v == false then redis.call('HSET', KEYS[1], 'lockUtil', ARGV[2]) //如果锁已经过期或者不存在，则更新锁 redis.call('HSET', KEYS[1], 'lockOwner', ARGV[3]) return &#123; v, 'LOCKED' &#125; end return &#123;v, lu&#125; `, []string&#123;key&#125;, []interface&#123;&#125;&#123;now(), now() + int64(c.Options.LockExpire/time.Second), owner&#125;) debugf(\"luaGet return: %v, %v\", res, err) if err != nil &#123; return nil, err &#125; return res.([]interface&#123;&#125;), nil&#125;func (c *Client) luaSet(key string, value string, expire int, owner string) error &#123; _, err := callLua(c.rdb.Context(), c.rdb, `-- luaSet local o = redis.call('HGET', KEYS[1], 'lockOwner') if o ~= ARGV[2] then return end redis.call('HSET', KEYS[1], 'value', ARGV[1]) redis.call('HDEL', KEYS[1], 'lockUtil') redis.call('HDEL', KEYS[1], 'lockOwner') redis.call('EXPIRE', KEYS[1], ARGV[3]) `, []string&#123;key&#125;, []interface&#123;&#125;&#123;value, owner, expire&#125;) return err&#125; 加锁和解锁 123456789101112131415161718192021222324252627282930//加锁func (c *Client) LockForUpdate(key string, owner string) error &#123; lockUtil := math.Pow10(10) res, err := callLua(c.rdb.Context(), c.rdb, ` -- luaLock local lu = redis.call('HGET', KEYS[1], 'lockUtil') local lo = redis.call('HGET', KEYS[1], 'lockOwner') if lu == false or tonumber(lu) &lt; tonumber(ARGV[2]) or lo == ARGV[1] then redis.call('HSET', KEYS[1], 'lockUtil', ARGV[2]) redis.call('HSET', KEYS[1], 'lockOwner', ARGV[1]) return 'LOCKED' end return lo `, []string&#123;key&#125;, []interface&#123;&#125;&#123;owner, lockUtil&#125;) if err == nil &amp;&amp; res != \"LOCKED\" &#123; return fmt.Errorf(\"%s has been locked by %s\", key, res) &#125; return err&#125;//解锁func (c *Client) UnlockForUpdate(key string, owner string) error &#123; _, err := callLua(c.rdb.Context(), c.rdb, ` -- luaUnlock local lo = redis.call('HGET', KEYS[1], 'lockOwner') if lo == ARGV[1] then redis.call('HSET', KEYS[1], 'lockUtil', 0) redis.call('HDEL', KEYS[1], 'lockOwner') end `, []string&#123;key&#125;, []interface&#123;&#125;&#123;owner&#125;) return err&#125; 读取缓存 1234567891011121314151617181920// new a client for rockscache using the default optionsrc := rockscache.NewClient(redisClient, NewDefaultOptions())v, err := rc.Fetch(\"key1\", 300, func()(string, error) &#123; // fetch data from database or other sources return \"value1\", nil&#125;)func (c *Client) Fetch(key string, expire time.Duration, fn func() (string, error)) (string, error) &#123; ex := expire - c.Options.Delay - time.Duration(rand.Float64()*c.Options.RandomExpireAdjustment*float64(expire)) v, err, _ := c.group.Do(key, func() (interface&#123;&#125;, error) &#123; //同样使用共享调用进行操作 if c.Options.DisableCacheRead &#123; //缓存崩溃直接读数据库 return fn() &#125; else if c.Options.StrongConsistency &#123; //强一致性 return c.strongFetch(key, ex, fn) &#125; return c.weakFetch(key, ex, fn) &#125;) return v.(string), err&#125; 这里也提供了忽略锁的操作 12345678910111213func (c *Client) RawGet(key string) (string, error) &#123; return c.rdb.HGet(c.rdb.Context(), key, \"value\").Result()&#125;func (c *Client) RawSet(key string, value string, expire time.Duration) error &#123; err := c.rdb.HSet(c.rdb.Context(), key, \"value\", value).Err() if err == nil &#123; //如果过期操作失败了，那么缓存可能永远不过期(根据AOF策略，默认每秒) //操作失败可能是网络或者redis宕机，如果是宕机，那么key可能都还没有落盘。所以这里得考虑网络异常情况 err = c.rdb.Expire(c.rdb.Context(), key, expire).Err() &#125; return err&#125; 强一致性获取 1234567891011121314151617181920212223242526272829303132333435363738394041func (c *Client) weakFetch(key string, expire time.Duration, fn func() (string, error)) (string, error) &#123; debugf(\"weakFetch: key=%s\", key) owner := shortuuid.New() r, err := c.luaGet(key, owner) for err == nil &amp;&amp; r[0] == nil &amp;&amp; r[1].(string) != locked &#123; debugf(\"empty result for %s locked by other, so sleep %s\", key, c.Options.LockSleep.String()) time.Sleep(c.Options.LockSleep) r, err = c.luaGet(key, owner) &#125; if err != nil &#123; return \"\", err &#125; if r[1] != locked &#123; return r[0].(string), nil &#125; if r[0] == nil &#123; return c.fetchNew(key, expire, owner, fn) &#125; go withRecover(func() &#123; _, _ = c.fetchNew(key, expire, owner, fn) &#125;) return r[0].(string), nil&#125;func (c *Client) strongFetch(key string, expire time.Duration, fn func() (string, error)) (string, error) &#123; debugf(\"strongFetch: key=%s\", key) owner := shortuuid.New() r, err := c.luaGet(key, owner) for err == nil &amp;&amp; r[1] != nil &amp;&amp; r[1] != locked &#123; // locked by other debugf(\"locked by other, so sleep %s\", c.Options.LockSleep) time.Sleep(c.Options.LockSleep) r, err = c.luaGet(key, owner) &#125; if err != nil &#123; return \"\", err &#125; if r[1] != locked &#123; // normal value return r[0].(string), nil &#125; return c.fetchNew(key, expire, owner, fn)&#125; 12345678910111213141516func (c *Client) fetchNew(key string, expire time.Duration, owner string, fn func() (string, error)) (string, error) &#123; result, err := fn() //自定义读取数据 if err != nil &#123; //成功则删除锁 _ = c.UnlockForUpdate(key, owner) return \"\", err &#125; if result == \"\" &#123; //如果结果为空 if c.Options.EmptyExpire == 0 &#123; // if empty expire is 0, then delete the key err = c.rdb.Del(c.rdb.Context(), key).Err() return \"\", err &#125; expire = c.Options.EmptyExpire &#125; err = c.luaSet(key, result, int(expire/time.Second), owner) //更新缓存 return result, err&#125; 总结 应该根据场景来设计合适的方案解决缓存一致性问题 读多写少的场景下，可以选择采用 Cache-Aside 结合消费数据库日志做补偿 的方案 写多的场景下，可以选择采用 Write-Through 结合分布式锁的方案 写多的极端场景下，可以选择采用 Write-Behind 的方案 可以通过读取 binlog (阿里云canal)异步删除缓存缓存 参考文档 https://blog.csdn.net/qq_34827674/article/details/123463175 https://learn.lianglianglee.com/专栏/300分钟吃透分布式缓存-完 分布式之数据库和缓存双写一致性方式解析 Cache-Aside Pattern Scaling Memcache at Facebook https://www.w3cschool.cn/architectroad/architectroad-cache-architecture-design.html https://cloud.tencent.com/developer/article/1932934 https://segmentfault.com/a/1190000040976439 https://talkgo.org/t/topic/1505 https://github.com/dtm-labs/rockscache/blob/main/helper/README-cn.md","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"微服务-02-一致性Hash","slug":"Microservices/微服务-02-一致性Hash","date":"2021-07-28T14:29:14.000Z","updated":"2023-08-10T04:15:17.065Z","comments":true,"path":"2021/07/28/Microservices/微服务-02-一致性Hash/","link":"","permalink":"http://xboom.github.io/2021/07/28/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-02-%E4%B8%80%E8%87%B4%E6%80%A7Hash/","excerpt":"","text":"问题背景 以缓存为例，在整个微服务系统中，缓存可以是多个节点 一是为了提高稳定，单节点宕机情况下，整个存储就面临服务不可用 二是数据容错，同样单节点数据物理损毁，而多节点情况下，节点有备份，除非互为备份的节点同时损毁 为了更好的访问各个节点，使用了Hash算法。把任意长度的输入通过散列算法变换成固定长度的输出，该输出就是散列值，对应不同的节点。如果直接使用取模的方式，如： value=key%Nvalue = key \\% N value=key%N 其中N为缓存节点(访问单元)的数目，当节点因为异常而退出的时候 如果未感知N的变化，那么单个节点所有key都无效导致无法访问，容易引起缓存雪崩 如果感知到N的变化 N = N - 1，那么整个集群所有的 Key 都移位，更加容易引起缓存雪崩 所以就引入了一致性Hash(Consistent Hash) 实现原理 基础类型 最基础的一致性 hash 算法就是把节点直接分布到环上，从而划分出值域， key 经过 hash(key) 之后，落到不同的值域，则由对应的节点处理(图中逆时针) 新增节点 当在N2节点与N3节点之间新增N5节点的时候，只有d的位置受到了影响，其他不变 删除节点 当删除N2节点的时候，也只有c的位置收到的影响 这样仍然存在问题： 随机分布节点的方式使得很难均匀的分布哈希值域(物理节点较少) 在动态增加节点后，即使原来是均匀分布后面也不再均匀分布； 增删节点带来的一个较为严重的缺点是： 当一个节点异常时，该节点的压力全部转移到相邻的一个节点； 当一个新节点加入时只能为一个相邻节点分摊压力； 虚拟节点 针对基础一致性 hash 的缺点一种改进算法是引入虚节点(virtual node)的概念。这个本质的改动：值域不再由物理节点划分，而是由固定的虚拟节点划分，这样值域的不均衡就不存在了。 注意： 虚拟节点的个数要远大于物理节点的个数 虚拟节点的分布是交错的，如果只是虚拟节点增加而不交叉，会无法在删除/新增节点时分摊压力 每个物理节点对应的虚拟节点也是相等的，可以使节点平均分配 操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点（旁白：所以这部分元数据是需要存下来的); 那么这个时候再来删除和新增节点的时候，就能很好的分摊压力了 增节点的时候要能为多个节点分摊压力 删节点的时候要能让多个节点承担压力 存在的问题 如何通过Key的Hash值找到对应范围所属的节点 认为Hash计算的值是随机的，符合概率分布。计算完所有节点的Hash之后需要进行一个排序 排序完怎么做到虚拟节点的交叉 通过计算每个虚拟节点的Hash值，然后整体进行排序 技术内幕 这里看看go-zero是如何实现一致性Hash的 对象定义 根据原理可知，实现一个一致性Hash需要知道 物理节点对应的虚拟节点的数目 Hash函数 物理节点与虚拟节点的对应关系 环中虚拟节点的排序 123456789101112131415161718192021222324252627282930313233const ( TopWeight = 100 //权重 minReplicas = 100 //虚拟节点:物理节点 prime = 16777619 //名称前缀)type ( // Func defines the hash method. Func func(data []byte) uint64 // A ConsistentHash is a ring hash implementation. ConsistentHash struct &#123; hashFunc Func //Hash函数 replicas int //虚拟节点的数量 keys []uint64 //保存虚拟节点的Hash ring map[uint64][]interface&#123;&#125; //存储虚拟节点与物理节点的关系 nodes map[string]lang.PlaceholderType //存储节点的信息 lock sync.RWMutex &#125;)// 可以理解为确定node字符串值的序列化方法// 在遇到哈希冲突时需要重新对key进行哈希计算// 为了减少冲突的概率前面追加了一个质数prime来减小冲突的概率func innerRepr(v interface&#123;&#125;) string &#123; return fmt.Sprintf(\"%d:%v\", prime, v)&#125;// 可以理解为确定node字符串值的序列化方法func repr(node interface&#123;&#125;) string &#123; return mapping.Repr(node)&#125; 新增节点 增加节点副本数 123456789101112131415161718192021222324252627func (h *ConsistentHash) AddWithReplicas(node interface&#123;&#125;, replicas int) &#123; h.Remove(node) //如果同名则先删除 if replicas &gt; h.replicas &#123; //虚拟节点数量不能超过一致性Hash数量 replicas = h.replicas &#125; nodeRepr := repr(node) //找到节点表达式 h.lock.Lock() defer h.lock.Unlock() h.addNode(nodeRepr) //添加添加 //添加虚拟节点 for i := 0; i &lt; replicas; i++ &#123; hash := h.hashFunc([]byte(nodeRepr + strconv.Itoa(i))) //计算虚拟节点名称获取Hash值 h.keys = append(h.keys, hash) //保存虚拟节点 h.ring[hash] = append(h.ring[hash], node) //为虚拟节点添加物理节点 &#125; sort.Slice(h.keys, func(i, j int) bool &#123; //进行虚拟节点的排序 return h.keys[i] &lt; h.keys[j] &#125;)&#125;func (h *ConsistentHash) addNode(nodeRepr string) &#123; h.nodes[nodeRepr] = lang.Placeholder&#125; 注意： h.ring[hash]这里使用的是切片，为了防止虚拟节点存在冲突，也就是一个虚拟节点可能对应多个物理节点。 设置权重节点 1234567// 按权重添加节点// 通过权重来计算方法因子，最终控制虚拟节点的数量// 权重越高，虚拟节点数量越多func (h *ConsistentHash) AddWithWeight(node interface&#123;&#125;, weight int) &#123; replicas := h.replicas * weight / TopWeight h.AddWithReplicas(node, replicas)&#125; 删除节点 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (h *ConsistentHash) Remove(node interface&#123;&#125;) &#123; nodeRepr := repr(node) //找到节点表达式 h.lock.Lock() defer h.lock.Unlock() //如果不包含该节点直接退出 if !h.containsNode(nodeRepr) &#123; return &#125; for i := 0; i &lt; h.replicas; i++ &#123; hash := h.hashFunc([]byte(nodeRepr + strconv.Itoa(i))) //虚拟节点Hash值 index := sort.Search(len(h.keys), func(i int) bool &#123; //查找虚拟节点 return h.keys[i] &gt;= hash //二分法进行节点查找 &#125;) if index &lt; len(h.keys) &amp;&amp; h.keys[index] == hash &#123; //如果是要找的虚拟节点则删除这个虚拟节点 h.keys = append(h.keys[:index], h.keys[index+1:]...) &#125; h.removeRingNode(hash, nodeRepr) //删除虚拟节点对应物理节点 &#125; //删除物理节点 h.removeNode(nodeRepr)&#125;func (h *ConsistentHash) removeRingNode(hash uint64, nodeRepr string) &#123; if nodes, ok := h.ring[hash]; ok &#123; newNodes := nodes[:0] for _, x := range nodes &#123; //遍历物理节点 if repr(x) != nodeRepr &#123; //如果物理节点与指定的不一样 newNodes = append(newNodes, x) //就把节点存入新的节点中 &#125; &#125; //虚拟节点存在多个物理节点则仅保留新的，否则直接删除虚拟节点 if len(newNodes) &gt; 0 &#123; h.ring[hash] = newNodes &#125; else &#123; delete(h.ring, hash) &#125; &#125;&#125;func (h *ConsistentHash) removeNode(nodeRepr string) &#123; delete(h.nodes, nodeRepr)&#125; 注意： 使用二分法进行节点查找 由于一个虚拟节点对应多个物理节点，不能直接删除虚拟节点 获取节点 根据v顺时针找到最近的虚拟节点，再通过虚拟节点映射找到真实节点 1234567891011121314151617181920212223242526272829func (h *ConsistentHash) Get(v interface&#123;&#125;) (interface&#123;&#125;, bool) &#123; h.lock.RLock() defer h.lock.RUnlock() if len(h.ring) == 0 &#123; return nil, false &#125; // 二分查找 // 找到比Hash的第一个节点，可能返回的 index = len(h.keys) // 所以这里取余回到第一个节点 hash := h.hashFunc([]byte(repr(v))) //Key的Hash值 index := sort.Search(len(h.keys), func(i int) bool &#123; return h.keys[i] &gt;= hash &#125;) % len(h.keys) nodes := h.ring[h.keys[index]] //获取虚拟节点对应物理节点 switch len(nodes) &#123; case 0: return nil, false case 1: return nodes[0], true default: //存在Hash冲突 // 取模的方式存获取真实物理节点 innerIndex := h.hashFunc([]byte(innerRepr(v))) pos := int(innerIndex % uint64(len(nodes))) return nodes[pos], true &#125;&#125; 注意: 如果虚拟节点对应多个物理节点那么通过hash获取在节点中的位置 总结 通过增加虚拟节点的数目来使的hash的分布更加均匀 虚拟节点的应该是交叉的(实现中并不是严格交叉，而是通过Hash再排序让虚拟节点交叉存在) 参考链接 分布式系统基石一 搞懂一致性Hash https://talkgo.org/t/topic/3098","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"MySql-03-索引","slug":"MySql/MySql-03-索引","date":"2021-07-25T23:18:54.000Z","updated":"2023-05-28T09:03:58.122Z","comments":true,"path":"2021/07/26/MySql/MySql-03-索引/","link":"","permalink":"http://xboom.github.io/2021/07/26/MySql/MySql-03-%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引是一种为了提升查询效率的数据结构，B+树索引并不能找到一个给定键值的具体行。只能找到数据行所在的页，然后数据库通过把页读到内存中进行查找。每个数据页都通过一个双向链表来进行链接 数据页存放的是完整的每行的记录，而在非数据页的索引页中，存放的仅仅是键值及指向数据页的偏移量，而不是一个完整的行记录 索引模型 哈希表：O(1)的时间复杂度，适用于等值查询，不支持范围查询 有序数组：lgN的时间复杂度(二分查找)，支持等值查询和范围查询，但插入效率低，适用于静态存储引擎(数据不再变化) 搜索树：InnoDB使用B+树结构建立索引，父节点左子树所有节点的值小于父节点的值，右子树所有节点的值大于父节点的值。 InnoDB的为N叉树，差不多为1200 MySql默认一个节点的长度为16K，整数(bigint)字段索引长度为 8B，每个索引还跟着6B的指向其子树的指针；所以16K/14B ≈ 1170 在 InnoDB 中，每一张表其实就是多个 B+ 树，即一个主键索引树和多个非主键索引树。 执行查询的效率，使用主键索引 &gt; 使用非主键索引 &gt; 不使用索引 为什么主键比非主键快(主键索引和非主键索引)？ 主键索引的b+树的叶子节点存储的是具体的行数据，非叶子节点存储的是主键的值。叶子节点之间通过链表连接，也称为 聚簇索引 非主键索引的叶子节点存储的是主键的值，所以通过非主键索引查询数据时，先找到主键，再去主键索引上根据主键找到具体的行数据，也称 二级索引。这个过程叫做回表 如果不使用索引进行查询，则从主索引 B+ 树的叶子节点进行遍历 例如：存在下列这样一个表 12345mysql&gt; create table T( id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6) 则 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引(clustered index) 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引(secondary index) 联合索引 联合索引(Composite Index)是指同时包含多个列的索引。 最左匹配原则：指的就是如果你的 SQL 语句中用到了联合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个联合索引去进行匹配，当遇到范围查询(&gt;、&lt;、between、like)就会停止匹配 假设对(a,b)字段建立一个索引 如果条件为下面是可以匹配索引的 12a = 1a = 1 and b = 2 如果条件为下面也是可以匹配索引的，因为优化器会优化语句 1b = 2 and a =1 如果条件为下面就不能匹配索引 1b = 2 如果使用对(a,b,c,d)建立索引，条件为下面内容，那么，a,b,c三个字段能用到索引，而d就匹配不到 1a = 1 and b = 2 and c &gt; 3 and d = 4 看一下几个联合问题就懂了 如果sql为 SELECT * FROM table WHERE a = 1 and b = 2 and c = 3; 那么如何建立索引 答：(a,b,c)或者(c,b,a)或者(b,a,c)都可以，重点要的是将区分度高的字段放在前面，区分度低的字段放后面。像性别、状态这种字段区分度就很低 如果sql为 SELECT * FROM table WHERE a &gt; 1 and b = 2; 那么如何建立索引 答：对(b,a)建立索引。如果建立的是(a,b)索引，那么只有a字段能用得上索引，毕竟最左匹配原则遇到范围查询就停止匹配。 如果对(b,a)建立索引那么两个字段都能用上，优化器会调整where后a,b的顺序，让语句用上索引 如果sql为 SELECT * FROM table WHERE a &gt; 1 and b = 2 and c &gt; 3; 那么如何建立索引 答: (b,a)或者(b,c)都可以 如果sql为 SELECT * FROM table WHERE a = 1 and b = 2 and c &gt; 3; 那么如何建立索引 答：(b,a,c)或者(a,b,c)都可以 如果sql为 SELECT * FROM table WHERE a = 1 ORDER BY b;那么如何建立索引 答：对(a,b)建索引，当a = 1的时候，b相对有序，可以避免再次排序！ 如果sql为 SELECT * FROM table WHERE a &gt; 1 ORDER BY b;那么如何建立索引 答：对(a)建立索引，因为a的值是一个范围，这个范围内b值是无序的，没有必要对(a,b)建立索引 如果sql为 SELECT * FROMtableWHERE a = 1 AND b = 2 AND c &gt; 3 ORDER BY c;那么如何建立索引 答：对(a，b)建立索引，此时c排序是用不到索引的 如果sql为 SELECT * FROM table WHERE a IN (1,2,3) and b &gt; 1; 那么如何建立索引 答：对(a，b)建立索引，因为IN在这里可以视为等值引用，不会中止索引匹配，所以还是(a,b)! 如果sql为 SELECT * FROM table WHERE a = 1 AND b IN (1,2,3) AND c &gt; 3 ORDER BY c;那么如何建立索引 答：对(a，b)建立索引，此时c排序是用不到索引的。 它的结构与单列索引（单列的非聚簇索引）有一些不同。以下是联合索引的一般结构： 排序顺序：联合索引按照索引键的排序顺序组织数据。联合索引的键由多个列组成，并且按照从左到右的顺序进行排序。 索引键值：联合索引的叶子节点存储了联合索引键的值。这些索引键值是根据多个列的值生成的，用于定位到相应的行数据。 主键引用：为了获取具体的行数据，联合索引的叶子节点通常会包含指向主键的引用。这个主键引用可以是主键的值，或者是指向包含主键值的位置的指针。通过主键引用，可以从联合索引快速地定位到相应的行数据。 覆盖索引：如果联合索引包含了查询所需的所有列，称为覆盖索引（Covering Index）。在这种情况下，可以直接从联合索引中获取所需的数据，而无需回表操作，提高查询性能。 索引失效的情况： 失效指的是，条件中如果有or，只要其中一个条件没有索引，其他字段有索引也不会使用 索引维护 在B+树中，所有记录节点都是按键值的大小顺序存放在同一层的叶子节点上，由各叶子节点进行连接 默认使用升序排序，在升序排序中，索引键的值按照从小到大的顺序进行排序。对于数字类型的列，从小到大表示较小的值排在前面；对于字符串类型的列，按照字典顺序排序，即按照字符的ASCII码进行比较 B+树的插入操作 Leaf Page 满 Index Page 满 操作 No No 1. 直接将数据插入叶子节点 Yes No 1. 拆分叶子节点2. 将中间的额节点存放入 Index Page中3. 小于中间节点的记录放左边4. 大于或等于中间节点的记录方右边 Yes Yes 1. 拆分叶子节点2. 小于中间接待你的记录方左边3. 大于或等于中间节点的记录放右边4. 拆分Index Page5. 小于中间节点的记录放左边6. 大于中间节点的记录放右边7. 中间节点放入上一层 Index Page 旋转发生在Leaf Page已经满了，但是其左右兄弟节点并没有满的情况下。B+树并不会急于拆分页的操作，而是将记录移动所在页的兄弟节点上。通常情况下，左兄弟会被首先检查用来做旋转操作 B+树的删除操作 叶子节点小于填充因子 中间节点小于填充因子 操作 No No 1. 直接将记录从叶子节点删除，如果该节点还是Index Page的节点，用该节点的右节点代替 Yes No 1. 合并叶子节点和它的兄弟节点，同时更新Index Page Yes Yes 1. 合并叶子节点和它的兄弟节点2. 更新Index Page3. 合并Index Page 和它的兄弟节点 最好使用自增主键做为主索引，如上图所示：参见 B+树的插入与删除 如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。 如果新插入的 ID 值为 400，需要逻辑上挪动后面的数据，空出位置。而更糟的情况是： 如果 R5 所在数据页满了，则申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂，影响性能 原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程，影响空间利用率 所以在主键选择的时候： 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 在KV场景下只有一个索引，且必须是唯一索引，在直接使用业务字段做主键 索引要根据表中的每一行的记录值来创建，所以需要全表扫描 加字段或修改字段，也要修改每一行记录中的对应列的数据，所以也要全表扫描 查询过程 执行查询语句 select id from T where id = 5 id为普通索引，查询过程为： 先通过 B+ 树从树根开始，按层搜索到叶子节点及数据页 然后在数据页内部通过二分法来定位记录 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索 InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB 更新过程 redo log是物理日志，记录的是数据页的修改，但是如果数据页不在内存中怎么办呢？都没有去修改数据页，redo log中记什么？所以这时候change buffer登场了，如果修改的不是唯一索引，而是普通索引，不用再去磁盘随机IO了，直接将这个修改记录在change buffer中。也就是说 如果数据页在内存，那就修改数据页，写redo log， 如果数据页不在内存，修改的也不是唯一索引，而是普通索引，那就写change buffer。并把这个change buffer写入到redo log，防止更新丢失。 比如在表中插入一条数据 1mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2); 假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图 数据表空间：就是一个个的表数据文件，对应的磁盘文件就是“表名.ibd”； 系统表空间：用来放系统信息，如数据字典等，对应的磁盘文件是“ibdata1” 更新语句的操作如下： Page 1在内存中，直接更新内存 Page 2 不在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中 做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。同时 写入 change buffer 和 数据表空间都是后台空间，不影响更新的响应时间 然后执行查询语句，如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了 1select * from t where k in (k1, k2) 读 Page 1 的时候，直接从内存返回。 WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗 change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成； redo log 记录的是change buffer页的变化。 change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？ change buffer有一部分在内存有一部分在ibdata. 做purge操作,应该就会把change buffer里相应的数据持久化到ibdata redo log里记录了数据页的修改以及change buffer新写入的信息 如果掉电,持久化的change buffer数据已经purge,不用恢复。主要分析没有持久化的数据 情况又分为以下几种: change buffer写入,redo log虽然做了fsync但未commit,binlog未fsync到磁盘,这部分数据丢失 change buffer写入,redo log写入但没有commit,binlog以及fsync到磁盘,先从binlog恢复redo log,再从redo log恢复change buffer change buffer写入,redo log和binlog都已经fsync.那么直接从redo log里恢复。 merge 的执行流程是这样的：从磁盘读入数据页到内存（老版本的数据页）；从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 普通索引和唯一索引的查询性能几乎一样, 但是写性能是普通索引快, 因为可以用到change buffer, 唯一索引会导致内存命中率下降 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。 设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50% 设置为0 表示关闭change buffer功能 索引选择 在 MySQL 中一张表可以支持多个索引。但写 SQL 语句的时候并没有主动指定使用哪个索引，MySql是怎么决定使用哪个索引的 1force Index(a) #强制使用指定的索引 使用下面三条SQL语句查看查询过程 123set long_query_time&#x3D;0;select * from t where a between 10000 and 20000; &#x2F;*Q1*&#x2F;select * from t force index(a) where a between 10000 and 20000;&#x2F;*Q2*&#x2F; 将慢查询的日志阀值设置为0，则接下来所有查询操作都记录到慢查询日志中 Q1 使用默认查询语句 Q2 强制使用索引a进行查询 优化器的逻辑 行数 临时表 是否排序 扫描行数是怎么判断的？ MySql在执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好 使用 show index 方法，看到一个索引的基数。 如果正确的使用索引 使用强制语句 force index(a) 引导 MySQL 使用我们期望的索引，如 order by b limit 1 改成 order by b,a limit 1 优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序(b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历)，所以即使扫描行数多，也判定为代价更小。现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a 索引失效的情况： 不使用索引列进行查询：如果查询条件中没有使用索引列，数据库引擎将无法使用索引进行优化，而是需要进行全表扫描。这种情况下索引将失效，查询性能会下降。 使用函数或表达式对索引列进行操作：当在索引列上应用函数、运算符或表达式时，索引可能无法被利用。数据库引擎无法直接在索引上执行这些操作，而是需要在数据行级别上执行，导致索引失效。 列类型不匹配：如果查询条件中的列类型与索引列的类型不匹配，索引也无法被使用。例如，索引列是字符串类型，但查询条件中使用了数字类型，或者索引列是日期类型，但查询条件中使用了字符串类型。 数据量过小：如果表中的数据量非常小，即使存在索引，数据库引擎可能会选择全表扫描而不是使用索引。这是因为全表扫描的成本较低，使用索引反而增加了查询的开销。 索引列上存在数据类型转换：如果查询条件中的值需要进行数据类型转换才能与索引列进行比较，索引可能会失效。例如，查询条件中的值是字符串类型，而索引列是整数类型，会导致索引失效。 索引列上存在NULL值：某些情况下，如果索引列包含NULL值，查询条件中包含对该列的判断，索引可能无法被使用。 数据分布不均匀：如果索引列的数据分布不均匀，即某些值的出现频率较高，而其他值的出现频率较低，索引的选择性较低，可能导致索引失效。 要避免索引失效，需要根据查询条件、数据类型、数据分布等因素进行索引的设计和优化。同时，使用正确的查询方式，避免对索引列进行函数操作或类型转换，以确保索引能够被有效利用，提高查询性能。 参考链接 《高性能myql》 《InnoDB技术内幕-索引与算法》 https://time.geekbang.org/column/article/70848 https://www.cnblogs.com/rjzheng/p/12557314.html","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql-04-锁","slug":"MySql/MySql-04-锁","date":"2021-07-25T03:20:33.000Z","updated":"2023-06-28T14:42:08.382Z","comments":true,"path":"2021/07/25/MySql/MySql-04-锁/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql-04-%E9%94%81/","excerpt":"","text":"学习 mysql 的锁主要的目的就是 mysql 都有什么锁 平时写语句的时候都回用到什么样的锁 如何排查死锁问题 lock与latch 数据库中lock与latch都称为锁，但两者有截然不同的意思 latch 称为闩锁(轻量级)，要求锁的时间必须非常短。在Innodb引擎中，有分mutex(互斥锁)和rwlock(读写锁)。用来保证并发线程操作临界资源的正确性，没有死锁检测机制 lock的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit或rollback后进行释放(补孕酮事务隔离级别释放的时间可能不一样) 全局锁 如果数据库需要进行全局逻辑备份 在支持事务的存储引擎中，在可重复读隔离级别下开启一个事务，数据库备份的同时可以更新(mysqldump工具) 在不支持事务的存储引擎中，需要通过FTWRL方法进行全局加锁备份 要全库只读，为什么不使用set global readonly=true? 有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。 且在slave上，用户有超级权限的话readonly是失效的 在异常处理机制上有差异。 如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。 如果将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 全局锁是对整个数据库实例加锁，整个数据库处于只读状态，下面语句将会被阻塞： 数据更新语句(数据的增删改) 数据定义语句(建表、修改表结构等) 更新类事务的提交语句 加锁 Flush tables with read lock(FTWRL) 解锁 unlock tables 整个数据库只读可能存在的问题： 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟 表级锁 MySql里面表级别的锁有两种： 表锁 元数据锁（meta data lock，MDL) 表锁的语法是 lock tables … read/write，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放 需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象 MDL(metadata lock)，MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。 DDL操作对表加MDL读锁，保证的是表结构不能修改，而与表数据无关，是可以CRUD的 DML操作对表加MDL写锁，保证的是表结构不能被并行修改，同时表数据也不能读了 如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的 虽然MDL是默认添加的，但还是会碰到一个问题：给一个小表加个字段，导致整个库挂了 因为sessionB加了MDL读锁，导致后面的sessionC阻塞。如果sessionB一直没有完成select，那么sessionC申请写锁被阻塞，将会导致后面的sessionD等申请读锁都被阻塞。这个时候客户端如果有频繁重试的逻辑就会导致不停的和数据库建立连接，把连接池打满导致库不可用 事务中的 MDL 锁，在语句执行开始时申请，但在语句结束后并不会马上释放，而会等到整个事务提交后释放 如何安全的给数据库加字段？ 首先需要解决长事务问题：事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务 如果要变更的表是一个热点表，虽然数据量不大但请求很频繁，而你不得不加个字段，你该怎么做呢？ 比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程 行锁 行锁是由各个引擎自己实现的，而有的引擎并不支持行锁如MyISAM InnoDB实现了如下两种标准的行级锁： 共享锁(S Lock)，允许事务读一行数据 排他锁(X Lock)，允许事务删除或更新一行数据 InnoDB支持多粒度(granular)锁，允许事务在行级上的锁和表级上的锁同时存在，称为意向锁(Intention Lock)，表级别的锁 意向共享锁(IS Lock)，事务想要获得一张表中某几行的共享锁 意向排他锁(IX Lock)，事务想要获得一张表中某几行的排他锁 由于InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫描以外的任何请求 IS IX S X IS 兼容 兼容 兼容 不兼容 IX 兼容 兼容 不兼容 不兼容 S 兼容 不兼容 兼容 不兼容 X 不兼容 不兼容 不兼容 不兼容 若将上锁的对象看成一颗树，那么对最下层的对象上锁，首先要对上层对象上锁 当事务A commit结束之后，事务B才执行 InnoDB存储引擎有3种行锁的算法，分别是： Record Lock: 单个行记录上的锁 Gap Lock: 间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock: Gap Lock + Record Lock，锁定一个范围，并且锁定记录本身 在REPEATABLE READ下，存储引擎使用Next-Key Locking机制来避免幻读(一个事务中，两次读到的数据数量不一致) 其他锁 一致性非锁写读 在默认配置即事务隔离级别为REPEATABLE READ模式下，InnoDB存储引擎的SELECT操作使用一致性非锁定读 一致性非锁定读(consistent nonlocking read)是指InnoDB存储引擎通过行多版本控制(multi versioning)的方式来读取当前执行时间数据库中的行的数据。如果读取的行正在执行DELETE或UPDATE操作，读取操作不会等待行上锁的释放而是去读取行的一个快照 快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。undo用来在事务中进行回滚的，因此快照数据本身没有额外的开销 一个行记录可能不止一个快照数据，由此的并发控制，称为多版本并发控制(Multi Version Concurrency Control, MVCC) 在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据 在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本 一致性锁定读 某些情况下需要显示对数据库读取操作加锁已保证数据逻辑一致性。对于SELECT的只读操作，InnoDB存储引擎对于SELECT语句支持两种一致性的锁定读(locking read) SELECT … FOR UPDATE : 对读取的行记录加一个X锁，其他事务不能加上任何锁 SELECT … LOCK IN SHARE MODE： 对读取的行记录加一个S锁，其他事务锁定的行加S锁，但是如果加X锁，会被阻塞 自增长与锁 在InnoDB存储引擎的内存结构中，对每个含有自增长值的表都有一个自增长在计数器(auto-increment counter)，对含有自增长的计数器的表进行插入操作时，这个计数器会被初始化，执行如下的语句来得到计数器的值： select MAX(auto_inc_col) FROM t FOR UPDATE; 插入操作会依据这个计数器值加1服务自增长列，称为AUTO-INC Locking。它并不是在一个事务完成后才释放，而是在完成对自增长值插入的SQL语句后立即释放 自增长值的列必须是缩影，同时必须是索引的第一列，不是则抛出异常 死锁和死锁检查 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略： 一种策略是，直接进入等待直到超时。超时时间可通过参数 innodb_lock_wait_timeout 来设置(默认50s)。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑 不能将第一种策略的超时时间设置的太短，可能导致误伤了不是死锁的情况 每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。死锁检测会消耗大量的CPU。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的 怎么解决由这种热点行更新导致的性能问题呢？ 可以考虑将一行改成逻辑上的多行来减少冲突。如果一个账户的金额等于这10条记录的总和，这样给一个账号添加金额的时候可以随机选择一条来进行，这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗 参考链接 https://time.geekbang.org/column/article/69862 https://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&amp;mid=2651961508&amp;idx=1&amp;sn=9f31a95e5b8ec16fa0edc7de6087d2a1&amp;chksm=bd2d0d788a5a846e3bf16d300fb9723047bd109fd22682c39bdf7ed4e77b167e333460f6987c&amp;scene=21#wechat_redirect","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"微服务-01-布隆过滤器","slug":"Microservices/微服务-01-布隆过滤器","date":"2021-07-24T16:36:57.000Z","updated":"2023-08-10T04:15:11.865Z","comments":true,"path":"2021/07/25/Microservices/微服务-01-布隆过滤器/","link":"","permalink":"http://xboom.github.io/2021/07/25/Microservices/%E5%BE%AE%E6%9C%8D%E5%8A%A1-01-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","excerpt":"","text":"如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢。三种结构的检索时间复杂度分别为O(n)，O(logn)，O(1) 哈希函数 哈希函数的概念是：将任意大小的输入数据转换成特定大小的输出数据的函数，转换后的数据称为哈希值或哈希编码，也叫散列值。下面是一幅示意图 所有散列函数都有如下基本特性： 如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。 散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“散列碰撞（collision）”。 布隆过滤器 BloomFilter 是由一个固定大小的二进制向量或者位图（bitmap）和一系列映射函数组成的。 在初始状态时，对于长度为 m 的位数组，它的所有位都被置为0，如下图所示： 当有变量被加入集合时，通过 K 个映射函数将这个变量映射成位图中的 K 个点，把它们置为 1（假定有两个变量都通过 3 个映射函数） 查询某个变量的时候我们只要看看这些点是不是都是 1 就可以大概率知道集合中有没有它了 如果这些点有任何一个 0，则被查询变量一定不在； 如果都是 1，则被查询变量很可能存在 Coding 添加元素 将要添加的元素给 k 个哈希函数 得到对应于位数组上的 k 个位置 将这k个位置设为 1 查询元素 将要查询的元素给k个哈希函数 得到对应于位数组上的k个位置 如果k个位置有一个为 0，则肯定不在集合中 如果k个位置全部为 1，则可能在集合中 特点 一个元素如果判断结果为存在的时候元素不一定存在，但是判断结果为不存在的时候则一定不存在。 布隆过滤器可以添加元素，但是不能删除元素。同一个位置可能被多个值引用，删掉元素会导致误判率增加 解决办法： 定时异步重建布隆过滤器 计数Bloom Filter 过滤器的值存储到Redis中 代码实现 总结 优点： 相比于其它的数据结构，布隆过滤器存储空间和插入/查询时间都是常数 O(K)，另外，散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 布隆过滤器可以表示全集，其它任何数据结构都不能； 缺点： 随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加 1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 怎么降低误判几率： 增加二进制位数 增加Hash次数 布隆过滤器数据其实也是存在Redis中的 应用场景： 数据库防止穿库 Google Bigtable，Apache HBase和Apache Cassandra以及Postgresql 使用BloomFilter来减少不存在的行或列的磁盘查找。避免代价高昂的磁盘查找会大大提高数据库查询操作的性能。 如同一开始的业务场景。如果数据量较大，不方便放在缓存中。需要对请求做拦截防止穿库。 缓存宕机 缓存宕机的场景，使用布隆过滤器会造成一定程度的误判。原因是除了Bloom Filter 本身有误判率，宕机之前的缓存不一定能覆盖到所有DB中的数据，当宕机后用户请求了一个以前从未请求的数据，这个时候就会产生误判。当然，缓存宕机时使用布隆过滤器作为应急的方式，这种情况应该也是可以忍受的。 WEB拦截器 相同请求拦截防止被攻击。用户第一次请求，将请求参数放入BloomFilter中，当第二次请求时，先判断请求参数是否被BloomFilter命中。可以提高缓存命中率 恶意地址检测 chrome 浏览器检查是否是恶意地址。 首先针对本地BloomFilter检查任何URL，并且仅当BloomFilter返回肯定结果时才对所执行的URL进行全面检查（并且用户警告，如果它也返回肯定结果）。 比特币加速 bitcoin 使用BloomFilter来加速钱包同步 参考链接 布隆过滤器 布隆过滤器","categories":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"}],"tags":[{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"}]},{"title":"MySql-02-文件","slug":"MySql/MySql-02-文件","date":"2021-07-24T02:44:44.000Z","updated":"2023-05-29T16:43:12.368Z","comments":true,"path":"2021/07/24/MySql/MySql-02-文件/","link":"","permalink":"http://xboom.github.io/2021/07/24/MySql/MySql-02-%E6%96%87%E4%BB%B6/","excerpt":"","text":"构成Mysql和InnoDB存储引擎表的各类文件包括： 参数文件：初始化参数、内存结构大小等设置 日志文件：记录MySql实例的各种类型活动 socket文件：当用UNIX域套接字方式进行连接时所需要的文件 pid文件：MySql实例的进程ID文件(作用是？) 存储引擎文件：每个引擎会有自己的文件保存数据(如记录和索引) 参数文件 当MySql实例启动时，会先读参数文件(如果没有则使用默认值)，用来初始化数据库 1234# 1. 查看数据库参数文件mysql --help |grep my.cnf# 2. 查看数据库参数SHOW VARIABLES 日志文件 MySql常见的日志文件有： 错误日志文件(error log) 二进制日志文件(binlog) 慢查询日志文件(slow query log) 查询日志文件(log) 慢查询日志 开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能 可用于定位可能存在问题的SQL语句，默认情况下并不启动慢查询日志 12345678#查询慢查询阀值(等于阀值的并不会记录)SHOW VARIABLES LIKE 'long_query_time'#查询慢查询开关SHOW VARIABLES LIKE 'log_slow_queries'#慢查询没有使用索引(如果运行SQL而没有使用索引)SHOW VARIABLES LIKE 'log_queries_not_using_indexes'#表示每分钟允许记录到slow log且未使用索引的SQL语句(默认为0，防止语句频繁记录到slow log导致文件不断增大)SHOW VARIABLES LIKE 'log_throttle_queries_not_using_indexes' 12#查看慢查询日志mysqldumpslow nh122-190-slow.log MySQL5.1 开始 可以将慢查询的日志记录放入一张表中，名为 slow_log 123456789101112#指定慢查询输出的格式，默认为FILE#TABLE则将日志写入slow_log表中SHOW VARIABLES LIKE &#39;log_output&#39;#指定逻辑IO次数超过的记录写入slow log(默认100)SHOW VARIABLES LIKE &#39;long_query_io&#39;SHOW VARIABLES LIKE &#39;slow_query_type&#39; #表示slow log的启动方式#0 表示不将sql语句记录到slow log#1 表示根据运行时间将sql语句记录到slow log#2 表示根据逻辑IO次数将sql语句记录到slow log#3 表示根据运行时间及逻辑IO次数将sql语句记录到slow log 逻辑IO：包含所有的读取，不管是磁盘还是缓冲池 物理IO：指从磁盘进行IO读取的次数 查看日志文件：记录了对MySql数据库所有请求的信息，无论这些请求是否得到正确的执行。默认文件名为：主机名.log。和慢查询日志一样，可以将查询日志放入mysql架构下的general_log表中 二进制日志 记录了对MySql数据库执行更改的所有操作，但不包括select和show这类操作，因为这类对数据本身并没有修改 1SHOW BINLOG EVENTS #查看二进制日志 主要有以下几点作用： 恢复(recovery)：某些数据的恢复需要二进制日志。例如在一个数据库全备文件恢复后，用户可通过二进制日志进行point-in-time的恢复 复制(replication)：通过复制和执行二进制日志使一台远程MySql数据库(一般为slave或standby)与一台MySql数据库(master或primary)进行实时同步 审计(audit)：用户可以通过二进制日志中的信息来进行审计，判断是否有对数据库进行注入攻击 12#启动二进制文件my.cnflog-bin[=name] 如果不指定name，则默认二进制日志文件名为主机名，后缀为二进制日志的序列号(bin_log.00001)，所在路径为数据库所在目录(datadir) 其他影响二进制日志记录的参数： max_binlog_size：指定单个二进制日志文件的最大值，超过则新建二进制日志文件，后缀序列号+1 并记录到.index文件中(默认1G) binlog_cache_size： 当使用事务的表，所有未提交的二进制日志会被记录到一个缓存中，等该事务提交再将缓冲写入二进制日志文件。缓存大小由binlog_cache_size决定(默认32K) binlog_cache_size是基于会话的，但一个线程开始一个事务，就会自动分配一个binlog_cache_size 当事务记录大小大于缓冲时，回将缓冲中的日志写入一个临时文件中 缓冲区大小的设置可以通过 SHOW GLOBASL STATUS命令查看 binlog_cache_use：记录使用缓冲区二进制日志的次数 binlog_cache_disk_use：记录使用临时文件写二进制日志的次数 binlog-do-db：表示需要写入哪些库的日志 binlog-ignore-db：表示需要忽略写入哪些库的日志 log-slave-update：如果是slave角色，则不会将从master取得并执行的二进制日志写入自己的二进制日志文件中。如果需要写入，则设置log–slave-update。如果搭建master=&gt;slae=&gt;slave架构复制，必须设置该值 Binlog-format：日志存储的不同格式(STATEMENT/ROW/MIX) 其他文件 套接字文件：UNIX系统下本地连接MySql可采用UNIX域套接字方式，需要一个套接字文件， 12#查看套接字文件位置SHOW VARIABLES LIKE 'socket' PID文件：当MySQL实例启动，会将自己的进程ID写入到pid文件 12#查看PID文件位置SHOW VARIABLES LIKE 'pid_file' InnoDB存储引擎文件 表空间文件 InnoDB将存储的数据按表空间进行存放。默认配置下会有一个初始大小为10MB的默认表空间文件，名为ibdata1的文件。 1Innodb_data_file_path = /db/ibdata1:2000M;/dr2/db/ibdata2:2000M:autoextend 将/db/ibdata1和/dr2/db/ibdata2两个文件组成表空间。 若两个文件处于不同的磁盘上，磁盘的负载可能被平均。可以提供数据库的整体性能 如果文件ibdata2用完了，则可以继续自动增长(autoextend) 设置innodb_data_file_path之后，所有基于InnoDB存储引擎的表数据都会记录到该共享表空间 设置innodb_file_per_table,将每个基于InnoDB的表都产生一个独立表空间，命令规则为：表名.ibd 注意：单独表空间文件仅存储该表的数据、索引和插入缓冲BITMAP等信息，其余信息还存放在默认表空间中 单独表空间是指将每个表或索引存储在单独的表空间文件中。这种方式可以通过使用InnoDB存储引擎的&quot;innodb_file_per_table&quot;选项来启用。当&quot;innodb_file_per_table&quot;选项设置为ON时，每个表或索引都会存储在一个以表名命名的.ibd文件中，而不是默认的表空间文件。这种方式可以提供更好的灵活性和管理性，例如可以独立地备份、还原或迁移单个表或索引 默认表空间：默认表空间是MySQL数据库的默认存储方式。在默认情况下，表和索引数据存储在数据库目录下的与数据库名相同的文件中。例如，对于名为&quot;mydatabase&quot;的数据库，表和索引数据会存储在&quot;mydatabase.ibd&quot;文件中。默认表空间适用于InnoDB存储引擎和其他支持表空间概念的存储引擎 Redo Log 文件 默认情况下，在InnoDB存储引擎的数据目录下会有两个名为ib_logfile0和ib_logfile1的文件，称为重做日志文件(redo log file)。用于记录对于InnoDB存储引擎的事务日志。 每个InnoDB存储引擎至少有1个重做日志文件组(group)，每个文件组至少有2个重做日志文件，如默认的ib_logfile0和ib_logfile1。 为了得到更高的可靠性，可设置多个镜像日志组(mirrored log groups)，将不同的文件组放在不同的磁盘上 采用循环写入的方式运行：在三个重做日志文件的重做日志文件组中，InnoDB存储引擎先写重写日志文件ib_logfile0，当到达文件最后时，会切换直重做日志文件1，当文件1也满了切到文件2，最后回到文件0，重复循环 innodb_log_file_size：指定每个日志文件大小(&lt;512GB) innodb_log_files_in_group：日志文件组中重做日志文件的数量，默认为2 innodb_mirrored_log_groups：指定日志镜像文件组的数量，默认为1(若磁盘高可用，如磁盘阵列，可不开) Innodb_log_group_home_dir：指定日志文件组所在的目录 重做日志文件不能设置的太大，如果设置太大，恢复时需要太长时间，如果设置太小，可能导致一个事务的日志需要多次切换重做日志文件，会导致频繁的发生 async checkpoint 重做日志文件与二进制日志区别： 二进制日志记录所有与数据库相关的日志，而InnoDB存储的重做日志只记录该InnoDB本身的事务日志 二进制日志记录的都是关于一个事务的具体操作内容，即该日志为逻辑日志。而重做日志记录的是关于每个页(Page)的更改的物理情况 写入时间不同，二进制日志仅在事务提交前进行提交，即只写磁盘一次，不论事务多大。而在事务进行的过程中，会不断有重做日志条目(redo entry)被写入到重做日志文件中 重做日志由四部分组成： redo_log_type占用1字节，表示重做日志的类型 space表示空间的ID，但采用压缩的方式，占用空间可能小于4字节 page_no表示页的偏移量，铜梁采用压缩的方式 redo_log_body表示每个重做日志的数据部分，恢复时需要调用相应的函数进行解析 重做日志文件的操作不是直接写，而是先写入一个重做日志缓冲(redo log buffer)，再按照一定的条件顺序地写入日志文件 重做日志缓冲进行磁盘写入时，是按512字节也就是一个扇区的大小进行写入。因为扇区是写入的最小单位，因此可以保证写入必定是成功的 写入条件： 主线程中每秒会将重做日志缓冲写入磁盘的重做日志文件中，不论事务是否已经提交 另外一个由参数 innodb_flush_log_at_trx_commit控制，表示提交(commit)操作时，处理重做日志方式 0 代表等待主线程每秒的刷新 1 表示执行commit时将重做日志缓冲同步写到磁盘，伴有fsync的调用 2 表示重做日志异步写到磁盘，即写到文件系统的缓存中，不能保证在执行commit时肯定会写入重做日志文件 为了保证事务的持久性，必须将innodb_flush_log_at_trx_commit设置为1，即使宕机也能通过重做日志文件恢复 0和2都有可能恢复时部分事务丢失，不同在于，为2时如果数据库宕机而操作系统没有宕机，此时日志保存在文件系统缓存中，恢复时同样能保证数据不丢失 Undo Log 文件 Undo Log通常存储在Undo Log文件中，这些文件位于InnoDB存储引擎的表空间中。每个Undo Log文件的大小由innodb_undo_log_truncate参数控制，默认大小为10MB。 Undo Log文件记录了事务执行过程中对数据所做的修改操作，以及旧数据的备份。在事务提交之前，对数据的修改操作都会先记录到Undo Log中。如果事务回滚或需要进行一致性读取，可以使用Undo Log中的信息来还原数据到事务之前的状态。 Undo Log文件的命名规则通常是undo_&lt;ID&gt;.log，其中&lt;ID&gt;是一个递增的标识符，表示Undo Log文件的编号。 Undo Log文件是InnoDB存储引擎的特定功能，其他存储引擎如MyISAM并不使用Undo Log文件。 写入过程 只要redo log和binlog保证持久化到磁盘，就能确保MySql异常重启后，数据可以恢复 Bin Log 的写入机制 首先需要明确，不是所有的操作都会记录到bin log中 SELECT语句：SELECT语句用于读取数据，它通常不会写入二进制日志，因为它只涉及数据的查询和检索，而没有对数据进行修改。 DDL语句（Data Definition Language）：DDL语句用于创建、修改或删除数据库对象（例如表、索引、视图等），例如CREATE、ALTER和DROP语句。这些操作通常不会写入二进制日志，因为它们可以通过备份和还原数据库来进行恢复。 TRUNCATE TABLE语句：TRUNCATE TABLE语句用于快速删除表中的所有数据。与DELETE语句不同，TRUNCATE TABLE语句不会写入二进制日志，因为它是一种快速的操作，可以通过删除底层数据文件来完成。 内部临时表操作：在某些情况下，MySQL使用内部临时表来执行查询和排序操作。这些内部临时表的创建和操作通常不会写入二进制日志 如果MySQL服务器的日志格式设置为&quot;ROW&quot;格式，那么即使某些语句本身不会写入二进制日志，但对于表的修改操作（例如INSERT、UPDATE、DELETE）的行级别变化仍然会被写入二进制日志。因此，即使某些语句本身不会被记录，表的修改仍然可以通过解析二进制日志进行恢复或复制 事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中 系统为每个线程分配了一片binlog cache内存，但共用一份binlog文件 参数binlog_cache_size控制单个线程内binlog cache大小，一个事务的binlog是不能拆分的(不论事务多大，都要确保一次性写入)，如果超过这个大小就要暂存到磁盘 事务提交的时候，执行器把binlog cache 里完整的事务写入binlog中，并清空binlog cache 12ssize_t write(int fd, const void *buf, size_t count);int fsync(int fd); write 是把日志写入到文件系统的page cache，内存中，没有持久化到磁盘，所以速度比较快，函数返回并不代表已经写入磁盘 fsync 是将数据持久化到磁盘，占用磁盘的IOPS，通知内核将数据写到硬盘中 write 和 fsync 的时机，是由参数 sync_binlog 控制的 sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync(如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志) 当执行UPDATE语句时，是否写入二进制日志（Binary Log）取决于二进制日志的设置以及执行的上下文。下面是一些常见的情况： 二进制日志已启用：如果二进制日志已启用（通过配置文件或运行时设置），则UPDATE语句通常会被写入二进制日志。这是默认行为，旨在确保数据更改能够在主从复制等场景中进行传播。 非事务环境：如果不在事务环境中执行UPDATE语句，它通常会立即写入二进制日志。每个UPDATE语句都会作为一个单独的事件记录到二进制日志中。 事务环境：如果在事务中执行UPDATE语句，并且事务尚未提交，则UPDATE语句不会立即写入二进制日志。在事务提交之前，对同一事务的多个修改操作会被合并为一个事件写入二进制日志。当事务提交时，包含UPDATE操作的完整事务将被写入二进制日志。 redo log的写入机制 redo log存在三种状态： 存在redo log buffer中，物理上是在MySql进程内存中 写到磁盘(write)，但是没有持久化(fsync)，物理上是在文件系统的page cache 持久化到磁盘，对应的是hard disk 为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数 0 表示每次事务提交时都只是把 redo log 留在 redo log buffer 中，等待主线程每秒刷新 1 表示每次事务提交时都将 redo log 直接持久化到磁盘(fsync)； 2 表示每次事务提交时都只是把 redo log 写到 page cache(文件系统缓存) 另外InnoDB有一个后台线程，每隔1s把redo log buffer中的日志，调用write写到文件系统的page cache,然后调用fsync持久化到磁盘 注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也可能已经持久化到磁盘的 实际上，除了后台线程没每秒一次的轮询操作，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中 redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache 并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘 一条update语句的执行过程： 执行器先找引擎取 ID=2 这一行 如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器 否则，需要先从磁盘读入内存，然后再返回 执行器拿到引擎给的行数据，更新行得到新的一行数据，再调用引擎接口写入这行新数据 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务 执行器生成这个操作的 binlog，并把 binlog 写入磁盘 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成 问题 为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的 一个事务的 binlog 必须连续写，整个事务完成后，再一起写到文件里。而 redo log 记录的是，中间有生成的日志可以写到 redo log buffer 中。其他事务提交的时候可以被一起写到磁盘中 为什么执行器更新数据之后不直接提交事务接口，而是和引擎有一个交互(这里是redo log)，其他引擎并没有redo log 更新的时候数据量太大，内存放不下怎么办 LRU 参考链接 《MySql技术内幕》 https://time.geekbang.org/column/article/76161","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql-01-一条sql","slug":"MySql/MySql-01-一条sql","date":"2021-07-21T23:20:02.000Z","updated":"2023-05-31T14:38:35.508Z","comments":true,"path":"2021/07/22/MySql/MySql-01-一条sql/","link":"","permalink":"http://xboom.github.io/2021/07/22/MySql/MySql-01-%E4%B8%80%E6%9D%A1sql/","excerpt":"","text":"MySQL整体架构如下所示，下面将从一个SQL语句的执行来看看它经历了什么 Server层 连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接，连接默认端口是3306，可通过修改my.cnf配置文件指定端口 连接命令：mysql -h$ip -P$port -u$user -p 如何查看 MySQL 服务被多少个客户端连接了？ 12345678mysql&gt; show processlist;+----+-----------------+-----------+------+---------+------+------------------------+------------------+| Id | User | Host | db | Command | Time | State | Info |+----+-----------------+-----------+------+---------+------+------------------------+------------------+| 5 | event_scheduler | localhost | NULL | Daemon | 121 | Waiting on empty queue | NULL || 8 | root | localhost | NULL | Query | 0 | init | show processlist |+----+-----------------+-----------+------+---------+------+------------------------+------------------+2 rows in set (0.00 sec) 共有两个用户连接了 MySQL 服务，其中 id 为 5 的用户的 Command 列的状态为 Daemon ，意味着该用户是 MySQL守护进程用户，空闲的时长是 121 秒(Time 列)，另外一个就是我登录并执行查询语句 Command：连接的当前命令，可能的取值包括： Sleep：连接处于空闲状态。 Query：正在执行查询操作。 Connect：连接正在建立中。 Execute：正在执行存储过程或函数。 Binlog Dump：正在读取二进制日志。 Daemon：表示当前连接是一个后台守护进程，通常是 MySQL 服务器的内部线程或任务 如果客户端太长时间没动静，连接器会自动断开。由参数wait_timeout控制的，默认值是8小时(28800秒) 1234567mysql&gt; show variables like 'wait_timeout';+---------------+-------+| Variable_name | Value |+---------------+-------+| wait_timeout | 28800 |+---------------+-------+1 row in set (0.04 sec) 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可在每次执行一个比较大的操作后，通过执 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态 一个处于空闲状态的连接被服务端主动断开后，kill connection +6; 这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到这样的报错ERROR 2013 (HY000): Lost connection to MySQL server during query MySQL 的连接数有限制吗？ MySQL 服务支持的最大连接数由 max_connections 参数控制，比如我的 MySQL 服务默认是 151 个，超过这个值，系统就会拒绝接下来的连接请求，并报错提示Too many connections。 1234567mysql&gt; show variables like 'max_connections';+-----------------+-------+| Variable_name | Value |+-----------------+-------+| max_connections | 151 |+-----------------+-------+1 row in set (0.00 sec) 怎么解决长连接占用内存的问题？ MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。如果长连接累计很多，将导致 MySQL 服务占用内存太大，有可能会被系统强制杀掉，这样会发生 MySQL 服务异常重启的现象。 定期断开长连接。既然断开连接后就会释放连接占用的内存资源，可以定期断开长连接。 设置wait_timeout自动释放长时间空闲连接 客户端主动重置连接。MySQL 5.7 版本实现了 mysql_reset_connection() 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection() 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存 连接建立完成后，就可以执行 select 语句了，执行查询缓存。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中，key 是查询的语句，value 是查询的结果 MySql8.0 之后查询缓存被删掉 如果是频繁更新的表格，那么缓存基本没用 1234#关闭缓存SET GLOBAL query_cache_type = OFF;#查看缓存是否被禁用SHOW VARIABLES LIKE 'query_cache_type'; 分析器 分析器的作用就是将 Mysql 语句解析成数据库自己认识的样子 第一件事情，词法分析。MySQL 会根据你输入的字符串识别出关键字出来，构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。 第二件事情，语法分析。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果我们输入的 SQL 语句语法不对，就会在解析器这个阶段报错 12mysql&gt; show variables likes 'max_connections';ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'likes 'max_connections'' at line 1 注意，表不存在或者字段不存在，并不是在分析器里做的 执行SQL 经过分析器后，接着就要进入执行 SQL 查询语句的流程了，每条SELECT 查询语句流程主要可以分为下面这三个阶段： prepare 阶段，也就是预处理阶段； optimize 阶段，也就是优化阶段； execute 阶段，也就是执行阶段； 预处理阶段 预处理阶段将执行 检查 SQL 查询语句中的表或者字段是否存在； 将 select * 中的 * 符号，扩展为表上的所有列； 比如表格不存在 12mysql&gt; select * from test;ERROR 1046 (3D000): No database selected 优化器 当语句被解析成自己认识的样子之后，并不是立即就执行，而是按照自己认为效率最高的方式去执行，即最优查询路径 索引使用：如果表中存在适当的索引，则 MySQL 可以使用索引来查找需要更新的行，从而避免全表扫描，提高查询效率。 要想知道使用哪个索引，可以使用 explain &lt;sql&gt; 1234567mysql&gt; explain select * from students where id = 1;+----+-------------+----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | students | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 优化 WHERE 子句：WHERE 子句用于指定需要更新的行。MySQL 优化器会根据 WHERE 子句的复杂度和索引情况来选择最优的查询路径。 最小化日志写入：MySQL 的 InnoDB 存储引擎支持事务和回滚机制，更新操作会生成事务日志，影响性能。优化器会尝试最小化日志写入，提高性能。 减少锁的使用：更新操作可能会引起表锁或行锁，锁的使用会对性能产生负面影响。优化器会尝试减少锁的使用 执行器 当优化器选出最优索引等步骤后，会去调用存储引擎接口，开始执行被MySQL解析过和优化过的SQL语句，用三种方式执行过程 主键索引查询 全表扫描 索引下推 主键索引查询 1234567mysql&gt; explain select * from students where id = 1;+----+-------------+----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | students | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+----------+------------+-------+---------------+---------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 查询条件用到主键索引，而且是等值查询，同时主键 id 是唯一，不会有 id 相同的记录，所以优化器决定选用访问类型为 const 进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的： 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 id = 1 交给存储引擎，让存储引擎定位符合条件的第一条记录。 存储引擎通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；(符合索引的记录) 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，否则跳过该记录。(符合条件的记录) 全表扫描 123456789select * from product where name = 'iphone';mysql&gt; explain select * from students where name = 'iphone';+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+| 1 | SIMPLE | students | NULL | ALL | NULL | NULL | NULL | NULL | 2 | 50.00 | Using where |+----+-------------+----------+------------+------+---------------+------+---------+------+------+----------+-------------+1 row in set, 1 warning (0.00 sec) 查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的： 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，让存储引擎读取表中的第一条记录； 执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的（是的没错，Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。 执行器查询的过程是一个 while 循环，因为优化器选择的访问类型为 all，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器(Server层)，执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端； 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器(Server层) 返回了读取完毕的信息； 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。 索引下推 索引下推能够减少二级索引在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了 12345678910111213141516mysql&gt; select * from students;+----+------------+------+------------+-------+| id | name | age | student_id | score |+----+------------+------+------------+-------+| 1 | John Doe | 20 | 2021001 | 20 || 2 | Jane Smith | 22 | 2021002 | 95 || 3 | Alice | 20 | 1 | 16 || 4 | Bob | 22 | 2 | 91 || 5 | Charlie | 21 | 3 | 9 || 6 | David | 19 | 4 | 73 || 7 | Emma | 23 | 5 | 34 || 8 | Frank | 20 | 6 | 53 || 9 | Grace | 22 | 7 | 63 || 10 | Henry | 21 | 8 | 54 |+----+------------+------+------------+-------+10 rows in set (0.00 sec) 执行如下的sql语句 1234567mysql&gt; explain SELECT * FROM students FORCE INDEX (idx_age_score) WHERE age &gt; 19 AND score = 53;+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | students | NULL | range | idx_age_score | idx_age_score | 5 | NULL | 9 | 10.00 | Using index condition |+----+-------------+----------+------------+-------+---------------+---------------+---------+------+------+----------+-----------------------+1 row in set, 1 warning (0.00 sec) age 字段能用到联合索引，但是 reward 字段则无法利用到索引，可以在 索引 查看原因。在不使用索引下推(MySQL 5.6 之前的版本）时，执行器与存储引擎的执行流程是这样的： Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，定位到 age &gt; 19 的第一条记录； 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后进行回表操作，将完整的记录返回给 Server 层； Server 层在判断该记录的 score 是否等于 53，如果成立则将其发送给客户端；否则跳过该记录； 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层； 如此往复，直到存储引擎把表中的所有记录读完。 没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给 Server，接着 Server 再判断该记录的 score 是否等于 53。而使用索引下推后，判断记录的 score 是否等于 53 的工作交给了存储引擎层，过程如下 ： Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age &gt; 19 的第一条记录； 存储引擎定位到二级索引后，先不执行回表操作，而是先判断一下该索引中包含的列(score列）的条件（score 是否等于 53）是否成立。如果条件不成立，则直接跳过该二级索引。如果成立，则执行回表操作，将完成记录返回给 Server 层。 Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。 如此往复，直到存储引擎把表中的所有记录读完。 使用了索引下推后，虽然 score 列无法使用到联合索引，但因为它包含在联合索引(age，score)里，所以直接在存储引擎过滤出满足 score = 53 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。如果 Extr 部分显示了 “Using index condition”，说明使用了索引下推。 这里强制使用联合索引实现索引下推，MySQL 优化器根据查询的复杂度、数据分布、索引统计信息等因素决定执行计划 bin log MySQL的二进制日志bin log是服务层文件而不是存储引擎，记录数据库事务的二进制文件，它可以用于数据恢复、复制和备份等操作。在MySQL中，bin log文件的大小是由max_binlog_size参数来控制的，默认值为1073741824(即1GB) 1234567mysql&gt; show variables like 'max_binlog_size';+-----------------+------------+| Variable_name | Value |+-----------------+------------+| max_binlog_size | 1073741824 |+-----------------+------------+1 row in set (0.02 sec) 既然是文件，就会有以下问题 文件的作用 文件路径与大小 文件的内容 文件是如何清理的 文件是如何写入的 接下来一一解答： 内容 MySQL 在完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写入 binlog 文件，以下是会被写入二进制日志 binlog 的一些操作： 数据更改操作(DML)：包括插入(INSERT)、更新(UPDATE)、删除(DELETE)等操作。以便进行数据恢复或复制。 数据定义操作(DDL)：某些数据定义语句也会被写入二进制日志，如创建表(CREATE TABLE)、修改表结构(ALTER TABLE)、删除表(DROP TABLE)等。记录数据库结构的更改。 事务控制语句：事务的开始(BEGIN、START TRANSACTION)、提交(COMMIT)和回滚(ROLLBACK)，以便进行事务的恢复和复制。 系统变量修改：一些修改数据库全局或会话级别系统变量的语句(如 SET GLOBAL 和 SET SESSION)也会被写入二进制日志。 那么不会写入的就有 查询语句(SELECT)：由于查询语句不会对数据进行修改，因此没必要将其记录到二进制日志中。 系统函数调用：对于仅调用系统函数的操作，例如获取当前时间或执行数学运算等，通常不会被写入二进制日志。 非事务性操作：对于没有显式或隐式事务包装的操作，例如单个的插入、更新或删除语句，如果没有事务的参与，它们可能不会被写入二进制日志。暂时不太理解 临时表的数据更改：对于临时表的数据更改，如果没有启用 binlog 的 row 格式，这些操作可能不会被写入二进制日志。 格式 这里说到的binlog的格式，其实就是binlog记录的内容的格式 STATMENT：基于 SQL 语句的复制(statement-based replication, SBR)，每一条会修改数据的 SQL 语句会记录到 bin log 中 不需要记录每一行的变化，减少了 bin log 日志量，节约了 IO , 从而提高了性能 在某些情况下会导致主从数据不一致，比如执行sysdate()、sleep()等 ROW：基于行的复制(row-based replication, RBR)，不记录每条SQL语句的上下文信息，仅需记录哪条数据被修改了 不会出现某些特定情况下的存储过程、或 function、或 trigger 的调用和触发无法被正确复制的问题 会产生大量的日志，尤其是 alter table 的时候会让日志暴涨 MIXED：基于 STATMENT 和 ROW 两种模式的混合复制( mixed-based replication, MBR )，一般的复制使用 STATEMENT 模式保存 bin log ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 bin log 修改MySQL的binlog格式步骤 打开MySQL配置文件（my.cnf或my.ini）。 定位到[mysqld]部分，该部分包含MySQL服务器的全局配置。 添加或修改 binlog_format 保存并关闭配置文件。 重启MySQL服务器，以使配置更改生效 写入时机 bin log 的刷盘策略通过sync_binlog来修改，默认为 0，表示先写入 os cache， 1234567mysql&gt; show variables like 'sync_binlog';+---------------+-------+| Variable_name | Value |+---------------+-------+| sync_binlog | 1 |+---------------+-------+1 row in set (0.01 sec) 0 表示在提交事务的时候，数据不会直接到磁盘中。如果宕机，bin log数据仍然会丢失。 1表示每次事务提交时，数据会写入磁盘并刷新到持久化存储设备。这确保了在事务提交后，二进制日志的内容已经持久化，并且可以用于数据恢复和复制。 sync_binlog仅控制二进制日志的刷新策略，并不影响事务的持久性。事务的持久性仍然由存储引擎和文件系统的设置决定 事务 文件过大 当binlog文件过大时，可能会导致存储空间不足或者文件系统的限制，解决方案： 改变binlog文件路径：可以将binlog文件存储在具有更大空间的磁盘分区上，或者使用分布式文件系统来扩展存储容量 log_bin：指定binlog文件的基本名称，不包括文件扩展名。默认情况下，它的值为mysql-bin。例如，如果设置为mysql-bin，则生成的binlog文件名为mysql-bin.000001、mysql-bin.000002等。 log_bin_index：指定binlog索引文件的名称。默认情况下，它的值为与log_bin参数相同的基本名称，附加.index扩展名。例如，如果log_bin为mysql-bin，则索引文件名为mysql-bin.index。 datadir：指定MySQL数据目录的路径，即数据库文件所在的目录。binlog文件默认存储在数据目录下的data子目录中 压缩binlog文件：使用压缩算法对binlog文件进行压缩，以减少文件的大小。通过设置binlog_compression参数启用压缩。 定期删除旧的binlog文件：可以通过定期清理和删除旧的binlog文件来释放空间。可以使用PURGE BINARY LOGS语句删除过期的binlog文件。 1234#删除早于xxx时间文件PURGE BINARY LOGS BEFORE '2022-01-01 00:00:00';#删除指定binlog文件PURGE BINARY LOGS TO 'mysql-bin.000003'; 调整binlog文件的生命周期：可以通过调整expire_logs_days参数来限制binlog文件的保留时间。将其设置为较短的值可以减少文件的积累。 1234567mysql&gt; show variables like 'expire_logs_days';+------------------+-------+| Variable_name | Value |+------------------+-------+| expire_logs_days | 0 |+------------------+-------+1 row in set (0.00 sec) 0 表示不会自动删除binlog日志 启用binlog文件的循环复用：可以使用log_bin_trust_function_creators参数启用binlog文件的循环复用。当binlog文件达到最大大小限制时，会重新使用最旧的文件，以避免无限增长。 1234567mysql&gt; show variables like 'log_bin_trust_function_creators';+---------------------------------+-------+| Variable_name | Value |+---------------------------------+-------+| log_bin_trust_function_creators | OFF |+---------------------------------+-------+1 row in set (0.00 sec) 说完了binlog，那么接着上面一条语句的执行，以Innodb为例来说明接下来的操作 bin log 并不属于存储引擎，与存储引擎的 redo log 是有区别的： 性质 redo log bin log 文件大小 redo log 的大小是固定的（配置中也可以设置，一般默认的就足够了） bin log 可通过配置参数max_binlog_size设置每个bin log文件的大小 实现方式 redo log是InnoDB引擎层实现的（也就是说是 Innodb 存储引擎独有的） bin log是 MySQL 层实现的，所有引擎都可以使用 bin log日志 记录方式 redo log 采用循环写的方式记录，当写到结尾时，会回到开头循环写日志。 bin log通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上 使用场景 redo log适用于崩溃恢复(crash-safe) bin log适用于主从复制和数据恢复 Inndb 存储引擎 到了存储引擎就到了实际与磁盘打交道的地方，如果在写入过程中出现崩溃，那么就会出现： 未提交事务，写入后崩溃(比如修改三个数据，程序还没修改完，但数据库已经将其中一个或两个数据的变动写入磁盘，此时出现崩溃) 已提交事务，写入前崩溃(程序已经修改完三个数据，但数据库还未将全部三个数据的变动都写入磁盘，此时出现崩溃） 由于写入中间状态与崩溃都无法避免，只能在崩溃恢复后采取补救措施，于是就提出来写日志的方式。但这里又有问题，在提交事务写写日志还是在在提交之后写日志？ 如果所有对数据的真实修改都必须发生在事务提交之后，即成功写入日志之后。在此之前，即使磁盘IO有足够的空闲，即使某个事务修改的数据量非常庞大，占用了大量的内存缓冲区，都不允许在事务提交前写入磁盘，因此这种方式对数据库性能的提升十分不利。提出了“提前写入日志(Write-Ahead Logging)的日志改进方案，允许在事务提交之前写入变动数据的意思 而对于提前写入磁盘，在数据库崩溃后需要回滚的数据，给出的解决办法是增加另外一种被称为Undo Log的日志类型，当变动数据写入磁盘前，必须先记录Undo Log，以便在事务回滚或者崩溃恢复时根据Undo Log对提前写入的数据变动进行擦除。 redo log 知道了redo log日志，接下来来看看它是如何解决崩溃的 首先了解一个东西叫 redo log buffer，也就是该文件缓存用来加快IO，既然是文件缓存就会有刷盘策略 延迟写(Delayed Write): 在延迟写策略下，当事务提交时，相关的redo log记录会先写入redo log buffer，而不是立即写入磁盘的redo log文件。只有在满足一定条件时，才会将redo log buffer中的日志刷写到磁盘。这种策略可以提高事务的执行性能，减少磁盘IO操作的频率。常见的条件包括事务提交频率、日志量大小等。 异步刷新（Asynchronous Flushing）：在异步刷新策略下，当事务提交时，相关的redo log记录会先写入redo log buffer，并触发一个后台线程，负责将redo log buffer中的日志异步刷新到磁盘的redo log文件中。这个刷新操作不会阻塞事务的提交，提高了事务的并发性能。异步刷新策略通常通过一定的策略控制，例如设置合适的刷新间隔、刷新量、I/O线程的数量等。 刷盘策略配置 innodb_flush_log_at_trx_commit默认值为1 0：不进行刷盘操作，性能最高，但存在数据丢失的风险，使用异步刷新。 1(默认)：每次事务提交时立即刷写到磁盘，最安全，但性能较低。 2：每次事务提交时将日志写入操作系统缓存，但不等待刷写到磁盘，性能较高，但存在数据丢失的风险。 innodb_flush_log_at_timeout：用于控制异步刷新策略的超时时间。 当innodb_flush_log_at_trx_commit设置为2时，该参数指定了多久后如果没有新的事务提交，则将日志刷写到磁盘，默认值为1秒 提交事务成功的标志是什么？指缓存 redo log buffer 刷入到了磁盘中？ 答：只有当 redo log 完全持久化到磁盘中之后，MySQL才会将该事务标记为以提交，这个过程称为 redo log 的持久化 如果存在多个事务，是否存在多个redo log buffer ？ 答：每个数据库实例，只有一个 redo log buffer redo log 是固定大小的，通常以ib_logfile0、ib_logfile1等文件名的形式存在于数据目录下。每个redo log文件的大小由MySQL配置参数innodb_log_file_size控制，默认大小为48MB。比如一组 4 个文件，每个文件的大小是 48MB，总共就可以记录 216MB 的操作 1234567mysql&gt; SHOW VARIABLES LIKE 'innodb_log_file_size';+----------------------+----------+| Variable_name | Value |+----------------------+----------+| innodb_log_file_size | 50331648 |+----------------------+----------+1 row in set (0.10 sec) 从头开始写，写到末尾就又回到开头循环写，如下面这个图所示 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。 checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 在一个类似环的地方，write pos 会不断的记录新的操作，而 checkpoint 在后面不断的追赶(将记录更新到数据文件)。有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe 当 write pos 追上 checkpoint(redo log满了)怎么办? 为了避免Redo Log被写满，MySQL会暂停新的写入操作，直到有足够的空间可用 有了 redo log 日志之后， update 的逻辑是怎样的？ 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成 所以更新操作并不是把某条记录查询到内存中对其做修改就行，而是将对应记录所在页都加载到内存中，然后通过执行器(是服务层而不是直接由引擎直接更新的！！！)，这里分为了两阶段提交 如果先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。这时候 binlog 里面就没有记录这个语句。如果用这个 binlog 恢复临时库的话，就会少了这一次更新 如果先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。 所以，为了保证事务分为三步操作： redo log 的 prepare阶段 写binlog 阶段 redo log 的 commit 阶段 当在第2步之前崩溃时，重启恢复后发现没有commit，回滚，备份恢复没有binlog 当在第3步之前崩溃时，重启恢复后发现没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份恢复有binlog 既然bin log就有所有的操作，不能解决崩溃问题，为什么还要redo log? 假设先提交事务再写binlog，可能事务提交数据更新之后数据库崩了，还没来得及写binlog。binlog进行主从复制数据不一致 假设先写binlog再提交事务更新数据库，有可能写binlog成功之后数据库崩掉而导致数据库更新失败，这样也会导致主从数据库不一致或者无法恢复数据库。 bin log 只记录逻辑操作，并无操作状态，即无法确定该操作是否完成。redo log是有状态的，不会直接检查binlog。只有在redo log状态为prepare时，才会去检查binlog是否存在，否则只校验redo log是否是 commit 就可以了。 buffer Pool 在上一节中提到，查询与更新都会先查询一下缓存，判断是否存在数据，如果没有则将磁盘数据页加载到内容中，而这个内存也是一个很重要的InnoDB 组件 buffer Pool 查看缓冲池大小 1234567mysql&gt; SHOW VARIABLES LIKE 'innodb_buffer_pool_size';+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 134217728 |+-------------------------+-----------+1 row in set (0.03 sec) undo log 记录语句执行前的记录值 将数据加载到 Buffer Pool 的同时还会往 undo log 中插入一条日志，也就是将 id = 1 原来的值记录下来 结合bin log，MySQL 在提交事务的时候，不仅会将 redo log buffer 中的数据写入到 redo log 文件中，也会将本次修改的数据记录到 bin log 文件中，同时会将本次修改的bin log文件名和修改的内容在bin log中的位置记录到redo log中，最后还会在redo log写入 commit 标记，表示本次事务被成功的提交了 准备更新一条 SQL 语句 MySQL(innodb)会先去缓冲池 BufferPool 中去查找这条数据，没找到就会去磁盘中查找，如果查找到就会将这条数据加载到缓冲池 BufferPool 中 在加载到 Buffer Pool 的同时，会将这条数据的原始记录保存到 undo log 文件中 innodb 会在 Buffer Pool 中执行更新操作 更新后的数据会记录在 redo log buffer 中 MySQL 提交事务的时候，会将 redo log buffer 中的数据写入到 redo log 文件中，刷磁盘可以通过 innodb_flush_log_at_trx_commit 参数来设置 值为 0 表示不刷入磁盘 值为 1 表示立即刷入磁盘 值为 2 表示先刷到 os cache myslq 重启的时候会将 redo 日志恢复到缓冲池中 MySQL有一个后台线程，在某个时机将 Buffer Pool 中的数据刷到 MySQL 数据库中，这样就将内存和数据库的数据保持统一了。 参考链接 https://pdai.tech/md/db/sql-mysql/sql-mysql-execute.html https://xiaolincoding.com/mysql/transaction/mvcc.html#读提交是如何工作的 https://time.geekbang.org/column/article/68963","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"Go-05-Map","slug":"Go/Go-05-Map","date":"2021-07-11T04:39:49.000Z","updated":"2023-07-15T05:36:15.048Z","comments":true,"path":"2021/07/11/Go/Go-05-Map/","link":"","permalink":"http://xboom.github.io/2021/07/11/Go/Go-05-Map/","excerpt":"","text":"什么是Map Map是一种通过key来获取value的一个数据结构，其底层存储方式为数组。 一种特殊的数据结构：一种元素对（pair）的无序集合，pair 的一个元素是 key，对应的另一个元素是 value 一种引用类型，未初始化的 map 的值是 nil，可以通过如下方式声明： 12var map1 map[keytype]valuetypevar map1 map[string]int key 可以是任意可以用 == 或者 != 操作符比较的类型，比如 string、int、float。所以数组、切片和结构体不能作为 key，但是指针和接口类型可以。如果要用结构体作为 key 可以提供 Key() 和 Hash() 方法，这样可以通过结构体的域计算出唯一的数字或者字符串的 key。 value 可以是任意类型的；通过使用空接口类型,可以存储任意值，但是使用这种类型作为值时需要先做一次类型断言 Hash表 使用Hash需要解决哈希碰撞的问题，常见方法的就是开放寻址法和拉链法。 需要注意的是，这里提到的哈希碰撞不是多个键对应的哈希完全相等，可能是多个哈希的部分相等，例如：两个键对应哈希的前四个字节相同 开放寻址法 开放寻址法核心思想是依次探测和比较数组中的元素以判断目标键值对是否存在于哈希表中，使用开放寻址法来实现哈希表，那么实现哈希表底层的数据结构就是数组，不过因为数组的长度有限，想哈希表写入 (author, draven) 这个键值对时会从如下的索引开始遍历 1index := hash(\"author\") % array.len 向当前哈希表写入新的数据时，如果发生了冲突，就会将键值对写入到下一个索引不为空的位置 当 Key3 与已经存入哈希表中的两个键值对 Key1 和 Key2 发生冲突时，Key3 会被写入 Key2 后面的空闲位置。当我们再去读取 Key3 对应的值时就会先获取键的哈希并取模，这会先帮助我们找到 Key1，找到 Key1 后发现它与 Key 3 不相等，所以会继续查找后面的元素，直到内存为空或者找到目标元素 当需要查找某个键对应的值时，会从索引的位置开始线性探测数组，找到目标键值对或者空内存就意味着这一次查询操作的结束。 开放寻址法中对性能影响最大的是装载因子，它是数组中元素的数量与数组大小的比值。随着装载因子的增加，线性探测的平均用时就会逐渐增加，这会影响哈希表的读写性能。当装载率超过 70% 之后，哈希表的性能就会急剧下降，而一旦装载率达到 100%，整个哈希表就会完全失效，这时查找和插入任意元素的时间复杂度都是 𝑂(𝑛)O(n) 的，这时需要遍历数组中的全部元素，所以在实现哈希表时一定要关注装载因子的变化 拉链法 与开放地址法相比，平均查找的长度也比较短，各个用于存储节点的内存都是动态申请的，可以节省比较多的存储空间。 实现拉链法一般会使用数组加上链表，一些编程语言会在拉链法的哈希中引入红黑树以优化性能，拉链法会使用链表数组作为哈希底层的数据结构，可以将它看成可以扩展的二维数组 如上图所示，当我们需要将一个键值对 (Key6, Value6) 写入哈希表时，键值对中的键 Key6 都会先经过一个哈希函数，哈希函数返回的哈希会帮助我们选择一个桶，和开放地址法一样，选择桶的方式是直接对哈希返回的结果取模： 1index := hash(\"Key6\") % array.len 选择了 2 号桶后就可以遍历当前桶中的链表了，在遍历链表的过程中会遇到以下两种情况： 找到键相同的键值对 — 更新键对应的值； 没有找到键相同的键值对 — 在链表的末尾追加新的键值对； 如果要在哈希表中获取某个键对应的值，会经历如下的过程 Key11 展示了一个键在哈希表中不存在的例子，当哈希表发现它命中 4 号桶时，它会依次遍历桶中的链表，然而遍历到链表的末尾也没有找到期望的键，所以哈希表中没有该键对应的值。 在一个性能比较好的哈希表中，每一个桶中都应该有 0~1 个元素，有时会有 2~3 个，很少会超过这个数量。计算哈希、定位桶和遍历链表三个过程是哈希表读写操作的主要开销，使用拉链法实现的哈希也有装载因子这一概念： 装载因子:=元素数量÷桶数量装载因子:=元素数量÷桶数量 与开放地址法一样，拉链法的装载因子越大，哈希的读写性能就越差。在一般情况下使用拉链法的哈希表装载因子都不会超过 1，当哈希表的装载因子较大时会触发哈希的扩容，创建更多的桶来存储哈希中的元素，保证性能不会出现严重的下降。如果有 1000 个桶的哈希表存储了 10000 个键值对，它的性能是保存 1000 个键值对的 1/10，但是仍然比在链表中直接读写好 1000 倍 Map结构 1234567891011121314151617181920// A header for a Go map.type hmap struct &#123; count int //表示当前hash表元素个数 flags uint8 //记录当前hash表状态，map是非线程安全的 B uint8 //表示当前哈希表持有的 buckets 数量，桶的数量都是2的倍数，该字段存储对数，也就是len(buckets) == 2^B noverflow uint16 // overflow 的 bucket 近似数 hash0 uint32 // 哈希种子，能为哈希函数的结果引入随机性， 值在创建哈希表时确定，并在调用哈希函数时作为参数传入 buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // 哈希在扩容时用于保存之前 buckets 的字段，它的大小是当前 buckets 的一半 nevacuate uintptr // 指示扩容进度，小于此地址的 buckets 迁移完成 extra *mapextra // optional fields&#125;type mapextra struct &#123; overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap&#125; 这里B是map的bucket数组长度的对数，每个bucket里面存储了kv对。buckets是一个指针，指向实际存储的bucket数组的首地址。 bucket的结构体如下: 123456789// A bucket for a Go map.type bmap struct &#123; //top hash通常包含该bucket中每个键的hash值的高八位 //如果tophash[0]小于mintophash，则tophash[0]为桶疏散状态 //bucketCnt 的初始值是8 tophash [bucketCnt]uint8 //注意：将所有键打包在一起，然后将所有值打包在一起，使得代码比交替key/elem/key/elem/...更复杂。 //但它允许我们消除可能需要的填充，例如map[int64]int8./后面跟一个溢出指针&#125;&#125; 上面这个数据结构并不是 golang runtime 时的结构，在编译时候编译器会给它动态创建一个新的结构，如下： 1234567type bmap struct &#123; topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr&#125; bmap 就是我们常说的“bucket”结构，每个 bucket 里面最多存储 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置） 当map的key和value 每个 bmap 都能存储 8 个键值对，当哈希表中存储的数据过多，单个桶无法装满时就会使用 extra.nextOverflow 中桶存储溢出数据 当作为函数传参时候，传递的实例其实是指针 map是一个hash表，数据被存放在一组buckets(滚筒)中，每个buckets包含8个键值对，低位的buckets用来给数据选择存储的buckets，每个存储buckets包含每个哈希的一些高阶位，以区分单个存储buckets中的条目 如果一个buckets中超过8个，那么将使用其他buckets存储 当哈希表增长时，分配一组新buckets是原来的两倍大，并且会将旧数据复制到新buckets中存储 映射迭代器遍历存储桶数组，并按行走顺序返回键（存储桶编号，然后是溢出链顺序，然后是存储桶索引）。 为了维持迭代语义，我们绝不会在键的存储桶中移动键（如果这样做，键可能会返回0或2次）。 在增加表时，迭代器将继续在旧表中进行迭代，并且必须检查新表是否将要迭代的存储桶（“撤离”）到新表中 常用常量 1234567891011121314151617181920212223242526272829303132333435363738394041424344const ( // 一个桶能装的最大键值对 1&lt;&lt;3 bucketCntBits = 3 bucketCnt = 1 &lt;&lt; bucketCntBits //负载因子计算 uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen) loadFactorNum = 13 loadFactorDen = 2 /* if t.key.size &gt; maxKeySize &amp;&amp; (!t.indirectkey() || t.keysize != uint8(sys.PtrSize)) || t.key.size &lt;= maxKeySize &amp;&amp; (t.indirectkey() || t.keysize != uint8(t.key.size)) &#123; throw(\"key size wrong\") &#125; if t.elem.size &gt; maxElemSize &amp;&amp; (!t.indirectelem() || t.elemsize != uint8(sys.PtrSize)) || t.elem.size &lt;= maxElemSize &amp;&amp; (t.indirectelem() || t.elemsize != uint8(t.elem.size)) &#123; throw(\"elem size wrong\") &#125; */ maxKeySize = 128 maxElemSize = 128 //bmap 偏移 int64 topHash dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64 &#125;&#123;&#125;.v) emptyRest = 0 // 这bmap中这一格为空，在更高的索引中没有更多的非空细胞 emptyOne = 1 // 这bmap中这一格为空 evacuatedX = 2 // 元素可得，但已经迁移到新桶的前半部分 evacuatedY = 3 // 元素可得，但已经迁移到新桶的后半部分 evacuatedEmpty = 4 // 格子为空，bucket已经迁移. minTopHash = 5 // ophash 的最小正常值 // flags iterator = 1 // 可能有迭代器在使用buckets oldIterator = 2 // 可能有迭代器在使用oldbuckets hashWriting = 4 // 有协程正在写 sameSizeGrow = 8 // 等量扩容 // sentinel bucket ID for iterator checks noCheck = 1&lt;&lt;(8*sys.PtrSize) - 1) 常用函数 1234//查找下一个overflow 的bmapfunc (b *bmap) overflow(t *maptype) *bmap &#123; return *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))&#125; 创建Map 通过调用make来创建map，底层为 makemap 函数 123456789101112131415161718192021222324252627282930313233func makemap(t *maptype, hint int, h *hmap) *hmap &#123; mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem &gt; maxAlloc &#123; hint = 0 &#125; // 初始化 hmap if h == nil &#123; h = new(hmap) &#125; h.hash0 = fastrand() // 找到一个 B使 hint &gt; B &amp;&amp; hint &gt; (2^B) * 6.5 为false 否则B就增大 // 当hint为1 则 B == 0 表示存1个数，只需要用一个桶 // 当hint为6 则 B == 0 表示存6个数，也只需要一个桶 // 当hint为7 则 B == 1 表示存7个树，就需要用两个桶 B := uint8(0) for overLoadFactor(hint, B) &#123; B++ &#125; h.B = B //这里使用了懒加载 if h.B != 0 &#123; var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil &#123; h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow &#125; &#125; return h&#125; 注意： 返回的是*hmap而 func makeslice(et *_type, len, cap int) slice返回的是结构体 虽然Go 语言中的函数传参都是值传递，在函数内部，map参数会影响外部 而slice部分会，因为slice内部的数组是指针类型，对数组的修改会影响外部，但是长度和容量不会 判断装载因子，13 * 1&lt;&lt;B / 2 即 桶数目的6.5倍 1234//判断装载因子 func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125; Key定位 key 经过哈希计算后得到哈希值，共 64 个 bit 位(64位机，32位机忽略)，计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。如果 B = 5，则桶数量即 buckets 数组的长度是 2^5 = 32 Key的Hash值计算为： 110010111 | 000011110110110010001111001010100010010110010101010 │ 00110 低5 位 00110，6号桶 高 8 位10010111，十进制为151 在6好bucket中找到tophash值(HOBhash)为151的key，对应2号槽位， 这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入 如果冲突了怎么办 是怎么存入的，什么顺序 查找怎么办 如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket 迁移过程中怎么查找 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; //如果map为空，或者map中数量为0 则返回0值 if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]) &#125; //如果写冲突，则直接报错 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; // 计算哈希值，加入hash0引入随机值 hash := t.hasher(key, uintptr(h.hash0)) // 比如 B=5，那 m 就是31，二进制是全 1 // m = 1&lt;&lt;B - 1 m := bucketMask(h.B) // b 就是 bucket 的地址，hash&amp;m*uintptr(t.bucketsize) 第几个bucket b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) //如果oldbuckets不为空，说明发生了扩容 if c := h.oldbuckets; c != nil &#123; // 如果不是同 size 扩容（看后面扩容的内容） // 对应条件 1 的解决方案 if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. // 新 bucket 数量是老的 2 倍 m &gt;&gt;= 1 &#125; // 求出 key 在老的 map 中的 bucket 位置 oldb := (*bmap)(add(c, (hash&amp;m)*uintptr(t.bucketsize))) // 如果 oldb 没有搬迁到新的 bucket // 那就在老的 bucket 中寻找 if !evacuated(oldb) &#123; b = oldb &#125; &#125; // 计算出高 8 位的 hash // 相当于右移 56 位，只取高8位 top := tophash(hash)bucketloop: for ; b != nil; b = b.overflow(t) &#123; //如果b中没有，则查看b的overflow for i := uintptr(0); i &lt; bucketCnt; i++ &#123; //遍历topHash if b.tophash[i] != top &#123; //如果topHash不等于top if b.tophash[i] == emptyRest &#123; //如果位置为空，后面都没有非空元素 break bucketloop //说明不是顺序填充，相等的topHash值会存到下一个bmap &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) //key 是指针 if t.indirectkey() &#123; //解引用 k = *((*unsafe.Pointer)(k)) &#125; //如果key相等(可能存在topHash相等，key不想等的情况) if t.key.equal(key, k) &#123; //定位value的位置 //b bmap 地址 // dataOffset(数据对齐) // bucketCnt*t.keysize 8个key的大小(固定8个) //i*t.elemsize 第i个元素 e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) // val解引用 if t.indirectelem() &#123; e = *((*unsafe.Pointer)(e)) &#125; return e &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0])&#125; 这里的dataOffset 是 key 相对于 bmap 起始地址的偏移，包含了topHash数组 int64 1234dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64&#125;&#123;&#125;.v) 因此 bucket 里 key 的起始地址就是 unsafe.Pointer(b)+dataOffset minTopHash minTopHash当一个 cell 的 tophash 值小于 minTopHash 时，标志这个 cell 的状态。因为这个状态值是放在 tophash 数组里，为了和正常的哈希值区分开，会给 key 计算出来的哈希值一个增量：minTopHash。这样就能区分正常的 tophash 值和表示状态的哈希值 源码里判断是否搬迁完毕使用函数evacuated，当第一个元素状态为迁移标识符中的三个 1234func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; emptyOne &amp;&amp; h &lt; minTopHash&#125; Map获取 Go 语言中读取 map 有两种语法：带 comma 和 不带 comma，两种分别对应不同的函数 123// src/runtime/hashmap.gofunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointerfunc mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) &#123; //这里不会panic if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]), false &#125; //并发读写错误 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + (hash&amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil &#123; if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. m &gt;&gt;= 1 &#125; oldb := (*bmap)(unsafe.Pointer(uintptr(c) + (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) &#123; b = oldb &#125; &#125; top := tophash(hash)bucketloop: for ; b != nil; b = b.overflow(t) &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; if t.key.equal(key, k) &#123; e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() &#123; e = *((*unsafe.Pointer)(e)) &#125; return e, true &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0]), false&#125; Map扩容 使用哈希表的目的就是要快速查找到目标 key，然而，随着向 map 中添加的 key 越来越多，key 发生碰撞的概率也越来越大。bucket 中的 8 个 cell 会被逐渐塞满，查找、插入、删除 key 的效率也会越来越低。 Go有一个衡量标准 装载因子 来描述存储情况 1loadFactor := count / (2^B) 触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容： 装载因子超过阈值，源码里定义的阈值是 6.5。 overflow 的 bucket 数量过多： 当 B &lt; 15，即 bucket 总数 2^B &lt; 2^15 时，如果 overflow 的 bucket 数量超过 2^B； 当 B &gt;= 15，即 bucket 总数 2^B &gt;= 2^15，如果 overflow 的 bucket 数量超过 2^15 123456789101112131415161718if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again&#125;// 装载因子超过 6.5func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125;// overflow buckets 太多func tooManyOverflowBuckets(noverflow uint16, B uint8) bool &#123; if B &gt; 15 &#123; B = 15 &#125; // The compiler doesn't see here that B &lt; 16; mask B to generate shorter shift code. return noverflow &gt;= uint16(1)&lt;&lt;(B&amp;15)&#125; 第 1 点：每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时，表明很多 bucket 都快要装满了。在这个时候进行扩容是有必要的。 扩容策略：将 B 加 1，bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。而且，新 bucket 只是最大数量变为原来最大数量（2^B）的 2 倍（2^B * 2） B 还没有变，新bucket怎么记录大小呢？ 第 2 点：是对第 1 点的补充。当 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket） 扩容策略：开辟一个新 bucket 空间，将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。原来在 overflow bucket 中的 key 可以移动到 bucket 中来。 一个极端的情况：如果插入 map 的 key 哈希都一样，就会落到同一个 bucket 里，超过 8 个就会产生 overflow bucket，结果也会造成 overflow bucket 数过多。移动元素其实解决不了问题，因为这时整个哈希表已经退化成了一个链表，操作效率变成了 O(n)。 Go map 的扩容采取了一种称为“渐进式”地方式，并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。 准备扩容 hashGrow() 函数表示开始扩容前的处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344func hashGrow(t *maptype, h *hmap) &#123; bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) &#123; //判断是否超过负载因子 bigger = 0 h.flags |= sameSizeGrow //没有说明是第二种情况 &#125; oldbuckets := h.buckets //先将桶指向旧的指针 //h.B+bigger 说明是桶是原来的两倍 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) //&amp;^按位置为0 flags := h.flags &amp;^ (iterator | oldIterator) if h.flags&amp;iterator != 0 &#123; flags |= oldIterator &#125; // 提交 grow 的动作 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets //搬迁进度为0 h.nevacuate = 0 //overflow buckets数量为0 h.noverflow = 0 //搬迁extra到老的extra if h.extra != nil &amp;&amp; h.extra.overflow != nil &#123; // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil &#123; throw(\"oldoverflow is not nil\") &#125; h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil &#125; if nextOverflow != nil &#123; if h.extra == nil &#123; h.extra = new(mapextra) &#125; h.extra.nextOverflow = nextOverflow &#125; // the actual copying of the hash table data is done incrementally // by growWork() and evacuate().&#125; 扩容工作 123456789func growWork(t *maptype, h *hmap, bucket uintptr) &#123; // 确认搬迁老的 bucket 对应正在使用的 bucket evacuate(t, h, bucket&amp;h.oldbucketmask()) // 再搬迁一个 bucket，以加快搬迁进程 if h.growing() &#123; evacuate(t, h, h.nevacuate) &#125;&#125; h.growing 用来判断是否在搬迁 1234func (h *hmap) growing() bool &#123; return h.oldbuckets != nil&#125;//当oldbuckets非空，表示正在扩容 bucket&amp;h.oldbucketmask()是为了确认搬迁的 bucket 是我们正在使用的 bucket。oldbucketmask() 函数返回扩容前的 map 的 bucketmask。 bucketmask，作用就是将 key 计算出来的哈希值与 bucketmask 相与，得到的结果就是 key 应该落入的桶。比如 B = 5，则返回11111 搬迁核心evacuate 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156type evacDst struct &#123; b *bmap // 当前搬迁的桶 i int // b中的k/v索引 k unsafe.Pointer // 指针指向当前k存储位置 e unsafe.Pointer // 指针指向当前v存储位置&#125;func evacuate(t *maptype, h *hmap, oldbucket uintptr) &#123; //定位老的buckets地址(桶的开始位置+桶的数目*桶的大小) b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) //返回旧的桶数，如果B为5，那么结果为32 newbit := h.noldbuckets() //如果没有搬迁过 if !evacuated(b) &#123; //xy 包含高低两个搬迁目的地 var xy [2]evacDst //低搬迁目的地 x := &amp;xy[0] //记录新的buckets地址(桶开始的位置+桶数目*大小) ==&gt; 地址是连续的? TODO x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) //k的位置为bmap开始的位置+topHash x.k = add(unsafe.Pointer(x.b), dataOffset) //v的位置为k结束的位置 x.e = add(x.k, bucketCnt*uintptr(t.keysize)) //如果不是等量扩容 if !h.sameSizeGrow() &#123; //使用y来进行搬迁 y := &amp;xy[1] //y.b表示获取新桶的地址 y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) //y.b表示key开始的地址 y.k = add(unsafe.Pointer(y.b), dataOffset) //y.e表示v开始的地址 y.e = add(y.k, bucketCnt*uintptr(t.keysize)) &#125; // 遍历所有的 bucket，包括 overflow buckets // b 是老的 bucket 地址 for ; b != nil; b = b.overflow(t) &#123; k := add(unsafe.Pointer(b), dataOffset) e := add(k, bucketCnt*uintptr(t.keysize)) // 遍历 bucket 中的所有 cell for i := 0; i &lt; bucketCnt; i, k, e = i+1, add(k, uintptr(t.keysize)), add(e, uintptr(t.elemsize)) &#123; // 当前 cell 的 top hash 值 top := b.tophash[i] // 如果 cell 为空，即没有 key if isEmpty(top) &#123; //x &lt;= emptyOne b.tophash[i] = evacuatedEmpty // 那就标志它被\"搬迁\"过 continue // 继续下个 cell &#125; // 正常不会出现这种情况 // 未被搬迁的 cell 只可能是 emptyRest 或者 emptyOne // 正常的 top hash（大于 minTopHash） if top &lt; minTopHash &#123; throw(\"bad map state\") &#125; k2 := k // 如果 key 是指针，则解引用 if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; var useY uint8 //如果不是等量扩容 if !h.sameSizeGrow() &#123; //计算hash值 hash := t.hasher(k2, uintptr(h.hash0)) //如果正在遍历map 且 出现过相同key 的两者相同(float变得NAN) if h.flags&amp;iterator != 0 &amp;&amp; !t.reflexivekey() &amp;&amp; !t.key.equal(k2, k2) &#123; useY = top &amp; 1 top = tophash(hash) &#125; else &#123; if hash&amp;newbit != 0 &#123; useY = 1 &#125; &#125; &#125; if evacuatedX+1 != evacuatedY || evacuatedX^1 != evacuatedY &#123; throw(\"bad evacuatedN\") &#125; b.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY dst := &amp;xy[useY] // evacuation destination if dst.i == bucketCnt &#123; dst.b = h.newoverflow(t, dst.b) dst.i = 0 dst.k = add(unsafe.Pointer(dst.b), dataOffset) dst.e = add(dst.k, bucketCnt*uintptr(t.keysize)) &#125; dst.b.tophash[dst.i&amp;(bucketCnt-1)] = top // mask dst.i as an optimization, to avoid a bounds check if t.indirectkey() &#123; *(*unsafe.Pointer)(dst.k) = k2 // copy pointer &#125; else &#123; typedmemmove(t.key, dst.k, k) // copy elem &#125; if t.indirectelem() &#123; *(*unsafe.Pointer)(dst.e) = *(*unsafe.Pointer)(e) &#125; else &#123; typedmemmove(t.elem, dst.e, e) &#125; dst.i++ // These updates might push these pointers past the end of the // key or elem arrays. That's ok, as we have the overflow pointer // at the end of the bucket to protect against pointing past the // end of the bucket. dst.k = add(dst.k, uintptr(t.keysize)) dst.e = add(dst.e, uintptr(t.elemsize)) &#125; &#125; // 如果没有协程在使用老的 buckets，就把老 buckets 清除掉，帮助gc if h.flags&amp;oldIterator == 0 &amp;&amp; t.bucket.ptrdata != 0 &#123; b := add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)) // Preserve b.tophash because the evacuation // state is maintained there. ptr := add(b, dataOffset) n := uintptr(t.bucketsize) - dataOffset memclrHasPointers(ptr, n) &#125; &#125; // 更新搬迁进度 // 如果此次搬迁的 bucket 等于当前进度 if oldbucket == h.nevacuate &#123; advanceEvacuationMark(h, t, newbit) &#125;&#125;func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) &#123; //进度+1 h.nevacuate++ // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. //尝试往后看 1024 个 bucket stop := h.nevacuate + 1024 if stop &gt; newbit &#123; stop = newbit &#125; // 寻找没有搬迁的 bucket for h.nevacuate != stop &amp;&amp; bucketEvacuated(t, h, h.nevacuate) &#123; h.nevacuate++ &#125; // 现在 h.nevacuate 之前的 bucket 都被搬迁完毕 if h.nevacuate == newbit &#123; // newbit == # of oldbuckets h.oldbuckets = nil // 清除老的 overflow bucket // 回忆一下：[0] 表示当前 overflow bucket // [1] 表示 old overflow bucket if h.extra != nil &#123; h.extra.oldoverflow = nil &#125; // 清除正在扩容的标志位 h.flags &amp;^= sameSizeGrow &#125;&#125; 搬迁的目的就是将老的 buckets 搬迁到新的 buckets。 应对条件 1，新的 buckets 数量是之前的一倍，所以要重新计算 key 的哈希，才能决定它到底落在哪个 bucket。例如，原来 B = 5，计算出 key 的哈希后，只用看它的低 5 位，就能决定它落在哪个 bucket。扩容后，B 变成了 6，因此需要多看一位，它的低 6 位决定 key 落在哪个 bucket。这称为 rehash 应对条件 2，新的 buckets 数量和之前相等。因此可以按序号来搬，比如原来在 0 号 bucktes，到新的地方后，仍然放在 0 号 buckets。 因此，某个 key 在搬迁前后 bucket 序号可能和原来相等，也可能是相比原来加上 2^B（原来的 B 值），取决于 hash 值 第 6 bit 位是 0 还是 1。 那么为什么遍历 map 是无序的？ 第一种情况插入时遍历：map 在扩容后，会发生 key 的搬迁，原来落在同一个 bucket 中的 key，可能就落在了不同的槽中 第二种情况不插入遍历：在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了 123456789101112//测试 map中存入6个值，则按照规则，只会有一个桶，通过十次遍历d := make(map[int]int)for i := 0; i &lt; 6; i++ &#123; d[i] = i&#125;for i := 0; i &lt; 10; i++ &#123; fmt.Printf(\"count:%d \\n\", i) for _, v := range d &#123; fmt.Print(v) &#125;&#125;//结果发现，每次开始的位置没有变化，但是整体的前后顺序不变 “迭代 map 的结果是无序的”这个特性是从 go 1.0 开始加入的 再明确一个问题：如果扩容后，B 增加了 1，意味着 buckets 总数是原来的 2 倍，原来 1 号的桶“裂变”到两个桶。 例如，原始 B = 2，1号 bucket 中有 2 个 key 的哈希值低 3 位分别为：010，110。由于原来 B = 2，所以低 2 位 10 决定它们落在 2 号桶，现在 B 变成 3，所以 010、110 分别落入 2、6 号桶 evacuate 函数每次只完成一个 bucket 的搬迁工作会有 2 层循环，外层遍历 bucket 和 overflow bucket，内层遍历 bucket 的所有 cell 源码里提到 X, Y part，桶的数量是原来的 2 倍，前一半桶被称为 X part，后一半桶被称为 Y part。 一个 bucket 中的 key 可能会分裂落到 2 个桶，一个位于 X part，一个位于 Y part。所以在搬迁一个 cell 之前，需要知道这个 cell 中的 key 是落到哪个 Part。很简单，重新计算 cell 中 key 的 hash，并向前“多看”一位，决定落入哪个 Part,如果 tophash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part 有一个特殊情况是：有一种 key，每次对它计算 hash，得到的结果都不一样。这个 key 就是 math.NaN() 的结果，它的含义是 not a number，类型是 float64。当它作为 map 的 key，在搬迁的时候，会遇到一个问题：再次计算它的哈希值和它当初插入 map 时的计算出来的哈希值不一样 你可能想到了，这样带来的一个后果是，这个 key 是永远不会被 Get 操作获取的！当我使用 m[math.NaN()] 语句的时候，是查不出来结果的。这个 key 只有在遍历整个 map 的时候，才有机会现身。所以，可以向一个 map 插入任意数量的 math.NaN() 作为 key 12345678910111213//通过tophash值与新算出来的哈希值进行运算得到if top&amp;1 != 0 &#123; // top hash 最低位为 1 // 新算出来的 hash 值的 B 位置 1 hash |= newbit&#125; else &#123; // 新算出来的 hash 值的 B 位置 0 hash &amp;^= newbit&#125;// hash 值的 B 位为 0，则搬迁到 x part// 当 B = 5时，newbit = 32，二进制低 6 位为 10 0000useX = hash&amp;newbit == 0 确定了要搬迁到的目标 bucket 后，搬迁操作就比较好进行了。将源 key/value 值 copy 到目的地相应的位置。 设置 key 在原始 buckets 的 tophash 为 evacuatedX 或是 evacuatedY，表示已经搬迁到了新 map 的 x part 或是 y part。新 map 的 tophash 则正常取 key 哈希值的高 8 位 下面通过图来宏观地看一下扩容前后的变化 扩容前，B = 2，共有 4 个 buckets，lowbits 表示 hash 值的低位。假设我们不关注其他 buckets 情况，专注在 2 号 bucket。并且假设 overflow 太多，触发了等量扩容（对应于前面的条件 2） 扩容完成后，overflow bucket 消失了，key 都集中到了一个 bucket，更为紧凑了，提高了查找的效率。 假设触发了 2 倍的扩容，那么扩容完成后，老 buckets 中的 key 分裂到了 2 个 新的 bucket。一个在 x part，一个在 y 的 part。依据是 hash 的 lowbits。新 map 中 0-3 称为 x part，4-7 称为 y part 注意，上面的两张图忽略了其他 buckets 的搬迁情况，表示所有的 bucket 都搬迁完毕后的情形。实际上，我们知道，搬迁是一个“渐进”的过程，并不会一下子就全部搬迁完毕。所以在搬迁过程中，oldbuckets 指针还会指向原来老的 []bmap，并且已经搬迁完毕的 key 的 tophash 值会是一个状态值，表示 key 的搬迁去向。 Map遍历 正常情况下，遍历所有的 bucket 以及它后面挂的 overflow bucket，然后挨个遍历 bucket 中的所有 cell。每个 bucket 中包含 8 个 cell，从有 key 的 cell 中取出 key 和 value，这个过程就完成了。 而扩容过程不是一个原子的操作，它每次最多只搬运 2 个 bucket，所以如果触发了扩容操作，那么在很长时间里，map 的状态都是处于一个中间态：有些 bucket 已经搬迁到新家，而有些 bucket 还待在老地方，它又是怎样遍历的呢？ 先是调用 mapiterinit 函数初始化迭代器，然后循环调用 mapiternext 函数进行 map 迭代，其中迭代器的结构体定义是： 1234567891011121314151617181920212223242526272829type hiter struct &#123; // key 指针 key unsafe.Pointer // value 指针 value unsafe.Pointer // map 类型，包含如 key size 大小等 t *maptype // map header h *hmap // 初始化时指向的 bucket buckets unsafe.Pointer // 当前遍历到的 bmap bptr *bmap overflow [2]*[]*bmap // 起始遍历的 bucet 编号 startBucket uintptr // 遍历开始时 cell 的编号（每个 bucket 中有 8 个 cell） offset uint8 // 是否从头遍历了 wrapped bool // B 的大小 B uint8 // 指示当前 cell 序号 i uint8 // 指向当前的 bucket bucket uintptr // 因为扩容，需要检查的 bucket checkBucket uintptr&#125; 之前说到每次遍历都是无序的 12345678910// 生成随机数 rr := uintptr(fastrand())if h.B &gt; 31-bucketCntBits &#123; r += uintptr(fastrand()) &lt;&lt; 31&#125;// 从哪个 bucket 开始遍历it.startBucket = r &amp; (uintptr(1)&lt;&lt;h.B - 1)// 从 bucket 的哪个 cell 开始遍历it.offset = uint8(r &gt;&gt; h.B &amp; (bucketCnt - 1)) 例如，B = 2，那 uintptr(1)&lt;&lt;h.B - 1 结果就是 3，低 8 位为 0000 0011，将 r 与之相与，就可以得到一个 0~3 的 bucket 序号；bucketCnt - 1 等于 7，低 8 位为 0000 0111，将 r 右移 2 位后，与 7 相与，就可以得到一个 0~7 号的 cell。 于是，在 mapiternext 函数中就会从 it.startBucket 的 it.offset 号的 cell 开始遍历，取出其中的 key 和 value，直到又回到起点 bucket，完成遍历过程。 假设我们有下图所示的一个 map，起始时 B = 1，有两个 bucket，后来触发了扩容（这里不要深究扩容条件，只是一个设定），B 变成 2。并且， 1 号 bucket 中的内容搬迁到了新的 bucket，1 号裂变成 1 号和 3 号；0 号 bucket 暂未搬迁。老的 bucket 挂在在 *oldbuckets 指针上面，新的 bucket 则挂在 *buckets 指针上面 这时，我们对此 map 进行遍历。假设经过初始化后，startBucket = 3，offset = 2。于是，遍历的起点将是 3 号 bucket 的 2 号 cell，下面这张图就是开始遍历时的状态 标红的表示起始位置，bucket 遍历顺序为：3 -&gt; 0 -&gt; 1 -&gt; 2。 因为 3 号 bucket 对应老的 1 号 bucket，因此先检查老 1 号 bucket 是否已经被搬迁过。判断方法就是： 1234func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; empty &amp;&amp; h &lt; minTopHash&#125; 如果 b.tophash[0] 的值在标志值范围内，即在 (0,4) 区间里，说明已经被搬迁过了 在本例中，老 1 号 bucket 已经被搬迁过了。所以它的 tophash[0] 值在 (0,4) 范围内，因此只用遍历新的 3 号 bucket。 依次遍历 3 号 bucket 的 cell，这时候会找到第一个非空的 key：元素 e。到这里，mapiternext 函数返回，这时我们的遍历结果仅有一个元素： 由于返回的 key 不为空，所以会继续调用 mapiternext 函数。 继续从上次遍历到的地方往后遍历，从新 3 号 overflow bucket 中找到了元素 f 和 元素 g。 遍历结果集也因此壮大： 新 3 号 bucket 遍历完之后，回到了新 0 号 bucket。0 号 bucket 对应老的 0 号 bucket，经检查，老 0 号 bucket 并未搬迁，因此对新 0 号 bucket 的遍历就改为遍历老 0 号 bucket。那是不是把老 0 号 bucket 中的所有 key 都取出来呢？ 并没有这么简单，回忆一下，老 0 号 bucket 在搬迁后将裂变成 2 个 bucket：新 0 号、新 2 号。而我们此时正在遍历的只是新 0 号 bucket（注意，遍历都是遍历的 *bucket 指针，也就是所谓的新 buckets）。所以，我们只会取出老 0 号 bucket 中那些在裂变之后，分配到新 0 号 bucket 中的那些 key。 因此，lowbits == 00 的将进入遍历结果集： 和之前的流程一样，继续遍历新 1 号 bucket，发现老 1 号 bucket 已经搬迁，只用遍历新 1 号 bucket 中现有的元素就可以了。结果集变成： 继续遍历新 2 号 bucket，它来自老 0 号 bucket，因此需要在老 0 号 bucket 中那些会裂变到新 2 号 bucket 中的 key，也就是 lowbit == 10 的那些 key。 这样，遍历结果集变成： 最后，继续遍历到新 3 号 bucket 时，发现所有的 bucket 都已经遍历完毕，整个迭代过程执行完毕。 顺便说一下，如果碰到 key 是 math.NaN() 这种的，处理方式类似。核心还是要看它被分裂后具体落入哪个 bucket。只不过只用看它 top hash 的最低位。如果 top hash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part。据此决定是否取出 key，放到遍历结果集里。 map 遍历的核心在于理解 2 倍扩容时，老 bucket 会分裂到 2 个新 bucket 中去。而遍历操作，会按照新 bucket 的序号顺序进行，碰到老 bucket 未搬迁的情况时，要在老 bucket 中找到将来要搬迁到新 bucket 来的 key Map插入 通过汇编语言可以看到，向 map 中插入或者修改 key，最终调用的是 mapassign 函数。 插入或修改 key 的语法是一样的，前者操作的 key 在 map 中不存在，而后者操作的 key 存在 map 中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; if h == nil &#123; panic(plainError(\"assignment to entry in nil map\")) &#125; if h.flags&amp;hashWriting != 0 &#123; //表示正在写 throw(\"concurrent map writes\") &#125; hash := t.hasher(key, uintptr(h.hash0)) //获取hash值 //在调用t.hasher之后设置hashWriting，因为t.hasher可能会出现紧急情况，在这种情况下，我们实际上并未执行写操作 h.flags ^= hashWriting //设置标记位，hashWriting //如果bucket数组一开始为空，则初始化 if h.buckets == nil &#123; h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) &#125;again: bucket := hash &amp; bucketMask(h.B) // 定位存储在哪一个bucket中 if h.growing() &#123; //如果现正在扩容，则扩容 growWork(t, h, bucket) &#125; //得到bucket的结构体 b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) //获取高八位hash值 var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointerbucketloop: for &#123; //死循环 for i := uintptr(0); i &lt; bucketCnt; i++ &#123; //循环bucket中的tophash数组 if b.tophash[i] != top &#123; //如果hash不相等 if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil &#123; //判断是否为空，为空则插入 inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) &#125; if b.tophash[i] == emptyRest &#123; //为空，且后续也没有非空cell break bucketloop &#125; continue &#125; //到这里说明高八位hash一样，获取已存在的key k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; //判断两个key是否相等，不相等就循环下一个 if !t.key.equal(key, k) &#123; continue &#125; // already have a mapping for key. Update it. if t.needkeyupdate() &#123; typedmemmove(t.key, k, key) &#125; //获取已存在的value elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done &#125; //如果上一个bucket没能插入，则通过overflow获取链表上的下一个bucket ovf := b.overflow(t) if ovf == nil &#123; break &#125; b = ovf &#125; //如果超过负载，则扩容 if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again &#125; //如果当前bmap满了，就新建一个 if inserti == nil &#123; // all current buckets are full, allocate a new one. newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) &#125; //存储元素 if t.indirectkey() &#123; kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem &#125; if t.indirectelem() &#123; vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem &#125; typedmemmove(t.key, insertk, key) *inserti = top h.count++done: //更新状态，返回元素 if h.flags&amp;hashWriting == 0 &#123; //如果是状态异常 throw(\"concurrent map writes\") &#125; h.flags &amp;^= hashWriting //设置标志位状态表示完成 if t.indirectelem() &#123; elem = *((*unsafe.Pointer)(elem)) &#125; return elem&#125; 整体来看，核心还是一个双层循环，外层遍历 bucket 和它的 overflow bucket，内层遍历整个 bucket 的各个 cell：对 key 计算 hash 值，根据 hash 值按照之前的流程，找到要赋值的位置（可能是插入新 key，也可能是更新老 key），对相应位置进行赋值。 注意： 函数首先会检查 map 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。 通过前文我们知道扩容是渐进式的，如果 map 处在扩容的过程中，那么当 key 定位到了某个 bucket 后，需要确保这个 bucket 对应的老 bucket 完成了迁移过程。即老 bucket 里的 key 都要迁移到新的 bucket 中来（分裂到 2 个新 bucket），才能在新的 bucket 中进行插入或者更新的操作。 上面说的操作是在函数靠前的位置进行的，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。 现在到了定位 key 应该放置的位置了，所谓找准自己的位置很重要。准备两个指针，一个（inserti）指向 key 的 hash 值在 tophash 数组所处的位置，另一个(insertk)指向 cell 的位置（也就是 key 最终放置的地址），当然，对应 value 的位置就很容易定位出来了。这三者实际上都是关联的，在 tophash 数组中的索引位置决定了 key 在整个 bucket 中的位置（共 8 个 key），而 value 的位置需要“跨过” 8 个 key 的长度。 在循环的过程中，inserti 和 insertk 分别指向第一个找到的空闲的 cell。如果之后在 map 没有找到 key 的存在，也就是说原来 map 中没有此 key，这意味着插入新 key。那最终 key 的安置地址就是第一次发现的“空位”（tophash 是 empty）。 如果这个 bucket 的 8 个 key 都已经放置满了，那在跳出循环后，发现 inserti 和 insertk 都是空，这时候需要在 bucket 后面挂上 overflow bucket。当然，也有可能是在 overflow bucket 后面再挂上一个 overflow bucket。这就说明，太多 key hash 到了此 bucket。 在正式安置 key 之前，还要检查 map 的状态，看它是否需要进行扩容。如果满足扩容的条件，就主动触发一次扩容操作。 这之后，整个之前的查找定位 key 的过程，还得再重新走一次。因为扩容之后，key 的分布都发生了变化。 最后，会更新 map 相关的值，如果是插入新 key，map 的元素数量字段 count 值会加 1；在函数之初设置的 hashWriting 写标志出会清零。 另外，有一个重要的点要说一下。前面说的找到 key 的位置，进行赋值操作，实际上并不准确。我们看 mapassign 函数的原型就知道，函数并没有传入 value 值，所以赋值操作是什么时候执行的呢？ 1func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer 答案还得从汇编语言中寻找。我直接揭晓答案，有兴趣可以私下去研究一下。mapassign 函数返回的指针就是指向的 key 所对应的 value 值位置，有了地址，就很好操作赋值了。 Map删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) &#123; //如果map为空，直接panic if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return &#125; //如果正在写，则异常并发读写 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map writes\") &#125; //计算hash值 hash := t.hasher(key, uintptr(h.hash0)) //设置标志位 h.flags ^= hashWriting //获取槽 bucket := hash &amp; bucketMask(h.B) if h.growing() &#123; //如果在扩容，直接触发扩容 growWork(t, h, bucket) &#125; b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) bOrig := b top := tophash(hash)search: for ; b != nil; b = b.overflow(t) &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; //双循环，如果topHash不一致，且不为emptyRest则继续 if b.tophash[i] == emptyRest &#123; //否则下一个overflow break search &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) k2 := k if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; if !t.key.equal(key, k2) &#123; continue &#125; // Only clear key if there are pointers in it. if t.indirectkey() &#123; *(*unsafe.Pointer)(k) = nil &#125; else if t.key.ptrdata != 0 &#123; memclrHasPointers(k, t.key.size) &#125; e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() &#123; *(*unsafe.Pointer)(e) = nil &#125; else if t.elem.ptrdata != 0 &#123; memclrHasPointers(e, t.elem.size) &#125; else &#123; memclrNoHeapPointers(e, t.elem.size) &#125; b.tophash[i] = emptyOne // If the bucket now ends in a bunch of emptyOne states, // change those to emptyRest states. // It would be nice to make this a separate function, but // for loops are not currently inlineable. if i == bucketCnt-1 &#123; if b.overflow(t) != nil &amp;&amp; b.overflow(t).tophash[0] != emptyRest &#123; goto notLast &#125; &#125; else &#123; if b.tophash[i+1] != emptyRest &#123; goto notLast &#125; &#125; for &#123; b.tophash[i] = emptyRest if i == 0 &#123; if b == bOrig &#123; break // beginning of initial bucket, we're done. &#125; // Find previous bucket, continue at its last entry. c := b for b = bOrig; b.overflow(t) != c; b = b.overflow(t) &#123; &#125; i = bucketCnt - 1 &#125; else &#123; i-- &#125; if b.tophash[i] != emptyOne &#123; break &#125; &#125; notLast: h.count-- //数量减1 break search &#125; &#125; if h.flags&amp;hashWriting == 0 &#123; throw(\"concurrent map writes\") &#125; //读写恢复 h.flags &amp;^= hashWriting&#125; 当然，我们只关心 mapdelete 函数。它首先会检查 h.flags 标志，如果发现写标位是 1，直接 panic，因为这表明有其他协程同时在进行写操作。 计算 key 的哈希，找到落入的 bucket。检查此 map 如果正在扩容的过程中，直接触发一次搬迁操作。 删除操作同样是两层循环，核心还是找到 key 的具体位置。寻找过程都是类似的，在 bucket 中挨个 cell 寻找。 找到对应位置后，对 key 或者 value 进行“清零”操作 最后，将 count 值减 1，将对应位置的 tophash 值置成 Empty Map类型 12345678910111213141516171819202122232425262728293031type maptype struct &#123; typ _type key *_type elem *_type bucket *_type // internal type representing a hash bucket // function for hashing keys (ptr to key, seed) -&gt; hash hasher func(unsafe.Pointer, uintptr) uintptr keysize uint8 // size of key slot elemsize uint8 // size of elem slot bucketsize uint16 // size of bucket flags uint32&#125;type _type struct &#123; size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; Map进阶 Key可以是float型 从语法上看，是可以的。Go 语言中只要是可比较的类型都可以作为 key。除开 slice，map，functions 这几种类型，其他类型都是 OK 的。具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。这些类型的共同特征是支持 == 和 != 操作符，k1 == k2 时，可认为 k1 和 k2 是同一个 key。如果是结构体，则需要它们的字段值都相等，才被认为是相同的 key chan是怎么比较的？ 删除Key之后，内存是直接释放吗？ 首先看看删除逻辑代码 1234567891011121314if t.indirectkey() &#123; //如果key是指针 *(*unsafe.Pointer)(k) = nil //则将key置位nil&#125; else if t.key.ptrdata != 0 &#123; //key中含有指针 memclrHasPointers(k, t.key.size)&#125;e := add(unsafe.Pointer(b),dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize))if t.indirectelem() &#123; //如果val是指针 *(*unsafe.Pointer)(e) = nil&#125; else if t.elem.ptrdata != 0 &#123; //如果val含有指针 memclrHasPointers(e, t.elem.size)&#125; else &#123; memclrNoHeapPointers(e, t.elem.size) //执行飞对指针&#125;b.tophash[i] = emptyOne 其中 1234567891011func memclrHasPointers(ptr unsafe.Pointer, n uintptr) &#123; bulkBarrierPreWrite(uintptr(ptr), 0, n) //添加写屏障 memclrNoHeapPointers(ptr, n) //clears n bytes starting at ptr 非堆&#125;func typedmemclr(typ *_type, ptr unsafe.Pointer) &#123; if writeBarrier.needed &amp;&amp; typ.ptrdata != 0 &#123; //类型含有指针字段 bulkBarrierPreWrite(uintptr(ptr), 0, typ.ptrdata) &#125; memclrNoHeapPointers(ptr, typ.size)&#125; 首先 memclrHasPointers 与 typedmemclr 的区别仅仅是 调用者是否直到清理的类型中是否含有堆指针 参考文献 如何设计并实现一个线程安全的Map https://www.kancloud.cn/kancloud/the-way-to-go/72489 https://blog.csdn.net/u010853261/article/details/99699350 https://cloud.tencent.com/developer/article/1468799 https://www.jianshu.com/p/7782d82f5154 https://qcrao.com/2019/05/22/dive-into-go-map/ https://github.com/golang/go/issues/20135","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-10-defer","slug":"Go/Go-10-defer","date":"2021-07-03T15:07:15.000Z","updated":"2023-07-15T05:36:37.180Z","comments":true,"path":"2021/07/03/Go/Go-10-defer/","link":"","permalink":"http://xboom.github.io/2021/07/03/Go/Go-10-defer/","excerpt":"","text":"defer原理 defer语句会进入一个栈，在函数return前按先进后出的顺序执行(原因是后面定义的函数可能会依赖前面的资源) 在defer函数定义时，对外部变量的引用是有两种方式的: 作为函数参数，则在defer定义时就把值传递给defer，并被cache起来； 作为闭包引用，则会在defer函数真正调用时根据整个上下文确定当前的值。 defer后的语句在执行的时候，函数调用的参数会被复制。如果此变量是一个“值”，那么就和定义的时候是一致的。如果此变量是一个“引用”，那么就可能和定义的时候不一致。 123456789func main() &#123; var whatever [3]struct&#123;&#125; for i := range whatever &#123; defer func() &#123; fmt.Println(i) &#125;() &#125;&#125; 执行结果为： 123222 因为闭包是根据上下文确定当前的值 123456789101112131415type number intfunc (n number) print() &#123; fmt.Println(n) &#125;func (n *number) pprint() &#123; fmt.Println(*n) &#125;func main() &#123; var n number defer n.print() defer n.pprint() defer func() &#123; n.print() &#125;() defer func() &#123; n.pprint() &#125;() n &#x3D; 3&#125; 执行结果是： 12343330 第四个defer语句是闭包，引用外部函数的n, 最终结果是3 第三个defer语句是闭包同第三个 第二个defer语句，n是引用，最终求值是3 第一个defer语句，对n直接求职，开始的时候n=0，所以最后是0 defer返回 返回语句 return xxx编译后编程三条命令 返回值 = xxx 调用defer函数 空的return 1234567func f() (r int) &#123; t :&#x3D; 5 defer func() &#123; t &#x3D; t + 5 &#125;() return t&#125; 可以看成 1234567891011121314func f() (r int) &#123; t :&#x3D; 5 &#x2F;&#x2F; 1. 赋值指令 r &#x3D; t &#x2F;&#x2F; 2. defer被插入到赋值与返回之间执行，这个例子中返回值r没被修改过 func() &#123; t &#x3D; t + 5 &#125; &#x2F;&#x2F; 3. 空的return指令 return&#125; 结果为5 第二例子： 123456func f() (r int) &#123; defer func(r int) &#123; r &#x3D; r + 5 &#125;(r) return 1&#125; 可以看成： 123456789101112func f() (r int) &#123; &#x2F;&#x2F; 1. 赋值 r &#x3D; 1 &#x2F;&#x2F; 2. 这里改的r是之前传值传进去的r，不会改变要返回的那个r值 func(r int) &#123; r &#x3D; r + 5 &#125;(r) &#x2F;&#x2F; 3. 空的return return&#125; 结果为 1 参考链接 https://qcrao.com/2019/02/12/how-to-keep-off-trap-of-defer/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-08-Channel","slug":"Go/Go-08-Channel","date":"2021-07-03T07:41:44.000Z","updated":"2023-07-15T05:36:31.782Z","comments":true,"path":"2021/07/03/Go/Go-08-Channel/","link":"","permalink":"http://xboom.github.io/2021/07/03/Go/Go-08-Channel/","excerpt":"","text":"channel底层数据结构 12345678910111213141516171819202122232425type hchan struct &#123; // chan 里元素数量 qcount uint // chan 底层循环数组的长度 dataqsiz uint // 指向底层循环数组的指针 // 只针对有缓冲的 channel buf unsafe.Pointer // chan 中元素大小 elemsize uint16 // chan 是否被关闭的标志 closed uint32 // chan 中元素类型 elemtype *_type // element type // 已发送元素在循环数组中的索引(第几个位置 1 ~ len) sendx uint // send index // 已接收元素在循环数组中的索引第几个位置 1 ~ len) recvx uint // receive index // 等待接收的 goroutine 队列 recvq waitq // list of recv waiters // 等待发送的 goroutine 队列 sendq waitq // list of send waiters // 保护 hchan 中所有字段 lock mutex&#125; 其中需要注意的是： buf指向底层循环数组，如果是非缓冲则指向hchan地址 sendq，recvq 分别表示被阻塞的 goroutine，这些 goroutine 由于尝试读取 channel 或向 channel 发送数据而被阻塞(部分通过直接复制memmove的方式传输未被阻塞，也就不在链表中) waitq 是 sudog 的一个双向链表，而 sudog 实际上是对 goroutine 的一个封装 1234type waitq struct &#123; first *sudog last *sudog&#125; lock 用来保证每个读 channel 或写 channel 的操作都是原子的 chan初创建 chan的创建分为含有缓冲区和非缓冲区，都通过 func makechan(t *chantype, size int64) *hchan实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func makechan64(t *chantype, size int64) *hchan &#123; if int64(int(size)) != size &#123; panic(plainError(\"makechan: size out of range\")) &#125; return makechan(t, int(size))&#125;func makechan(t *chantype, size int) *hchan &#123; elem := t.elem //1. 安全检查 if elem.size &gt;= 1&lt;&lt;16 &#123; throw(\"makechan: invalid channel element type\") &#125; if hchanSize%maxAlign != 0 || elem.align &gt; maxAlign &#123; throw(\"makechan: bad alignment\") &#125; //2. 元素分配是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem &gt; maxAlloc-hchanSize || size &lt; 0 &#123; panic(plainError(\"makechan: size out of range\")) &#125; var c *hchan switch &#123; case mem == 0: //a. 非缓冲或者元素大小为0(struct&#123;&#125;) // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. c.buf = c.raceaddr() case elem.ptrdata == 0: //b. 不含有指针，buf指向数组起始阶段 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: //c. 元素含有指针 c = new(hchan) c.buf = mallocgc(mem, elem, true) &#125; c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(&amp;c.lock, lockRankHchan) ... return c&#125; 以上有几点注意： 针对创建不同的chan初始化过程 非缓冲chan 只需要分配一个hchanSize 缓冲区chan 不含有指针，则分配一个连续的内存 缓冲区chan 含有指针，分别分配hchan和 指针需要的内存大小 chan 分配在堆中，返回一个指针*hchan sendq 和 recvq 链表由goroutine初始化时候创建，不在这里创建 chan接收 接收的操作有两种写法 123456789// entry points for &lt;- c from compiled codefunc chanrecv1(c *hchan, elem unsafe.Pointer) &#123; chanrecv(c, elem, true)&#125;func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) &#123; _, received = chanrecv(c, elem, true) return&#125; received 反应channel是否被关闭 接收值会放到elem所指向的指针，如果忽略接收值，则elem为nil 第三个参数 都使用 true 表示阻塞模式， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/* 1. 如果 ep 是 nil，说明忽略了接收值。 2. 如果 block == false，即非阻塞型接收，在没有数据可接收的情况下，返回 (false, false) 3. 否则，如果 c 处于关闭状态，将 ep 指向的地址清零，返回 (true, false)// 否则，用返回值填充 ep 指向的内存地址。返回 (true, true)// 如果 ep 非空，则应该指向堆或者函数调用者的栈*///判断hchan是否有数据func empty(c *hchan) bool &#123; // dataqsiz 一旦初始化chan就不会变化 if c.dataqsiz == 0 &#123; //如果是非缓冲，当存储发送挂起协程的sendq链表为空，表示没有数据 return atomic.Loadp(unsafe.Pointer(&amp;c.sendq.first)) == nil &#125; //缓冲区则判断是否有数据 return atomic.Loaduint(&amp;c.qcount) == 0&#125;// chanbuf(c, i) 指向c的缓冲区第i个元素func chanbuf(c *hchan, i uint) unsafe.Pointer &#123; return add(c.buf, uintptr(i)*uintptr(c.elemsize))&#125;func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; //忽略debug //如果是 nil的channel if c == nil &#123; //如果不阻塞，直接返回 (false, false) if !block &#123; return &#125; //否则，接收一个nil的channel, goroutine 挂起 gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) //不会执行到这里 throw(\"unreachable\") &#125; // 非阻塞模式下，快速检查不用获取锁 if !block &amp;&amp; empty(c) &#123; if atomic.Load(&amp;c.closed) == 0 &#123; //如果chan没有关闭，返回(false, false) return &#125; //这里又做了一次为空判断，防止在关闭检查的时候收到的数据 if empty(c) &#123; // The channel is irreversibly closed and empty. if ep != nil &#123; //如果为空，则返回一个默认值 typedmemclr(c.elemtype, ep) &#125; return true, false &#125; &#125; lock(&amp;c.lock) //获取原子锁 //如果关闭了，且不含有缓冲数据，则ep指向默认值，返回(true, false) if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; unlock(&amp;c.lock) if ep != nil &#123; typedmemclr(c.elemtype, ep) &#125; return true, false &#125; //则获取sendq中阻塞的发送协程进行接收 if sg := c.sendq.dequeue(); sg != nil &#123; //找到一个等待的发送人。 如果缓冲区大小为 0，则接收值直接来自发送者。 //否则，从队列头接收并将发送者的值添加到队列的尾部 recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; //如果没有发送者，且当前缓冲区有数据， //则将数据复制给ep，且清理对应位置缓冲区数据，qcount-- if c.qcount &gt; 0 &#123; //1. 获取对应缓冲区数据 qp := chanbuf(c, c.recvx) //2. 将数据复制给ep if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; //3. 清理对应的缓冲区数据 typedmemclr(c.elemtype, qp) //4. 接收索引+1，如果 等于缓冲区大小，表示结束，从头开始 c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; //4. 缓冲区数目-1 并释放原子锁 c.qcount-- unlock(&amp;c.lock) return true, true &#125; if !block &#123; unlock(&amp;c.lock) return false, false &#125; //没有数据，也没有发送者，则将要阻塞这个协程 gp := getg() //获取当前协程指针 mysg := acquireSudog() // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. //保存代接收数据的地址 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil //进入等待链表中 c.recvq.enqueue(mysg) atomic.Store8(&amp;gp.parkingOnChan, 1) //将当前goroutine 挂起 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) //当goroutine被唤起 if mysg != gp.waiting &#123; //当前协程等待的不是创建的 mysg throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil gp.activeStackChans = false closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed&#125; 需要注意的是 同一个 chan 不能被重新开启 在缓冲区chan buf满了的情况下，发送协程阻塞，则接收者还是会优先处理缓冲区数据 12345678910111213141516171819202122232425262728293031323334353637383940//如果是非缓冲型的，就直接从发送者的栈拷贝到接收者的栈func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) &#123; src := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125;//接收数据的时候发现有协程阻塞func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; if c.dataqsiz == 0 &#123; if ep != nil &#123; //非缓冲直接从sender拷贝到ep recvDirect(c.elemtype, sg, ep) &#125; &#125; else &#123;//非缓冲区 //1. 获取缓冲区数据 qp := chanbuf(c, c.recvx) // 将数据从缓冲区拷贝给receiver if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 将sender的数据拷贝到缓冲区 typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ //索引+1，则刚才加了数据需要循环一遍才能拿到 if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz &#125; sg.elem = nil gp := sg.g // 解锁 unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒发送的 goroutine。需要等到调度器的光临 goready(gp, skip+1)&#125; chan发送 ch &lt;- 3 最终会转换成 chansend 函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105func full(c *hchan) bool &#123; if c.dataqsiz == 0 &#123; //如果是非缓冲区，且没有接收协程，则表示满了 return c.recvq.first == nil &#125; //如果是缓冲区，当缓冲数据等于缓冲区大小 则表示满了 return c.qcount == c.dataqsiz&#125;func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; //如果为空 if c == nil &#123; if !block &#123; //非阻塞模式下 return false &#125; //阻塞模式下，直接挂起 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\"unreachable\") &#125; //... // 如果 channel 未关闭且 channel 没有多余的缓冲空间。直接返回false if !block &amp;&amp; c.closed == 0 &amp;&amp; full(c) &#123; return false &#125; //1. 加锁 lock(&amp;c.lock) //如果channel 关闭，则直接panic if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError(\"send on closed channel\")) &#125; //如果接收协程阻塞存在，说明缓冲区没有数据，直接将sender数据拷贝给receiver if sg := c.recvq.dequeue(); sg != nil &#123; // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; //如果有缓冲空间 if c.qcount &lt; c.dataqsiz &#123; //1. 直接获取缓冲发送位置 qp := chanbuf(c, c.sendx) //2. 将数据直接拷贝进缓冲区 typedmemmove(c.elemtype, qp, ep) //3. 发送位置+1(表示下一个进来的数据存放的位置) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; //3. 数量+1 c.qcount++ unlock(&amp;c.lock) return true &#125; //非阻塞模式直接返回false if !block &#123; unlock(&amp;c.lock) return false &#125; //阻塞当前协程 gp := getg() mysg := acquireSudog() // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) //将协程加入到等待队列中 atomic.Store8(&amp;gp.parkingOnChan, 1) //挂起当前协程 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // 确保正在发送的值保持活动状态，直到接收者复制它 KeepAlive(ep) // 被唤起 if mysg != gp.waiting &#123; throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil gp.activeStackChans = false if gp.param == nil &#123; if c.closed == 0 &#123; throw(\"chansend: spurious wakeup\") &#125; // 被唤醒后，channel 关闭了。坑爹啊，panic panic(plainError(\"send on closed channel\")) &#125; gp.param = nil mysg.c = nil releaseSudog(mysg) return true&#125; 需要注意的是： KeepAlive 的作用和原理是? 123456789101112131415161718192021222324252627282930313233func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; //sg.elem 指向接收到的值存放的位置，如 val &lt;- ch，指的就是 &amp;val if sg.elem != nil &#123; // 直接拷贝内存（从发送者到接收者） sendDirect(c.elemtype, sg, ep) sg.elem = nil &#125; // sudog 上绑定的 goroutine gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒接收的 goroutine. skip 和打印栈相关，暂时不理会 goready(gp, skip+1)&#125;// 向一个非缓冲型的 channel 发送数据、从一个无元素的（非缓冲型或缓冲型但空）的 channel// 接收数据，都会导致一个 goroutine 直接操作另一个 goroutine 的栈// 由于 GC 假设对栈的写操作只能发生在 goroutine 正在运行中并且由当前 goroutine 来写// 所以这里实际上违反了这个假设。可能会造成一些问题，所以需要用到写屏障来规避func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) &#123; // src 在当前 goroutine 的栈上，dst 是另一个 goroutine 的栈 // 直接进行内存\"搬迁\" // 如果目标地址的栈发生了栈收缩，当我们读出了 sg.elem 后 // 就不能修改真正的 dst 位置的值了 // 因此需要在读和写之前加上一个屏障 dst := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125; 关闭chan 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func closechan(c *hchan) &#123; if c == nil &#123; //关闭一个空chan会panic panic(plainError(\"close of nil channel\")) &#125; lock(&amp;c.lock) if c.closed != 0 &#123; //关闭一个关闭的chan会panic unlock(&amp;c.lock) panic(plainError(\"close of closed channel\")) &#125; c.closed = 1 //修改chan状态为关闭 var glist gList // release all readers for &#123; //1. 所有接收协程出链表 sg := c.recvq.dequeue() if sg == nil &#123; break &#125; //2. 如果元素不为空，则清除并设置为空 if sg.elem != nil &#123; typedmemclr(c.elemtype, sg.elem) sg.elem = nil &#125; if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil glist.push(gp) &#125; // release all writers (they will panic) for &#123; //从发送链表中获取协程 sg := c.sendq.dequeue() if sg == nil &#123; break &#125; sg.elem = nil if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil //形成协程链表 glist.push(gp) &#125; unlock(&amp;c.lock) // Ready all Gs now that we've dropped the channel lock. for !glist.empty() &#123; // 取最后一个 gp := glist.pop() gp.schedlink = 0 //环形协程 goready(gp, 3) &#125;&#125; chan应用 针对不同的chan会有不同的效果： 控制并发数 12345678910111213var limit = make(chan int, 3)func main() &#123; // ………… for _, w := range work &#123; go func() &#123; limit &lt;- 1 w() &lt;-limit &#125;() &#125; // …………&#125; 参考链接 Go问题集 https://qcrao.com/2019/07/22/dive-into-go-channel/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-20-编程模式","slug":"Go/Go-20-编程模式","date":"2021-06-18T17:05:45.000Z","updated":"2023-07-15T05:37:41.013Z","comments":true,"path":"2021/06/19/Go/Go-20-编程模式/","link":"","permalink":"http://xboom.github.io/2021/06/19/Go/Go-20-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"深度比较 12345678910111213141516171819202122import ( \"fmt\" \"reflect\")func main() &#123; v1 := data&#123;&#125; v2 := data&#123;&#125; fmt.Println(\"v1 == v2:\",reflect.DeepEqual(v1,v2)) //prints: v1 == v2: true m1 := map[string]string&#123;\"one\": \"a\",\"two\": \"b\"&#125; m2 := map[string]string&#123;\"two\": \"b\", \"one\": \"a\"&#125; fmt.Println(\"m1 == m2:\",reflect.DeepEqual(m1, m2)) //prints: m1 == m2: true s1 := []int&#123;1, 2, 3&#125; s2 := []int&#123;1, 2, 3&#125; fmt.Println(\"s1 == s2:\",reflect.DeepEqual(s1, s2)) //prints: s1 == s2: true&#125; 接口编程 1234567891011121314151617type Shape interface &#123; Sides() int Area() int&#125;type Square struct &#123; len int&#125;func (s* Square) Sides() int &#123; return 4&#125;func main() &#123; s := Square&#123;len: 5&#125; fmt.Printf(\"%d\\n\",s.Sides())&#125;var _ Shape = (*Square)(nil) //接口实现校验 声明一个 _ 变量（没人用）会把一个 nil 的空指针从 Square 转成 Shape，这样，如果没有实现完相关的接口方法，编译器就会报错： cannot use (*Square)(nil) (type *Square) as type Shape in assignment: *Square does not implement Shape (missing Area method) 性能 如果需要把数字转换成字符串，使用 strconv.Itoa() 比 fmt.Sprintf() 要快一倍左右。 尽可能避免把String转成[]Byte ，这个转换会导致性能下降。 如果在 for-loop 里对某个 Slice 使用 append()，请先把 Slice 的容量扩充到位，这样可以避免内存重新分配以及系统自动按 2 的 N 次方幂进行扩展但又用不到的情况，从而避免浪费内存。 使用StringBuffer 或是StringBuild 来拼接字符串，性能会比使用 + 或 +=高三到四个数量级。 尽可能使用并发的 goroutine，然后使用 sync.WaitGroup 来同步分片操作。 避免在热代码中进行内存分配，这样会导致 gc 很忙。尽可能使用 sync.Pool 来重用对象。 使用 lock-free 的操作，避免使用 mutex，尽可能使用 sync/Atomic包（关于无锁编程的相关话题，可参看《无锁队列实现》或《无锁 Hashmap 实现》）。 使用 I/O 缓冲，I/O 是个非常非常慢的操作，使用 bufio.NewWrite() 和 bufio.NewReader() 可以带来更高的性能。 对于在 for-loop 里的固定的正则表达式，一定要使用 regexp.Compile() 编译正则表达式。性能会提升两个数量级。 需要更高性能的协议，就要考虑使用 protobuf 或 msgp 而不是 JSON，因为 JSON 的序列化和反序列化里使用了反射。 使用 Map 的时候，使用整型的 key 会比字符串的要快，因为整型比较比字符串比较要快 错误检查 1234567891011121314151617181920func parse(r io.Reader) (*Point, error) &#123; var p Point if err := binary.Read(r, binary.BigEndian, &amp;p.Longitude); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.Latitude); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.Distance); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationGain); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationLoss); err != nil &#123; return nil, err &#125;&#125; 太多的 if err != nil {} 12345678910111213141516171819202122func parse(r io.Reader) (*Point, error) &#123; var p Point var err error read := func(data interface&#123;&#125;) &#123; if err != nil &#123; //如果失败则不执行 return &#125; err = binary.Read(r, binary.BigEndian, data) &#125; read(&amp;p.Longitude) read(&amp;p.Latitude) read(&amp;p.Distance) read(&amp;p.ElevationGain) read(&amp;p.ElevationLoss) if err != nil &#123; return &amp;p, err &#125; return &amp;p, nil&#125; 上述代码存在一个变量err和一个内部函数 read 12345678910111213141516171819202122232425262728type Reader struct &#123; r io.Reader err error&#125;func (r *Reader) read(data interface&#123;&#125;) &#123; if r.err == nil &#123; r.err = binary.Read(r.r, binary.BigEndian, data) &#125;&#125;func parse(input io.Reader) (*Point, error) &#123; var p Point r := Reader&#123;r: input&#125; r.read(&amp;p.Longitude) r.read(&amp;p.Latitude) r.read(&amp;p.Distance) r.read(&amp;p.ElevationGain) r.read(&amp;p.ElevationLoss) if r.err != nil &#123; return nil, r.err &#125; return &amp;p, nil&#125; 主要应用于相同业务处理场景 错误包装 1234567891011121314import \"github.com/pkg/errors\" //第三方库//错误包装if err != nil &#123; return errors.Wrap(err, \"read failed\")&#125;// Cause接口switch err := errors.Cause(err).(type) &#123;case *MyError: // handle specificallydefault: // unknown error&#125; 多参数 1234567891011121314func main() &#123; slice1 := make([]interface&#123;&#125;, 0) slice2 := make([]interface&#123;&#125;, 1) //不要这样使用 会初始化1的nil slice1 = append(slice1, 1, \"2\") slice2 = append(slice2, 1, \"2\") test(slice1) //args [[1 2]] num:1 test(slice1...) //args [1 2] num:2 test(slice2) //args [[&lt;nil&gt; 1 2]] num:1 test(slice2...) //args [&lt;nil&gt; 1 2] num:3 &#125;func test(args ...interface&#123;&#125;) &#123; fmt.Printf(\"args %v num:%d \\n\", args, len(args))&#125; Map-Reduce 1234567891011121314151617181920212223242526272829303132333435363738func main() &#123; var list = []string&#123;\"Hao\", \"Chen\", \"MegaEase\"&#125; x := Reduce(list, func(s string) int &#123; return len(s) &#125;) fmt.Printf(\"%v\\n\", x) //15 var intset = []int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125; out := Filter(intset, func(n int) bool &#123; return n%2 == 1 &#125;) fmt.Printf(\"%v\\n\", out) //[1 3 5 7 9] out = Filter(intset, func(n int) bool &#123; return n &gt; 5 &#125;) fmt.Printf(\"%v\\n\", out) //[6 7 8 9 10]&#125;func Reduce(arr []string, fn func(s string) int) int &#123; sum := 0 for _, it := range arr &#123; sum += fn(it) &#125; return sum&#125;func Filter(arr []int, fn func(n int) bool) []int &#123; var newArray = []int&#123;&#125; for _, it := range arr &#123; if fn(it) &#123; newArray = append(newArray, it) &#125; &#125; return newArray&#125; 健壮的泛型Map 123456789101112131415161718192021222324func verifyFuncSignature(fn reflect.Value, types ...reflect.Type) bool &#123; //Check it is a funciton if fn.Kind() != reflect.Func &#123; return false &#125; // NumIn() - returns a function type's input parameter count. // NumOut() - returns a function type's output parameter count. if (fn.Type().NumIn() != len(types)-1) || (fn.Type().NumOut() != 1) &#123; return false &#125; // In() - returns the type of a function type's i'th input parameter. for i := 0; i &lt; len(types)-1; i++ &#123; if fn.Type().In(i) != types[i] &#123; return false &#125; &#125; // Out() - returns the type of a function type's i'th output parameter. outType := types[len(types)-1] if outType != nil &amp;&amp; fn.Type().Out(0) != outType &#123; return false &#125; return true&#125; 健壮的泛型Reduce 123456789101112131415161718192021222324252627282930313233func Reduce(slice, pairFunc, zero interface&#123;&#125;) interface&#123;&#125; &#123; sliceInType := reflect.ValueOf(slice) if sliceInType.Kind() != reflect.Slice &#123; panic(\"reduce: wrong type, not slice\") &#125; len := sliceInType.Len() if len == 0 &#123; return zero &#125; else if len == 1 &#123; return sliceInType.Index(0) &#125; elemType := sliceInType.Type().Elem() fn := reflect.ValueOf(pairFunc) if !verifyFuncSignature(fn, elemType, elemType, elemType) &#123; t := elemType.String() panic(\"reduce: function must be of type func(\" + t + \", \" + t + \") \" + t) &#125; var ins [2]reflect.Value ins[0] = sliceInType.Index(0) ins[1] = sliceInType.Index(1) out := fn.Call(ins[:])[0] for i := 2; i &lt; len; i++ &#123; ins[0] = out ins[1] = sliceInType.Index(i) out = fn.Call(ins[:])[0] &#125; return out.Interface()&#125; 健壮的泛型 Filter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func Filter(slice, function interface&#123;&#125;) interface&#123;&#125; &#123; result, _ := filter(slice, function, false) return result&#125;func FilterInPlace(slicePtr, function interface&#123;&#125;) &#123; in := reflect.ValueOf(slicePtr) if in.Kind() != reflect.Ptr &#123; panic(\"FilterInPlace: wrong type, \" + \"not a pointer to slice\") &#125; _, n := filter(in.Elem().Interface(), function, true) in.Elem().SetLen(n)&#125;var boolType = reflect.ValueOf(true).Type()func filter(slice, function interface&#123;&#125;, inPlace bool) (interface&#123;&#125;, int) &#123; sliceInType := reflect.ValueOf(slice) if sliceInType.Kind() != reflect.Slice &#123; panic(\"filter: wrong type, not a slice\") &#125; fn := reflect.ValueOf(function) elemType := sliceInType.Type().Elem() if !verifyFuncSignature(fn, elemType, boolType) &#123; panic(\"filter: function must be of type func(\" + elemType.String() + \") bool\") &#125; var which []int for i := 0; i &lt; sliceInType.Len(); i++ &#123; if fn.Call([]reflect.Value&#123;sliceInType.Index(i)&#125;)[0].Bool() &#123; which = append(which, i) &#125; &#125; out := sliceInType if !inPlace &#123; out = reflect.MakeSlice(sliceInType.Type(), len(which), len(which)) &#125; for i := range which &#123; out.Index(i).Set(sliceInType.Index(which[i])) &#125; return out.Interface(), len(which)&#125; 反射不适用于高性能的地方 类型检查 Type Assert 对变量进行 .(type)的转型操作返回两个值，分别是 variable 和 error variable 是被转换好的类型，error 表示如果不能转换类型，则会报错 12345678910111213141516171819202122232425//Container is a generic container, accepting anything.type Container []interface&#123;&#125;//Put adds an element to the container.func (c *Container) Put(elem interface&#123;&#125;) &#123; *c = append(*c, elem)&#125;//Get gets an element from the container.func (c *Container) Get() interface&#123;&#125; &#123; elem := (*c)[0] *c = (*c)[1:] return elem&#125;intContainer := &amp;Container&#123;&#125;intContainer.Put(7)intContainer.Put(42)// assert that the actual type is intelem, ok := intContainer.Get().(int)if !ok &#123; fmt.Println(\"Unable to read an int from intContainer\")&#125;fmt.Printf(\"assertExample: %d (%T)\\n\", elem, elem) 反射 1234567891011121314151617181920212223242526type Container struct &#123; s reflect.Value&#125;func NewContainer(t reflect.Type, size int) *Container &#123; if size &lt;=0 &#123; size=64 &#125; return &amp;Container&#123; s: reflect.MakeSlice(reflect.SliceOf(t), 0, size), &#125;&#125;func (c *Container) Put(val interface&#123;&#125;) error &#123; if reflect.ValueOf(val).Type() != c.s.Type().Elem() &#123; return fmt.Errorf(“Put: cannot put a %T into a slice of %s\", val, c.s.Type().Elem())) &#125; c.s = reflect.Append(c.s, reflect.ValueOf(val)) return nil&#125;func (c *Container) Get(refval interface&#123;&#125;) error &#123; if reflect.ValueOf(refval).Kind() != reflect.Ptr || reflect.ValueOf(refval).Elem().Type() != c.s.Type().Elem() &#123; return fmt.Errorf(\"Get: needs *%s but got %T\", c.s.Type().Elem(), refval) &#125; reflect.ValueOf(refval).Elem().Set( c.s.Index(0) ) c.s = c.s.Slice(1, c.s.Len()) return nil&#125; 参考文献 https://coolshell.cn/articles/21128.html https://github.com/robpike","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Redis入门1-数据结构","slug":"Redis/Redis入门1-数据结构","date":"2021-05-12T14:07:15.000Z","updated":"2023-03-23T14:36:11.487Z","comments":true,"path":"2021/05/12/Redis/Redis入门1-数据结构/","link":"","permalink":"http://xboom.github.io/2021/05/12/Redis/Redis%E5%85%A5%E9%97%A81-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"这一章主要描述Redis的基本数据结构以及应用 Redis的数据结构 类型 简介 特性 场景 String(字符串) 二进制安全 可以包含任何数据,比如jpg图片或者序列化的对象,一个键最大能存储512M — Hash(字典) 键值对集合,即编程语言中的Map类型 适合存储对象,并且可以像数据库中update一个属性一样只修改某一项属性值(Memcached中需要取出整个字符串反序列化成对象修改完再序列化存回去) 1. 存储、读取、修改用户属性 2. 购物车 List(列表) 链表(双向链表) 增删快,提供了操作某一段元素的API 1. 最新消息排行等功能(比如朋友圈的时间线) 2. 消息队列 3. 最近联系人 Set(集合) 哈希表实现,元素不重复 1、添加、删除,查找的复杂度都是O(1) 2、为集合提供了求交集、并集、差集等操作 1. 标签 2. 去重 3. 关注模型 4. 抽奖 Sorted Set(有序集合) 将Set中的元素增加一个权重参数score,元素按score有序排列 数据插入集合时,已经进行天然排序 1. 时间轴 2. 排行榜 字符串应用场景 123456789101112SET key val //存入字符串键值对MSET key value [key val ...] //批量存储字符串键值对SETNX key value //存入一个不存在的字符串键值对GET key //获取一个字符串键值MGET key [...] //批量获取键值对DEL key [key ...] //删除一个键//原子加减INCR key //将key中存储的数字值加1DECR key //将key中存储的数字值减1INCRBY key increment //将key所存储的值加上incrementDECR key increment //将key所存储的值加上decrement 在原子加减中，如果 val 不是数字，那么将报错 (error) ERR value is not an integer or out of range SETNX 只会存入一个不存在的值，如果存入成功则返回1，否则返回0 应用1：单值缓存 12345678//单值缓存set key valueget key//对象缓存set key value(json)mset k1 v1 k2 v2mget k1 k2 应用2：分布式锁 由于SETNX(set if not exist)无法设置key的有效期，导致锁可能成为死锁，可以使用set完成锁操作，并且是原子操作 1set key value [EX seconds] [PX milliseconds] [NX/XX] EX seoncds: 设置失效时长，单位秒 PX milliseconds: 设置失效时长，单位毫秒 NX: key不存在时设置value，成功返回OK，失败返回(nil) XX: key存在时设置value，成功返回OK，失败返回(nil) 应用3：计数器 比如记录文章阅读次数(如果使用数据库，则需要考虑并发锁)，incrby命令可以实现原子性的递增 12INCR article:readcount:&#123;文章id&#125;GET article:readcount:&#123;文章id&#125; Hash应用场景 12345678HSET key field value //存储一个哈希表的键值HSETNX key field value //存储一个不存在的哈希表KeyHMSET key field value [field value ...] //在一个哈希表key中存储多个键值对HGET key field //获取hash表中key对应的field键值对HMGT key field [field ...] //批量获取哈希表key中多个field的值HDEL key field [field ...] //删除哈希表中key的多个field的值HLEN key //返回哈希表中key的field的数量HINCRBY key field increment //为哈希表key中filed的值加上增量 应用1：购物车 第一步，新增商品到购物车中 1hset cart:1001 10088 1 cart代表的购物车 :1001 这里代表的是用户id 10088 代表的是商品id 1 代表数量 第二步，增加商品数量1 1hincrby cert:1001 10088 1 第三步，获取购物车中商品总量 12345#获取数量hlen cert:1001#获取所有键值对hgetall cert:1001 第四步，删除购物车内容 12#删除购物车hdel cert:1001 优点： 同类数据归类整合存储方便数据管理 相比string操作消耗内存与cpu更小 相比string存储更节省空间 缺点： 过期功能不能使用在field上，只能用在key上 hash的会分配槽位，Redis集群架构下集群中会导致数据过于集中，没办法做分片 List应用场景 1234567lpush key value [...value] #将一个或多个值value插入到key列表的表头(最左边)rpush key value [...value] #将一个或多个值value插入到key列表的表尾(最右边)lpop key #移除并返回key列表的头元素rpop key #移除并返回key列表的尾元素lrange key start stop #返回列表key中制定区间的元素，区间异偏移量start和stop指定blpop key [key ...] timeout #从key表头弹出一个元素，若没有元素则阻塞等待timeout秒，如果timeout = 0,则一直阻塞brpop key [key ...] timeout #从key表尾弹出一个元素，若没有元素则阻塞等待timeout秒，如果timeout = 0,则一直阻塞 应用1：最近联系人 123456#1. 首先存储新的联系人lpush contact:我的id 小明的IDlpush contact:我的id 小强的IDlpush contact:我的id 小白的ID#2. 只保留2位ltrim contact:我的id 0 2 #返回小强的ID 、小白的ID Set 应用场景 1234567891011121314SADD key member [member ...] //往集合key中存入元素，元素存在则忽略，不存在则新建SREM key member [member ...] //从集合中删除元素SMEMBERS key //获取即可key中所有元素SCARD key //获取集合key的元素个数SISMEMBER key member //判断key是否存在于集合中SRANDMEMBER key [count] //从集合key中选出count个元素，元素不从key中删除SPOP key [count] //从结合key中选出count个元素，元素从key中删除SINTER key [key ...] //交集运算SINTERSTORE destionation key [key ...] //交集运算的结果存入新集合 destionation中SUNION key[key ...] //并集运算SUNIONSTORE destionation key [key ...] //并集运算的结果存入新集合 destionation中SDIFF key [key ...] //差集运算SDIFFSOTRE destionation key [key ...] //差集运算的结果存入新集合 destionation中 应用1：去重 由于Set是一种无序，不重复的数据结构，所以用户的操作可以存储在Set中，避免重复操作 12345#重复添加 k/vSADD myset \"hello\"SADD myset \"hello\"SADD myset \"world\"SMEMBERS myset #只得到 “hello” \"world\" 两个值 应用2：标签 记录博客点赞、收藏、标签数。首先是知道自己被谁点赞、收藏，另外是需要知道自己点赞、收藏了哪些 123456789101112#1. 点赞(消息被谁点赞)SADD like:&#123;消息ID&#125; &#123;用户ID&#125;#2. 取消点赞(删除消息的点赞)SREM like:&#123;消息ID&#125; &#123;用户ID&#125;#3. 检查用户是否点赞过SISMEMBER like:&#123;消息ID&#125; &#123;用户ID&#125;#4. 获取点赞的用户列表SMEMBERS like:&#123;消息ID&#125;#5. 获取点赞用户数SCARD like:&#123;消息ID&#125;#6. 消息贴标签SADD tag:&#123;消息ID&#125; &#123;标签ID&#125; 应用3：关注模型 12345678910111213#1. 你关注的人SADD following:我的ID &#123;张三、李四、王五&#125;#2. 小明关注的人SADD following:小明ID &#123;张三、王五、刘六&#125;#3. 小强关注的人SADD following:小强ID &#123;七七、小八&#125;#4. 我跟小明的共同关注SINTER following:我的ID following:小明ID #张三、王五#5. 我关注的人也在关注小明SISMEMBER following:我的ID 小白ID #我是否关注了小白SISMEMBER following:小白ID 小明ID #小白是否关注了小明#6. 我可能认识的人(我关注了小明、小明关注的就是我可能认识的人)SDIFF following:我的ID following:小明 应用4：抽奖 123456#1. 将用户加入集合SADD key &#123;userID&#125;#2. 查看参与抽奖的所有用户SMEMBERS key#3. 抽取count名中奖者SRANDMEMBER key [couunt] / SPOP key [count] SortedSet 应用场景 12345678ZADD key score1 member1 [score2 member2 ...] #往集合key中存入元素，或更新成员分数ZCARD key #获取集合key的元素个数ZCOUNT key min max #计算在有序集合中指定区间分数的成员数ZINCRBY key increment member #有序集合中对指定成员的分数加上增量 ZINTERSTORE destination numkeys key [key ...] # 计算给定的多个有序集的交集并将结果集存储在新的有序集合 destination 中ZLEXCOUNT key min max #在有序集合中计算指定字典区间内成员数量ZREM key member [member ...] #移除有序集合中的一个或多个成员ZSCORE key member #返回有序集中，成员的分数值 应用1：时间轴 12# 每隔5min记录一次温度ZADD temperature:我的ID 123124 30.5 234235 33.5 应用2：排行榜 123456789# 添加用户得分ZADD leaderboard 90 \"Alice\"ZADD leaderboard 80 \"Bob\"ZADD leaderboard 70 \"Charlie\"ZADD leaderboard 60 \"Dave\"ZADD leaderboard 50 \"Eva\"# 获取排行榜前三名ZREVRANGE leaderboard 0 2 WITHSCORES 参考链接 https://www.runoob.com/redis/redis-data-types.html http://doc.redisfans.com https://segmentfault.com/a/1190000038173551","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门0-设计","slug":"Redis/Redis入门0-设计","date":"2021-04-13T16:07:16.000Z","updated":"2023-03-23T14:31:25.545Z","comments":true,"path":"2021/04/14/Redis/Redis入门0-设计/","link":"","permalink":"http://xboom.github.io/2021/04/14/Redis/Redis%E5%85%A5%E9%97%A80-%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"前言 Redis的设计与实现主要关注以下几点： 高性能：线程模型(CPU)、数据结构(内存)、AOF(IO)、epoll网络框架(网络) 高可靠：主从复制、哨兵模式、RDB 高课拓展：数据分片，负载均衡 想系统的学习Redis参考将其章节分为 数据结构 底层结构 结构对象 数据库 RDB AOF 集群 缓存 事务 发布与订阅 Lua脚本 Stream 多版本 实战经验 常见问题 参考资料 《Redis设计与实现》 https://pdai.tech/md/db/nosql-redis/db-redis-overview.html https://time.geekbang.org/column/100056701?tab=catalog https://time.geekbang.org/column/intro/100084301?tab=catalog","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"MySql-09-存储引擎","slug":"MySql/MySql-09-存储引擎","date":"2021-02-17T23:20:02.000Z","updated":"2023-05-28T16:15:55.501Z","comments":true,"path":"2021/02/18/MySql/MySql-09-存储引擎/","link":"","permalink":"http://xboom.github.io/2021/02/18/MySql/MySql-09-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/","excerpt":"","text":"提到数据库就会说到存储引擎，这里做一下简单的比较 特征 MyISAM BDB Memory InnoDB Archive NDB 存储限制 是 64TB 是 事务处理 是 是 锁定粒度 表 页 表 行 行 行 MVCC/快照读 是 是 是 地理空间支持 是 B-Tree 索引 是 是 是 是 是 Hash 索引 是 是 全文搜索索引 是 数据缓存 是 是 是 索引缓存 是 是 是 压缩数据 是 是 加密数据 是 是 是 是 是 是 大批量插入速度 高 高 高 低 非常高 高 存储成本 低 低 N/A 高 非常低 低 内存成本 低 低 中等 高 低 高 集群数据库支持 是 数据复制支持 是 是 是 是 是 是 外键支持 是 备份/时间点恢复 是 是 是 是 是 是 查询缓存支持 是 是 是 是 是 是 数据字典更新统计 是 是 是 是 是 这里主要对 InnoDB 与 MyISAM 进行比较 MyISAM 特点： 不支持事务处理：MyISAM 不支持事务的原子性和一致性，因此在并发写入的情况下可能出现数据不一致的问题。 表级锁定：MyISAM 使用表级锁定，即当对某个表进行写操作时，会锁定整个表，这可能导致并发性能较差。 较快的读取性能：由于没有事务处理和行级锁定的开销，MyISAM 在读取操作上具有较高的性能。 支持全文搜索：MyISAM 提供全文索引功能，使得对文本字段进行高效的全文搜索成为可能。 较低的存储和内存成本：相比其他存储引擎，MyISAM 具有较低的存储和内存消耗。 InnoDB 特点： 支持事务处理：InnoDB 支持事务的原子性、一致性、隔离性和持久性（ACID 特性），能够保证数据的完整性。 行级锁定：InnoDB 使用行级锁定，可以实现更好的并发性能，不同事务可以同时修改不同行的数据。 支持外键约束：InnoDB 支持定义外键约束，可以确保数据的引用完整性。 支持崩溃恢复：InnoDB 具有崩溃恢复机制，能够在数据库崩溃或停电后自动恢复数据的一致性。 支持 MVCC：InnoDB 使用多版本并发控制（MVCC）来处理读写冲突，可以实现更好的并发性能。 MyISAM 适用场景： 读取密集型应用：MyISAM 在读取操作上具有较好的性能，适合于读取密集型的应用，如数据报表、数据分析等场景。 高并发读取：如果应用需要处理大量并发的读取请求，而对数据一致性和事务支持要求较低，MyISAM 可能是一个选择。 非关键数据存储：如果存储的数据不太重要，或者有备份机制保证数据可恢复性，可以考虑使用 MyISAM InnoDB 适用场景： 事务处理：如果应用需要支持事务的原子性、一致性、隔离性和持久性，或者需要进行复杂的数据操作和更新，应选择 InnoDB。 并发写入：InnoDB 使用行级锁定，支持高并发的写入操作，适合于高并发写入的应用场景，如电子商务、社交网络等。 数据完整性和安全性要求高：如果应用需要强制执行外键约束，保证数据引用的完整性，或者需要更高的数据安全性，使用 InnoDB 是一个较好的选择。 数据库崩溃恢复：InnoDB 具有崩溃恢复机制，能够在数据库崩溃或停电后自动恢复数据的一致性，适用于对数据可靠性要求较高的应用场景。 参考链接 https://pdai.tech/md/db/sql-mysql/sql-mysql-execute.html","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"Go-02-Slice","slug":"Go/Go-02-Slice","date":"2020-12-12T08:39:51.000Z","updated":"2023-07-15T05:33:03.191Z","comments":true,"path":"2020/12/12/Go/Go-02-Slice/","link":"","permalink":"http://xboom.github.io/2020/12/12/Go/Go-02-Slice/","excerpt":"","text":"Go 语言的函数参数传递，只有值传递，没有引用传递 数组与切片 在 Go 中，与 C 数组变量隐式作为指针使用不同，Go 数组是值类型，赋值和函数传参操作都会复制整个数组数据 123456789101112131415161718192021222324252627func main() &#123; arrayA := [2]int&#123;100, 200&#125; var arrayB [2]int arrayB = arrayA fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB) arrayA[0] = 200 testArray(arrayA) fmt.Println(arrayA) fmt.Println(arrayB) testArray(arrayB)&#125;func testArray(x [2]int) &#123; fmt.Printf(\"func Array : %p , %p, %p, %v\\n\", &amp;x, &amp;x[0], &amp;x[1], x) x[0] = 300&#125;//运行结果为：arrayA : 0xc00010c010 , 0xc00010c010, 0xc00010c018, [100 200]arrayB : 0xc00010c020 , 0xc00010c020, 0xc00010c028, [100 200]func Array : 0xc00010c060 , 0xc00010c060, 0xc00010c068, [200 200][200 200][100 200]func Array : 0xc00010c0a0 , 0xc00010c0a0, 0xc00010c0a8, [100 200] 分析： 当直接使用 arrayB = arrayA 进行的是值复制，也就是说Go数组是值类型 将数组赋值给函数的时候，也是对整个数组进行复制，函数内改变值并不会改变外面的数组 上面的例子换成切片会怎么样？ 1234567891011121314151617181920212223242526func main() &#123; arrayA := []int&#123;100, 200, 300&#125; arrayB := arrayA[0:2] fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v, %d\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB, cap(arrayB)) arrayA[0] = 200 testArray(arrayB) fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v, %d\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB, cap(arrayB))&#125;func testArray(x []int) &#123; fmt.Printf(\"func Array : %p , %p, %p, %v %d\\n\", &amp;x, &amp;x[0], &amp;x[1], x, cap(x)) x[0] = 300 x = append(x, 400) fmt.Printf(\"func Array : %p , %p, %p, %v %d\\n\", &amp;x, &amp;x[0], &amp;x[1], x, cap(x))&#125;//运行结果为:arrayA : 0xc00000c060 , 0xc000014140, 0xc000014148, [100 200 300]arrayB : 0xc00000c080 , 0xc000014140, 0xc000014148, [100 200], 3func Array : 0xc00000c0e0 , 0xc000014140, 0xc000014148, [200 200] 3func Array : 0xc00000c0e0 , 0xc000014140, 0xc000014148, [300 200 400] 3arrayA : 0xc00000c060 , 0xc000014140, 0xc000014148, [300 200 400]arrayB : 0xc00000c080 , 0xc000014140, 0xc000014148, [300 200], 3 分析： 当直接使用 arrayB = arrayA 进行的是其实是值复制，地址空间不一致，但是指向的内存空间一致 切片做参数传递的时候,但函数内容修改切片内容并不会影响外部切片，改变底层数组会影响外部切片 切片数据结构 切片（slice）切片是一个引用类型，是对数组一个连续片段的引用。这个片段是由起始和终止索引标识的一些项的子集。终止索引标识的项不包括在切片内(左闭右开)。切片提供了一个与指向数组的动态窗口。 给定项的切片索引可能比相关数组的相同元素的索引小。和数组不同的是，切片的长度可以在运行时修改，最小为 0 最大为相关数组的长度：切片是一个长度可变的数组 12345type slice struct &#123; array unsafe.Pointer //指向数组的指针 len int //当前切片长度 cap int //当前切片容量&#125; uintptr is an integer type that is large enough to hold the bit pattern of any pointer unsafe.Pointer 通用指针,类似C中的 void * 切片创建 首先这里会有一个常见函数，math.MulUintptr 将两个参数相乘来判断是否溢出 123456789//MulUintptr返回a * b以及乘法是否溢出。在受支持的平台上，这是编译器固有的功能。//true 越界;false 没有越界func MulUintptr(a, b uintptr) (uintptr, bool) &#123; if a|b &lt; 1&lt;&lt;(4*sys.PtrSize) || a == 0 &#123; return a * b, false &#125; overflow := b &gt; MaxUintptr/a return a * b, overflow&#125; 解析： sys.PtrSize 在64位机器中为8 12const PtrSize = 4 &lt;&lt; (^uintptr(0) &gt;&gt; 63) // unsafe.Sizeof(uintptr(0)) but an ideal constconst MaxUintptr = ^uintptr(0) &lt; 1&lt;&lt;(4*sys.PtrSize)相当于&lt; 1&lt;&lt;32 ,而8位计算机中最大的是 2^64-1,所以判断a和b都小于2^32即可 否则 b &gt; MaxUintptr/a 如果 b 大于最大值除以a,则说明 a*b 超过最大值即越界 make与字面量切片 支持通过make或字面量的方式进行创建切片 12slice1 := make([]int, 4, 6) //makeslice2 := []int&#123;1,2,3,4&#125; //字面量，注意[]内不要写容量，否则是数组 再来看分片函数 makeslice 12345678910111213141516func makeslice(et *_type, len, cap int) unsafe.Pointer &#123; mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem &gt; maxAlloc || len &lt; 0 || len &gt; cap &#123; mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem &gt; maxAlloc || len &lt; 0 &#123; panicmakeslicelen() &#125; panicmakeslicecap() &#125; // 分配内存 // 小对象从当前P 的cache中空闲数据中分配 // 大的对象 (size &gt; 32KB) 直接从heap中分配 // runtime/malloc.go return mallocgc(mem, et, true)&#125; 解析： 根据 数据类型大小 x 切片容量 判断是否会越界 如果 越界或超过最大分配长度maxAlloc(32位与64位不一致)，长度大于容器。则尝试 根据 数据类型大小 x 切片长度 判断溢出 如果 长度溢出则 panicmakeslicelen: panic报错 “makeslice: len out of range” 如果 容量溢出则 panicmakeslicecap: panic报错 “makeslice: cap out of range” 否则分配内存： 1func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer &#123;&#125; 所以切片其实分配了容量大小的内存，只是访问不到，也被初始化 空切片与nil切片 123var slice []int //nil切片silce := make( []int , 0 ) //空切片slice := []int&#123; &#125; //空切片 空切片和 nil 切片的区别在于：空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素。 不管是使用 nil 切片还是空切片，对其调用内置函数 append，len 和 cap 的效果都是一样的 切片扩容 扩容原理 growslice用来处理在使用append时候的切片扩容，那么它的规则如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100func growslice(et *_type, old slice, cap int) slice &#123; if raceenabled &#123; callerpc := getcallerpc() racereadrangepc(old.array, uintptr(old.len*int(et.size)), callerpc, funcPC(growslice)) &#125; if msanenabled &#123; msanread(old.array, uintptr(old.len*int(et.size))) &#125; //当新容量比旧容量还有小的时候，直接panic报错 if cap &lt; old.cap &#123; panic(errorString(\"growslice: cap out of range\")) &#125; if et.size == 0 &#123; //如果切片元素大小为0，还调用了扩容方法，那么就新生成一个新的容量的切片返回 return slice&#123;unsafe.Pointer(&amp;zerobase), old.len, cap&#125; &#125; newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap &#123; //1. 当指定容量大于旧切片2倍，则使用指定容量 newcap = cap &#125; else &#123; //2. 指定容量小于旧切片2倍 if old.len &lt; 1024 &#123; newcap = doublecap //2.1. 如果旧切片长度小于1024，则使用容量为旧容量两倍 &#125; else &#123; //2.2. 如果旧切片长度大于1024且小于指定容量，则新容量循环增加自身的四分之一直到大于指定容量 for 0 &lt; newcap &amp;&amp; newcap &lt; cap &#123; newcap += newcap / 4 &#125; if newcap &lt;= 0 &#123; //2.3. 如果旧切片长度等于0，则新容量等于旧容量 newcap = cap &#125; &#125; &#125; var overflow bool var lenmem, newlenmem, capmem uintptr // Specialize for common values of et.size. // For 1 we don't need any division/multiplication. // For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch &#123; case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) &gt; maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) &gt; maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): var shift uintptr if sys.PtrSize == 8 &#123; // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) &amp; 63 &#125; else &#123; shift = uintptr(sys.Ctz32(uint32(et.size))) &amp; 31 &#125; lenmem = uintptr(old.len) &lt;&lt; shift newlenmem = uintptr(cap) &lt;&lt; shift capmem = roundupsize(uintptr(newcap) &lt;&lt; shift) overflow = uintptr(newcap) &gt; (maxAlloc &gt;&gt; shift) newcap = int(capmem &gt;&gt; shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) &#125; if overflow || capmem &gt; maxAlloc &#123; panic(errorString(\"growslice: cap out of range\")) &#125; var p unsafe.Pointer if et.ptrdata == 0 &#123; //先将 P 地址加上新的容量得到新切片容量的地址，然后将新切片容量地址后面的 capmem-newlenmem 个 bytes 这块内存初始化。为之后继续 append() 操作腾出空间 p = mallocgc(capmem, nil, false) // memclrNoHeapPointers clears n bytes starting at ptr memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) &#125; else &#123; // 重新申请 capmen 这个大的内存地址，并且初始化为0值 p = mallocgc(capmem, et, true) if lenmem &gt; 0 &amp;&amp; writeBarrier.enabled &#123; // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem-et.size+et.ptrdata) &#125; &#125; //将 lenmem 这个多个 bytes 从 old.array地址 拷贝到 p 的地址处 memmove(p, old.array, lenmem) return slice&#123;p, old.len, newcap&#125; //返回新切片&#125; 上述就是扩容的实现。主要需要关注的有两点: 扩容时候的策略 首先判断，如果新申请容量（cap）大于2倍的旧容量（old.cap），最终容量（newcap）就是新申请的容量（cap） 否则判断，如果旧切片的长度小于1024，则最终容量(newcap)就是旧容量(old.cap)的两倍，即（newcap=doublecap） 否则判断，如果旧切片长度大于等于1024，则最终容量（newcap）从旧容量（old.cap）开始循环增加原来的 1/4，即（newcap=old.cap,for {newcap += newcap/4}）直到最终容量（newcap）大于等于新申请的容量(cap)，即（newcap &gt;= cap） 如果最终容量（cap）计算值溢出，则最终容量（cap）就是新申请容量（cap） 扩容是生成全新的内存地址还是在原来的地址后追加 扩容实例 实例1： 123456789101112131415161718192021222324252627func main() &#123; s := make([]int, 0) oldCap := cap(s) for i := 0; i &lt; 2048; i++ &#123; s = append(s, i) newCap := cap(s) if newCap != oldCap &#123; fmt.Printf(\"[%d -&gt; %4d] cap = %-4d | after append %-4d cap = %-4d\\n\", 0, i-1, oldCap, i, newCap) oldCap = newCap &#125; &#125;&#125;//运行结果为:[0 -&gt; 0] cap = 1 | after append 1 cap = 2 [0 -&gt; 1] cap = 2 | after append 2 cap = 4 [0 -&gt; 3] cap = 4 | after append 4 cap = 8 [0 -&gt; 7] cap = 8 | after append 8 cap = 16 [0 -&gt; 15] cap = 16 | after append 16 cap = 32 [0 -&gt; 31] cap = 32 | after append 32 cap = 64 [0 -&gt; 63] cap = 64 | after append 64 cap = 128 [0 -&gt; 127] cap = 128 | after append 128 cap = 256 [0 -&gt; 255] cap = 256 | after append 256 cap = 512 [0 -&gt; 511] cap = 512 | after append 512 cap = 1024[0 -&gt; 1023] cap = 1024 | after append 1024 cap = 1280[0 -&gt; 1279] cap = 1280 | after append 1280 cap = 1696[0 -&gt; 1695] cap = 1696 | after append 1696 cap = 2304 分析： 1280/1024 = 1.25 , 1696/1024 = 1.325, 2304/1696=1.3584,为什么每次增加的倍数都不一样呢？ * 实例2： 1234567891011121314func main() &#123; slice := []int&#123;10, 20, 30, 40&#125; newSlice := append(slice, 50) fmt.Printf(\"Before slice %v, Pointer %p, Pointer0 %p, len %d, cap %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"Before newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf(\"After slice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"After newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice))&#125;Before slice [10 20 30 40], Pointer 0xc00000c060, Pointer0 0xc000014140, len 4, cap 4Before newSlice = [10 20 30 40 50], Pointer = 0xc00000c080, Pointer0 0xc000016140, len = 5, cap = 8After slice = [10 20 30 40], Pointer = 0xc00000c060, Pointer0 0xc000014140, len = 4, cap = 4After newSlice = [10 30 30 40 50], Pointer = 0xc00000c080, Pointer0 0xc000016140, len = 5, cap = 8 分析： 当旧切片容量小于1024，新切片容量直接翻倍 这里分配了一个新地址，修改新分片，旧分片并不会改变 实例2： 12345678910111213141516171819func main() &#123; array := [4]int&#123;10, 20, 30, 40&#125; slice := array[0:2] newSlice := append(slice, 50) fmt.Printf(\"Before array %v, Poninter %p, Poninter0 %p \\n\", array, &amp;array, &amp;array[0]) fmt.Printf(\"Before slice %v, Pointer %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"Before newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf(\"After slice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"After newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) fmt.Printf(\"After array = %v, Poninter = %p, Pointer0 %p \\n\", array, &amp;array, &amp;array[0])&#125;Before array [10 20 50 40], Poninter 0xc000014140, Poninter0 0xc000014140 Before slice [10 20], Pointer 0xc00000c060, Pointer0 0xc000014140, len = 2, cap = 4Before newSlice = [10 20 50], Pointer = 0xc00000c080, Pointer0 0xc000014140, len = 3, cap = 4After slice = [10 30], Pointer = 0xc00000c060, Pointer0 0xc000014140, len = 2, cap = 4After newSlice = [10 30 50], Pointer = 0xc00000c080, Pointer0 0xc000014140, len = 3, cap = 4After array = [10 30 50 40], Poninter = 0xc000014140, Pointer0 0xc000014140 分析： 与实例1进行对比：由于slice还有容量可以扩容，所以执行 append() 操作以后，会在原数组上直接操作，这种情况下，扩容以后的数组还是指向原来的数组 实例1中由于没有容量进行扩容，所以执行append之后指向的就是一个全新的数组，修改值并不会影响原来的数组 由于数组是值类型，而切片是应引用类型，所以看到 &amp;array 与 &amp;array[0]的地址是一样的。但是 &amp;slice 与 &amp;slice[0]的地址是不一样的。另外两者指向的第一个值地址都是一样的，指向同一片内存。 由于容量导致结果不一致，极易产生bug。 实例3： 123456789101112131415161718192021222324252627func main() &#123; s := make([]int, 0) oldCap := cap(s) for i := 0; i &lt; 2048; i++ &#123; s = append(s, i) newCap := cap(s) if newCap != oldCap &#123; fmt.Printf(\"[%d -&gt; %4d] cap = %-4d | after append %-4d cap = %-4d\\n\", 0, i-1, oldCap, i, newCap) oldCap = newCap &#125; &#125;&#125;//运行结果为:[0 -&gt; -1] cap = 0 | after append 0 cap = 1 [0 -&gt; 0] cap = 1 | after append 1 cap = 2 [0 -&gt; 1] cap = 2 | after append 2 cap = 4 [0 -&gt; 3] cap = 4 | after append 4 cap = 8 [0 -&gt; 7] cap = 8 | after append 8 cap = 16 [0 -&gt; 15] cap = 16 | after append 16 cap = 32 [0 -&gt; 31] cap = 32 | after append 32 cap = 64 [0 -&gt; 63] cap = 64 | after append 64 cap = 128 [0 -&gt; 127] cap = 128 | after append 128 cap = 256 [0 -&gt; 255] cap = 256 | after append 256 cap = 512 [0 -&gt; 511] cap = 512 | after append 512 cap = 1024[0 -&gt; 1023] cap = 1024 | after append 1024 cap = 1280[0 -&gt; 1279] cap = 1280 | after append 1280 cap = 1696[0 -&gt; 1695] cap = 1696 | after append 1696 cap = 2304 分析： 切片拷贝 拷贝原理 123456789101112131415161718192021222324252627282930313233343536func slicecopy(toPtr unsafe.Pointer, toLen int, fmPtr unsafe.Pointer, fmLen int, width uintptr) int &#123; if fmLen == 0 || toLen == 0 &#123; return 0 &#125; n := fmLen if toLen &lt; n &#123; n = toLen &#125; if width == 0 &#123; return n &#125; if raceenabled &#123; //竞争检测 callerpc := getcallerpc() pc := funcPC(slicecopy) racereadrangepc(fmPtr, uintptr(n*int(width)), callerpc, pc) racewriterangepc(toPtr, uintptr(n*int(width)), callerpc, pc) &#125; if msanenabled &#123; // 如果开启了 The memory sanitizer (msan) msanread(fmPtr, uintptr(n*int(width))) msanwrite(toPtr, uintptr(n*int(width))) &#125; size := uintptr(n) * width if size == 1 &#123; // common case worth about 2x to do here // TODO: is this still worth it with new memmove impl? //如果只有一个元素，那么指针直接转换即可 *(*byte)(toPtr) = *(*byte)(fmPtr) // known to be a byte pointer &#125; else &#123; //如果不止一个元素，那么就把 size 个 bytes 从 fm.array 地址开始，拷贝到 to.array 地址之后 memmove(toPtr, fmPtr, size) &#125; return n&#125; 拷贝实例 实例1： 12345678910111213func main() &#123; array := []int&#123;10, 20, 30, 40&#125; slice := make([]int, 6) slice1 := make([]int, 3) n := copy(slice, array) m := copy(slice1, array) fmt.Println(n, slice) fmt.Println(m, slice1)&#125;//运行结果4 [10 20 30 40 0 0]3 [10 20 30] 分析： copy返回复制数目，slicecopy 方法最终结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了 实例2： 1234567891011func main() &#123; slice := []int&#123;10, 20, 30, 40&#125; for index, value := range slice &#123; fmt.Printf(\"value = %d , value-addr = %x , slice-addr = %x\\n\", value, &amp;value, &amp;slice[index]) &#125;&#125;value = 10 , value-addr = c000018088 , slice-addr = c000014140value = 20 , value-addr = c000018088 , slice-addr = c000014148value = 30 , value-addr = c000018088 , slice-addr = c000014150value = 40 , value-addr = c000018088 , slice-addr = c000014158 分析： 如果用 range 的方式去遍历一个切片， Value 其实是切片里面的值拷贝。所以每次打印 Value 的地址都不变 由于 Value 是值拷贝而非引用传递，所以直接改 Value 是达不到更改原切片值的目的的，需通过 &amp;slice[index] 获取真实的地址 参考文献 https://halfrost.com/go_slice/ https://www.kancloud.cn/kancloud/the-way-to-go/72489","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go-01-闭包","slug":"Go/Go-01-闭包","date":"2020-12-08T14:58:29.000Z","updated":"2023-07-15T05:32:50.730Z","comments":true,"path":"2020/12/08/Go/Go-01-闭包/","link":"","permalink":"http://xboom.github.io/2020/12/08/Go/Go-01-%E9%97%AD%E5%8C%85/","excerpt":"","text":"闭包概念 所谓闭包是指内层函数引用了外层函数中的变量或称为引用了自由变量的函数，其返回值也是一个函数 123闭包 = 函数 + 引用环境接受一个或多个函数作为输入 输出一个函数 函数 函数只是一段可执行代码，编译后就“固化”了，每个函数在内存中只有一份实例，得到函数的入口点便可以执行函数了。 在函数式编程语言中，函数是一等公民（First class value）：第一类对象，我们不需要像命令式语言中那样借助函数指针，委托操作函数，函数可以作为另一个函数的参数或返回值，可以赋给一个变量。函数可以嵌套定义，即在一个函数内部可以定义另一个函数，有了嵌套函数这种结构，便会产生闭包问题 匿名函数 函数可以像普通的类型（整型、字符串等）一样进行赋值、作为函数的参数传递、作为函数的返回值等。 匿名函数可以动态的创建，与之成对比的常规函数必须在包中编译前就定义完毕。匿名函数可以随时改变功能 Golang的函数只能返回匿名函数！ 闭包实例 实例1： 1234567891011121314151617181920212223242526272829303132//函数片段func add(base int) func(int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) //打印变量地址，可以看出来 内部函数时对外部传入参数的引用 f := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base += i return base &#125; return f&#125;//由 main 函数作为程序入口点启动func main() &#123; t1 := add(10) fmt.Println(t1(1), t1(2)) t2 := add(100) fmt.Println(t2(1), t2(2))&#125;//结果为0xc0000b40080xc0000b40080xc0000b400811 130xc0000b40200xc0000b40200xc0000b4020101 103 可以看出： 函数add返回一个函数，返回的这个函数就是闭包 调用同一个闭包函数，都是对同一个环境进行操作 函数add每进入一次，就形成了一个新的环境，对应的闭包中，函数都是同一个函数，环境却是引用不同的环境 实例2.1： 1234567891011121314151617//由 main 函数作为程序入口点启动func main() &#123; x, y := 1,2 defer func(a int)&#123; fmt.Println(\"defer x, y = \", a, y) //y为闭包引用 &#125;(x) //x值拷贝 调用时传入参数 x += 100 y += 200 fmt.Println(x, y)&#125;//结果为：101 202defer x, y = 1 202 实例2.2： 12345678910111213141516171819//由 main 函数作为程序入口点启动func main() &#123; for i := 0; i &lt; 3; i++ &#123; //多次注册延迟调用，相反顺序执行 defer func()&#123; fmt.Println(i) //闭包引用局部变量 &#125;() fmt.Print(i) if i == 2 &#123; fmt.Printf(\"\\n\") &#125; &#125;&#125;//结果为：012333 实例3： 123456789101112131415161718192021222324252627282930313233343536373839404142//返回加减函数，重点：内部函数时对外部变量的引用func calc(base int) (func(int) int, func(int) int) &#123; fmt.Printf(\"%p\\n\", &amp;base) add := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base += i return base &#125; sub := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base -= i return base &#125; return add, sub&#125;//由 main 函数作为程序入口点启动func main() &#123; f1, f2 := calc(100) fmt.Println(f1(1), f2(2)) //执行顺序：f1 f2 println fmt.Println(f1(3), f2(4)) fmt.Println(f1(5), f2(6)) fmt.Println(f1(7), f2(8))&#125;//结果为：0xc00001a0880xc00001a0880xc00001a088101 990xc00001a0880xc00001a088102 980xc00001a0880xc00001a088103 970xc00001a0880xc00001a088104 96 可以看出： 可利用go特性，同时返回两个闭包函数 相同父环境得两个闭包函数，使用的是相同的内存变量 实例4： 123456789101112131415161718//由 main 函数作为程序入口点启动func main() &#123; for i:=0; i&lt;5; i++ &#123; go func()&#123; fmt.Println(i) //i变量值也是引用.创建5个线程执行函数， for循环执行过程中可能执行完的时候，线程刚好处于i的某个值。 &#125;() &#125; time.Sleep(time.Second * 1)&#125;//结果为55555 闭包中的值是对源变量的引用。指向的是变量的当前值。当延迟调用函数体内某个变量作为defer匿名函数的参数，则在定义defer时已获得值拷贝，否则引用某个变量的地址(引用拷贝) 实例5(阻塞)： 123456789101112131415161718func main() &#123; //创建slice cs := make([](chan int), 10) for i := 0; i &lt; len(cs); i++ &#123; cs[i] = make(chan int) &#125; for i := range cs &#123; go func() &#123; cs[i] &lt;- i //创建线程，但是i是引用外部变量，不一定等线程执行的时候就是当前i值 &#125;() &#125; for i := 0; i &lt; len(cs); i++ &#123; t := &lt;-cs[i] //读取值的时候，可能会出现一只阻塞的情况 fmt.Println(t) &#125;&#125; 主要功能就是 创建10个线程执行函数，并向channal写入值。由于goroutine还没有开始，i的值已经跑到了最大9，使得这几个goroutine都取的i=9这个值，从而都向cs[9]发消息，导致执行t := &lt;-cs[i]时，cs[0]、cs[1]、cs[2] … 都阻塞起来了，从而导致了死锁 方案1(利用闭包传每次循环得参数)： 12345for i := range cs &#123; go func(index int) &#123; cs[index] &lt;- index &#125;(i)&#125; 方案2(利用阻塞管道，每次等待协程起来再执行下一次)： 12345678ch := make(chan int)for i := range cs &#123; go func() &#123; ch &lt;- 1 cs[i] &lt;- i &#125;() &lt;- ch&#125; 闭包原理 闭包是函数和它所引用的环境。像一个结构体 1234type Closure struct &#123; F func()() i *int&#125; 通过汇编查看闭包底层 1234567891011package mainfunc f(i int) func() int &#123; return func() int &#123; i++ return i &#125;&#125;//执行命令 go tool compile -S demo.go 得出结果为 可以看到LEAQ type.noalg.struct &#123; F uintptr; \"\".i *int &#125;(SB), CX 运行汇编命令得到得结果为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132➜ Go go tool compile -S demo.go&quot;&quot;.f STEXT size&#x3D;157 args&#x3D;0x10 locals&#x3D;0x20 0x0000 00000 (demo.go:3) TEXT &quot;&quot;.f(SB), ABIInternal, $32-16 0x0000 00000 (demo.go:3) MOVQ (TLS), CX 0x0009 00009 (demo.go:3) CMPQ SP, 16(CX) 0x000d 00013 (demo.go:3) PCDATA $0, $-2 0x000d 00013 (demo.go:3) JLS 147 0x0013 00019 (demo.go:3) PCDATA $0, $-1 0x0013 00019 (demo.go:3) SUBQ $32, SP 0x0017 00023 (demo.go:3) MOVQ BP, 24(SP) 0x001c 00028 (demo.go:3) LEAQ 24(SP), BP 0x0021 00033 (demo.go:3) FUNCDATA $0, gclocals·2589ca35330fc0fce83503f4569854a0(SB) 0x0021 00033 (demo.go:3) FUNCDATA $1, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB) 0x0021 00033 (demo.go:3) LEAQ type.int(SB), AX 0x0028 00040 (demo.go:3) MOVQ AX, (SP) 0x002c 00044 (demo.go:3) PCDATA $1, $0 0x002c 00044 (demo.go:3) CALL runtime.newobject(SB) 0x0031 00049 (demo.go:3) MOVQ 8(SP), AX 0x0036 00054 (demo.go:3) MOVQ AX, &quot;&quot;.&amp;i+16(SP) 0x003b 00059 (demo.go:3) MOVQ &quot;&quot;.i+40(SP), CX 0x0040 00064 (demo.go:3) MOVQ CX, (AX) 0x0043 00067 (demo.go:4) LEAQ type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;(SB), CX 0x004a 00074 (demo.go:4) MOVQ CX, (SP) 0x004e 00078 (demo.go:4) PCDATA $1, $1 0x004e 00078 (demo.go:4) CALL runtime.newobject(SB) 0x0053 00083 (demo.go:4) MOVQ 8(SP), AX 0x0058 00088 (demo.go:4) LEAQ &quot;&quot;.f.func1(SB), CX 0x005f 00095 (demo.go:4) MOVQ CX, (AX) 0x0062 00098 (demo.go:4) PCDATA $0, $-2 0x0062 00098 (demo.go:4) CMPL runtime.writeBarrier(SB), $0 0x0069 00105 (demo.go:4) JNE 131 0x006b 00107 (demo.go:4) MOVQ &quot;&quot;.&amp;i+16(SP), CX 0x0070 00112 (demo.go:4) MOVQ CX, 8(AX) 0x0074 00116 (demo.go:4) PCDATA $0, $-1 0x0074 00116 (demo.go:4) MOVQ AX, &quot;&quot;.~r1+48(SP) 0x0079 00121 (demo.go:4) MOVQ 24(SP), BP 0x007e 00126 (demo.go:4) ADDQ $32, SP 0x0082 00130 (demo.go:4) RET 0x0083 00131 (demo.go:4) PCDATA $0, $-2 0x0083 00131 (demo.go:4) LEAQ 8(AX), DI 0x0087 00135 (demo.go:4) MOVQ &quot;&quot;.&amp;i+16(SP), CX 0x008c 00140 (demo.go:4) CALL runtime.gcWriteBarrierCX(SB) 0x0091 00145 (demo.go:4) JMP 116 0x0093 00147 (demo.go:4) NOP 0x0093 00147 (demo.go:3) PCDATA $1, $-1 0x0093 00147 (demo.go:3) PCDATA $0, $-2 0x0093 00147 (demo.go:3) CALL runtime.morestack_noctxt(SB) 0x0098 00152 (demo.go:3) PCDATA $0, $-1 0x0098 00152 (demo.go:3) JMP 0 0x0000 65 48 8b 0c 25 00 00 00 00 48 3b 61 10 0f 86 80 eH..%....H;a.... 0x0010 00 00 00 48 83 ec 20 48 89 6c 24 18 48 8d 6c 24 ...H.. H.l$.H.l$ 0x0020 18 48 8d 05 00 00 00 00 48 89 04 24 e8 00 00 00 .H......H..$.... 0x0030 00 48 8b 44 24 08 48 89 44 24 10 48 8b 4c 24 28 .H.D$.H.D$.H.L$( 0x0040 48 89 08 48 8d 0d 00 00 00 00 48 89 0c 24 e8 00 H..H......H..$.. 0x0050 00 00 00 48 8b 44 24 08 48 8d 0d 00 00 00 00 48 ...H.D$.H......H 0x0060 89 08 83 3d 00 00 00 00 00 75 18 48 8b 4c 24 10 ...&#x3D;.....u.H.L$. 0x0070 48 89 48 08 48 89 44 24 30 48 8b 6c 24 18 48 83 H.H.H.D$0H.l$.H. 0x0080 c4 20 c3 48 8d 78 08 48 8b 4c 24 10 e8 00 00 00 . .H.x.H.L$..... 0x0090 00 eb e1 e8 00 00 00 00 e9 63 ff ff ff .........c... rel 5+4 t&#x3D;17 TLS+0 rel 36+4 t&#x3D;16 type.int+0 rel 45+4 t&#x3D;8 runtime.newobject+0 rel 70+4 t&#x3D;16 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0 rel 79+4 t&#x3D;8 runtime.newobject+0 rel 91+4 t&#x3D;16 &quot;&quot;.f.func1+0 rel 100+4 t&#x3D;16 runtime.writeBarrier+-1 rel 141+4 t&#x3D;8 runtime.gcWriteBarrierCX+0 rel 148+4 t&#x3D;8 runtime.morestack_noctxt+0&quot;&quot;.f.func1 STEXT nosplit size&#x3D;19 args&#x3D;0x8 locals&#x3D;0x0 0x0000 00000 (demo.go:4) TEXT &quot;&quot;.f.func1(SB), NOSPLIT|NEEDCTXT|ABIInternal, $0-8 0x0000 00000 (demo.go:4) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (demo.go:4) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (demo.go:4) MOVQ 8(DX), AX 0x0004 00004 (demo.go:5) MOVQ (AX), CX 0x0007 00007 (demo.go:5) INCQ CX 0x000a 00010 (demo.go:5) MOVQ CX, (AX) 0x000d 00013 (demo.go:6) MOVQ CX, &quot;&quot;.~r0+8(SP) 0x0012 00018 (demo.go:6) RET 0x0000 48 8b 42 08 48 8b 08 48 ff c1 48 89 08 48 89 4c H.B.H..H..H..H.L 0x0010 24 08 c3 $..go.cuinfo.packagename. SDWARFINFO dupok size&#x3D;0 0x0000 6d 61 69 6e main&quot;&quot;..inittask SNOPTRDATA size&#x3D;24 0x0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0010 00 00 00 00 00 00 00 00 ........runtime.memequal64·f SRODATA dupok size&#x3D;8 0x0000 00 00 00 00 00 00 00 00 ........ rel 0+8 t&#x3D;1 runtime.memequal64+0runtime.gcbits.01 SRODATA dupok size&#x3D;1 0x0000 01 .type..namedata.*struct &#123; F uintptr; i *int &#125;- SRODATA dupok size&#x3D;32 0x0000 00 00 1d 2a 73 74 72 75 63 74 20 7b 20 46 20 75 ...*struct &#123; F u 0x0010 69 6e 74 70 74 72 3b 20 69 20 2a 69 6e 74 20 7d intptr; i *int &#125;type.*struct &#123; F uintptr; &quot;&quot;.i *int &#125; SRODATA dupok size&#x3D;56 0x0000 08 00 00 00 00 00 00 00 08 00 00 00 00 00 00 00 ................ 0x0010 23 f3 08 44 08 08 08 36 00 00 00 00 00 00 00 00 #..D...6........ 0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0030 00 00 00 00 00 00 00 00 ........ rel 24+8 t&#x3D;1 runtime.memequal64·f+0 rel 32+8 t&#x3D;1 runtime.gcbits.01+0 rel 40+4 t&#x3D;5 type..namedata.*struct &#123; F uintptr; i *int &#125;-+0 rel 48+8 t&#x3D;1 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0runtime.gcbits.02 SRODATA dupok size&#x3D;1 0x0000 02 .type..namedata..F- SRODATA dupok size&#x3D;5 0x0000 00 00 02 2e 46 ....Ftype..namedata.i- SRODATA dupok size&#x3D;4 0x0000 00 00 01 69 ...itype.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125; SRODATA dupok size&#x3D;128 0x0000 10 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 ................ 0x0010 b7 43 25 9a 02 08 08 19 00 00 00 00 00 00 00 00 .C%............. 0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0040 02 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 ................ 0x0050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0070 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 ................ rel 32+8 t&#x3D;1 runtime.gcbits.02+0 rel 40+4 t&#x3D;5 type..namedata.*struct &#123; F uintptr; i *int &#125;-+0 rel 44+4 t&#x3D;6 type.*struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0 rel 48+8 t&#x3D;1 type..importpath.&quot;&quot;.+0 rel 56+8 t&#x3D;1 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+80 rel 80+8 t&#x3D;1 type..namedata..F-+0 rel 88+8 t&#x3D;1 type.uintptr+0 rel 104+8 t&#x3D;1 type..namedata.i-+0 rel 112+8 t&#x3D;1 type.*int+0gclocals·2589ca35330fc0fce83503f4569854a0 SRODATA dupok size&#x3D;10 0x0000 02 00 00 00 02 00 00 00 00 00 ..........gclocals·9fb7f0986f647f17cb53dda1484e0f7a SRODATA dupok size&#x3D;10 0x0000 02 00 00 00 01 00 00 00 00 01 ..........gclocals·33cdeccccebe80329f1fdbee7f5874cb SRODATA dupok size&#x3D;8 0x0000 01 00 00 00 00 00 00 00 ........ 参考链接 https://segmentfault.com/a/1190000019753885 https://www.cnblogs.com/landv/p/11589074.html","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"TCP-5-拥塞控制","slug":"TCP/TCP-5-拥塞控制","date":"2020-11-06T14:46:37.000Z","updated":"2023-06-02T11:36:53.002Z","comments":true,"path":"2020/11/06/TCP/TCP-5-拥塞控制/","link":"","permalink":"http://xboom.github.io/2020/11/06/TCP/TCP-5-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","excerpt":"","text":"引言 有了流量控制为什么还要有拥塞控制？ 流量控制是避免 发送方 的数据填满 接收方 的缓存。 在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大…. 所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。 拥塞控制 是避免 发送方 的数据填满整个网络。 在 发送方 调节所要发送数据的量，定义了 拥塞窗口 拥塞窗口和发送窗口有什么区别？ 拥塞窗口 cwnd 是发送方维护的一个 的状态变量，会根据网络的拥塞程度动态变化的。 前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，加入拥塞窗口概念后，发送窗口的值是swnd = min(cwnd, rwnd)，即拥塞窗口和接收窗口中的最小值。 拥塞窗口 cwnd 变化的规则：只要网络中没有出现拥塞， cwnd 就会增大； 但网络中出现了拥塞， cwnd 就减少； 怎么知道网络出现了阻塞？ 只要 发送方 没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就认为网络 出现了拥塞。 拥塞控制主要是四个算法： 慢启动 拥塞避免 拥塞发生 快速恢复 慢启动 TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据 包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？ 慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，就拥塞窗口 cwnd 的大小就会加 1。 假定拥塞窗口 cwnd 和发送窗口 swnd 相等，下面举个栗子： 连接建立完成后，一开始初始化 cwnd = 1 ，表示可以传一个 MSS 大小的数据。 当收到 1 个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个 当收到 2 个的 ACK 确认应答后，cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个 当收到 4 个的 ACK 确认到来的时候，4 个确认 cwnd 增加 4，于是就可以 比之前多发 4 个，所以这一次能够发送 8 个 可以看出慢启动算法，发包的个数是指数性的增长。 那慢启动涨到什么时候是个头呢？ 有一个叫慢启动门限 ssthresh （slow start threshold）状态变量 当 cwnd &lt; ssthresh 时，使用慢启动算法 当 cwnd &gt;= ssthresh 时，就会使用 拥塞避免算法 拥塞避免 当拥塞窗口 cwnd &gt;= ssthresh 慢启动门限 就会进入拥塞避免算法 一般来说 ssthresh 的大小是 65535 字节 = 2^16 - 1 那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd 接上前面的慢启动的例子，现假定 ssthresh 为 8 ： 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次 能够发送 9 个 MSS 大小的数据，变成了线性增长 拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶 段，但是增长速度缓慢了一些。 就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失 的数据包进行重传。 当触发了重传机制，也就进入了 拥塞发生算法 拥塞发生 当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种： 超时重传 快速重传 发生超时重传则就会使用拥塞发生算法。 这个时候，sshresh 和 cwnd 的值会发生变化： ssthresh 设为 cwnd/2 cwnd 重置为 1 接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦超时重传 ，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。 还有更好的方式 快速重传算法： 当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。 TCP 认为这种情况不严重，因为大部分没丢，只丢了小部分，则 ssthresh 和 cwnd 变化如下： cwnd = cwnd/2 设置为原来的一半; ssthresh = cwnd 进入快速恢复算法 快速恢复 快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也 不那么糟糕，所以没有必要像 RTO 超时那么强烈。 进入快速恢复之前， cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，设置为原来的一半 ssthresh = cwnd 然后进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了） 重传丢失的数据包 如果再收到重复的 ACK，那么 cwnd 增加 1 如果收到新数据的 ACK 后，设置 cwnd 为 ssthresh，接着就进入了拥塞避免算法 也就是没有像 超时重传 一夜回到解放前，而是还在比较高的值，后续呈线性增长","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-4-流量控制","slug":"TCP/TCP-4-流量控制","date":"2020-11-06T14:02:09.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/06/TCP/TCP-4-流量控制/","link":"","permalink":"http://xboom.github.io/2020/11/06/TCP/TCP-4-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/","excerpt":"","text":"引言 如果发送方一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。 流量控制可以让发送方根据接收方的实际接收能力控制发送 的数据量 如果窗口固定 客户端是接收方，服务端是发送方， 假设接收窗口和发送窗口相同都为 200 假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响 根据上图的流量控制，说明下每个过程(本次是把服务端作为发送方，所以没有画出服务端的接收窗口)： 客户端向服务端发送请求数据报文 服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 Usable 减少为 120 字 节，同时 SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。 客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节， RCV.NXT 也就指向 321，这意味着 客户端期望的下一个报文的序列号是 321，接着发送确认报文给服务端。 服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法在继续发送数据。 客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节， RCV.NXT 也就指向 441，接着 发送确认报文给服务端。 服务端收到对 80 字节数据的确认报文后， SND.UNA 指针往右偏移后指向 321，于是可用窗口 Usable 增大到 80。 服务端收到对 120 字节数据的确认报文后， SND.UNA 指针往右偏移后指向 441，于是可用窗口 Usable 增大到 200。 服务端可以继续发送了，于是发送了 160 字节的数据后， SND.NXT 指向 601，于是可用窗口 Usable 减少到 40。 客户端收到 160 字节后，接收窗口往右移动了 160 字节， RCV.NXT 也就是指向了 601，接着发送 确认报文给服务端。 服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 SND.UNA 指针 偏移了 160 后指向 601，可用窗口 Usable 也就增大至了 200。 操作系统缓冲区与滑动窗口的关系 前面的流量控制例子假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中 所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。 当应用进程没法及时读取缓冲区的内容时，也会对缓冲区造成影响。 操作系统的缓冲区，是如何影响发送窗口和接收窗口的呢？ 缓冲区与滑动窗口 考虑以下场景： 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 360 ； 服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。 根据上图的流量控制，说明下每个过程： 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140） 服务端收到 140 字节数据，但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100），最后发送确认信息时，将窗口大小通过给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 260 客户端发送 180 字节数据，此时可用窗口减少到 80 服务端收到 180 字节数据，但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区， 于是接收窗口收缩到了 80 （260 - 180），并在发送确认信息时，通过窗口大小给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 80 客户端发送 80 字节数据后，可用窗口耗尽 服务端收到 80 字节数据，但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是 接收窗口收缩到了 0，并在发送确认信息时，通过窗口大小给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 0。 可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变 缩小缓冲区 当服务端系统资源非常紧张，操心系统可能会直接减少接收缓冲区大小，应用程序会因为无法及时读取缓存数据，可能会出现丢包 说明下每个过程： 客户端发送 140 字节的数据，于是可用窗口减少到了 220 服务端因为现在非常的繁忙，操作系统把接收缓存减少了 100 字节，当收到对端 140 数据确认报文后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大 小从 360 收缩成了 100，最后发送确认信息时，通告窗口大小给对方 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户 端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40 服务端收到了 180 字节数据时，发现数据大小超过了接收窗口的大小，于是就把数据包丢失了 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗 口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。 所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。所以TCP 规定不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段间再减少缓存 窗口关闭 TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控 制。 如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭 窗口关闭存在潜在危险：接收方向发送方通告窗口大小时，是通过 ACK 报文来通告的。 当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个ACK报文丢失呢？ 这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，就会出现相互等待的死锁现象 TCP 是如何解决窗口关闭时，潜在的死锁现象呢？ 为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 窗口探测 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器； 如果接收窗口不是 0，那么死锁的局面就可以被打破了。 窗口探查探测的次数一般为 3 此次，每次次大约 30-60 秒（不同的实现可能会不一样，如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接 糊涂窗口综合症 如果接收方太忙了，来不及取走接收窗口里的数据，就会导致发送方的发送窗口越来越小。 到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。 要知道，我们的 TCP + IP 头有 40 个字节，如果只是为了传几个字节不太划算 考虑以下场景： 接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下： 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据； 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节； 每个过程的窗口大小不断减少了，并且发送的数据都是比较小的了 糊涂窗口综合症的现象： 接收方可以通告一个小的窗口 而发送方可以发送小数据 解决糊涂窗口综合症的办法是： 让接收方不通告小窗口给发送方：当 窗口大小 小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会 向发送方通告窗口为 0 ，也就阻止了发送方再发数据过来。 等到接收方处理了一些数据后，窗口大小 &gt;= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。 让发送方避免发送小数据：使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据： 要等到窗口大小 &gt;= MSS 或是 数据大小 &gt;= MSS 收到之前发送数据的 ack 回包 只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件 Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。 可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每 个应用自己的特点来关闭） 1setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&amp;value, sizeof(int))","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-3-滑动窗口","slug":"TCP/TCP-3-滑动窗口","date":"2020-11-02T15:53:18.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/02/TCP/TCP-3-滑动窗口/","link":"","permalink":"http://xboom.github.io/2020/11/02/TCP/TCP-3-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"","text":"引言 TCP 是每发送一个数据，都要进行一次确认应答。如果上一个数据包收到应答再发送下一个。数据包往返时间越长，通信的效率就越低。 为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，也不会降低网络通信的效率 有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值 窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。 假设窗口大小为 3 个 TCP 段，那么发送方就可以连续发送 3 个 TCP 段，若中途若有 ACK 丢失，可以通过下一个确认应答进行确认 图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通话下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据接收方都收到了。这个模式就叫累计确认或者累计应答 窗口的大小由哪一方决定？ TCP 头里有一个字段叫 Window ，即窗口大小。 接收端告知发送端自己还有多少缓冲区，发送方数据大小不能超过这个窗口大小，否则接收方无法正常接收数据 发送方的滑动窗口 下图发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口： #1 是已发送并收到 ACK确认的数据：1~31 字节 #2 是已发送但未收到 ACK确认的数据：32~45 字节 #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节 #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后 在下图，当发送方把数据全部都一下发送出去后，可用窗口为 0 ，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据 在下图，当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 52~56 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了 程序是如何表示发送方的四个部分的呢？ TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移） SND.WND：表示发送窗口的大小（大小是由接收方指定的） SND.UNA：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节 SND.NXT：是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的 第一个字节 指向 #4 的第一个字节是个相对指针，它需要 SND.NXT 指针加上 SND.WND 大小的偏移量，就可以指向 #4 的第一个字节了 可用窗口大小的计算就是： 可用窗口大 = SND.WND -（SND.NXT - SND.UNA） 接收方的滑动窗口 接收窗口相对简单一些，根据处理的情况划分成三个部分： #1 + #2 是已成功接收并确认的数据（等待应用进程读取）； #3 是未收到数据但可以接收的数据； #4 未收到数据并不可以接收的数据； 其中三个接收部分，使用两个指针进行划分: RCV.WND ：表示接收窗口的大小，它会通告给发送方 RCV.NXT ：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节 指向 #4 的第一个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND 大小的偏移量，就可以指向 #4 的第一个字节 接收窗口和发送窗口的大小是相等的吗？ 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。 因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发 送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。 滑动窗口是固定的吗？ 如果是固定窗口，就会存在两个问题： 如果说窗口过小，当传输比较大的数据的时候需要不停的对数据进行确认，会造成很大的延迟。 如果说窗口的大小定义的过大。假设发送方一次发送100个数据。但接收方只能处理50个数据。这样每次都会只对这50个数据进行确认。发送方下一次还是发送100个数据，但是接受方还是只能处理50个数据。这样就有不必要的数据来拥塞我们的链路。 窗口左边沿向右移动称为窗口合拢，发生在数据被发送和确认后(重复ACK会会被丢弃，左边沿不会左移)。 窗口右边沿向右移动称为窗口张开，发生在另一端的接收进程读取已经确认的数据并释放了TCP的接收缓存时 窗口右边沿向左移动称为窗口收缩。RFC建议不使用这种方式。但TCP必须能够在某一端这种情况时进行处理 如果左边沿到达右边沿，则称其为一个零窗口，此时发送方不能够发送任何数据","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"Linux-C01-Tools","slug":"Linux/Linux-C01-Tools","date":"2020-11-02T02:24:18.000Z","updated":"2023-08-10T04:27:07.678Z","comments":true,"path":"2020/11/02/Linux/Linux-C01-Tools/","link":"","permalink":"http://xboom.github.io/2020/11/02/Linux/Linux-C01-Tools/","excerpt":"","text":"前言 Linux性能分析 CPU性能 磁盘I/O性能 内存性能 网络性能 Linux大神Brendan D.Gregg的Linux Performance Tools Virtual Memory vmstat vmstat：Virtual Meomory Statistics(虚拟内存统计)，可对操作系统的虚拟内存、进程、CPU活动进行监控 vmstat 命令详解 使用 man vmstat 查看 vmstat 的使用说明 123456789101112131415161718192021222324252627vmstat [options] [delay [count]]# 参数详解delay 表示间隔更新的时间。如果不指定，则只会打印一次从系统启动到现在的平均值count 更新次数，在没有计数的情况下，定义延迟时，默认值为无限-a, --active -f, --forks 显示从启动到现在forks次数-m, --slabs 显示内核slab缓冲区信息-n, --one-header 标题只显示一次而不是定期显示-s, --state 显示各种事件计数器和内存统计信息的表。此显示不重复-d, --disk 显示磁盘统计-D, --disk-sum 显示磁盘摘要信息-p, --partition device 显示分区统计信息-S, --unit character 在1000(k),1024(k),1000000(m)或1048576(m)字节之间切换输出。不会更改交换(si/so)或块(bi/bo)字段-t, --timestamp 每一行显示时间戳-w, --wide 宽输出模式（适用于内存量较大的系统，其中默认输出模式会出现不需要的列中断）。输出宽度超过80个字符每行NOTES 1. vmstat does not require special permissions. 2. These reports are intended to help identify system bottlenecks. Linux vmstat does not count itself as a running process. 3. All linux blocks are currently 1024 bytes. Old kernels may report blocks as 512 bytes, 2048 bytes, or 4096 bytes. 4. Since procps 3.1.9, vmstat lets you choose units (k, K, m, M). Default is K (1024 bytes) in the default mode. FILES /proc/meminfo /proc/stat /proc/*/stat vmstat常用命令 vmstat 2 1 间隔2s采集一次服务器状态，1表示只采集一次 12345678910111213141516171819202122232425262728293031323334353637[xboom@localhost ~]$ vmstat 2 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 698692 70384 80 136748 159 827 4004 965 250 511 5 4 91 0 0#FIELD DESCRIPTION FOR VM MODEProcs r: The number of runnable processes (running or waiting for run time). b: The number of processes in uninterruptible sleep. Memory swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. inact: the amount of inactive memory. (-a option) active: the amount of active memory. has been recently userd (-a option) Swap si: Amount of memory swapped in from disk (kb/s) so: Amount of memory swapped to disk (kb/s) IO bi: Blocks received from a block device (blocks/s). default 1024bytes per block bo: Blocks sent to a block device (blocks/s).System in: The number of interrupts per second, including the clock. cs: The number of context switches per second.CPU These are percentages of total CPU time. us: Time spent running non-kernel code. (user time, including nice time) sy: Time spent running kernel code. (system time) id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time. wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle. st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown. Active memory: This information is currently in memory, and has been recently used. Inactive memory: This information in memory is not actively being used, but was recently used. For example, if you’ve been using Mail and then quit it, the RAM that Mail was using is marked as Inactive memory. This Inactive memory is available for use by another application, just like Free memory. However, if you open Mail before its Inactive memory is used by a different application, Mail will open quicker because its Inactive memory is converted to Active memory, instead of loading Mail from the slower hard dis vmstat -d 查看磁盘信息 123456789101112131415161718192021222324[xboom@localhost ~]$ vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur secsda 73171 8499 7041491 19482 12371 181181 1645664 10221 0 23sr0 52 0 2125 55 0 0 0 0 0 0dm-0 46855 0 6696761 13980 5743 0 143320 2504 0 17dm-1 32794 0 266008 6924 187764 0 1502112 308321 0 10#FIELD DESCRIPTION FOR DISK MODEReads total: Total reads completed successfully merged: grouped reads (resulting in one I/O) sectors: Sectors read successfully ms: milliseconds spent readingWrites total: Total writes completed successfully merged: grouped writes (resulting in one I/O) sectors: Sectors written successfully ms: milliseconds spent writingIO cur: I/O in progress s: seconds spent for I/O vmstat -p dev 查看分区情况 12345678[root@localhost ~]# vmstat -p /dev/sda1sda1 reads read sectors writes requested writes 343 68386 34 456#FIELD DESCRIPTION FOR DISK PARTITION MODE reads: Total number of reads issued to this partition,分区的读的次数 read sectors: Total read sectors for partition,分区的读扇区的次数 writes : Total number of writes issued to this partition,分区写的次数 requested writes: Total number of write requests made for partition,分区写请求次数 vmstat -m 查看系统slab信息 123456789101112[root@localhost ~]# vmstat -mCache Num Total Size Pagesisofs_inode_cache 92 92 704 46fuse_inode 36 36 896 36nf_conntrack 102 102 320 51#FIELD DESCRIPTION FOR SLAB MODE cache: Cache name num: Number of currently active objects total: Total number of available objects size: Size of each object pages: Number of pages with at least one active object slab:由于内核会有许多小对象，这些对象构造销毁十分频繁，比如i-node，dentry，这些对象如果每次构建的时候就向内存要一个页(4kb),这样就会非常浪费，为了解决这个问题，就引入了一种新的机制来处理在同一个页框中如何分配小存储区，而slab可以对小对象进行分配,这样就不用为每一个对象分配页框，从而节省了空间，内核对一些小对象创建析构很频繁，slab对这些小对象进行缓冲,可以重复利用,减少内存分配次数 vmstat -f显示从系统启动至今的fork(创建进程的系统调用)数量 12[root@localhost ~]# vmstat -f 4319 forks Block Device Int-块设备IO接口 pidstat pidstat命令用于监视Linux内核当前正在管理的各个任务 pidstat 命令详解 使用 man pidstat 查看 pidstat 的使用说明, 进程(任务)相关统计 1234567891011121314151617181920212223242526272829303132333435363738394041pidstat [ -d ] [ -H ] [ -h ] [ -I ] [ -l ] [ -R ] [ -r ] [ -s ] [ -t ] [ -U [ username ] ] [ -u ] [ -V ] [ -v ] [ -w ] [ -C comm ] [ -G process_name ] [ --human ] [ -p &#123; pid [,...] | SELF | ALL &#125; ] [ -T &#123; TASK | CHILD | ALL &#125; ] [ interval [ count ] ] [ -e program args ]#描述 1. 未指定 -p &lt;pid&gt; 将默认使用-p ALL，仅显示活动任务(统计值非0的任务) 2. 使用 -T 查看子进程信息 3. interval参数表示每次报告的时间间隔，0(或没有参数)表示自系统启动以来的的任务统计信息。如果没有设置为0，则可以与 count 一起使用，count表示指定间隔时间之后生成的报告次数。如果没有则是连续报告#参数详解-C comm 仅显示命令名称包含字符串comm的任务。 该字符串可以是正则表达式-d 报告I/O统计信息（仅内核2.6.20及更高版本）-e program args 给定参数args执行程序，并使用pidstat对其进行监视。当程序终止时，pidstat停止-G process_name 仅显示命令名称包含字符串process_name的进程。该字符串可以是正则表达式。 如果选项-t与选项-G一起使用，则属于该进程的线程为也会显示， 即使其命令名称不包含字符串process_name-H 以秒为单位显示时间戳-h 所有活动都单行显示，报告末尾没有平均统计信息。目的是使它易于被其他程序解析--human 以人类可读的格式打印尺寸（例如1.0k，1.2M等），设置与指标关联的其他任何默认单位（例如，千字节，扇区...）。-I 在多处理器环境中，指示任务CPU使用率（如选项-u所示）应除以处理器总数.-l 显示完成的进程命令和它的参数-p &#123; pid [,...] | SELF | ALL &#125; 选择要报告其统计信息的任务。 pid 指定进程标识号 SELF 报告pidstat进程本身的统计信息 ALL 报告系统管理的所有任务的统计信息。-R 报告实时优先级和调度策略信息 prio 任务的优先级 policy 任务的调度策略-r 报告页面错误和内存利用率。当报告单个任务的统计信息时，可能会显示以下值：-s 报告堆栈利用率-T &#123; TASK | CHILD | ALL &#125; 此选项指定pidstat命令必须监视的内容。 TASK 表示将报告单个任务的统计信息（这是默认选项） CHILD 表示报告所选任务及其子进程 ALL 表示要报告单个任务的统计信息，并针对所选全局报告任务和子进程 注意：任务及其所有子项的全局统计信息不适用于pidstat的所有选项 同样，这些统计信息不一定与当前时间间隔有关：子进程的统计信息仅在完成或被杀死时才收集进程-t 显示与所选任务关联的线程的统计信息 TGID 线程组标识符 TID 线程标识符-U [ username ] 显示正在监视的任务的真实用户名，而不是UID。如果指定了用户名，则仅显示属于指定用户的任务 -u 显示CPU利用率-v 内核表信息-w 显示进程上下文切换情况 pidstat常用命令 pidstat 2 5 每两秒钟显示一次系统中每个活动任务的CPU统计信息的五个报告 1234567891011121314151617181920212223[xboom@localhost ~]$ pidstat 2 5Linux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时10分13秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分16秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分16秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分18秒 0 5569 0.00 0.50 0.00 0.00 0.50 0 kworker/0:1-ata_sff23时10分18秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分18秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分20秒 0 4130 0.50 0.00 0.00 0.00 0.50 0 sssd_kcm23时10分20秒 1000 5605 0.50 0.00 0.00 0.00 0.50 0 pidstat23时10分20秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分22秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分22秒 UID PID %usr %system %guest %wait %CPU CPU Command平均时间: UID PID %usr %system %guest %wait %CPU CPU Command平均时间: 0 4130 0.10 0.00 0.00 0.00 0.10 - sssd_kcm平均时间: 0 5569 0.00 0.10 0.00 0.00 0.10 - kworker/0:1-ata_sff平均时间: 1000 5605 0.10 0.30 0.00 0.00 0.40 - pidstat pidstat -d 报告I/O统计信息（仅内核2.6.20及更高版本） 123456789101112131415[xboom@localhost ~]$ pidstat -dLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时11分56秒 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command23时11分56秒 1000 2050 0.99 0.00 0.00 12 dbus-daemon23时11分56秒 1000 2052 0.81 0.00 0.00 0 gdm-wayland-ses23时11分56秒 1000 2056 28.68 0.07 0.00 6 gnome-session-b23时11分56秒 1000 2104 197.84 0.05 0.00 199 gnome-shell23时11分56秒 1000 2167 4.47 0.00 0.00 6 XwaylandkB_rd/s 每秒从磁盘读取任务的千字节数kB_wr/s 每秒磁盘写入任务的千字节数kB_ccwr/s 任务已取消其写入磁盘的千字节数。 当任务截断一些脏页缓存时，可能会发生这种情况。 在这种情况下，一些IO另一个任务已被考虑将不会发生iodelay 任务的块I/O延迟，以时钟周期为单位。该指标包括等待同步块I/O完成和交换块I/O完成 pidstat -r 报告页面错误和内存利用率 1234567891011121314151617181920[xboom@localhost ~]$ pidstat -rLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时16分42秒 UID PID minflt/s majflt/s VSZ RSS %MEM Command23时16分42秒 0 1 3.02 0.23 245432 5032 0.62 systemd23时16分42秒 0 725 0.19 0.19 104716 2700 0.33 systemd-journal23时16分42秒 0 757 0.65 0.02 118924 420 0.05 systemd-udevd23时16分42秒 32 885 0.05 0.00 67124 144 0.02 rpcbind23时16分42秒 0 888 0.03 0.01 143108 48 0.01 auditd23时16分42秒 0 890 0.03 0.00 48488 20 0.00 sedispatch23时16分42秒 998 927 6.24 0.12 1634948 2484 0.31 polkitdminflt/s 任务每秒发生的次要错误，不需要从磁盘中加载页majflt/s 任务每秒发生的主要错误，需要从磁盘中加载页VSZ 虚拟地址大小，虚拟内存的使用KBRSS 常驻集合大小，非交换区五里内存使用KB%MEM 任务当前使用的可用物理内存份额当报告任务及其所有子任务的全局统计信息时，可能会显示以下值：minflt-nr 任务及其所有子项所产生的次要错误总数majflt-nr 任务及其所有子项所产生的主要错误总数 pidstat -s 报告堆栈利用率 123456789101112[xboom@localhost ~]$ pidstat -sLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时18分36秒 UID PID StkSize StkRef Command23时18分36秒 1000 2016 132 16 systemd23时18分36秒 1000 2037 132 4 pulseaudio23时18分36秒 1000 2050 132 12 dbus-daemon23时18分36秒 1000 2052 132 4 gdm-wayland-ses23时18分36秒 1000 2056 132 8 gnome-session-bStkSize 为任务保留为堆栈的内存量（以KB为单位），但不一定使用。StkRef 任务引用的用作堆栈的内存量（以KB为单位） pidstat -u 报告CPU利用率 12345678910111213141516171819[xboom@localhost ~]$ pidstat -uLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时20分26秒 UID PID %usr %system %guest %wait %CPU CPU Command23时20分26秒 0 1 0.00 0.03 0.00 0.07 0.03 0 systemd23时20分26秒 0 2 0.00 0.00 0.00 0.00 0.00 0 kthreadd23时20分26秒 0 9 0.00 0.01 0.00 0.03 0.01 0 ksoftirqd/023时20分26秒 0 10 0.00 0.00 0.00 0.81 0.00 0 rcu_sched%usr 用户太CPU占比，不包含虚拟处理器(虚拟处理器)所花费时间%system 内核态CPU占比 %guest 虚拟机CPU占比 %wait 等待IO的CPU占比 %CPU 总的CPU利用率，如果在多处理器环境中，添加参数 -I,该值将除以总的CPU个数CPU 进程运行的CPU id当报告任务及其所有子任务的全局统计信息时，可能会显示以下值：usr-ms 用户态任务及其所有子项花费的毫秒总数system-ms 内核态任务及其所有子项花费的毫秒总数guest-ms 虚拟处理器任务及其所有子项花费的毫秒总数 pidstat -v 内核表信息 12345678910[xboom@localhost ~]$ pidstat -vLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时22分03秒 UID PID threads fd-nr Command23时22分03秒 1000 2016 1 25 systemd23时22分03秒 1000 2037 3 36 pulseaudio23时22分03秒 1000 2050 2 71 dbus-daemonthreads 当前任务关联线程数fd-nr 当前任务关联文件描述符 pidstat -w 显示上下文切换 123456789101112131415[xboom@localhost ~]$ pidstat -wLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时23分28秒 UID PID cswch/s nvcswch/s Command23时23分28秒 0 1 0.79 0.27 systemd23时23分28秒 0 2 0.03 0.00 kthreadd23时23分28秒 0 3 0.00 0.00 rcu_gp23时23分28秒 0 4 0.00 0.00 rcu_par_gp23时23分28秒 0 6 0.00 0.00 kworker/0:0H-kblockdcswch/s 每秒执行的任务的自愿上下文切换总数。 自愿的上下文切换：进程无法获取所需资源导致,如 I/O、内存等系统系统资源不足。nvcswch/s 每秒执行的任务的非自愿上下文切换总数。 非自愿的上下文切换：是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。 比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换 I/O Controller iostat iostat命令用于通过观察设备活动的时间及其平均传输速率来监视系统输入/输出设备的负载。 iostat命令生成可用于更改系统配置的报告，以更好地平衡物理磁盘之间的输入/输出负载 iostat 命令详解 使用 man iostat 查看 iostat 的使用说明, 进程(任务)相关统计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667iostat [ -c ] [ -d ] [ -h ] [ -k | -m ] [ -N ] [ -s ] [ -t ] [ -V ] [ -x ] [ -y ] [ -z ] [ -j &#123; ID | LABEL | PATH | UUID | ... &#125; ] [ -o JSON ] [ [ -H ] -g group_name ] [ --human ] [ -p [ device [,...] | ALL ] ] [ device [...] | ALL ] [ interval [ count ] ]#描述 1.iostat命令生成的第一个报告提供有关自系统启动以来的时间的统计信息，除非使用了-y选项（在这种情况下，将忽略此第一个报告）。 2.每个后续报告涵盖自上一个报告以来的时间。每次运行iostat命令时，都会报告所有统计信息。 该报告由一个CPU标头行和随后的CPU统计信息行组成。 在多处理器系统上，CPU统计信息是系统范围内所有处理器的平均值。将显示设备标题行，后跟配置的每个设备的一行统计信息。 3.interval参数指定每个报告之间的时间间隔（以秒为单位）。可以与interval参数一起指定count参数。如果指定了count参数，count的值将确定间隔间隔秒生成的报告数。如果没有指定count参数而未指定count参数，则iostat命令将连续生成报告。# iostat命令生成两种类型的报告，即CPU利用率报告和设备利用率报告。# CPU使用率报告iostat命令生成的第一个报告是CPU利用率报告。对于多处理器系统，CPU值是所有处理器之间的全局平均值。 该报告具有以下格式： #磁盘利用率报告设备报告提供了基于每个物理设备或分区的统计信息。 如果未输入设备或分区，则将显示系统使用的每个设备的统计信息，并提供内核维护的统计信息。 如果在命令行上指定了ALL关键字，则将显示系统定义的每个设备的统计信息，包括从未使用过的设备。 除非设置了环境变量POSIXLY_CORRECT，否则默认情况下传输速率以1K块为单位，在这种情况下，将使用512字节的块。 sec/s (kB/s, MB/s): 每秒从设备读取或写入设备的扇区数（千字节，兆字节） rqm/s: 每秒排队到设备中的 I/O 请求数 areq-sz: 发出给设备的 I/O 请求的平均大小（以千字节为单位）注意：在以前的版本中，此字段称为avgrq-sz，以扇区表示 await: 发出给要服务的设备的 I/O 请求的平均时间（毫秒）。 这包括队列中的请求所花费的时间以及为请求服务所花费的时间 # 参数详解-c 显示CPU利用率-d 显示设备利用率-g group_name &#123; device [...] | ALL &#125; 显示一组设备的统计信息 iostat命令报告列表中每个设备的统计信息， 然后报告显示为group_name并由列表中所有设备组成的组的全局统计信息。 ALL关键字表示系统定义的所有块设备都应包含在组中。-H 此选项必须与选项-g一起使用，并指示仅显示该组的全局统计信息，而不显示该组中各个设备的统计信息。-h 使设备使用情况报告更易于人阅读。--human使用此选项隐式启用。--human 以可读格式打印的尺寸（例如KB, M等）。使用此选项显示的单位将取代与度量标准关联的任何其他默认单位（例如千字节，扇区...）-j &#123; ID | LABEL | PATH | UUID | ... &#125; [ device [...] | ALL ] 显示永久设备名称。 选项ID，LABEL等指定持久名称的类型。这些选项不受限制，只有先决条件是 /dev/disk 中必须存在具有所需持久名称的目录。 （可选）可以在所选的持久名称类型中指定多个设备。 因为持久性设备名称通常很长，所以选择-k 显示统计信息（KB/s）-m 显示统计信息（M/s）。-N 显示任何设备映射器设备的注册设备映射器名称。用于查看LVM2统计信息。-o JSON 以JSON（JavaScript对象表示法）格式显示统计信息。JSON输出字段顺序未定义，将来可能会添加新字段。-p [ &#123; device [,...] | ALL &#125; ] 显示系统使用的块设备及其所有分区的统计信息。 如果在命令行上输入了设备名称，则对其进行统计并显示其所有分区。 ALL关键字指示必须显示系统定义的所有块设备和分区的统计信息，包括那些从未使用过的 如果在此选项之前定义了选项-j，则可以使用所选的持久名称类型指定在命令行上输入的设备。-s 在80个字符的宽屏幕中显示报告的简短（窄版）版本。-t 打印显示的每个报告的时间。时间戳格式可能取决于S_TIME_FORMAT环境变量的值（请参见下文）。-V 打印版本号然后退出-x 显示扩展统计信息。-y 如果以给定的时间间隔显示多个记录，则自系统启动以来忽略带有统计信息的第一个报告。-z 告诉iostat忽略在采样期间没有任何活动的任何设备的输出。#环境变量：iostat命令考虑以下环境变量：POSIXLY_CORRECT 设置此变量后，传输速率以512字节块而不是默认的1K块显示。#BUG /proc filesystem must be mounted for iostat to work. Kernels older than 2.6.x are no longer supported. The average service time (svctm field) value is meaningless, 1. I/O statistics are now calculated at block level 2， we don't know when the disk driver starts to process a request.#FILES /proc/stat contains system statistics. /proc/uptime contains system uptime. /proc/diskstats contains disks statistics. /sys contains statistics for block devices. /proc/self/mountstats contains statistics for network filesystems. /dev/disk contains persistent device names. iostat常用命令 iostat自启动报告以来，显示所有CPU和设备的单个历史记录 123456Device: 此列提供 /dev 目录中列出的设备（或分区）名称tps: 表示每秒发送到设备的I/O请求次数。多个逻辑请求可以组合成对设备的单个I/O请求。传输的大小不确定。Blk_read/s(kB_read/s, MB_read/s): 表示从设备读取的数据量，以每秒的块数（千字节，兆字节）表示。块等同于扇区，因此大小为512BBlk_wrtn/s(kB_wrtn/s, MB_wrtn/s): 表示写入设备的数据量，以每秒的块数（千字节，兆字节）表示。Blk_read (kB_read, MB_read): 读取的总块数（千字节，兆字节）。Blk_wrtn (kB_wrtn, MB_wrtn): 写入的总块数（千字节，兆字节）。 iostat -x 显示IO拓展信息 123456789101112131415161718Device: 此列提供 /dev 目录中列出的设备（或分区）名称r/s: 每秒设备完成的读取请求数（合并后）w/s: 每秒设备完成的写请求数（合并后）rsec/s (rkB/s, rMB/s): 每秒从设备读取的扇区数（千字节，兆字节）wsec/s (wkB/s, wMB/s): 每秒写入设备的扇区数（千字节，兆字节）rrqm/s: 每秒合并到设备中的读取请求数wrqm/s: 每秒合并到设备中的写入请求数%rrqm: 读取请求(已合并)的百分比%wrqm: 写入请求(已合并)的百分比r_await: 发送给要服务的设备的读取请求的平均时间（以毫秒为单位）包括队列中的请求所花费的时间以及为请求服务所花费的时间w_await: 发布给要服务的设备的写请求的平均时间（以毫秒为单位）包括队列中的请求所花费的时间以及为请求服务所花费的时间aqu-sz: 发出到设备的请求的平均队列长度。注意：在以前的版本中 此字段称为avgqu-szrareq-sz: 发出给设备的读请求的平均大小（以千字节为单位）wareq-sz: 发出给设备的写请求的平均大小（以千字节为单位）svctm: 发出给设备的 I/O 请求的平均服务时间（以毫秒为单位）警告！不再信任此字段。 将来的sysstat版本中将删除此字段%util: 向设备发出 I/O 请求的经过时间的百分比（设备的带宽利用率） 当该值接近100％时，发生设备饱和用于串行服务请求的设备 对于并行处理请求的设备（例如RAID阵列和现代SSD）此数字并不反映其性能限制 参考链接 Linux vmstat命令详解","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"TCP-2-超时与重传","slug":"TCP/TCP-2-超时与重传","date":"2020-11-01T03:00:54.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/11/01/TCP/TCP-2-超时与重传/","link":"","permalink":"http://xboom.github.io/2020/11/01/TCP/TCP-2-%E8%B6%85%E6%97%B6%E4%B8%8E%E9%87%8D%E4%BC%A0/","excerpt":"","text":"引言 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。针对TCP数据包丢失情况，常用以下四种重传机制解决： 超时重传 快速重传 SACK D-SACK 超时重传 在发送时设置一个定时器，当定时器溢出时还没收到ACK确认，它就重传该数据。TCP会在以下两种情况发生超时重传 数据包丢失 确认应答丢失 那么怎样决定超时间隔和重传的频率? RTT(Round-Trip Time 往返时延):表示数据往返两端一次所需要的时间 RTO(Retransmission Timeout 超时重传时间):表示数据包发送超过该时间还没有收到回复，则进行超时重传 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发 所以 超时重传时间 RTO 的值应该略大于报文往返 RTT 的值 由于网络是波动的，报文往返 RTT 是经常波动变化的，那系统是怎么确定RTO的值的呢？ 系统通过采样的方式来估计往返时间RTT： 采样 RTT 的时间进行加权平均，算出一个 SRTT(smoothed round-trip time)的值，而且这个值是不断变化的，因为网络状况不断地变化。 采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况 [RFC6298] 建议使用 Jacobaon/Karels 算法 计算 RTO 123456789# 第一次计算RTO,其中R1为第一次测量的RTT值, SRTT是计算平滑的RTT, DevRTR 是计算平滑的 RTT 与最新 RTT 的差距。SRTT = R1DevRTT = R1/2RTO = μ * SRTT + ∂ * DevRTT = μ * R1 + ∂ * R1 / 2# 后续计算RTO,其中R2为最新测量的RTTSRTT = SRTT + α * (RTT - SRTT) = R1 + α * (R2 - R1)DevRTT = (1 - β) * DevRTT + β * (|RTT - STRR|) = (1 - β) * R1 / 2 + β * (|R2 - R1|)RTO = μ * SRTT + ∂ * DevRTT 在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4 通过大量实验调试出来的 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍：即每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。 快速重传 超时触发重传存在的问题是，超时周期可能相对较长。 就有了快速重传机制来解决超时重发的时间等待 发送方发出了 1，2，3，4，5 份数据 第一份 Seq1 先送到了，于是就 Ack 回 2(表示2以前的都收到了，其他发送方接下来从2开始发送) 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2; 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到; 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失 的Seq2 最后，接收到收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段 SACK **快速重传( Selective Acknowledgment 选择性确认)**重传的时候，怎么知道是重传之前的一个，还是重传所有。发送端并不清楚这连续的三个 Ack 2 是由于哪个请求而传回来的。比如快速重传例子，是重传 Seq2 还是重传 Seq2、Seq3、Seq4、Seq5 。为了解决不知道重传哪些 TCP 报文，就有 SACK。 在 TCP 头部 Option 字段里加一个 SACK 的东西，将缓存的地图(几组不连续块的第一个序列号与不连续块的最后一个序列号)发送给发送方， 这样发送方就可以知道哪些数据收到了，哪些数据没收到，只重传丢失的数据。 如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现 只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。 如果要支持 SACK ，必须双方都要支持。在 Linux 下，通过 net.ipv4.tcp_sack 参数打开这个功能(Linux 2.4 后默认打开) Duplicate SACK Duplicate SACK 又称 D-SACK ，使用 **SACK 来通知发送方有哪些数据被重复接收了 ** 比如1,ACK丢包： 接收方发给发送方的两个 ACK 确认应答都丢失了，发送方超时后，重传第一个数据包(3000 ~ 3499) 接收方发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉发送方 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据 都已收到，所以这个 SACK 就代表着 D-SACK 。 这样发送方就知道了，数据没有丢，是接收方的 ACK 确认报文丢了 比如2,网络延时： 数据包(1000~1499) 被网络延迟了，导致发送方没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但重传后，被延迟的数据包(1000~1499)又到了接收方; 所以接收方回了一个SACK=1000~1500，因为ACK已经到了3000，所以这个SACK是D- SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而 是因为网络延迟了。 可见，D-SACK 有这么几个好处: 可以让发送方知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以判断是不是发送方的数据包被网络延迟了; 可以判断网络中是不是把发送方的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能(Linux 2.4 后默认打开)","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"Linux-B03-malloc","slug":"Linux/Linux-B03-malloc","date":"2020-11-01T02:24:18.000Z","updated":"2023-08-10T04:27:16.600Z","comments":true,"path":"2020/11/01/Linux/Linux-B03-malloc/","link":"","permalink":"http://xboom.github.io/2020/11/01/Linux/Linux-B03-malloc/","excerpt":"","text":"基础概念 每个进程都有独立的虚拟地址空间，进程访问的虚拟地址并不是真正的物理地址 虚拟地址是通过每个进程的页表(在每个进程的内核虚拟地址空间)与物理地址进行映射，获得真正物理地址； 如果虚拟地址对应物理地址不在物理内存中，则产生缺页中断，真正分配物理地址，同时更新进程的页表；如果此时物理内存已耗尽，则根据内存替换算法淘汰部分页面至物理磁盘中 空间操作 CPU通过地址总线、数据总线和控制总线实现对内存的访问 数据总线：在CPU和内存之间传递数据的通道 控制总线：在CPU和内存之间传递各种控制/状态信号的通道 地址总线：传递地址信号，以确定所要访问的内存地址 当 CPU 执行内存中一条指令的时候，首先把 VMA（虚拟内存区域）中的逻辑地址转换为线性地址，转化过程通过 MMU（内存管理单元）实现 系统在内存分配的时候，其实并没有申请相应的物理页帧，只有在真正赋值的时候才会申请物理页帧。这也是 VSZ（进程虚拟内存大小）和 RSS（常驻物理内存大小）的最大区别。 空间分配 Linux 使用虚拟地址空间， 对32位操作系统而言，它的寻址空间(虚拟存储空间)为4G(2的32次方)，将最高的1G字节(从虚拟地址0xC0000000到0xFFFFFFFF)，供内核使用，称为内核空间，而将较低的3G字节(从虚拟地址0x00000000到0xBFFFFFFF)，供各个进程使用，称为用户空间。 由低地址到高地址分别为： 只读段：该部分空间只能读不可写(包括：代码段、rodata 段(C常量字符串和#define定义的常量) ) 数据段：保存全局变量、静态变量的空间 堆 ：就是平时所说的动态内存， malloc/new 大部分都来源于此。其中堆顶的位置可通过函数 brk 和 sbrk 进行动态调整。 文件映射区域：如动态库、共享内存等映射物理空间的内存，一般是 mmap 函数所分配的虚拟地址空间 栈：用于维护函数调用的上下文空间，一般为 8M ，可通过 ulimit –s 查看，每个线程都有自己专属的栈 内核虚拟空间：用户代码不可见的内存区域，由内核管理(页表就存放在内核虚拟空间) 分配原理 进程分配内存有两种方式，分别由两个系统调用完成：brk 和 mmap (不考虑共享内存) brk 是将数据段（.data）的最高地址指针 _edata 往高地址推 mmap 是在进程的虚拟地址空间中（堆和栈中间，称为“文件映射区域”的地方）找一块空闲的虚拟内存。 两种方式分配的都是虚拟内存，没有分配物理内存。第一次访问已分配的虚拟地址空间的时候，发生缺页中断进程会陷入内核态，执行以下操作： 检查要访问的虚拟地址是否合法 查找/分配一个物理页 填充物理页内容（读取磁盘，或者直接置0，或者什么都不做） 如果需要读取磁盘，那么这次缺页就是 majfit(major fault：大错误) 否则就是 minflt(minor fault：小错误) 建立映射关系（虚拟地址到物理地址的映射关系） 重复执行发生缺页中断的那条指令 使用命令查看缺页中断的次数 分配过程 第一种情况：malloc小于128K的内存，使用brk 将_edata往高地址推(只分配虚拟空间，不对应物理内存(因此没有初始化)，第一次读/写数据时，引起内核缺页中断，内核才分配对应的物理内存，然后虚拟地址空间建立映射关系)，如下图： 进程启动的时候，其（虚拟）内存空间的初始布局(1)所示 进程调用A=malloc(30K)以后，内存空间 (2), malloc函数会调用brk系统调用，将_edata指针往高地址推30K，就完成虚拟内存分配 _edata+30K只是完成虚拟地址的分配，A这块内存现在还是没有物理页与之对应的，等到进程第一次读写A这块内存的时候，发生缺页中断，这个时候，内核才分配A这块内存对应的物理页。也就是说，如果用malloc分配了A这块内容，然后从来不访问它，那么，A对应的物理页是不会被分配的。 进程调用B=malloc(40K)以后，内存空间如(3) 第二种情况：malloc 大于 128K 的内存，使用 mmap 分配（munmap 释放） 进程调用C=malloc(200K)以后，内存空间如(4) 进程调用D=malloc(100K)以后，内存空间如(5) 进程调用free©以后，C对应的虚拟内存和物理内存一起释放 默认情况下，malloc函数分配内存，如果请求内存大于128K（可由M_MMAP_THRESHOLD选项调节），那就不是去推_edata(指向数据段)指针了，而是利用mmap系统调用，从堆和栈的中间分配一块虚拟内存 这样子做是因为 brk分配的内存需要等到高地址内存释放以后才能释放（例如，在B释放之前，A是不可能释放的，因为只有一个_edata 指针，这就是内存碎片产生的原因，先进后出），而mmap分配的内存可以单独释放 进程调用free(B)以后，如(7)所示:B对应的虚拟内存和物理内存都没有释放，因为只有一个_edata指针，如果往回推，那么D这块内存怎么办呢？当然，B这块内存，是可以重用的，如果这个时候再来一个40K的请求，那么malloc很可能就把B这块内存返回回去了 进程调用free(D)以后，如图(8)所示：B和D连接起来，变成一块140K的空闲内存，当最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作(trim)。在上一个步骤free的时候，发现最高地址空闲内存超过128K，于是内存紧缩，变成(9)所示 总结: 当开辟的空间小于 128K 时，调用 brk()函数，malloc 的底层实现是系统调用函数 brk()，其主要移动指针 _enddata(此时的 _enddata 指的是 Linux 地址空间中堆段的末尾地址，不是数据段的末尾地址) 当开辟的空间大于 128K 时，mmap()系统调用函数来在虚拟地址空间中（堆和栈中间，称为“文件映射区域”的地方）找一块空间来开辟 参考链接 malloc 底层实现原理","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"},{"name":"Memory","slug":"Memory","permalink":"http://xboom.github.io/tags/Memory/"}]},{"title":"Linux-A04-进程","slug":"Linux/Linux-A04-进程","date":"2020-10-30T02:24:18.000Z","updated":"2023-08-10T04:27:29.096Z","comments":true,"path":"2020/10/30/Linux/Linux-A04-进程/","link":"","permalink":"http://xboom.github.io/2020/10/30/Linux/Linux-A04-%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"基础知识 进程就是一个程序的执行流程，内部保存程序运行所需的资源 在UNIX系统中，直邮fork系统调用才可以创建进程 12345678910111213#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main() &#123; pid_t id = fork(); if (id &lt; 0) &#123; perror(\"fork\\n\"); &#125; else if (id == 0) &#123; // 子进程 printf(\"子进程\\n\"); &#125; else &#123; // 父进程 printf(\"父进程\\n\"); &#125; return 0;&#125; 进程创建之后，父子进程都有各自不同的地址空间，其中一个进程在其地址空间的修改对另一个进程不可见。子进程的初始化空间是父进程的一个副本，这里涉及两个不同地址空间，不可写的内存区是共享的，某些UNIX的实现使程序正文在两者间共享，因为它是不可修改的。 还有一种写时复制共享技术，子进程共享父进程的所有内存，一旦两者之一想要修改部分内存，则这块内存被复制确保修改发生在当前进程的私有内存区域 PCB 进程控制块(PCB),操作系统为每个进程都维护一个PCB，用来保存与该进程有关的各种状态信息。进程可以抽象理解为就是一个PCB，PCB是进程存在的唯一标志，用PCB来描述进程的基本情况以及运行变化的过程 PCB包含进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号和调度信息，以及其它在进程由运行态转换到就绪态或阻塞态时必须保存的信息，从而保证该进程随后能再次启动，就像从未中断过一样。 中断发生后操作系统最底层做了什么？ 硬件压入堆栈程序计数器 硬件从中断向量装入新的程序计数器 汇编语言过程保存寄存器的值 汇编语言过程设置新的堆栈 C中断服务例程运行（典型的读和缓冲输入） 调度程序决定下一个将运行的进程 C过程返回到汇编代码 汇编语言过程开始运行新的当前进程 进程控制块中存储的信息 进程标识信息：如本进程的标识，本进程的父进程标识，用户标识等。 处理机状态信息保护区：用于保存进程的运行现场信息： 用户可见寄存器：用户程序可以使用的数据，地址等寄存器 控制和状态寄存器：程序计数器，程序状态字 栈指针：过程调用、系统调用、中断处理和返回时需要用到它 进程控制信息： 调度和状态信息：用于操作系统调度进程使用 进程间通信信息：为支持进程间与通信相关的各种标识、信号、信件等，这些信息存在接收方的进程控制块中 存储管理信息：包含有指向本进程映像存储空间的数据结构 进程所用资源：说明由进程打开使用的系统资源，如打开的文件等 有关数据结构连接信息：进程可以连接到一个进程队列中，或连接到相关的其他进程的PCB 进程状态 进程因为等待输入而被阻塞，进程从运行态转换到阻塞态 调度程序选择了另一个进程执行时，当前程序就会从运行态转换到就绪态 被调度程序选择的程序会从就绪态转换到运行态 当阻塞态的进程等待的一个外部事件发生时，就会从阻塞态转换到就绪态，此时如果没有其他进程运行时，则立刻从就绪态转换到运行态！ 12345678910pid=fork(); // 创建一个与父进程一样的子进程pid=waitpid(); // 等待子进程终止s=execve(); // 替换进程的核心映像exit(); // 终止进程运行并返回状态值s=sigaction(); // 定义信号处理的动作s=sigprocmask(); // 检查或更换信号掩码s=sigpending(); // 获得阻塞信号集合s=sigsuspend(); // 替换信号掩码或挂起进程alarm(); // 设置定时器pause(); // 挂起调用程序直到下一个信号出现 进程调度 一个CPU同一时刻只会有一个进程处于运行状态，操作系统通过调度算法选择下一个要运行的进程 什么时候进行调度 系统调用创建一个新进程后，需要决定是运行父进程还是运行子进程 一个进程退出时需要做出调度决策，需要决定下一个运行的是哪个进程 当一个进程阻塞在I/O和信号量或者由于其它原因阻塞时，必须选择另一个进程运行 当一个I/O中断发生时，如果中断来自IO设备，而该设备现在完成了工作，某些被阻塞的等待该IO的进程就成为可运行的就绪进程了，是否让新就绪的进程运行，或者让中断发生时运行的进程继续运行，或者让某个其它进程运行，这就取决于调度程序的抉择了 调度算法可以分类 非抢占式调度算法：让进程运行直至被阻塞，或者直到该进程自动释放CPU。在时钟中断发生时不会进行调度，在处理完时钟中断后，如果没有更高优先级的进程等待，则被中断的进程会继续执行。简单来说，调度程序必须等待事件结束。 非抢占方式引起进程调度的条件： 进程执行结束，或发生某个事件而不能继续执行 正在运行的进程因有I/O请求而暂停执行 进程通信或同步过程中执行了某些原语操作（wait、block等） 抢占式调度算法：进程运行固定时段。如果时段结束时进程仍在运行，就被挂起，而调度程序挑选另一个进程运行，进行抢占式调度处理，需要在时间间隔的末端发生时钟中断，以便CPU控制返回给调度程序，如果没有可用的时钟，那么非抢占式调度就是唯一的选择。简单来说，防止单一进程长时间独占CPU资源。 进程间通信 匿名管道 匿名管道就是pipe，pipe只能在父子进程间通信，且数据是单向流动（半双工通信） 使用方式： 父进程创建管道，会得到两个文件描述符，分别指向管道的两端； 父进程创建子进程，从而子进程也有两个文件描述符指向同一管道； 父进程可写数据到管道，子进程就可从管道中读出数据，从而实现进程间通信，下面的示例代码中通过pipe实现了每秒钟父进程向子进程都发送消息的功能 123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;int main() &#123; int _pipe[2]; int ret = pipe(_pipe); if (ret &lt; 0) &#123; perror(\"pipe\\n\"); &#125; pid_t id = fork(); if (id &lt; 0) &#123; perror(\"fork\\n\"); &#125; else if (id == 0) &#123; // 子进程 close(_pipe[1]); int j = 0; char _mesg[100]; while (j &lt; 100) &#123; memset(_mesg, '\\0', sizeof(_mesg)); read(_pipe[0], _mesg, sizeof(_mesg)); printf(\"%s\\n\", _mesg); j++; &#125; &#125; else &#123; // 父进程 close(_pipe[0]); int i = 0; char *mesg = NULL; while (i &lt; 100) &#123; mesg = \"父进程来写消息了\"; write(_pipe[1], mesg, strlen(mesg) + 1); sleep(1); ++i; &#125; &#125; return 0;&#125; 平时也会使用 | 关于管道的命令行，如 ls | less 创建管道 为ls创建一个进程，设置stdout为管道写端 为less创建一个进程，设置stdin为管道读端 高级管道 通过popen将另一个程序当作一个新的进程在当前进程中启动，它算作当前进程的子进程，高级管道只能用在有亲缘关系的进程间通信，这种亲缘关系通常指父子进程，下面的GetCmdResult函数可以获取某个Linux命令执行的结果，实现方式就是通过popen 123456789101112131415161718192021222324252627282930std::string GetCmdResult(const std::string &amp;cmd, int max_size = 10240) &#123; char *data = (char *)malloc(max_size); if (data == NULL) &#123; return std::string(\"malloc fail\"); &#125; memset(data, 0, max_size); const int max_buffer = 256; char buffer[max_buffer]; // 将标准错误重定向到标准输出 FILE *fdp = popen((cmd + \" 2&gt;&amp;1\").c_str(), \"r\"); int data_len = 0; if (fdp) &#123; while (!feof(fdp)) &#123; if (fgets(buffer, max_buffer, fdp)) &#123; int len = strlen(buffer); if (data_len + len &gt; max_size) &#123; cout &lt;&lt; \"data size larger than \" &lt;&lt; max_size; break; &#125; memcpy(data + data_len, buffer, len); data_len += len; &#125; &#125; pclose(fdp); &#125; std::string ret(data, data_len); free(data); return ret;&#125; 命名管道 匿名管道有个缺点就是通信的进程一定要有亲缘关系，而命名管道就不需要这种限制。 命名管道其实就是一种特殊类型的文件，所谓的命名其实就是文件名，文件对各个进程都可见，通过命名管道创建好特殊文件后，就可以实现进程间通信。 通过mkfifo创建一个特殊的类型的文件，参数读者看名字应该就了解，一个是文件名，一个是文件的读写权限： 1int mkfifo(const char* filename, mode_t mode); 当返回值为0时，表示该命名管道创建成功，至于如何通信，其实就是个读写文件的问题 消息队列 队列想必大家都知道，像FIFO一样，这里可以有多个进程写入数据，也可以有多个进程从队列里读出数据，但消息队列有一点比FIFO还更高级，它读消息不一定要使用先进先出的顺序，每个消息可以赋予类型，可以按消息的类型读取，不是指定类型的数据还存在队列中。本质上MessageQueue是存放在内核中的消息链表，每个消息队列链表会由消息队列标识符表示，这个消息队列存于内核中，只有主动的删除该消息队列或者内核重启时，消息队列才会被删除 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// 创建和访问一个消息队列int msgget(key_t, key, int msgflg);// 用来把消息添加到消息队列中int msgsend(int msgid, const void *msg_ptr, size_t msg_sz, int msgflg);// msg_ptr是结构体数据的指针，结构第一个字段要有个类型：struct Msg &#123; long int message_type; // 想要传输的数据&#125;;// 从消息队列中获取消息int msgrcv(int msgid, void *msg_ptr, size_t msg_st, long int msgtype, int msgflg);// 用来控制消息队列，不同的command参数有不同的控制方式int msgctl(int msgid, int command, struct msgid_ds *buf);#include &lt;errno.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/msg.h&gt;#include &lt;chrono&gt;#include &lt;iostream&gt;#include &lt;thread&gt;using namespace std;#define BUFFER_SIZ 20typedef struct &#123; long int msg_type; char text[BUFFER_SIZ];&#125; MsgWrapper;void Receive() &#123; MsgWrapper data; long int msgtype = 2; int msgid = msgget((key_t)1024, 0666 | IPC_CREAT); if (msgid == -1) &#123; cout &lt;&lt; \"msgget error \\n\"; return; &#125; while (true) &#123; if (msgrcv(msgid, (void *)&amp;data, BUFFER_SIZ, msgtype, 0) == -1) &#123; cout &lt;&lt; \"error \" &lt;&lt; errno &lt;&lt; endl; &#125; cout &lt;&lt; \"read data \" &lt;&lt; data.text &lt;&lt; endl; if (strlen(data.text) &gt; 6) &#123; // 发送超过6个字符的数据，结束 break; &#125; &#125; if (msgctl(msgid, IPC_RMID, 0) == -1) &#123; cout &lt;&lt; \"msgctl error \\n\"; &#125; cout &lt;&lt; \"Receive ok \\n\";&#125;void Send() &#123; MsgWrapper data; long int msgtype = 2; int msgid = msgget((key_t)1024, 0666 | IPC_CREAT); if (msgid == -1) &#123; cout &lt;&lt; \"msgget error \\n\"; return; &#125; data.msg_type = msgtype; for (int i = 0; i &lt; 10; ++i) &#123; memset(data.text, 0, BUFFER_SIZ); char a = 'a' + i; memset(data.text, a, 1); if (msgsnd(msgid, (void *)&amp;data, BUFFER_SIZ, 0) == -1) &#123; cout &lt;&lt; \"msgsnd error \\n\"; return; &#125; std::this_thread::sleep_for(std::chrono::seconds(1)); &#125; memcpy(data.text, \"1234567\", 7); if (msgsnd(msgid, (void *)&amp;data, BUFFER_SIZ, 0) == -1) &#123; cout &lt;&lt; \"msgsnd error \\n\"; return; &#125;&#125;int main() &#123; std::thread r(Receive); r.detach(); std::thread s(Send); s.detach(); std::this_thread::sleep_for(std::chrono::seconds(20)); return 0;&#125; 消息队列收发消息自动保证了同步，不需要由进程自己来提供同步方法，而命名管道需要自行处理同步问题 消息队列接收数据可以根据消息类型有选择的接收特定类型的数据，不需要像命名管道一样默认接收数据。 消息队列有一个缺点就是发送和接收的每个数据都有最大长度的限制 共享内存 可开辟中一块内存，用于各个进程间共享，使得各个进程可以直接读写同一块内存空间，就像线程共享同一块地址空间一样，该方式基本上是最快的进程间通信方式，因为没有系统调用干预，也没有数据的拷贝操作，但由于共享同一块地址空间，数据竞争的问题就会出现，需要自己引入同步机制解决数据竞争问题。 共享内存只是一种方式，它的实现方式有很多种，主要的有mmap系统调用、Posix共享内存以及System V共享内存等。通过这三种“工具”共享地址空间后，通信的目的自然就会达到 信号量 信号量semaphore，是操作系统中一种常用的同步与互斥的机制； 信号量允许多个进程（计数值&gt;1）同时进入临界区； 如果信号量的计数值为1，一次只允许一个进程进入临界区，这种信号量叫二值信号量； 信号量可能会引起进程睡眠，开销较大，适用于保护较长的临界区； 与读写自旋锁类似，linux内核也提供了读写信号量的机制； 流程分析 可以将信号量比喻成一个盒子，初始化时在盒子里放入N把钥匙，钥匙先到先得，当N把钥匙都被拿走完后，再来拿钥匙的人就需要等待了，只有等到有人将钥匙归还了，等待的人才能拿到钥匙 1234567891011struct semaphore &#123; raw_spinlock_t lock; //自旋锁，用于count值的互斥访问 unsigned int count; //计数值，能同时允许访问的数量，也就是上文中的N把锁 struct list_head wait_list; //不能立即获取到信号量的访问者，都会加入到等待列表中&#125;;struct semaphore_waiter &#123; struct list_head list; //用于添加到信号量的等待列表中 struct task_struct *task; //用于指向等待的进程，在实际实现中，指向current bool up; //用于标识是否已经释放&#125;; down接口用于获取信号量 如果sem-&gt;count &gt; 0时，也就是盒子里边还有多余的锁，直接自减并返回了 当sem-&gt;count == 0时，表明盒子里边的锁被用完了，当前任务会加入信号量的等待列表中，设置进程的状态，并调用schedule_timeout来睡眠指定时间，实际上这个时间设置的无限等待，也就是只能等着被唤醒，当前任务才能继续运行； up用于释放信号量 如果等待列表为空，表明没有多余的任务在等待信号量，直接将sem-&gt;count自加即可。 如果等待列表非空，表明有任务正在等待信号量，那就需要对等待列表中的第一个任务（等待时间最长）进行唤醒操作，并从等待列表中将需要被唤醒的任务进行删除操作 信号量缺点 Semaphore与Mutex在实现上有一个重大的区别：ownership。Mutex被持有后有一个明确的owner，而Semaphore并没有owner，当一个进程阻塞在某个信号量上时，它没法知道自己阻塞在哪个进程（线程）之上； 没有ownership会带来以下几个问题： 在保护临界区的时候，无法进行优先级反转的处理； 系统无法对其进行跟踪断言处理，比如死锁检测等； 信号量的调试变得更加麻烦； 因此，在Mutex能满足要求的情况下，优先使用Mutex 其他接口 信号量提供了多种不同的信号量获取的接口，介绍如下： 12345678/* 未获取信号量时，进程轻度睡眠：TASK_INTERRUPTIBLE */int down_interruptible(struct semaphore *sem)/* 未获取到信号量时，进程中度睡眠：TASK_KILLABLE */int down_killable(struct semaphore *sem)/* 非等待的方式去获取信号量 */int down_trylock(struct semaphore *sem)/* 获取信号量，并指定等待时间 */int down_timeout(struct semaphore *sem, long timeout) 读写信号量 读写自旋锁，读写信号量的功能类似，它能有效提高并发性，包含以下特点： 允许多个读者同时进入临界区 读者与写者不能同时进入临界区（读者与写者互斥） 写者与写者不能同时进入临界区（写者与写者互斥） 读写信号量的数据结构与信号量的结构比较相似： 12345678910111213141516struct rw_semaphore &#123; atomic_long_t count; //用于表示读写信号量的计数 struct list_head wait_list; //等待列表，用于管理在该信号量上睡眠的任务 raw_spinlock_t wait_lock; //锁，用于保护count值的操作#ifdef CONFIG_RWSEM_SPIN_ON_OWNER struct optimistic_spin_queue osq; /* spinner MCS lock */ //MCS锁，参考上一篇文章Mutex中的介绍 /* * Write owner. Used as a speculative check to see * if the owner is running on the cpu. */ struct task_struct *owner; //当写者成功获取锁时，owner会指向锁的持有者#endif#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;; 读写自旋锁，读写自旋锁中的lock字段，bit[31]用于写锁的标记，bit[30:0]用于读锁的统计，而读写信号量的count字段也大体类似； 孤儿进程与僵尸进程 在linux中，正常情况下子进程是通过父进程创建的，父进程无法预测子进程结束时间，需要调用 wait/waitpid 系统调用取的子进程的终止状态 孤儿进程 如果父进程先退出，而它的子进程还在运行，那么子进程被称为孤儿进程。孤儿进程被根进程(进程号为1)所收养，由根进程管理 僵尸进程 任何一个子进程(根进程除外)在退出之后，并非马上消失，内核释放该进程资源，包括打开的文件，占用的内存等。但是仍然为其保留一定的信息 进程号 the process ID 退出状态 the Termination status of the process 运行时间 the amount of CPU time taken by the process 留下一个称为 僵尸进程(Zombie)的数据结构，直到父进程通过 wait/waitpid 来取时才会释放。如果父进程不调用 wait/waitpid 的话，那么保留的信息就不会释放，其进程号就会一直被占用，但系统所使用的进程号时有限的，如果存在大量僵尸进程，可能因为没有可用进程号导致系统不能产生新的进程 通过命令 ulimit -a 查看 max user processes 通过 ps 命令查看，状态为 Z 定位僵尸进程 ps -A -ostat,ppid,pid,cmd |grep -e '^[Zz]’ -A 参数列出所有进程 -o 自定义输出字段 stat（状态）、ppid（进程父id）、pid（进程id）、cmd（命令） 因为状态为z或者Z的进程为僵尸进程，所以我们使用grep抓取stat状态为zZ进程 僵尸进程ID:3457，父进程ID:3425 僵尸进程ID:3533，父进程ID:3511 处理僵尸进程 使用 kill -HUP &lt;僵尸进程ID&gt; 往往无法杀死僵尸进程，-hup会让进程挂起/休眠。此时需要使用 kill -HUP &lt;僵尸进程父ID&gt; 来杀死进程(注意这里有可能杀死根进程，杀的时候注意) 杀死后可通过上述查询命令查看僵尸进程是否存在 僵尸进程代码解决 方法一：wait/waitpid 函数: pid_t wait(int *status) 父进程在调用wait函数之后将自己阻塞，由wait自动分析是否当前进程的某个子进程已经退出 如果找到了变成僵尸的子进程，wait就会收集这个子进程信息，并将它测底销毁后返回 如果没有找到，wait就会一直阻塞在这里，直到有一个出现为止。其中参数 status 用来保存被收集进程退出时的一些状态，是一个指向int类型的指针(不关心如何死掉，直接使用 wait(NULL) )，一个wait也只能处理一个僵尸进程 方法2：将父进程中对SIGCHILD信号的处理函数设为 使用信号，引入 sinal.h，子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程(SIG_IGN 或直接忽略) 1234567891011121314151617181920212223242526272829303132333435363738394041 1 #include &lt;stdio.h&gt; 2 #include &lt;unistd.h&gt; 3 #include &lt;errno.h&gt; 4 #include &lt;stdlib.h&gt; 5 #include &lt;signal.h&gt; 6 7 static void sig_child(int signo); 8 9 int main()10 &#123;11 pid_t pid;12 //创建捕捉子进程退出信号13 signal(SIGCHLD,sig_child);14 pid = fork();15 if (pid &lt; 0)16 &#123;17 perror(\"fork error:\");18 exit(1);19 &#125;20 else if (pid == 0)21 &#123;22 printf(\"I am child process,pid id %d.I am exiting.\\n\",getpid());23 exit(0);24 &#125;25 printf(\"I am father process.I will sleep two seconds\\n\");26 //等待子进程先退出27 sleep(2);28 //输出进程信息29 system(\"ps -o pid,ppid,state,tty,command\");30 printf(\"father process is exiting.\\n\");31 return 0;32 &#125;33 34 static void sig_child(int signo)35 &#123;36 pid_t pid;37 int stat;38 //处理僵尸进程39 while ((pid = waitpid(-1, &amp;stat, WNOHANG)) &gt;0)40 printf(\"child %d terminated.\\n\", pid);41 &#125; **方法3：**将子进程成为孤儿进程，从而其父进程变为根进程，通过根进程处理子进程 进程P 创建子进程 Pc1 之后，子进程再创建子进程 Pcc1， 父进程等待第一个子进程 Pc1退出，让Pc1退出，Pcc1 处理问题称为孤儿进程，由根进程处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt; int main()&#123; pid_t pid; //创建第一个子进程 pid = fork(); if (pid &lt; 0) &#123; perror(\"fork error:\"); exit(1); &#125; //第一个子进程 else if (pid == 0) &#123; //子进程再创建子进程 printf(\"I am the first child process.pid:%d\\tppid:%d\\n\",getpid(),getppid()); pid = fork(); if (pid &lt; 0) &#123; perror(\"fork error:\"); exit(1); &#125; //第一个子进程退出 else if (pid &gt;0) &#123; printf(\"first procee is exited.\\n\"); exit(0); &#125; //第二个子进程 //睡眠3s保证第一个子进程退出，这样第二个子进程的父亲就是根进程里 sleep(3); printf(\"I am the second child process.pid: %d\\tppid:%d\\n\",getpid(),getppid()); exit(0); &#125; //父进程处理第一个子进程退出 if (waitpid(pid, NULL, 0) != pid) //等待进程ID为 pid 的子进程，只要子进程没有结束，就一直等下去 &#123; perror(\"waitepid error:\"); exit(1); &#125; exit(0); return 0;&#125; 参考文献 https://www.cnblogs.com/Anker/p/3271773.html https://mp.weixin.qq.com/s/wTicQwTu8Ta8gLv2fZ3rCA https://mp.weixin.qq.com/s/Lu1nqXfrGPsFcFHH_R_rYA","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"Linux-A01-内核入门","slug":"Linux/Linux-A01-内核入门","date":"2020-10-25T02:24:18.000Z","updated":"2023-08-10T04:27:35.296Z","comments":true,"path":"2020/10/25/Linux/Linux-A01-内核入门/","link":"","permalink":"http://xboom.github.io/2020/10/25/Linux/Linux-A01-%E5%86%85%E6%A0%B8%E5%85%A5%E9%97%A8/","excerpt":"","text":"编译内核代码 下载内核代码 从国内镜像网站下载最新的内核源码linux-5.9.1，并进行解压 12xz -d linux-5.9.1.tar.xztar xvf linux-5.9.1.tar 配置内核 对内核进行配置是为了得到内核配置文件.config。通过对内核进行配置，可以使未来编译成功的内核增加或减少对一些内核特性的支持。对内核进行配置有多种方法：基于文本/图形等，这里采用make menuconfig方式： 由于该配置方式基于ncurses库，所以在启动配置界面前要先安装ncurses库 12yum install -y ncurses-devel # make menuconfig requires the ncurses librariesmake menuconfig 配置界面成功如下 对内核按照默认的配置方式进行编译，因此当配置菜单启动后直接退出并保存即可。此时就在内核源码根目录下生成了.config文件 编译内核 编译内核包含两部分的工作， 其一是编译内核，即编译配置选项中标记为Y的那部分，这部分内核最终形成bzIamge镜像文件； 其二是编译内核模块，即编译配置选项中标记为M的那部分内核，这部分形成以.ko结尾的内核模块目标文件。 上述两部分编译工作可以依次通过make bzImage和make modules完成，也可以通过一条make命令直接完成。 编译内核的整个过程比较漫长，可以对make加-j参数来提高编译的效率。在make时使用该选项会为编译过程分配n个并发任务，这样可以缩短编译时间。n的取值为cpu个数的二倍。 1make -j4 如果不想看到垃圾信息又不想错过告警和错误信息，可以使用重定向文件写入，编译信息会写到文件中，同时错误和告警信息都会在屏幕上显示 1make &gt; ../detritus 安装内核 安装过程分为两部分，首先对内核模块进行安装，这个过程会将刚刚编译内核模块时生成的内核模块复制到/lib/modules/5.9.1/目录下，其中5.9.1为对应的内核版本。使用的命令如下： 1sudo make modules_install 接着使用下述命令安装编译好的内核： 1sudo make install 安装内核的过程主要完成了以下的工作： 将编译内核时生成的内核镜像bzImage拷贝到/boot目录下，并将这个镜像命名为vmlinuz-5.9.1。如果使用x86的cpu，则该镜像位于arch/x86/boot/目录下（处于正在编译的内核源码下）。 将~/linux-5.9.1/目录下的System.map拷贝到/boot/目录下，重新命名为System.map-5.9.1。该文件中存放了内核的符号表。 将~/linux-5.9.1/目录下的.config拷贝到/boot/目录下，重新命名为config-5.9.1。 创建initrd.img文件 initrd.img即为初始化的ramdisk文件，它是一个镜像文件，将一些最基本的驱动程序和命令工具打包到镜像文件里。该镜像文件的作用是在系统还没有挂载根分区前，系统需要执行一些操作，比如挂载scsi驱动，此时将initrd文件释放到内存中，作为一个虚拟的根分区，然后执行相关脚本，运行insmod命令加载需要的模块。 1sudo mkinitramfs 5.9.1 -o /boot/initrd.img-5.9.1 更新grub 最后一步则是更新grub启动菜单，使用下面的命令则可以自动更新启动菜单： 1sudo update-grub2 这样会将刚才编译好的内核放在启动菜单的首位，如果需要修改启动菜单中默认系统的启动顺序，则修改/boot/grub/grub.cfg文件中的set default=的值即可 内核就这样编译完毕了 Hello Kernel 编写一个Hello.c文件 1234567891011121314151617181920212223242526272829#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;//必选//模块许可声明MODULE_LICENSE(\"GPL\");//模块加载函数static int hello_init(void)&#123; printk(KERN_ALERT \"hello,I am edsionte\\n\"); return 0;&#125;//模块卸载函数static void hello_exit(void)&#123; printk(KERN_ALERT \"goodbye,kernel\\n\");&#125;//模块注册module_init(hello_init);module_exit(hello_exit);//可选MODULE_AUTHOR(\"xboom dove\");MODULE_DESCRIPTION(\"This is a simple example!\\n\");MODULE_ALIAS(\"A simplest example\"); 一个模块程序的中，模块加载函数，模块卸载函数以及模块许可声明是必须有的 编写了模块加载函数后，还必须用module_init(mode_name);的形式注册这个函数。当接下来用insmod加载模块时，内核会自动去寻找并执行内核加载函数，完成一些初始化工作。类似的当使用rmmod命令时，内核会自动去执行内核卸载函数。 注意这里的printk函数，可以简单的理解为它是内核中的printf函数，初次使用很容易将其打成printf 编写Makefile文件 12345678910111213141516171819obj-m += hello.o#generate the pathCURRENT_PATH:=$(shell pwd)#the current kernel version numberLINUX_KERNEL:=$(shell uname -r)#the absolute pathLINUX_KERNEL_PATH:=/lib/modules/$(LINUX_KERNEL)/build#complie objectall: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules#cleanclean: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean 在当前目录下运行make命令，就会生成hello.ko文件，即模块目标文件。接下来： 使用insmod hello.ko 添加该模块 如果报错 “Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel” Centos: yum install elfutils-libelf-devel Ubuntu： 12apt install libelf-devapt install libssl-dev 查看已经加载的模块 lsmod 12[root@localhost linuxDemo]# lsmod |grep hellohello 16384 0 使用rmmod hello.ko 卸载该模块 使用 dmesg命令查看printk中显示的语句 12345[339722.818996] hello: loading out-of-tree module taints kernel.[339722.819088] hello: module verification failed: signature and/or required key missing - tainting kernel[339722.819609] hello,I am edsionte[339764.633084] goodbye,kernel[339786.267993] hello,I am edsionte linux内核从3.7 开始加入模块签名检查机制，如果内核选项CONFIG_MODULE_SIG和CONFIG_MODULE_SIG_FORCE打开的话， 当加载模块时内核会检查模块的签名，如果签名不存在或者签名内容不一致，会强制退出模块的加载 提示&quot;module verification failed: signature and/or required key missing - tainting kernel&quot; 关闭方式： 在Makefile中添加 CONFIG_MODULE_SIG=n 然后重新重新编译内核 其实第一步insmod还有另外一种办法： 将hello.ko文件拷贝到/lib/module/#uname -r#/目录下， #uname -r#意思是，在终端中输入 uname -r后显示的内核版本及名称，例如mini2440中#uname -r#就是2.6.32.2-FriendlyARM。 然后depmod（会在/lib/modules/#uname -r#/目录下生成modules.dep和modules.dep.bb文件，表明模块的依赖关系） 最后modprobe hello（注意这里无需输入.ko后缀）即可 两种方法的区别： modprobe和insmod类似，都是用来动态加载驱动模块的，区别在于modprobe可以解决load module时的依赖关系，它是通过/lib/modules/#uname -r/modules.dep(.bb)文件来查找依赖关系的；而insmod不能解决依赖问题。 参考文档 Linux内核新手区 《Linux内核设计与实现》","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"TCP-1-传输控制协议","slug":"TCP/TCP-1-传输控制协议","date":"2020-07-26T09:25:09.000Z","updated":"2022-06-25T07:00:00.000Z","comments":true,"path":"2020/07/26/TCP/TCP-1-传输控制协议/","link":"","permalink":"http://xboom.github.io/2020/07/26/TCP/TCP-1-%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"引言 TCP提供一种面向连接的，可靠的字节流服务 面向连接的：指通信通过三次握手建立TCP连接 可靠的： 三次握手建立连接，四次挥手释放连接； ARQ协议(Automatic Repeat-reQuest)即自动重传请求来保证数据传输的正确性； 使用分组窗口和滑动窗口协议来保证接收方能够及时处理接收到的数据，进行流量控制； 使用慢开始、拥塞避免、拥塞发生、快恢复进行拥塞控制 字节流：不在字节流中添加边界标识符 如何唯一确定一个TCP连接？ 答：源地址+源端口+目的地址+目的端口，源地址和目的地址的字段(32位)是在IP头部，源端口和目的端口的字段(16位)在TCP头部。 有一个IP的服务器监听一个端口，它的TCP最大连接数是多少？ 答：首先，存在文件描述符限制，Socket都是文件，所以通过ulimit配置文件描述符的限制；其次，存在内存限制，每个TCP连接都要占用一定内存，操作系统的内存是有限的 TCP数据结构 源端口(Source Port, 2B = 16bits)和目的端口(Destination Port, 2B = 16bit)加上IP层源IP和目的IP唯一确定一个TCP连接，一个IP地址和一个端口号称为一个插口(socket)。 序列号(Sequence Number, 4B = 32bits)标识了TCP发送端到TCP接收端的数据流的一个字节，该字节代表着包含该序列号的报文段的数据报文段的第一个字节(想象两程序直接一个方向上流动的数据流，TCP给每个字节赋予了一个序列号)。如果TCP报文中flags标志位为SYN，该序列号表示初始化序列号(ISN)，此时第一个数据应该是从序列号ISN+1开始(因为SYN标识消耗了一个序号，同理FIN标识也要占用一个序号)。序号是32bits的无符号数，序号到达最大值 2^32 - 1后又从0开始。 初始序列号可被视为一个32位的计数器，该计数器数值每4微妙加1，防止出现与其他连接的序列号重叠 确认序列号(Acknowledgment Number, 4B = 32bits)表示TCP发送确认的一端所期望接受的下一个数据分片的序列号。该序号在TCP分片中Flags标志位为ACK时生效。 TCP为应用层提供全双工服务。这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据序号。 首部长度(Data Offset/Header Length, 4bits)也叫数据偏移，该字段表示 TCP 所传输的数据部分应该从 TCP 包的哪个位置开始计算，可以看作是 TCP 首部的长度。它表示TCP头包含了多少4字节的Words。因为4bits在十进制中能表示的最大值为15，32bits表示4个字节，那么Data Offset的最大可表示15*4=60个字节。所以TCP报头长度最大为60字节。如果options fields为0的话，报文头长度为20个字节。 预留字段(Reserved field, 6bits)值全为零。预留给以后使用。 标志位(Flags, 6bits)表示TCP包特定的连接状态。一个标签位占1bit，从低位到高位值依次为 FIN: 表示发送者已经发送完数据。通常用在发送者发送完数据的最后一个包中 SYN: 在连接建立时用来同步序号。当SYN=1而ACK=0时，表明这是一个连接请求报文。对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此,SYN置1就表示这是一个连接请求或连接接受报文。 RST: 表示复位TCP连接 PSH: 通知接收端处理接收的报文，而不是将报文缓存到buffer中 ACK: 只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 URG: 表示紧急指针字段有效 补充的标志还有ECE,CWR占用保留字段2bits ECE: Explicit Congestion Notification。ECN回显。 CWR: Congestion Window Reduced。拥塞窗口减(发送方降低它的发送速率)，CWR 标志与后面的 ECE 标志都用于 IP 首部的 ECN 字段，ECE 标志为 1 时，则通知对方已将拥塞窗口缩小。 详见RFC3168。 窗口大小(Window, 2B = 16bits)表示滑动窗口的大小，用来告诉发送端接收端的buffer space的大小。接收端buffer大小用来控制发送端的发送数据数率，从而达到流量控制。最大值为2^16 = 65535B。若窗口为 0，则表示可以发送窗口探测，以了解最新的窗口大小，但这个数据必须是 1 个字节。 校验和(Checksum, 2B = 16bits)奇偶校验是对整个的 TCP 报文段(包括 TCP 头部和 TCP 数据，以 16 位字进行计算所得)由发送端计算和存储，并由接收端进行验证。 紧急指针(Urgent pointer, 2B = 16bits)只有在 URG 控制位为 1 时有效。该字段的数值表示本报文段中紧急数据的指针。从数据部分的首位到紧急指针所在的位置为止是紧急数据。 选项和填充(Option和pading)可变长度。表示TCP可选选项以及填充位。当选项不足32bits时，填充字段加入额外的0填充。最常见的可选字段: 最长报文大小MSS(Maximum Segment Size)在通信的第一个报文段(为建立连接而设置SYN标志为1的那个段)中指明这个选项，表示本端所能接受的最大报文段的长度(不包含TCP与IP头部)。选项长度不一定是32位的整数倍，所以要加填充位，即在这个字段中加入额外的零，以保证TCP头是32的整数倍 最大段大小的均值为1460，加上40字节(TCP头部和IP头部)组成1500字节 最大传输单元典型值 由于IPv6的头部比IPv4的头部长20字节，最大报文大小也减少20字节 TCP选择确认(SACK)选项 窗口缩放选项 时间戳选项和防回绕序列号 用户超时选项 认证选项 数据(Data)长度可变。用来存储上层协议的数据信息。可以为空。比如在连接建立和连接中止时。 TCP三次握手 第一次握手：建立连接时，客户端发送syn包(syn=x)到服务器，并进入SYN_SENT状态，等待服务器确认； 第二次握手：服务器收到syn包，确认客户的SYN(ack=x+1)，同时自己也发送一个SYN包(syn=y)，即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(seq=x+1, ack=y+1），此包发送完毕，客户端和服务器进入ESTABLISHED(TCP连接成功）状态，完成三次握手。 第三次握手是可以带数据的，前两次的握手不能带数据 TCP四次挥手 第一次挥手：客户端进程发出连接释放请求FIN(seq=u)，并且停止发送数据。客户端进入FIN-WAIT-1(终止等待1)状态。FIN报文段即使不携带数据，也要消耗一个序号。 第二次挥手： 服务器收到连接释放请求，发出确认报文ACK(ack=u+1)，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。整个TCP处于半关闭状态，即客户端已经没数据需要发送，但服务器若发送数据，客户端依然要接受。 客户端收到服务器的确认请求后ACK后，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。 第三次挥手： 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。 第四次挥手：服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。 套接字API还提供半关闭操作，使用shutdown()代替close()可以实现TCP半关闭 MSL与MTT的区别？ TCP状态转换 箭头表示因报文段传输、接收以及计时器超时而引发的状态转换 红色箭头表示客户端行为 虚线箭头表示服务器行为 TIME_WAIT状态又称为2MSL等待状态，在此状态TCP会等待2倍的最大段生存期(Maximum Segment Lifetime, MSL)的时间。这样做的目的是： 让TCP重新发送最终的ACK已避免丢失的情况。重新发送最终的ACK并不是因为TCP重传了ACK，而是因为通信另一方重传了它的FIN。TCP总是重传FIN，直到它收到一个最终的ACK。 当TCP处于TIME_WAIT状态的时候，通信双方将该连接(四元组)定义为不可重新使用。只有当2MSL等待结束，或一条新链接使用的初始序列号超过了连接之前的实例所使用的最高序列号时，或允许使用时间戳选项来区分之前连接实例的报文段以避免混淆时，这条连接才能重新使用。 当一个连接处于TIME_WAIT状态时，任何延迟到达的报文都会被丢弃。 FIN_WAIT_2状态可以一直保持这个状态，也就是对端一直处于CLOSE_WAIT状态，在Linux中可以通过调整变量net.ipv4.tcp_fin_timeout来设置计时器的秒数(默认60s) 问题解答 为什么在TCP首部的开始便是源和目的的端口号？ 答：一个ICMP差错报文必须至少返回引起差错的IP数据报中除了IP首部的前8个字节。当TCP收到一个ICMP差错报文时，它需要检查两个端口号以决定差错对应于哪个连接。因此，端口号必须包含在TCP首部的前8个字节里 首部校验和是怎么计算的? 为什么连接的时候是三次握手，关闭的时候却是四次握手？ 答：当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，只有等到Server端所有报文都发送完，才能发送FIN报文，因此不能一起发送。 为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？ 答：网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间，一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 为什么不能用两次握手进行连接？ 答：3次握手完成三个重要的功能 三次握手才可以阻止重复历史连接的初始化(主要原因) 三次握手可以同步双方的初始序列号 三次握手才可以避免资源浪费 两次握手可能存在死锁的情况：计算机S和C之间的通信，假定C给S发送一个连接请求分组，S收到了这个分组，并发送了确认应答分组。按照两次握手的协定，S认为连接已经成功地建立了，可以开始发送数据分组。可是，C在S的应答分组在传输中被丢失的情况下，将不知道S是否已准备好，不知道S建立什么样的序列号，C甚至怀疑S是否收到自己的连接请求分组。在这种情况下，C认为连接还未建立成功，将忽略S发来的任何数据分组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。 如果已经建立了连接，但是客户端突然出现故障了怎么办？ 答：TCP设有保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 TCP与UDP的区别 目标和源端口：告诉UDP协议应该把报文发往哪个进城 包长度：保存了UDP首部的长度和数据的长度之和 校验和：校验和是为了提供可靠的UDP首部和数据 答：如下 连接 TCP是面向链接的传输层协议，传输数据前先要建立连接 UDP是不需要连接，即刻传输数据 服务对象 TCP是一对一的两点服务，即一条连接只有两个端点 UDP支持一对一、一对多、多对多的交互方式 可靠性 TCP是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达 UDP是尽最大努力交付，不保证可靠交付(什么叫尽最大努力？？) 拥塞控制、流量控制 TCP有拥塞控制和流量控制机制，保证数据的可靠性 UDP则没有 首部开销 TCP首部长度较长、会有一定的开销，没有使用选项字段是20字节 UDP首部只有8个字节，并且固定不变 传输方式 TCP是流式传输、没有边界，但保证顺序和可靠 UDP是一个包一个包的发送，是有边界的，可能丢失或乱序 分片不同 TCP的数据大小如果大于MSS大小，则会在传输层进行分片。中途丢失分片，也只会重传分片 UDP的数据大小如果大于MTU大小，则会在IP层分片并在IP层组装。如果中途丢失分片。则需要传输所有，所以UDP包大小最好小于MTU 应用场景 TCP面向链接，保证数据可靠。如 FTP文件传输、HTTP/HTTPS UDP面向无连接，可随时发送。如 DNS、SNMP、视频/音频等多媒体通信、广播通信 为什么UDP头部有包长度 字段，而TCP头部长度则没有 答：TCP数据的长度 = IP总长度 - IP首部 - TCP头部。UDP为啥不用这个公式，可能是为了保证数据包长度为4字节的整数倍 如何在Linux系统中查看TCP状态 答：通过命令 netstat -antp 参考链接 TCP:传输控制协议 TCP的三次握手和四次挥手 TCP状态转换图解析 TCP状态转换解析","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]}],"categories":[{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/categories/Unix-NetWrok/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"http://xboom.github.io/categories/Data-Structure/"},{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/categories/Algorithms/"},{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"},{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"},{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/categories/Algorithms-To-Live-By/"},{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/categories/Grpc/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/categories/Design-Patterns/"},{"name":"C++","slug":"C","permalink":"http://xboom.github.io/categories/C/"},{"name":"Interesting Algorithm","slug":"Interesting-Algorithm","permalink":"http://xboom.github.io/categories/Interesting-Algorithm/"},{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/categories/Distributed-Transaction/"},{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"},{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"},{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"},{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/categories/Docker/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"},{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/categories/Microservices/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"},{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"http://xboom.github.io/tags/Unix/"},{"name":"Unix NetWrok","slug":"Unix-NetWrok","permalink":"http://xboom.github.io/tags/Unix-NetWrok/"},{"name":"Heap","slug":"Heap","permalink":"http://xboom.github.io/tags/Heap/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"http://xboom.github.io/tags/Data-Structure/"},{"name":"Libevent","slug":"Libevent","permalink":"http://xboom.github.io/tags/Libevent/"},{"name":"Algorithms","slug":"Algorithms","permalink":"http://xboom.github.io/tags/Algorithms/"},{"name":"List","slug":"List","permalink":"http://xboom.github.io/tags/List/"},{"name":"DHCP","slug":"DHCP","permalink":"http://xboom.github.io/tags/DHCP/"},{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/tags/Protocol/"},{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"},{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://xboom.github.io/tags/Distributed-System/"},{"name":"Raft","slug":"Raft","permalink":"http://xboom.github.io/tags/Raft/"},{"name":"Algorithms To Live By","slug":"Algorithms-To-Live-By","permalink":"http://xboom.github.io/tags/Algorithms-To-Live-By/"},{"name":"Grpc","slug":"Grpc","permalink":"http://xboom.github.io/tags/Grpc/"},{"name":"Design Patterns","slug":"Design-Patterns","permalink":"http://xboom.github.io/tags/Design-Patterns/"},{"name":"C++","slug":"C","permalink":"http://xboom.github.io/tags/C/"},{"name":"Concurrency","slug":"Concurrency","permalink":"http://xboom.github.io/tags/Concurrency/"},{"name":"PrimerPlus","slug":"PrimerPlus","permalink":"http://xboom.github.io/tags/PrimerPlus/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"Distributed Transaction","slug":"Distributed-Transaction","permalink":"http://xboom.github.io/tags/Distributed-Transaction/"},{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"},{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"},{"name":"Quic","slug":"Quic","permalink":"http://xboom.github.io/tags/Quic/"},{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"},{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/tags/Docker/"},{"name":"HTTP","slug":"HTTP","permalink":"http://xboom.github.io/tags/HTTP/"},{"name":"Microservices","slug":"Microservices","permalink":"http://xboom.github.io/tags/Microservices/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"},{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"},{"name":"Memory","slug":"Memory","permalink":"http://xboom.github.io/tags/Memory/"}]}