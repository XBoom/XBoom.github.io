{"meta":{"title":"XBoom Dove","subtitle":"","description":"","author":"XBoom Dove","url":"http://xboom.github.io","root":"/"},"pages":[{"title":"Repositories","date":"2022-07-03T09:03:24.278Z","updated":"2020-09-02T15:24:35.801Z","comments":false,"path":"repository/index.html","permalink":"http://xboom.github.io/repository/index.html","excerpt":"","text":""},{"title":"关于","date":"2022-06-25T07:09:26.668Z","updated":"2020-09-02T15:24:35.803Z","comments":false,"path":"about/index.html","permalink":"http://xboom.github.io/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2022-06-25T07:09:26.674Z","updated":"2020-09-02T15:24:35.806Z","comments":false,"path":"categories/index.html","permalink":"http://xboom.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-06-25T07:09:26.674Z","updated":"2020-09-02T15:24:35.805Z","comments":false,"path":"tags/index.html","permalink":"http://xboom.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"go-zero4-协程池1","slug":"GoZero/GoZero4-协程池1","date":"2022-07-06T13:53:58.640Z","updated":"2022-07-06T14:47:08.333Z","comments":true,"path":"2022/07/06/GoZero/GoZero4-协程池1/","link":"","permalink":"http://xboom.github.io/2022/07/06/GoZero/GoZero4-%E5%8D%8F%E7%A8%8B%E6%B1%A01/","excerpt":"","text":"问题背景 需要协程池吗？ 虽然协程非常轻量级的，一般用不上协程池。协程池的作用 无休止地创建大量goroutine，势必会因为对大量go 程的创建、调度和销毁带来性能损耗。 为了解决这个问题，可以引入协程池 解决方案 协程池需要什么？ 协程如何重用、任务如何执行 协程池支持自定义协程池大小 如果当前任务数量超过协程池大小，那么当前任务需要等待，等待时间支持超时退出 协程支持自定义退出 异常捕获，防止因为单个协程的异常处理导致整个协程池无法使用 协程池大致的逻辑如下图所示： 方案1 这是一个网上能搜到的 “100行实现一个协程池”， 第一步：定义一个任务 1234type Task struct &#123; Handler func(v ...interface&#123;&#125;) Params []interface&#123;&#125;&#125; 其实是将需要执行的协程方法，使用结构体封装起来 第二步：定义一个协程池 12345678910111213type Pool struct &#123; capacity uint64 //容量 runningWorkers uint64 //正在运行的协程树 status int64 //状态(防止在退出过程中，仍然在新建协程) chTask chan *Task //任务队列 PanicHandler func(interface&#123;&#125;)//panic处理函数 sync.Mutex //原子锁,用于保证runningWorkers原子数据变化&#125;const ( RUNNING = 1 STOPED = 0) 第三步：利用协程池启动一个协程执行任务 12345678910111213141516171819202122232425262728293031323334353637func (p *Pool) run() &#123; p.incRunning() //增加正在运行的协程数目 go func() &#123; defer func() &#123; p.decRunning() //结束任务 if r := recover(); r != nil &#123; if p.PanicHandler != nil &#123; p.PanicHandler(r) &#125; else &#123; log.Printf(\"Worker panic: %s\\n\", r) &#125; &#125; p.checkWorker() // check worker avoid no worker running &#125;() for &#123; select &#123; case task, ok := &lt;-p.chTask: if !ok &#123; return &#125; task.Handler(task.Params...) //任务处理 &#125; &#125; &#125;()&#125;//如果任务数量大于0，但是这个时候协程都退出，则再次构建一个协程func (p *Pool) checkWorker() &#123; p.Lock() defer p.Unlock() if p.runningWorkers == 0 &amp;&amp; len(p.chTask) &gt; 0 &#123; p.run() &#125;&#125; 协程池的协程不是常驻协程吗，为什么会出现协程数量为0，但是任务大于0的情况呢？ 答：工作协程可能因为 panic 都退出了，那么这个时候就需要有一个重新拉起协程去执行任务 第四步：生产任务 1234567891011121314151617181920func (p *Pool) Put(task *Task) error &#123; p.Lock() defer p.Unlock() if p.status == STOPED &#123; return ErrPoolAlreadyClosed &#125; // run worker if p.GetRunningWorkers() &lt; p.GetCap() &#123; p.run() &#125; // send task if p.status == RUNNING &#123; p.chTask &lt;- task &#125; return nil&#125; 为什么在存放任务的时候，会多一个协程池的判断呢？ 答：可能会出现协程池结束关闭的情况，如果这个时候又有新的任务，那就又会创建新的协程去执行 最后：关闭协程池 12345678910111213// Close close pool gracefulfunc (p *Pool) Close() &#123; if !p.setStatus(STOPED) &#123; // stop put task return &#125; for len(p.chTask) &gt; 0 &#123; // 等待所有的任务都被消费 time.Sleep(1e6) // 防止等待任务清空 cpu 负载突然变大, 这里小睡一下 &#125; close(p.chTask)&#125; 总结： 这里添加了协程池的状态，防止退出时候的任务增加 为什么在退出的时候，如果任务大于0，那么需要 sleep 一下？ 异常捕获之后再次检查是否有协程在执行任务，没有则添加一个协程 使用无缓冲channel进行任务执行，可能会出现添加任务阻塞的情况 任务是否可以添加一个运行超时时间，防止单个任务死锁？ 方案2 字节跳动开源的协程池，仓库地址：https://github.com/bytedance/gopkg/tree/develop/util/gopool 使用 生产者-消费者模式 设计协程池 第一步：**协程池 ** 具有的功能 12345678type Pool interface &#123; Name() string //协程池名称 SetCap(cap int32) //协程池容量 Go(f func()) //使用协程执行 f CtxGo(ctx context.Context, f func()) //使用协程执行f并支持参数 ctx SetPanicHandler(f func(context.Context, interface&#123;&#125;)) //设置协程处理函数 WorkerCount() int32 //返回正在运行的协程数量&#125; 第二步：协程池的结构 1234567891011type pool struct &#123; name string //协程池名称 cap int32 //协程池容量 config *Config //协程池配置 taskHead *task //任务头部 taskTail *task //任务尾部 taskLock sync.Mutex //任务原子锁(竞争) taskCount int32 //任务数量 workerCount int32 //正在运行的协程数量 panicHandler func(context.Context, interface&#123;&#125;) //Panic处理逻辑&#125; 第三步：看看 任务 task 的定义 123456789101112type task struct &#123; ctx context.Context f func() //执行函数 next *task //指向下一个任务的指针&#125;type taskList struct &#123; //使用双向链表将任务连接起来 sync.Mutex taskHead *task taskTail *task&#125; 第四步：查看协程池是怎么运行的 1234567891011121314151617181920212223242526272829var taskPool sync.Pool //对象池func (p *pool) CtxGo(ctx context.Context, f func()) &#123; t := taskPool.Get().(*task) //从对象池获取任务对象 t.ctx = ctx t.f = f p.taskLock.Lock() //获取任务写锁 if p.taskHead == nil &#123; //如果任务链表为空则新建，否则插入链表尾部 p.taskHead = t p.taskTail = t &#125; else &#123; p.taskTail.next = t p.taskTail = t &#125; p.taskLock.Unlock() //释放任务写锁 atomic.AddInt32(&amp;p.taskCount, 1) //增加任务数量 /* 1. 如果任务数量大于配置的数量 &amp;&amp; 正在执行的协程数量小于协程池容量，说明任务太多，还有空闲的协程 那么就开启一个新的协程处理 如果任务数量小于配置的数量 &amp;&amp; 正在执行的协程数量小于协程池容量。说明任务还不多，就让当前协程顺序执行 2. 正在执行的协程为0 */ if (atomic.LoadInt32(&amp;p.taskCount) &gt;= p.config.ScaleThreshold &amp;&amp; p.WorkerCount() &lt; atomic.LoadInt32(&amp;p.cap)) || p.WorkerCount() == 0 &#123; p.incWorkerCount() w := workerPool.Get().(*worker) w.pool = p w.run() &#125;&#125; 这里额外定义了一个 workPool，其实是消费者池 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354var workerPool sync.Pooltype worker struct &#123; //消费者其实是一个协程池 pool *pool&#125;func (w *worker) run() &#123; go func() &#123; for &#123; //这是一个常驻协程 var t *task w.pool.taskLock.Lock() //获取任务写锁 if w.pool.taskHead != nil &#123; //获取任务并将任务数量-1 t = w.pool.taskHead w.pool.taskHead = w.pool.taskHead.next atomic.AddInt32(&amp;w.pool.taskCount, -1) &#125; if t == nil &#123; //如果没有任务，那么worker销毁 // if there's no task to do, exit w.close() w.pool.taskLock.Unlock() w.Recycle() return &#125; w.pool.taskLock.Unlock() //释放任务写锁 func() &#123; defer func() &#123; if r := recover(); r != nil &#123; //异常处理 if w.pool.panicHandler != nil &#123; w.pool.panicHandler(t.ctx, r) &#125; else &#123; msg := fmt.Sprintf(\"GOPOOL: panic in pool: %s: %v: %s\", w.pool.name, r, debug.Stack()) logger.CtxErrorf(t.ctx, msg) &#125; &#125; &#125;() t.f() //执行任务 f &#125;() t.Recycle() //任务结束后，会收work &#125; &#125;()&#125;func (w *worker) close() &#123; //减少worker数目 w.pool.decWorkerCount()&#125;func (w *worker) zero() &#123; //释放缓存池 w.pool = nil&#125;func (w *worker) Recycle() &#123; //释放worker(存入缓存池) w.zero() workerPool.Put(w)&#125; 最后，它还定义了一个 poolMap 用于根据名称注册与使用 多个协程池 注意： 使用双向链表存储任务，表示它理论上支持无限个任务。后面的任务可能存在长时间等待的情况 使用任务上限的好处是，不是每来一个任务都开启一个协程，而是任务超过一定数量而又空闲的协程才开启新的协程去执行 方案3：ants 内容比较多，将在下一章详细描述 技术内幕 go-zero是如何实现协程池的，代码路径：core/threading 第一步，定义了recover逻辑，用于 panic 之后的清理操作 12345678910// core/rescue/recover.gofunc Recover(cleanups ...func()) &#123; for _, cleanup := range cleanups &#123; cleanup() &#125; if p := recover(); p != nil &#123; //logx.ErrorStack(p) &#125;&#125; 第二步，定义了一个安全运行goroutine的方案 GoSafe，包含处理panic逻辑 123456789func GoSafe(fn func()) &#123; go RunSafe(fn)&#125;func RunSafe(fn func()) &#123; defer rescue.Recover() fn()&#125; TaskRunner： 使用 limitChan 协程池 执行协程 123456789101112131415161718192021222324// TaskRunner 用于并发控制协程数量type TaskRunner struct &#123; limitChan chan lang.PlaceholderType&#125;// 创建 TaskRunner 对象func NewTaskRunner(concurrency int) *TaskRunner &#123; return &amp;TaskRunner&#123; limitChan: make(chan lang.PlaceholderType, concurrency), &#125;&#125;// 在 任务并发控制下执行 taskfunc (rp *TaskRunner) Schedule(task func()) &#123; rp.limitChan &lt;- lang.Placeholder //limitChan 类似一个并发锁 go func() &#123; defer rescue.Recover(func() &#123; &lt;-rp.limitChan &#125;) task() &#125;()&#125; 注意：当limitChan满那么任务执行会出现超时，缺乏超时逻辑 WorkerGroup：使用 wokers 并发执行任务 job 123456789101112131415161718192021type WorkerGroup struct &#123; job func() workers int&#125;// NewWorkerGroup returns a WorkerGroup with given job and workers.func NewWorkerGroup(job func(), workers int) WorkerGroup &#123; return WorkerGroup&#123; job: job, workers: workers, &#125;&#125;// Start starts a WorkerGroup.func (wg WorkerGroup) Start() &#123; group := NewRoutineGroup() for i := 0; i &lt; wg.workers; i++ &#123; group.RunSafe(wg.job) &#125; group.Wait()&#125; RoutineGroup: 多协程等待 123456789101112131415161718192021222324252627282930313233// RoutineGroup 多协程等待type RoutineGroup struct &#123; waitGroup sync.WaitGroup&#125;func NewRoutineGroup() *RoutineGroup &#123; return new(RoutineGroup)&#125;// 不要引用外部参数，可能被其他协程修改func (g *RoutineGroup) Run(fn func()) &#123; g.waitGroup.Add(1) go func() &#123; defer g.waitGroup.Done() fn() &#125;()&#125;//不要引用外部参数，可能被其他协程修改func (g *RoutineGroup) RunSafe(fn func()) &#123; g.waitGroup.Add(1) GoSafe(func() &#123; defer g.waitGroup.Done() fn() &#125;)&#125;// 等待所有协程结束func (g *RoutineGroup) Wait() &#123; g.waitGroup.Wait()&#125; 所以 go-zero的threading 并不是真正的协程池，仅仅是提供多种并发执行 goroutine的方法 总结 所以从目前来看，实现一个协程池都有哪些值得学习的地方呢？ 将需要使用临时协程执行的函数已任务的形式 任务 -- 协程池(Pool) -- 工人执行 协程池是有容量限制的，有了容量就有正在运行的协程数 协程池有状态防止在退出的时候仍然进行任务构建与执行 协程池有异常捕获机制，保证单个异常不会影响整个协程池 任务已任务合集的形式存在，让消费者并发消费 有了异常捕获与任务合集，为了防止工人都发生异常，而还有任务没有执行，则需要有工人唤起机制 可以使用本地缓存池进行工人的重复利用 下一节，将学习另外一个协程池 ants 的实现方式 参考文档 https://go-zero.dev/cn/docs/goctl/installation/","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero3-MapReduce","slug":"GoZero/GoZero3-MapReduce","date":"2022-07-03T08:37:40.000Z","updated":"2022-07-03T08:41:55.313Z","comments":true,"path":"2022/07/03/GoZero/GoZero3-MapReduce/","link":"","permalink":"http://xboom.github.io/2022/07/03/GoZero/GoZero3-MapReduce/","excerpt":"","text":"什么是MapReduce? MapReduce是Google提出了一个软件架构，用于大规模数据集的并行运算。 MapReduce通过把对数据集的大规模操作分发给网络上的每个节点实现可靠性；每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。 go-zero的MapReduce则借鉴其中的思想，接下来一起看下go-zero是如何应用这一思想的 问题背景 在微服务中开发中，如果多个服务串行依赖的话那么整个API的耗时将会大大增加。通过什么手段来优化？ 传输层面通过MQ的解耦特性来降低API的耗时 MQ通信效率没有grpc高(消息通过MQ服务器进行中转) 业务层面通过Go语言的WaitGroup工具来进行并发控制 自行封装Add与Done 实际业务场景中： 如果接口的多个依赖有一个出错，则期望能立即返回且不必等待所有依赖都执行完毕。已经完成的接口调用也应该回滚 多个依赖可能有部分依赖之间也存在着相互依赖，或者上下关系 go-zero的主要应用场景为：需要从不同的rpc服务中获取相应属性组装成复杂对象，比如要查询商品详情： 商品服务-查询商品属性 库存服务-查询库存属性 价格服务-查询价格属性 营销服务-查询营销属性 如果是串行调用的话响应时间会随着 rpc 调用次数呈线性增长，简单场景下使用 WaitGroup 也能够满足需求，但如果要对 rpc 调用返回的数据进行校验、数据加工转换、数据汇总呢？ go-zero通过mapreduce来处理这种对输入数据进行处理最后输出清洗数据的问题。是一种经典的模式：生产者消费者模式。将数据处理分为三个阶段： 数据生产 generate(查询，必选) 数据加工 mapper(加工，可选) 数据聚合 reducer(聚合，可选) 利用协程处理以及管道通信，实现数据的加速处理 应用场景 场景1 对数据批处理，比如对一批用户id，效验每个用户的合法性并且效验过程中有一个出错就认为效验失败，返回的结果为效验合法的用户id 123456789101112131415161718192021222324252627282930313233343536373839404142434445package mapreduceimport ( \"errors\" \"log\" \"github.com/zeromicro/go-zero/core/mr\")func Run(uids []int) ([]int, error) &#123; r, err := mr.MapReduce(func(source chan&lt;- interface&#123;&#125;) &#123; for _, uid := range uids &#123; source &lt;- uid &#125; &#125;, func(item interface&#123;&#125;, writer mr.Writer, cancel func(error)) &#123; uid := item.(int) ok, err := check(uid) if err != nil &#123; //如果校验逻辑有问题，这里执行cancel整个校验过程停止 cancel(err) &#125; if ok &#123; //如果校验失败，那么不返回该uid writer.Write(uid) &#125; &#125;, func(pipe &lt;-chan interface&#123;&#125;, writer mr.Writer, cancel func(error)) &#123; var uids []int for p := range pipe &#123; uids = append(uids, p.(int)) &#125; writer.Write(uids) &#125;) if err != nil &#123; log.Printf(\"check error: %v\", err) return nil, err &#125; return r.([]int), nil&#125;func check(uid int) (bool, error) &#123; // do something check user legal if uid == 0 &#123; return false, errors.New(\"uid wrong\") &#125; return true, nil&#125; 其实是利用N个协程等待数据生产者的数据传输然后转交给聚合逻辑处理 场景2 某些功能的结果往往需要依赖多个服务，比如商品详情的结果往往会依赖用户服务、库存服务、订单服务等等，一般被依赖的服务都是以rpc的形式对外提供，为了降低依赖的耗时我们往往需要对依赖做并行处理 1234567891011121314151617181920func productDetail(uid, pid int64) (*ProductDetail, error) &#123; var pd ProductDetail err := mr.Finish(func() (err error) &#123; pd.User, err = userRpc.User(uid) return &#125;, func() (err error) &#123; pd.Store, err = storeRpc.Store(pid) return &#125;, func() (err error) &#123; pd.Order, err = orderRpc.Order(pid) return &#125;) if err != nil &#123; log.Printf(\"product detail error: %v\", err) return nil, err &#125; return &amp;pd, nil&#125; 技术内幕 源码目录：core/mr/mapreduce.go 其中利用到的对外函数有 12345678910111213141516171819202122// MapReduce 包含数据生产、数据处理以及数据聚合阶段并返回结果func MapReduce(generate GenerateFunc, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123;&#125;// MapReduceChan 包含数据生产、数据处理以及数据聚合阶段并返回结果。其中利用chan代替数据生产func MapReduceChan(source &lt;-chan interface&#123;&#125;, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; panicChan := &amp;onceChan&#123;channel: make(chan interface&#123;&#125;)&#125; return mapReduceWithPanicChan(source, panicChan, mapper, reducer, opts...)&#125; // ForEach 只包含数据生产和数据处理阶段，但没有任何输出func ForEach(generate GenerateFunc, mapper ForEachFunc, opts ...Option) &#123;&#125;// FinishVoid 并行运行 fnsfunc FinishVoid(fns ...func()) &#123; // Finish 并行运行 fns，在任何错误时取消func Finish(fns ...func() error) error &#123;&#125; //WithWorkers 定义一个 mapreduce 有几个协程func WithWorkers(workers int) Option &#123;&#125; 数据生产阶段 首先定义 buildSource使用协程进行数据生产 12345678910111213141516//buildSource 使用协程执行generate 并将协程的参数一个非缓冲的channel返回。如果generate发生panic，则将错误写入 onceChanfunc buildSource(generate GenerateFunc, panicChan *onceChan) chan interface&#123;&#125; &#123; source := make(chan interface&#123;&#125;) go func() &#123; defer func() &#123; if r := recover(); r != nil &#123; panicChan.write(r) &#125; close(source) &#125;() generate(source) //返回的非缓冲隧道也是数据生产的入口 &#125;() return source&#125; 其中的 onceChan则是一个非阻塞会缓冲的channel，当channel中还有数据没有处理完，则直接返回 123456789101112type onceChan struct &#123; channel chan interface&#123;&#125; wrote int32&#125;func (oc *onceChan) write(val interface&#123;&#125;) &#123; if atomic.AddInt32(&amp;oc.wrote, 1) &gt; 1 &#123; return &#125; oc.channel &lt;- val&#125; 数据处理阶段 接着利用mapReduceWithPanicChan进行数据处理mapper和数据聚合reducer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func MapReduce(generate GenerateFunc, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; panicChan := &amp;onceChan&#123;channel: make(chan interface&#123;&#125;)&#125; source := buildSource(generate, panicChan) //使用协程执行generate并返回数据生产无缓冲隧道source return mapReduceWithPanicChan(source, panicChan, mapper, reducer, opts...) //将隧道和处理函数传入&#125;//source就是数据来源隧道func mapReduceWithPanicChan(source &lt;-chan interface&#123;&#125;, panicChan *onceChan, mapper MapperFunc, reducer ReducerFunc, opts ...Option) (interface&#123;&#125;, error) &#123; options := buildOptions(opts...) // output is used to write the final result output := make(chan interface&#123;&#125;) //...defer //collector 用于从mapper收集数据，消费者是数据聚合 collector := make(chan interface&#123;&#125;, options.workers) //done表示结束，所有的mappers和reducer都要结束 done := make(chan lang.PlaceholderType) writer := newGuardedWriter(options.ctx, output, done) var closeOnce sync.Once // use atomic.Value to avoid data race //... go func() &#123; //起一个协程进行 数据聚合 //panic.wirte() reducer(collector, writer, cancel) &#125;() go executeMappers(mapperContext&#123; //进行 数据处理 ctx: options.ctx, mapper: func(item interface&#123;&#125;, w Writer) &#123; mapper(item, w, cancel) &#125;, source: source, panicChan: panicChan, collector: collector, doneChan: done, workers: options.workers, &#125;) select &#123; //等待结果 case &lt;-options.ctx.Done(): cancel(context.DeadlineExceeded) return nil, context.DeadlineExceeded case v := &lt;-panicChan.channel: panic(v) case v, ok := &lt;-output: if err := retErr.Load(); err != nil &#123; return nil, err &#125; else if ok &#123; return v, nil &#125; else &#123; return nil, ErrReduceNoOutput &#125; &#125;&#125; 其中数据处理阶段又定义了一个协程进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142func executeMappers(mCtx mapperContext) &#123; var wg sync.WaitGroup defer func() &#123; wg.Wait() close(mCtx.collector) drain(mCtx.source) &#125;() var failed int32 pool := make(chan lang.PlaceholderType, mCtx.workers) //协程池 writer := newGuardedWriter(mCtx.ctx, mCtx.collector, mCtx.doneChan) for atomic.LoadInt32(&amp;failed) == 0 &#123; select &#123; case &lt;-mCtx.ctx.Done(): return case &lt;-mCtx.doneChan: return case pool &lt;- lang.Placeholder: //这里是定义的N个works的chan，也就是会在下面创建n个协程 item, ok := &lt;-mCtx.source if !ok &#123; //如果来源关闭，那么将pool数据释放 &lt;-pool return &#125; wg.Add(1) go func() &#123; defer func() &#123; if r := recover(); r != nil &#123; atomic.AddInt32(&amp;failed, 1) mCtx.panicChan.write(r) &#125; wg.Done() &lt;-pool &#125;() //item:生产的数据 //writer: 数据处理对象 mCtx.mapper(item, writer) //执行map &#125;() &#125; &#125;&#125; 数据聚合阶段 最后来看一下是数据处理 1234567891011121314type guardedWriter struct &#123; //写入接口 ctx context.Context //保证超时退出 channel chan&lt;- interface&#123;&#125; //接收数据，这里传输的就是 长度为 N 的 collect channel done &lt;-chan lang.PlaceholderType //主动结束退出&#125;func newGuardedWriter(ctx context.Context, channel chan&lt;- interface&#123;&#125;, done &lt;-chan lang.PlaceholderType) guardedWriter &#123; return guardedWriter&#123; ctx: ctx, channel: channel, done: done, &#125;&#125; Finish Finish逻辑只进行并发处理，其实内部是将执行函数做为数据生产的生产的数据，然后又数据处理逻辑进行处理 12345678910111213141516171819202122232425262728func Finish(fns ...func() error) error &#123; if len(fns) == 0 &#123; //n个外部调用 return nil &#125; return MapReduceVoid(func(source chan&lt;- interface&#123;&#125;) &#123; for _, fn := range fns &#123; source &lt;- fn //数据生产者将函数传入 &#125; &#125;, func(item interface&#123;&#125;, writer Writer, cancel func(error)) &#123; fn := item.(func() error) if err := fn(); err != nil &#123; //数据处理逻辑执行函数 cancel(err) //这里并没有写入，所以第三个函数其实并没有执行 &#125; &#125;, func(pipe &lt;-chan interface&#123;&#125;, cancel func(error)) &#123; &#125;, WithWorkers(len(fns)))&#125;//底层还是执行的MapReduce逻辑func MapReduceVoid(generate GenerateFunc, mapper MapperFunc, reducer VoidReducerFunc, opts ...Option) error &#123; _, err := MapReduce(generate, mapper, func(input &lt;-chan interface&#123;&#125;, writer Writer, cancel func(error)) &#123; reducer(input, cancel) &#125;, opts...) if errors.Is(err, ErrReduceNoOutput) &#123; return nil &#125; return err&#125; 总结： 适用于并发无顺序依赖的并发调用，如果是多个调用具有前后依赖关系，依然需要有先后调用顺序(废话) 也不存在回滚的操作，内部只是将不在等待处理结果直接退出 select 参考链接 https://talkgo.org/t/topic/1452 https://zh.wikipedia.org/wiki/MapReduce","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero2-共享调用","slug":"GoZero/GoZero2-共享调用","date":"2022-06-27T16:55:17.000Z","updated":"2022-07-02T14:45:14.537Z","comments":true,"path":"2022/06/28/GoZero/GoZero2-共享调用/","link":"","permalink":"http://xboom.github.io/2022/06/28/GoZero/GoZero2-%E5%85%B1%E4%BA%AB%E8%B0%83%E7%94%A8/","excerpt":"","text":"问题背景 并发场景下，可能会有多个线程（协程）同时请求同一份资源，如果每个请求都要走一遍资源的请求过程，除了比较低效之外，还会对资源服务造成并发的压力。 例如： 缓存失效的同时多个请求同时到达某服务请求相同资源，这些请求会继续访问DB做查询，会引起数据库压力瞬间增大。而使用 SharedCalls 可以使得同时多个请求只需要发起一次拿结果的调用，其他请求&quot;坐享其成&quot;。有效减少了资源服务的并发压力，可以有效防止缓存击穿 云平台服务众多，使用grpc通信的时候不期望每个服务之间都建立链接，而只在向对端发送消息的时候，才在服务之间建立通信。老的逻辑是当开始建立连接的时候会将创建改为正在创建链接，后续消息会因为正在建立链接会直接返回错误或阻塞等待结果(自行实现)，而使用SharedCalls可以短时间内等待链接建立然后继续发送消息 演示代码 1234567891011121314151617181920212223242526func main() &#123; const round = 5 var wg sync.WaitGroup barrier := syncx.NewSharedCalls() wg.Add(round) for i := 0; i &lt; round; i++ &#123; // 多个线程同时执行 go func() &#123; defer wg.Done() // 可以看到，多个线程在同一个 key 上去请求资源，获取资源的实际函数只会被调用一次 val, err := barrier.Do(\"once\", func() (interface&#123;&#125;, error) &#123; // sleep 1秒，为了让多个线程同时取 once 这个 key 上的数据 time.Sleep(time.Second) // 生成了一个随机的 id return stringx.RandId(), nil &#125;) if err != nil &#123; fmt.Println(err) &#125; else &#123; fmt.Println(val) &#125; &#125;() &#125; wg.Wait()&#125; 技术内幕 文件目录：core/syncx/singleflight.go SingleFlight 通过为并发的请求根据相同的key提供相同的结果 一共提供了 Do 与 DoEx 两种接口 123456789101112131415var SingleFlight interface &#123; Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) DoEx(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, bool, error)&#125;var call struct &#123; //代指一次调用 wg sync.WaitGroup //用于等待call结束 val interface&#123;&#125; err error&#125;var flightGroup struct &#123; calls map[string]*call lock sync.Mutex&#125; 首先查看两个基础函数 123456789101112131415161718192021222324252627282930func (g *flightGroup) createCall(key string) (c *call, done bool) &#123; // 先申请加锁 g.lock.Lock() if c, ok := g.calls[key]; ok &#123; //如果key存在，那么等待 // 拿到 call 以后，释放锁，此处 call 可能还没有实际数据，只是一个空的内存占位 g.lock.Unlock() //调用 wg.Wait，判断是否有其他 goroutine 正在申请资源，如果阻塞，说明有其他 goroutine 正在获取资源 c.wg.Wait() //等待相同的call的结束 // 当 wg.Wait 不再阻塞，表示资源获取已经结束，可以直接返回结果 return c, true &#125; c = new(call) //创建一个新的call c.wg.Add(1) //并为这个call添加一个的等待 g.calls[key] = c g.lock.Unlock() return c, false&#125;func (g *flightGroup) makeCall(c *call, key string, fn func() (interface&#123;&#125;, error)) &#123; defer func() &#123; g.lock.Lock() delete(g.calls, key) //删除 g.lock.Unlock() c.wg.Done() //结束call &#125;() c.val, c.err = fn() //执行函数并返回结果&#125; 再看实现逻辑 1234567891011121314151617181920212223242526func (g *flightGroup) Do(key string, fn func() (interface&#123;&#125;, error)) (interface&#123;&#125;, error) &#123; c, done := g.createCall(key) //根据key定义一个call if done &#123; //如果是等待结束了，且等待结束了，则返回Call的value return c.val, c.err &#125; //如果是新的，那么就首次执行call，并返回结果 g.makeCall(c, key, fn) return c.val, c.err&#125;func (g *flightGroup) DoEx(key string, fn func() (interface&#123;&#125;, error)) (val interface&#123;&#125;, fresh bool, err error) &#123; c, done := g.createCall(key) if done &#123; //等待结束 return c.val, false, c.err &#125; g.makeCall(c, key, fn) return c.val, true, c.err //新的结束&#125;// NewSingleFlight returns a SingleFlight.func NewSingleFlight() SingleFlight &#123; return &amp;flightGroup&#123; calls: make(map[string]*call), &#125;&#125; DoEx 相较于 Do 中增加了一个 bool 类型的返回值，表示返回的值是共享的还是首次拿到的 参考链接 https://talkgo.org/t/topic/968","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"go-zero1-架构","slug":"GoZero/GoZero1-架构","date":"2022-06-27T16:55:05.000Z","updated":"2022-07-02T14:39:40.822Z","comments":true,"path":"2022/06/28/GoZero/GoZero1-架构/","link":"","permalink":"http://xboom.github.io/2022/06/28/GoZero/GoZero1-%E6%9E%B6%E6%9E%84/","excerpt":"","text":"Go-Zero结构 主要功能 技术内幕 防止缓存击穿之共享调用 参考链接 https://github.com/zeromicro/go-zero/blob/master/readme-cn.md https://talkgo.org/c/go-zero/23","categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"}]},{"title":"ETCD入门3-RAFT协议","slug":"ETCD/ETCD入门3-RAFT协议","date":"2022-03-13T02:25:38.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2022/03/13/ETCD/ETCD入门3-RAFT协议/","link":"","permalink":"http://xboom.github.io/2022/03/13/ETCD/ETCD%E5%85%A5%E9%97%A83-RAFT%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"一致性是指在集群中的多个节点在状态上达成一致，常见的一致性协议有Raft、Paxos、ZAB 基本规则 任期(Term)：实际上是一个全局的、连续递增的整数。在raft中，每进行一次选举，任期就会+1 节点存在三种状态分别是： Leader节点：可读可写、处理日志复制、保证⼀致性、维护⼼跳 Follower节点：可进行读操作，写操作需要路由到Leader处理；参与投票，⼀个term任期内只能投⼀次; 当触发 election timeout 时，晋升为 candidate ;(注意读请求模式的线性读) Candidate节点：集群的候选者，会发起投票试图当选leader ; 通过 RequestVote 通知其他节点来投票; 集群中任意一个节点都处于这三个状态之一 如上图所示 当etcd节点刚启动的时候，状态机初始化状态为Follower 为了进行Leader 选举控制有两个时间 选举超时时间(election timeout): 每个Follower节点会因为超过选举超时时间还没有收到Leader的心跳而切换程Candidate状态并发起选举。一般为 150ms ~ 300ms，是一个一定范围的随机数(为了保证各节点不是同时发起选举请求) 心跳超时时间(heartbeat timeout): Leader节点向集群中其他Follower节点发送心跳消息的时间间隔 脑裂问题 原本一个集群，被分成了两个集群，同时出现了两个“大脑”，这就是所谓的“脑裂”现象 ETCD是否有可能出现脑裂？ 第一种情况：两边都得到过半票数而选举出Leader的情况，不可能出现 第二种情况：两边都无法得到过半票数而无法选举出Leader的情况，则服务端会返回客户端集群不可用 第三种情况：Leader 与 其他节点通信异常，导致其他节点重新选出 Leader Leader 知道与其他节点通信异常 由于 leader A ⽆法同步数据到多数节点, 造成 client 写请求失败 ; ⽹络分区后 follower c 长时间未收到⼼跳, 则触发选举且当选 ; client 按照策略⼀直尝试跟可用的节点进⾏请求 ; 当⽹络恢复后, leader A B 将降为 follower, 并且强制同步 Leader C 的数据 ; Leader 不知道与还不知道与其他节点通信异常 在 leader-A 里标记 client 请求为 未提交模式 ; follower c 首先触发超时发起选举投票, folower ED 收到更加新的 term 过去的 requestVote, 则回应同意当选 ; leader C 可以接收用户的请求, 由于可以拿到⼤多数的回应, 则可以正常提交数据 ; 当⽹络分区问题解决后, 由于旧集群的Leader A 的term 低于 Leader C , leader A 强制 同步 Leader C的数据; Leader选举 能称为Leader的条件有 当前集群⽆可用 leader ; 触发 election timeout ; term 任期最新 ; log 日志最新 ; 获取多数投票 ; 流程可以查看：http://thesecretlivesofdata.com/raft/ 需要注意的是： 超过集群半数节点应答，才将日志写入WAL中，也是就如果没有等到半数应答就挂了，那么数据就丢了 Follower 收到 Leader 的 复制日志 则直接将日志写入 WAL 中 当Follower收到 Vote Request 也会重置 election timeout 定时器(降低同时出现两个候选人的概率) Term 将持续直到某个节点因为心跳超时而从新发起选举 出现多个候选人都没有称为 Leader，那么选举失败，切换为Follower，等待下一个选举超时 Leader 宕机后，其他节点重新选出新 Leader。等到旧Leader 恢复，会因为收到新Leader心跳 而自己的 Term 落后，切换成 Follower，更新自己的Term，然后重新投出自己的选票 基本流程 将设ETCD集群中有三个节点(A、B、C) 初始化，所有节点起初都是 Follower 状态(Term = 0， 得票数 = 0) 节点 A 由于长时间未收到 Leader 的心跳消息，就会切换成为Candidate 状态并发起选举（节点 A 的选举计时器 （election timer）己被重置） 节点 A 首先会将自己的选票投给自己，并会向集群中其他节点发送选举请求（ Request Vote ）以获取其选票，此时的节点 B 和节点 C 还都是处于 Term=0 的任期之中，且都是 Follower 状态，均未投出 Term=1 任期中的选票，所以节点 B 和节点 C 在接收到节点 A 的选举请求后会将选票投给节点 A，任期变为1。 在节点 A 收到节点 B、 C 的投票之后，其收到了集群中超过半数的选票，所以在 Terrn = 1 的任期中，该集群的 Leader 节点就是节点 A，其他节点将切换成 Follower 状态， 集群中的节点除 了记录当期任期号(currentTerrn)，还会记录在该任期中当前节点的投票结果(VoteFor） 成为 Terrn = 1 任期的 Leader 节点之后，节点 A 会定期向集群中的其他节点发送心跳消息，防止节点 B 和节点 C 中的选举计时器(election timer) 超时而触发新一轮的选举: 当节点 B 和节点 C (Follower) 收到节点 A 的心跳消息之后会重置选举计时器 心跳超时时间（heartbeat timeout）需要远小于选举超时时间(election timeout) Candidate 发起选举前,会先尝试连接集群中的其他节点。如果连接不上，放弃本次选举。这个状态称之为：Prevote 问题1：如果Leader节点频繁宕机，或者选举反复进行，怎么办？ 要求：广播时间 &lt;&lt; 选举超时时间 &lt;&lt; 平均故障间隔时间 广播时间：节点直接发送心跳信息的完整返回时间 hearthbeat timeout：0.5ms~50ms 选举超时时间：election timer：200ms~1s 故障间隔时间：两次故障的平均时间：1个月或更多(保证节点的不要经常出古故障) 这样最大程度保证：不会频繁选举（广播时间 &lt;&lt; 选举超时时间），故障时间最多为200ms~1s（选举超时时间 &lt;&lt; 平均故障间隔时间） 问题2：是不是谁先发起了选举请求，谁就得到了Leader？ 不是，除了看先后顺序，还取决于Candidate节点的日志是不是最新最全的日志，否则拒绝投票，防止出现日志（即数据）丢失的情况 Prevote 当出现网络分区，节点因为选举超时而不断的进行 term 增加，而 leader 会因为遇到更新的 term 而退化为 follower，为了防止 term不断的新增，使用了 Prevote 措施，变为 candidate 的条件 询问其他节点是否有可用 leader ; 可连通绝⼤数节点 ; 日志复制 基本流程 流程动画：http://thesecretlivesofdata.com/raft/#replication 步骤如下： Leader 节点接到 client 请求后（如 set a=10）,将本请求计入本地log Leader 节点向 Follower1 和 Follower2 发送Append Entries消息（set a=10） Follower1 和 Follower2 将此信息计入本地log，并返回给 Leader Leader 将日志信息置为 已提交(Commited),然后状态机处理。 响应 client 请求 向 Follower1 和 Follower2 发送消息，该信息已提交 Follower1 和 Follower2 接到消息后，修改日志状态，交给自己的状态机处理 存在的问题：计入本地log是否是写入WAL，按照之前的流程是没有的，而是在提交之后，那么Follower又是什么时候写入WAL的？ 索引 集群中各个节点都会维护一个本地Log 用于记录更新操作，还会维护 commitlndex 和lastApplied 两个值，它们是本地Log 的索引值 commitlndex 表示的是当前节点已知的、最大的、己提交的日志索引值 lastApplied 表示的是当前节点最后一条被应用到状态机中的日志索引值。 当节点中的 commitlndex 值大于 lastApplied 值时，会将 lastApplied+ 1 ，并将 lastApplied 对应的日志应用到其状态机中。 除此之外，在Leader还需要了解集群中其他Follower 节点的这些信息，而决定下次发送Append Entries 消息中包含哪些日志记录。为此， Leader 节点会维护 nextlndex[]和 matchlndex[]两个数组，这两个数组中记录的都是日志索引值 nextlndex[] 记录了需要发送给每个Follower 节点的下一条日志的索引值 matchlndex[] 记录了己经复制给每个Follower 节点的最大的日志索引值 一个数组不就可以表达了吗？ 例如： Follower 节点中最后一条日志的索引值大于等于该 Follower 节点对应的 nextlndex 值，那么通过Append Entries 消息发送从nextlndex 开始的所有日志。之后， Leader节点会检测该 Follower 节点返回的相应响应， 如果成功则更新相应该Follower 节点对应的 nextlndex 值和matchlndex 值； 如果因为日志不一致而失败，则减少 nextlndex 值重试。 一致性复制 raft 中的每个日志记录都带有 term 和 log index 来唯⼀标识 ; 日志记录具有两个特性 如果两条不同节点的某两个日志记录具有相同的 term 和 index 号，则两条记录⼀定是完全相同的 ; 如果两条不同节点的某两个日志记录具有相同的 term 和 index 号，则两 条记录之前的所有记录也⼀定是完全相同的 ; 以此来判断 leader与 follower 之间的 日志是否相同 上图中：a 和 b 的情况是没有完全收到来自 leader 的 AppendEntries RPC, ⽽ c-f 则是带有不同时期的未提交的日志（有可能是他们当 leader 时产⽣的，但没有提交就 crash了） leader 从来不会覆盖，删除或者修改其日志 ; leader 会初始化⼀个数组 nextIndex[], 该结构对应的值表示本 leader 将给对应 follower 发送的下⼀条日志 index ; 若 follower 对比其前⼀条 log 不⼀致，则会拒绝 leader 发来的请求. 此时 leader 就将其在 nextIndex[] 中的对应值减⼀ ; leader 不断重试直到 follower 比对成功, 然后 follower 接受 AppendEntries RPC, ⼀个个的抛弃所有冲突的日志 ; leader 按照自身日志顺序将日志正常复制给 follower，并不断将 nextIndex[] 对应 值 +1，直到对应值 “追上” 自身日志的 index 为⽌; 为什么要一个一个对比，而不直接找到对应的位置，批量复制？=&gt; 通过数组存储的是索引，但是日志比较是通过 index 与 term 那数组为什么不存 index 与 term? =&gt; 因为本来每个节点的 操作都是不一样的，其他节点无法 异常情况 场景1： 两个Follower 节点不可用，Leader 如何处理请求 leader 通过⼼跳已知 follower 已挂, 则直接返回错误 ; leader 不知节点已挂, 同步数据时得知异常; 触发异常触发超时 场景2： 提交给Leader后，发生了crash leader 本地已记录 未提交日志, 重启后强制同步新 leader 数据 ; leader 还没记录未提交日志就crash, 丢了就丢了; 场景3 复制给 follower-1 后, leader就发⽣了 crash ? leader 发⽣了重启，集群触发新的选举 ; 由于 follower-1 的数据较新, 那么该节点会晋升 leader ; follower-1 会把 uncommited 的 v=3 同步给其他节点 ; follower-1 收到其他节点接收确认后, 进⾏提交日志 ; 场景4： follower 返回确认消息时, leader 发⽣了 crash ? 三个节点的数据已经⼀致, 都为 uncommited v=3 ; 这时谁当 Leader 都可以 ; 新leader当选后, 需要进⾏同步提交通知 ; 参考链接 《ETCD技术内幕》 http://www.xuyasong.com/?p=1706","categories":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/categories/ETCD/"}],"tags":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"}]},{"title":"ETCD入门2-ETCD架构","slug":"ETCD/ETCD入门2-ETCD架构","date":"2022-03-12T09:31:19.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2022/03/12/ETCD/ETCD入门2-ETCD架构/","link":"","permalink":"http://xboom.github.io/2022/03/12/ETCD/ETCD%E5%85%A5%E9%97%A82-ETCD%E6%9E%B6%E6%9E%84/","excerpt":"","text":"ETCD架构 Client：客户端库，封装对KVServer、Cluster、Auth、Lease、Watch的API API： Raft HTTP：主要用于ETCD集群各节点通信，基于Raft进行数据同步、选主 gRPC API: v3 使用 grpc实现服务端与客户端交互，而 v2 使用 http1.X 实现交互 HTTP: ETCD 支持 HTTP 对外交互接口(v2 与 v3 不能混用) Raft：选主、日志复制、ReadIndex等为保障节点数据一致性的具体算法实现 功能逻辑 KVServer模块 Auth鉴权模块 Lease租约模块：主动上报的检测机制 Compactor压缩模块 MVCC模块：treeIndex + boltdb，多版本并发控制，保存一组KV的多个历史版本 Quota: 配额模块，检查下当前 etcd db 大小加上你请求的 key-value 大小之和是否超过了配额(quota-backend-bytes，默认存储为2G, 最大配额为8G) 存储 WAL模块：Write Ahead Log，预写式日志，所有修改在提交前都要写入log文件，保障etcd崩溃了不丢失数据（redo/undo） Snapshot：防止WAL文件过多而设置的快照，存储etcd数据状态，某个快照前的日志会变为无效数据。（如每处理1w条日志就进行一次snapshot） boltdb模块：嵌入式 K/V 数据库，B+树，保存集群元数据与用户写入的数据 Quota 模块 如果超过配额，会产生一个告警(Alarm)请求，告警类型是 NO SPACE，并通过 Raft 日志同步给其它节点，告知 db 无空间了，并将告警持久化存储到 db 中。 最终，无论是 API 层 gRPC 模块还是负责将 Raft 侧已提交的日志条目应用到状态机的 Apply 模块，都拒绝写入，集群只读。 常见的 “etcdserver: mvcc: database space exceeded&quot; 错误就是因为Quota 模块检测到 db 大小超限导致的。 一方面默认 db 配额仅为 2G，当你的业务数据、写入 QPS、Kubernetes 集群规模增大后，你的 etcd db 大小就可能会超过 2G。 另一方面 etcd 是个 MVCC 数据库，保存了 key 的历史版本，当你未配置压缩策略的时候，随着数据不断写入，db 大小会不断增大，导致超限。 解决办法 1）首先当然是调大配额，etcd 社区建议不超过 8G。 如果填的是个小于 0 的数，就会禁用配额功能，这可能会让db 大小处于失控，导致性能下降，不建议你禁用配额。 2）检查 etcd 的压缩（compact）配置是否开启、配置是否合理。 压缩时只会给旧版本Key打上空闲（Free）标记，后续新的数据写入的时候可复用这块空间，db大小并不会减小。 如果需要回收空间，减少 db 大小，得使用碎片整理（defrag）， 它会遍历旧的 db 文件数据，写入到一个新的 db 文件。但是它对服务性能有较大影响，不建议你在生产集群频繁使用。 调整后还需要手动发送一个取消告警（etcdctl alarm disarm）的命令，以消除所有告警，否则因为告警的存在，集群还是无法写入。 写请求处理处理 图中流程如下： 客户端会与ETCD每个节点都建立连接，读写grpc请求则通过负载均衡算法随机发送到其中一个节点 etcd 节点收到请求后经过 gRPC 拦截器 进入逻辑处理流程(写入数据的etcd是leader节点，如果是非leader节点则路由到etcd leader节点去) Quota 进行配额校验，完成后转发给KVServer KVServer 模块生成并提交提案 打包提案：将 put 写请求内容打包成一个提案消息 请求限速、检查：在提交提案前，进行如下的一系列检查和限速 限速：如果 Raft 模块已提交的日志索引（committed index）比已应用到状态机的日志索引（applied index）超过了 5000，那么它就返回一个&quot;etcdserver: too many requests&quot;错误给 client。 鉴权：尝试去获取请求中的鉴权信息，若使用了密码鉴权、请求中携带了 token，如果 token 无效，则返回&quot;auth: invalid auth token&quot;错误给 client。 大包检查：检查写入的包大小是否超过默认的 1.5MB， 如果超过则返回&quot;etcdserver: request is too large&quot;错误给 client 通过检查后会生成一个唯一的 ID，将此请求关联到一个对应的消息通知 channel（用于接收结果，golang 响应处理方式），向Raft模块发起(Propose)一个提案(Proposal) KVServer 模块会等待此 put 请求，等待写入结果通过消息通知 channel 返回或者超时。etcd 默认超时时间是 7 秒（5 秒磁盘 IO 延时 +2*1 秒竞选超时时间），如果一个请求超时未返回结果，则可能会出现你熟悉的 etcdserver: request timed out 错误 Raft模块处理提案 raft模块会首先保存到raftLog的unstable存储部分(raft 仅仅是普通日志作用，unstable 表示存在内存中) 通过raft协议与集群中其他etcd节点进行交互应答 集群中其他节点向leader节点应答接收这条日志数据 当超过集群半数(N + 1) / 2节点应答这条日志数据时，将这条日志写入到WAL模块中 通知最上层的etcd server该日志已经commit etcd server调用applierV3模块将日志写入持久化存储中 首先生成 key 对应的 revision 存入treeindex中 然后通过revistion - val 持久话到 boltdb中(B+树结构) 超过配额怎么办？ 检查 etcd 的压缩（compact）配置是否开启、配置是否合理。 压缩时只会给旧版本Key打上空闲（Free）标记，后续新的数据写入的时候可复用这块空间，db大小并不会减小。 如果需要回收空间，减少 db 大小，得使用碎片整理（defrag）， 它会遍历旧的 db 文件数据，写入到一个新的 db 文件。但是它对服务性能有较大影响，不建议你在生产集群频繁使用。 调整后还需手动发送取消告警的命令etcdctl alarm disarm，以消除所有告警，否则因为告警的存在，集群还是无法写入 读请求处理流程 串行读与线性读 etcd 为了保证服务高可用，生产环境一般部署多个节点，多节点之间的数据由于延迟等关系可能会存在不一致的情况 不一致原因：虽然是强一致性，但 Leader 收到大部分节点响应就回复客户端，可能存在leader与未回复follower不一致的情况 根据业务场景对数据一致性差异的接受程度，etcd 中有两种读模式。 串行 (Serializable) 读：直接读状态机数据返回、无需通过 Raft 协议与集群进行交互，它具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。(可能出现当前KV不是最新的情况) 线性读：需要经过 Raft 协议模块，反应的是集群共识，因此在延时和吞吐量上相比串行读略差一点，适用于对数据一致性要求高的场景 由于串行读是直接从当前节点获取&quot;最新&quot;数据，可能出现不是真的最新，线性读通过先与集群达成共识再返回结果 线性读流程具体流程如下： 当 Follower 收到线性读请求时，首先执行 Read Index 请求， 从 Leader 获取集群最新的已提交的日志索引 (committed index) Leader 收到 Read Index 请求时，为防止脑裂等异常场景，会向 Follower 节点发送心跳确认，一半以上节点确认 Leader 身份后才能将已提交的索引 (committed index) 返回给请求节点。 Follower 节点拿到 Read index 后会和状态机的 applied index进行比较 如果 Read index 大于 applied index 则会等待，直到状态机已应用索引 (applied index) 大于等于 Leader 的已提交索引时 (committed Index)才会去通知读请求，数据已赶上 Leader可以去状态机中访问数据了 否则直接开始读取 readIndex 需要请求 leader，那为什么不直接让 leader 返回读请求的结果 主要是性能因素，如果将所有读请求都转发到 Leader，会导致 Leader 负载升高，内存、cpu、网络带宽资源都很容易耗尽。特别是expensive request场景，会让 Leader 节点性能会急剧下降。read index 机制的引入，使得每个follower节点都可以处理读请求，极大扩展提升了写性能。 读取流程 从 treeIndex 中获取 key 的版本号 再以版本号作为 boltdb 的 key，从 boltdb 中获取其 value 信息 etcd 出于数据一致性、性能等考虑，在访问 boltdb 前，首先会从一个内存读事务 buffer 中，二分查找你要访问 key 是否在 buffer 里面，若命中则直接返回。 若 buffer 未命中，此时就真正需要向 boltdb 模块查询数据了。boltdb 使用 B+ tree 来组织用户的 key-value 数据，获取 bucket key 对象后，通过 boltdb 的游标 Cursor 可快速在 B+ tree 找到 key hello 对应的 value 数据，返回给 client。 boltdb 通过 bucket 隔离集群元数据与用户数据 参考链接 http://www.jcxioo.com/2021/06/14/01_Kubernetes/etcd1 / https://www.codedump.info/post/20181125-etcd-server/#wal https://www.lixueduan.com/post/etcd/07-read-process/","categories":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/categories/ETCD/"}],"tags":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"}]},{"title":"ETCD入门1-ETCD概述","slug":"ETCD/ETCD入门1-ETCD概述","date":"2022-03-12T07:33:07.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2022/03/12/ETCD/ETCD入门1-ETCD概述/","link":"","permalink":"http://xboom.github.io/2022/03/12/ETCD/ETCD%E5%85%A5%E9%97%A81-ETCD%E6%A6%82%E8%BF%B0/","excerpt":"","text":"etcd是一个可靠的分布式KV存储系统 黄色：关联特性 红色：核心原理 特点 简单：提供定义明确且面向用户的API 安全：支持SSL证书验证 性能：基准压测支持1w+/sec写入 可靠：采用Raft协议保证分布式系统数据的可用性和一致性。 功能特性 KV存储、查询功能：支持精准查询、range操作、ttl机制、key版本等 强一致性：采用raft协议保证强一致性 高可用性：提供集群和leader选举机制 SSL认证机制 Lease 机制：即租约机制（TTL，Time To Live），Etcd 可以为存储的 Key-Value 对设置租约，当租约到期，Key-Value 将失效删除；同时也支持续约(Keepalive)，通过客户端可以在租约到期之前续约，以避免 Key-Value 对过期失效。 Lease 机制可以保证分布式锁的安全性，为锁对应的 Key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。 Revision 机制：每个 Key 带有一个 Revision 号，每进行一次事务便加一，因此它是全局唯一的，如初始值为 0，进行一次 put(key, value)，Key 的 Revision 变为 1，同样的操作，再进行一次，Revision 变为 2；换成 key1 进行 put(key1, value) 操作，Revision 将变为 3； 通过 Revision 的大小就可以知道写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免 “羊群效应” （也称“惊群效应”），实现公平锁。 Prefix 机制：即前缀机制，也称目录机制，例如，一个名为 /mylock 的锁，两个争抢它的客户端进行写操作，实际写入的 Key 分别为：key1=&quot;/mylock/UUID1&quot;,key2=&quot;/mylock/UUID2&quot;，其中，UUID 表示全局唯一的 ID，确保两个 Key 的唯一性。很显然，写操作都会成功，但返回的 Revision 不一样， 如何判断谁获得了锁呢？ 通过前缀“/mylock” 查询，返回包含两个 Key-Value 对的 Key-Value 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁，如果抢锁失败，则等待锁释放（对应的 Key 被删除或者租约过期），然后再判断自己是否可以获得锁。 Watch 机制：即监听机制，Watch 机制支持监听某个固定的 Key，也支持监听一个范围（前缀机制），当被监听的 Key 或范围发生变化，客户端将收到通知； 在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 Key-Value 列表获得 Revision 比自己小且相差最小的 Key（称为 Pre-Key），对 Pre-Key 进行监听，因为只有它释放锁，自己才能获得锁，如果监听到 Pre-Key 的 DELETE 事件，则说明 Pre-Key 已经释放，自己已经持有锁。 获取Pre-Key 的时候就已经释放了怎么办？Watch需要带上版本 应用场景 分布式锁 分布式锁 因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁 V3版本接口: 123456//用来新建一个mutexfunc NewMutex(s *Session, pfx string) *Mutex //它会阻塞直到拿到了锁，并且支持通过context来取消获取锁func (m *Mutex) Lock(ctx context.Context) error//解锁func (m *Mutex) Unlock(ctx context.Context) error 锁原理详解 TODO leader选举组件 利用分布式锁，实现Leader竞选。而 leader-follower模式来保证有状态服务的高可用 注册与发现 本质是：服务端通过(Lessor + keepalive) 进行服务注册与保活，客户端通过(Watch)做到KV实时变化实时感知 服务注册与发现 服务端注册信息到注册中心ETCD中，并通过心跳实时检测服务端(lessor + keepalive)。客户端从注册中心获取服务端信息才能建立链接(Watch) k8s 中已经有 service，是否还需要使用到注册中心？ 使用grpc构建k8s 服务通信，由于 grpc 使用的是长连接通信，导致消息在多副本情况下只会发往同一个副本 如何注册服务端信息的 对应 Key = &lt;标识&gt;.&lt;服务名称&gt;.&lt;版本&gt;. , Val =&lt;k=v&gt;&amp;&lt;k=v&gt; Key: 保存通信信息，当服务端出现变化的时候。客户端即使不解析 Val 也能收到变化 Val: 可以不定义或者定义每个服务(注意副本)特有信息 集群监控 通过 etcd 来进行监控实现起来非常简单并且实时性强。 前面几个场景已经提到 Watcher 机制，当某个节点消失或有变动时，Watcher 会第一时间发现并告知用户。 节点可以设置TTL key，比如每隔 30s 发送一次心跳使代表该机器存活的节点继续存在，否则节点消失。 分布式通知与协调 通过 etcd 进行低耦合的心跳检测。检测和被检测系统通过 etcd 进行关联降低系统的耦合性。 通过 etcd 完成系统调度。某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了 etcd 上某些目录节点的状态，而 etcd 就把这些变化通知给注册了 Watcher 的推送系统客户端，推送系统再作出相应的推送任务。 通过 etcd 完成工作汇报。大部分类似的任务分发系统，子任务启动后，到 etcd 来注册一个临时工作目录，并且定时将自己的进度进行汇报（将进度写入到这个临时目录），这样任务管理者就能够实时知道任务进度 消息订阅和发布 消息发布与订阅 配置中心需要考虑分权问题，每个服务对于同一个数据库的访问权限也是不一致的。所以配置信息也有可能不一致。当然不影响进行配置注册 配置中心 整个平台不同服务包含很多相同配置信息 ，例如: 数据库地址、链路追踪等。可以将这些配置信息注册到注册中心实现动态更新 这里还需要考虑分权问题：即每个服务能够访问的权限、区域不一致，那么每服务甚至副本配置都不一致 负载均衡 在k8s中一般针对多副本场景，由于 grpc 使用的是长连接，k8s service 无法满足负载场景，需要客户端通过获取副本信息实现负载均衡 分布式队列 分布式队列的常规用法与场景五中所描述的分布式锁的控制时序用法类似，即创建一个先进先出的队列，保证顺序。 另一种比较有意思的实现是在保证队列达到某个条件时再统一按顺序执行。这种方法的实现可以在 /queue 这个目录中另外建立一个 /queue/condition 节点。 condition 可以表示队列大小。比如一个大的任务需要很多小任务就绪的情况下才能执行，每次有一个小任务就绪，就给这个 condition 数字加 1，直到达到大任务规定的数字，再开始执行队列里的一系列小任务，最终执行大任务。 condition 可以表示某个任务在不在队列。这个任务可以是所有排序任务的首个执行程序，也可以是拓扑结构中没有依赖的点。通常，必须执行这些任务后才能执行队列中的其他任务。 condition 还可以表示其它的一类开始执行任务的通知。可以由控制程序指定，当 condition 出现变化时，开始执行队列任务。 参考链接 http://www.jcxioo.com/2021/06/14/01_Kubernetes/etcd1 / https://www.infoq.cn/article/etcd-interpretation-application-scenario-implement-principle","categories":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/categories/ETCD/"}],"tags":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"}]},{"title":"架构6-分布式缓存","slug":"FrameWork/架构6-分布式缓存","date":"2022-03-03T16:30:33.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2022/03/04/FrameWork/架构6-分布式缓存/","link":"","permalink":"http://xboom.github.io/2022/03/04/FrameWork/%E6%9E%B6%E6%9E%846-%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/","excerpt":"","text":"缓存知识点如下： 首先，要熟练掌握缓存的基础知识，了解缓存常用的分类、读写模式，熟悉缓存的七大经典问题及解决应对之策，同时要从缓存组件的访问协议、Client 入手，熟练掌握如何访问各种缓存组件，如 Memcached、Redis、Pika 等。 其次，要尽可能深入理解缓存组件的实现方案、设计原理，了解缓存的各种特性、优势和不足，这样在缓存数据与预期不一致时，能够快速定位并解决问题。 再次，还要多了解线上大中型系统是如何对缓存进行架构设计的。线上系统，业务功能丰富多变，跨域部署环境复杂，而且热点频发，用户习惯迥异。因此，缓存系统在设计之初就要尽量进行良好设计，规划好如何进行Hash及分布、如何保障数据的一致性、如何进行扩容和缩容。当然，缓存体系也需要伴随业务发展持续演进，这就需要对缓存体系进行持续的状态监控、异常报警、故障演练，以确保在故障发生时能及时进行人肉或自动化运维处理，并根据线上状况不断进行优化和改进。 最后，了解缓存在各种场景下的最佳实践，理解这些最佳实践背后的 Tradeoff，做到知其然知其所以然，以便在实际工作中能举一反三，把知识和经验更好的应用到工作实践中来 缓存的原理 缓存的基本思想、缓存的优点、缓存的代价 缓存的定义 缓存最初的含义，是指用于加速 CPU 数据交换的 RAM，即随机存取存储器，通常这种存储器使用更昂贵但快速的静态 RAM（SRAM）技术，用以对 DRAM进 行加速。这是一个狭义缓存的定义。 广义缓存的定义则更宽泛，任何可以用于数据高速交换的存储介质都是缓存，可以是硬件也可以是软件 缓存的基本思想 缓存构建的基本思想是利用时间局限性原理，通过空间换时间来达到加速数据获取的目的，同时由于缓存空间的成本较高，在实际设计架构中还要考虑访问延迟和成本的权衡问题。这里面有 3 个关键点。 一是时间局限性原理，即被获取过一次的数据在未来会被多次引用，比如一条微博被一个人感兴趣并阅读后，它大概率还会被更多人阅读，当然如果变成热门微博后，会被数以百万/千万计算的更多用户查看。 二是以空间换时间，因为原始数据获取太慢，所以我们开辟一块高速独立空间，提供高效访问，来达到数据获取加速的目的。 三是性能成本 Tradeoff，构建系统时希望系统的访问性能越高越好，访问延迟越低小越好。但维持相同数据规模的存储及访问，性能越高延迟越小，成本也会越高，所以在系统架构设计时，你需要在系统性能和开发运行成本之间做取舍。比如左边这张图，相同成本的容量，SSD 硬盘容量会比内存大 10～30 倍以上，但读写延迟却高 50～100 倍。 通过缓存可以 提升访问性能、降低网络拥堵、减轻服务负载、增强可扩展性 但同时 增加系统复杂度、增加成本以及 缓存一致性 缓存读写模式 业务系统读写缓存有 3 种模式： Cache Aside（旁路缓存） Read/Write Through（读写穿透） Write Behind Caching（异步缓存写入） Cache Aside Cache Aside 模式中，业务应用方对于 写: 更新 DB 后，直接将 key 从 cache 中删除，然后由 DB 驱动缓存数据的更新； 读: 先读 cache，如果 cache 没有，则读 DB，同时将从 DB 中读取的数据回写到 cache 写的时候为什么在更新完数据库之后将数据更新到缓存中？ 适用：对数据一致性要求高或者数据缓存更新复杂的业务 Read/Write Through Cache Aside 模式，业务应用需同时维护 cache 和 DB 两个数据存储方，过于繁琐，于是就有了 Read/Write Through 模式 在Read/Write Through模式下，业务应用只关注一个存储服务即可，业务方的读写 cache 和 DB 的操作，都由存储服务代理。存储服务处理 写：会首先查 cache，如果数据在 cache 中不存在，则只更新 DB，如果数据在 cache 中存在，则先更新 cache，然后更新 DB。 读：如果命中 cache 直接返回，否则先从 DB 加载，更新 cache 后返回响应 为什么在写操作先更新缓存，如果更新完缓存，但是数据库更新失败怎么办？ 为什么Cache Aside不能使用存储服务代理？ 适用：同一业务区分缓存(读写频率高的更新缓存并写入数据库，而操作频率低的直接写入数据库并不写入缓存) Write Behind Caching Write Behind Caching 模式与 Read/Write Through 模式类似，也由数据存储服务来管理 cache 和 DB 的读写。不同点是，数据更新时，Read/write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是改为异步批量的方式来更新 DB。 适用：变化频率特变高，对一致性要求不太高的业务(比如计数业务的点赞功能，执行异步批量写入) 缓存的分类 按宿主层次分类 按宿主层次分类的话，缓存一般可以分为本地 Cache、进程间 Cache 和远程 Cache。 本地 Cache 是指业务进程内的缓存，这类缓存由于在业务系统进程内，所以读写性能超高且无任何网络开销，但不足是会随着业务系统重启而丢失。 进程间 Cache 是本机独立运行的缓存，这类缓存读写性能较高，不会随着业务系统重启丢数据，并且可以大幅减少网络开销，但不足是业务系统和缓存都在相同宿主机，运维复杂，且存在资源竞争。 远程 Cache 是指跨机器部署的缓存，这类缓存因为独立设备部署，容量大且易扩展，在互联网企业使用最广泛。不过远程缓存需要跨机访问，在高读写压力下，带宽容易成为瓶颈。 本地 Cache 的缓存组件有 Ehcache、Guava Cache 等，开发者自己也可以用 Map、Set 等轻松构建一个自己专用的本地 Cache。进程间 Cache 和远程 Cache 的缓存组件相同，只是部署位置的差异罢了，这类缓存组件有 Memcached、Redis、Pika 等。 按存储介质分类 内存型缓存将数据存储在内存，读写性能很高，但缓存系统重启或 Crash 后，内存数据会丢失。 持久化型缓存将数据存储到 SSD/Fusion-IO 硬盘中，相同成本下，这种缓存的容量会比内存型缓存大 1 个数量级以上，而且数据会持久化落地，重启不丢失，但读写性能相对低 1～2 个数量级。Memcached 是典型的内存型缓存，而 Pika 以及其他基于 RocksDB 开发的缓存组件等则属于持久化型缓存。 缓存架构设计 根据缓存组件特点确定缓存组件后，需要以下三点来考虑缓存架构设计 分布式算法：取模还是一致性 Hash 进行分布 取模：方案简单，每个 key 只会存在确定的缓存节点 一致性Hash：方案相对复杂，但失效节点的数据访问均衡分散到其他正常存活的节点，从而保证系统稳定性 分布式读写访问如何实施：由缓存 Client 直接进行 Hash 分布定位读写 Client 直接读写：读写性能最佳，但需要 Client 感知分布策略 Proxy代理访问：业务开发友好，但需要考虑性能瓶颈 缓存的水平迁移:如果待缓存的数据量增长过快，会导致大量缓存数据被剔除，缓存命中率会下降，数据访问性能会随之降低，这样就需要将数据从缓存节点进行动态拆分，把部分数据水平迁移到其他缓存节点 部署与运维 设计完毕缓存的分布策略后，接下来就要考虑缓存的架构部署及运维管理了。架构部署主要考虑如何对缓存进行分池、分层、分 IDC，以及是否需要进行异构处理。 核心的、高并发访问的不同数据，需要分别分拆到独立的缓存池中，进行分别访问，避免相互影响；访问量较小、非核心的业务数据，则可以混存。 对海量数据、访问超过 10～100万 级的业务数据，要考虑分层访问，并且要分摊访问量，避免缓存过载。 如果业务系统需要多 IDC 部署甚至异地多活，则需要对缓存体系也进行多 IDC 部署，要考虑如何跨 IDC 对缓存数据进行更新，可以采用直接跨 IDC 读写，也可以采用 DataBus 配合队列机进行不同 IDC 的消息同步，然后由消息处理机进行缓存更新，还可以由各个 IDC 的 DB Trigger 进行缓存更新。 某些极端场景下，还需要把多种缓存组件进行组合使用，通过缓存异构达到最佳读写性能。 站在系统层面，要想更好得管理缓存，还要考虑缓存的服务化，考虑缓存体系如何更好得进行集群管理、监控运维等。 常见考量点 读写方式 首先是 value 的读写方式。是全部整体读写，还是只部分读写及变更？是否需要内部计算？比如，用户粉丝数，很多普通用户的粉丝有几千到几万，而大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表肯定不能采用整体读写的方式，只能部分获取。另外在判断某用户是否关注了另外一个用户时，也不需要拉取该用户的全部关注列表，直接在关注列表上进行检查判断，然后返回 True/False 或 0/1 的方式更为高效 参考链接 https://learn.lianglianglee.com/专栏/300分钟吃透分布式缓存-完","categories":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/categories/FrameWork/"}],"tags":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/tags/FrameWork/"}]},{"title":"ETCD入门6-多版本控制","slug":"ETCD/ETCD入门6-多版本控制","date":"2022-02-22T16:41:29.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2022/02/23/ETCD/ETCD入门6-多版本控制/","link":"","permalink":"http://xboom.github.io/2022/02/23/ETCD/ETCD%E5%85%A5%E9%97%A86-%E5%A4%9A%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/","excerpt":"","text":"悲观锁：悲观得认为并发事务会冲突，所以要先拿锁，拿到锁的作修改操作 读写锁（可多读，写时互斥）、互斥锁（线程互斥）等 控制粒度太大，高并发下大量事务会被阻塞 乐观锁：乐观得认为数据不会冲突，但发生冲突时要能检测到 逻辑时钟（时间戳排序） MVCC MVCC（Multiversion Concurrency Control）多版本并发控制机制，保存一个数据的多个历史版本以解决事务管理中数据隔离的问题，被主流数据库采用，包括Oracle, MySQL等 写一个数据项时，不是简单的用新值覆盖旧值，而是为这一项添加一个新版本的数据； 读一个数据项时，先确定版本，在根据版本找到对应的值； 这样保证了读写隔离，无需锁协调。读不会阻塞，适合“读多写少”的etcd 可以保存一个Key-Value数据的多个历史版本 每修改（新增/删除）一次都会生成一个新的数据记录 指定版本号读取数据时，实际访问的是版本号生成的那个时间点的快照 linux中的RCU功能？ 文中涉及的key表示用户输入的键值，K(or Key)表示boltdb持久化存储时的键值 treeIndex：基于B-tree实现的key的索引，保存用户key与版本号revision的映射关系 Backend：作持久化KV操作，目前支持boltdb ReadTx 读事务接口 BatchTx 写事务接口 Buffer 缓存 boltdb：基于B+ Tree的支持事务的KV嵌入式数据库 K：revision V：{用户的kv，版本号，lease信息等} etcd中的MVCC存储分为两部分： 内存中保存的key-revision映射，用于快速点查及范围查询 磁盘中保存的不同版本的真实数据 数据结构 B Tree 树内节点都存数据 叶子节点都在同一层且无相邻指针 度：每个节点的子节点的个数，“胖瘦程度” B+ Tree 数据只在叶子节点 所有叶子节点增加了一个链指针 revision 12345// &#123;txID, 0&#125;, &#123;txID, 1&#125;, &#123;txID, 2&#125;...type revision struct &#123; main int64 // 事务ID，逻辑时间戳，全局递增不重复 sub int64 // 当前事务内不同的修改操作的编号（put/del），从0开始递增&#125; 每一次事务中，每一个修改操作所绑定的 revision 一次为{txID, 0}, {txID, 1}… keyIndex 12345678910111213 type keyIndex struct &#123; key []byte // 用户的key modified revision // 最后一次修改key时的etcd版本号 generations []generation &#125;// generation保存了一个key若干代的版本号，每代都包含对key多次修改的版本号列表// 创建时第0代，删除后生成第1代...type generation struct &#123; ver int64 // 表示key的修改次数 created revision // 创建时的第一个版本号 revs []revision // 每次修改key时的revision追加进此切片&#125; treeIndex 树状索引，内存中维护的B树，加速查询key 树的每个节点都是一个keyIndex，其实现了Item接口Less方法，就可以通过给定的key快速查找对应的keyIndex 123456789// github.com/google/btree/btree.gotype Item interface &#123; Less(than Item) bool&#125;// github.com/etcd/mvcc/key_index.gofunc (ki *keyIndex) Less(b btree.Item) bool &#123; return bytes.Compare(ki.key, b.(*keyIndex).key) == -1&#125; Backend/boltdb K：版本号，如{2,0} V：mvccpb.KeyValue 包含用户的kv等信息 12345678type mvccpb.KeyValue struct &#123; Key []byte CreateRevision int64 // 创建时的版本号 ModRevision int64 // 最后一次修改时的版本号 Version int64 // 此key修改的次数 = keyIndex.generations[i].ver + 1 Value []byte // 用户数据 Lease int64 // 租约&#125; 流程 跟新key etcdctl put hello world 发起一个写事务 根据key从treeIndex B树中查对应的keyIndex信息 若查到为空，则全局版本号自增，默认是1，生成revision{2,0} 填充mvccpb.KeyValue结构体 根据带版本号的key通过Backend 的写事务接口batchTx将 key{2,0},value 为 mvccpb.KeyValue 保存到缓存中 写事务会在buffer中堆积，默认写事务大于1万才会由Backend异步执行持久化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (tw *storeTxnWrite) put(key, value []byte, leaseID lease.LeaseID) &#123; rev := tw.beginRev + 1 c := rev oldLease := lease.NoLease // 如果这个key已存在，则时候用它之前创建的 _, created, ver, err := tw.s.kvindex.Get(key, rev) if err == nil &#123; c = created.main oldLease = tw.s.le.GetLease(lease.LeaseItem&#123;Key: string(key)&#125;) &#125; tw.trace.Step(\"get key's previous created_revision and leaseID\") ibytes := newRevBytes() idxRev := revision&#123;main: rev, sub: int64(len(tw.changes))&#125; revToBytes(idxRev, ibytes) ver = ver + 1 kv := mvccpb.KeyValue&#123; Key: key, Value: value, CreateRevision: c, ModRevision: rev, Version: ver, Lease: int64(leaseID), &#125; d, err := kv.Marshal() if err != nil &#123; if tw.storeTxnRead.s.lg != nil &#123; tw.storeTxnRead.s.lg.Fatal( \"failed to marshal mvccpb.KeyValue\", zap.Error(err), ) &#125; else &#123; plog.Fatalf(\"cannot marshal event: %v\", err) &#125; &#125; tw.trace.Step(\"marshal mvccpb.KeyValue\") // 以revision为key，kv为value存入 tw.tx.UnsafeSeqPut(keyBucketName, ibytes, d) tw.s.kvindex.Put(key, idxRev) tw.changes = append(tw.changes, kv) tw.trace.Step(\"store kv pair into bolt db\") ...... func (t *batchTx) unsafePut(bucketName []byte, key []byte, value []byte, seq bool) &#123; ... &#125; lazy delete 延期删除 基于更新key的流程，在生成的新版本号后追加一个标志位t generation切片中追加一个空的，使得查询比当前版本号更大时拿到的是空 bolt mvccpb.KeyValue 结构体清空，只有key Watch会根据key上的标志位t生成Delete事件 重启时，可根据boltkey 上的标志位t来重建treeIndex内存树 真正的删除是通过压缩（compactor）异步完成 读取key etcdctl get hello 发起一个读事务 根据key从treeIndex B树中获取版本号，默认读最新，匹配有效generation后返回切片最后一个版本号给读事务 根据带版本号的key通过Backend 的ConcurrentReadTx 并发读接口去buffer缓存中查，未命中则再去boltdb找 参考链接 1.http://www.jcxioo.com/2021/07/22/01_Kubernetes/etcd mvcc/","categories":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/categories/ETCD/"}],"tags":[{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"}]},{"title":"k8s入门5-容器网络之网络栈","slug":"K8S/k8s入门5-容器网络","date":"2022-02-20T16:35:18.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2022/02/21/K8S/k8s入门5-容器网络/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A85-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/","excerpt":"","text":"容器网络 一个 Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的 网络栈包括：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。这些要素就构成了一个进程发起和响应网络请求的基本环境 回环设备是指拿一个大的镜像文件，如xxx.iso或xxx.img等，在此文件内建立一个文件系统，此文件就像一个新的磁盘或光盘设备一样使用。回环可以理解成回复重用，在已有设备上建立文件来模拟物理块设备 一个容器可以声明直接使用宿主机的网络栈（–net=host），即不开启 Network Namespace 1$ docker run –d –net=host --name nginx-host nginx 在这种情况下，容器启动后，直接监听的就是宿主机的 80 端口 优点：可以为容器提供良好的网络性能，不需要进行网络转发操作 缺点：不可避免地引入共享网络资源的问题，比如端口冲突 在大多数情况下，还是希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口 问题1：这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？ 在 Linux 中，能够起到虚拟交换机作用的网络设备是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上 Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信 问题2：如果将容器连接到网桥上? 答案是使用Veth Pair，它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里，使得 Veth Pair 被用作连接不同 Network Namespace 的“网线” 容器里 eth0 的网卡，正是一个 Veth Pair 设备在容器里的这一端 通过 route 命令查看 nginx-1 容器的路由表，可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则） 再查看Veth Pair 设备的另一端(宿主机的网络信息)，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth5d20cf2 Mac 无法查看，可以通过上面的host模型进行查看 1docker run -d --net=host --name nginx-host nginx 通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上 如果再在这台宿主机上启动另外一个容器，将会发现一个新的名字 veth7e63610 也插在docker0网桥上 如果在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址（172.17.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的，这是因为： nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机 同一个宿主机上的不同容器通过docker0网桥进行通信 要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址 ARP(Address Resolution Protocol)，是通过三层的 IP 地址找到对应的二层 MAC 地址的协议。 这个 eth0 网卡是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上，变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包，全部交给对应的网桥。 docker0 在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器。 有了目的 MAC 地址，nginx-1 容器的 eth0 网卡就 ping 包发出去。而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth5d20cf2 虚拟网卡上，然后直接流入到了 docker0 网桥里。 docker0 继续扮演二层交换机的角色。docker0 网桥根据数据包的目的 MAC 地址，在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：veth7e63610，然后把数据包发往这个端口。而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。 nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。 在实际的数据传递时，上述数据的传递过程在网络协议栈的不同层次，都有 Linux 内核 Netfilter 参与其中。通过打开 iptables 的 TRACE 功能查看到数据包的传输过程，在 系统日志 /var/log/syslog 或者 var/log/messages 里看到数据包传输的日志，具体方法如下所示 1234# 在宿主机上执行$ iptables -t raw -A OUTPUT -p icmp -j TRACE$ iptables -t raw -A PREROUTING -p icmp -j TRACE docker0 网桥的工作方式可以理解为，在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换 在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里 当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3， 请求数据包首先经过 docker0 网桥出现在宿主机上。 然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理 这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。 当遇到容器连不通“外网”的时候，先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常 问题4：如果在另外一台宿主机（比如：10.168.0.3）上，也有一个 Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢？ 在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了 创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上 构建这种容器网络的核心在于：需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络，被称为：Overlay Network（覆盖网络） 容器跨主机网络 Flannel 为了解决这个容器“跨主通信”的问题，出现了那么多的容器网络方案。Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现 Flannel 支持三种后端实现，也代表了三种容器跨主网络的主流实现方法 VXLAN host-gw UDP UDP 假设有两台宿主机： 宿主机 Node1 上有容器 container-1，它的 IP 地址是 100.96.1.2，docker0 网桥的地址是：100.96.1.1/24 宿主机 Node2 上有容器 container-2，它的 IP 地址是 100.96.2.3，docker0 网桥的地址是：100.96.2.1/24 第一步：container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。 第二步：由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。 第三步：这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示： 123456# 在Node 1上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2 可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。 flannel0 的设备类型是一个 TUN 设备，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能是在操作系统内核和用户应用程序之间传递 IP 包 当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。 反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向 当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。 那么 flannelId 又是如何知道这个IP地址对应的容器运行在Node 2上面呢？ 在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示： 1234$ etcdctl ls /coreos.com/network/subnets/coreos.com/network/subnets/100.96.1.0-24/coreos.com/network/subnets/100.96.2.0-24/coreos.com/network/subnets/100.96.3.0-24 所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3 12$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24&#123;\"PublicIP\":\"10.168.0.3\"&#125; 而对于 flanneld 来说，只要 Node 1 和 Node 2 是互通的，那么 flanneld 作为 Node 1 上的一个普通进程，就一定可以通过上述 IP 地址（10.168.0.3）访问到 Node 2，这没有任何问题 **flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。**这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。 当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。 通过这样一个普通的、宿主机之间的 UDP 通信，一个 UDP 包就从 Node 1 到达了 Node 2。而 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包。 而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。根据我前面讲解的 TUN 设备的原理，这正是一个从用户态向内核态的流动方向（Flannel 进程向 TUN 设备发送数据包），所以 Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示： 123456# 在Node 2上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.3 由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。 接下来的流程，docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里。而 container-2 返回给 container-1 的数据包，则会经过与上述过程完全相反的路径回到 container-1 中。 需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可： 12$ FLANNEL_SUBNET=100.96.1.1/24$ dockerd --bip=$FLANNEL_SUBNET ... 以上，就是基于 Flannel UDP 模式的跨主通信的基本原理了，如下所示。 可以看到，Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。 上述 UDP 模式有严重的性能问题，所以已经被废弃了。 相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示： 可以看到： 第一次，用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态； 第二次，IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程； 第三次，flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。 此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。这也正是造成 Flannel UDP 模式性能不好的主要原因 VXLAN 即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network） VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。 而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。 而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。 上述基于 VTEP 设备进行“隧道”通信的流程，如下所示： 可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。那么，与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。也就是说，来到了“隧道”的入口。为了方便叙述，我接下来会把这个 IP 包称为“原始 IP 包”。 为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1 这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。从图 3 的 Flannel VXLAN 模式的流程图中我们可以看到，10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。为了方便叙述，接下来我会把 Node 1 和 Node 2 上的 flannel.1 设备分别称为“源 VTEP 设备”和“目的 VTEP 设备”。而这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。 “源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。 目的 VTEP 设备 的 MAC 地址是什么？ 此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示： 123# 在Node 1上$ ip neigh show dev flannel.110.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT 这条记录的意思非常明确，即：IP 地址 10.1.16.0，对应的 MAC 地址是 5e:f8:4f:00:e3:37。 可以看到，最新版本的 Flannel 并不依赖 L3 MISS 事件和 ARP 学习，而会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录，直接下放到其他每台宿主机上 有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示： 可以看到，Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。需要注意的是，上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。所以图中的 Inner IP Header 字段，依然是 container-2 的 IP 地址，即 10.1.16.3。 但是，上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Outer Ethernet Frame）。 为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。 所以，跟 UDP 模式类似，在宿主机看来，它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备，发起了一次普通的 UDP 链接。它哪里会知道，这个 UDP 包里面，其实是一个完整的二层数据帧。这是不是跟特洛伊木马的故事非常像呢？不过，不要忘了，一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对应的宿主机地址是什么。也就是说，这个 UDP 包该发给哪台宿主机呢？在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。 不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示： 123# 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:375e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent 可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即：发往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node 2，UDP 包要发往的目的地就找到了。 所以接下来的流程，就是一个正常的、宿主机网络上的封包工作。 我们知道，UDP 包是一个四层数据包，所以 Linux 内核会在它前面加上一个 IP 头，即原理图中的 Outer IP Header，组成一个 IP 包。并且，在这个 IP 头里，会填上前面通过 FDB 查询出来的目的主机的 IP 地址，即 Node 2 的 IP 地址 10.168.0.3。然后，Linux 内核再在这个 IP 包前面加上二层数据帧头，即原理图中的 Outer Ethernet Header，并把 Node 2 的 MAC 地址填进去。这个 MAC 地址本身，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护。这时候，我们封装出来的“外部数据帧”的格式，如下所示： 这样，封包工作就宣告完成了。接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。以上，就是 Flannel VXLAN 模式的具体工作原理了。 Flannel 能负责保证二层网络（MAC 地址）的连通性吗？ CNI网络插件 UDP与VXLAN都需要容器连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作 网络插件是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。 Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0 以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已 Kubernetes 为 Flannel 分配的子网范围是 10.244.0.0/16。这个参数可以在部署的时候指定 1$ kubeadm init --pod-network-cidr=10.244.0.0/16 也可以在部署完成后，通过修改 kube-controller-manager 的配置文件来指定。这时候，假设 Infra-container-1 要访问 Infra-container-2（也就是 Pod-1 要访问 Pod-2），这个 IP 包的源地址就是 10.244.0.2，目的 IP 地址是 10.244.1.3。而此时，Infra-container-1 里的 eth0 设备，同样是以 Veth Pair 的方式连接在 Node 1 的 cni0 网桥上。所以这个 IP 包就会经过 cni0 网桥出现在宿主机上 此时，Node 1 上的路由表，如下所示： 12345678# 在Node 1上$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.244.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni010.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 因为我们的 IP 包的目的 IP 地址是 10.244.1.3，所以它只能匹配到第二条规则，也就是 10.244.1.0 对应的这条路由规则。可以看到，这条规则指定了本机的 flannel.1 设备进行处理。并且，flannel.1 在处理完后，要将 IP 包转发到的网关（Gateway），正是“隧道”另一端的 VTEP 设备，也就是 Node 2 的 flannel.1 设备。所以，接下来的流程，就跟上一篇文章中介绍过的 Flannel VXLAN 模式完全一样了 需要注意的是，CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。而此时，如果你用 docker run 单独启动一个容器，那么 Docker 项目还是会把这个容器连接到 docker0 网桥上。所以这个容器的 IP 地址，一定是属于 docker0 网桥的 172.17.0.0/16 网段。 Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面： 一方面，Kubernetes 项目并没有使用 Docker 的网络模型（CNM），所以它并不希望、也不具备配置 docker0 网桥的能力； 另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关。 Kubernetes 创建一个 Pod 的第一步，就是创建并启动一个 Infra 容器，用来“hold”住这个 Pod 的 Network Namespace。所以，CNI 的设计思想，就是：Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。 那么它是怎么完成这个网络栈的配置的呢？ 在部署 Kubernetes 的时候，有一个步骤是安装 kubernetes-cni 包，它的目的就是在宿主机上安装 CNI 插件所需的基础可执行文件，在安装完成后，你可以在宿主机的 /opt/cni/bin 目录下看到它们，如下所示： 1234567891011121314$ ls -al /opt/cni/bin/total 73088-rwxr-xr-x 1 root root 3890407 Aug 17 2017 bridge-rwxr-xr-x 1 root root 9921982 Aug 17 2017 dhcp-rwxr-xr-x 1 root root 2814104 Aug 17 2017 flannel-rwxr-xr-x 1 root root 2991965 Aug 17 2017 host-local-rwxr-xr-x 1 root root 3475802 Aug 17 2017 ipvlan-rwxr-xr-x 1 root root 3026388 Aug 17 2017 loopback-rwxr-xr-x 1 root root 3520724 Aug 17 2017 macvlan-rwxr-xr-x 1 root root 3470464 Aug 17 2017 portmap-rwxr-xr-x 1 root root 3877986 Aug 17 2017 ptp-rwxr-xr-x 1 root root 2605279 Aug 17 2017 sample-rwxr-xr-x 1 root root 2808402 Aug 17 2017 tuning-rwxr-xr-x 1 root root 3475750 Aug 17 2017 vlan 这些 CNI 的基础可执行文件，按照功能可以分为三类： 第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件。比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan。我在前面提到过的 Flannel、Weave 等项目，都属于“网桥”类型的 CNI 插件。所以在具体的实现中，它们往往会调用 bridge 这个二进制文件。 第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。 第三类，是由 CNI 社区维护的内置 CNI 插件。比如： flannel，就是专门为 Flannel 项目提供的 CNI 插件； tuning，是一个通过 sysctl 调整网络设备参数的二进制文件； portmap，是一个通过 iptables 配置端口映射的二进制文件； bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。 从这些二进制文件中，我们可以看到，如果要实现一个给 Kubernetes 用的容器网络方案，其实需要做两部分工作，以 Flannel 项目为例： 首先，实现这个网络方案本身。这一部分需要编写的，其实就是 flanneld 进程里的主要逻辑。比如，创建和配置 flannel.1 设备、配置宿主机路由、配置 ARP 和 FDB 表里的信息等等。然后，实现该网络方案对应的 CNI 插件。这一部分主要需要做的，就是配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。 由于 Flannel 项目对应的 CNI 插件已经被内置了，所以它无需再单独安装。而对于 Weave、Calico 等其他项目来说，我们就必须在安装插件的时候，把对应的 CNI 插件的可执行文件放在 /opt/cni/bin/ 目录下。 实际上，对于 Weave、Calico 这样的网络方案来说，它们的 DaemonSet 只需要挂载宿主机的 /opt/cni/bin/，就可以实现插件可执行文件的安装了。你可以想一下具体应该怎么做，就当作一个课后小问题留给你去实践了。 接下来，你就需要在宿主机上安装 flanneld（网络方案本身）。而在这个过程中，flanneld 启动后会在每台宿主机上生成它对应的 CNI 配置文件（它其实是一个 ConfigMap），从而告诉 Kubernetes，这个集群要使用 Flannel 作为容器网络方案。 这个 CNI 配置文件的内容如下所示： 12345678910111213141516171819$ cat /etc/cni/net.d/10-flannel.conflist &#123; \"name\": \"cbr0\", \"plugins\": [ &#123; \"type\": \"flannel\", \"delegate\": &#123; \"hairpinMode\": true, \"isDefaultGateway\": true &#125; &#125;, &#123; \"type\": \"portmap\", \"capabilities\": &#123; \"portMappings\": true &#125; &#125; ]&#125; Soft multi-tenancy 为什么说 Kubernetes 只有 soft multi-tenancy？ 参考链接 《极客时间-深入剖析 Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门13-容器网络之Service和Ingress","slug":"K8S/k8s入门13-容器网络之Service和Ingress","date":"2022-02-20T16:35:06.000Z","updated":"2022-02-20T16:36:12.237Z","comments":true,"path":"2022/02/21/K8S/k8s入门13-容器网络之Service和Ingress/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A813-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8BService%E5%92%8CIngress/","excerpt":"","text":"Service 暴露给外界的三种方法中有一个叫作 LoadBalancer 类型的 Service，它会为你在 Cloud Provider（比如：Google Cloud 或者 OpenStack）里创建一个与该 Service 对应的负载均衡服务 由于每个 Service 都要有一个负载均衡服务，为什么没有一个内置一个全局的负载均衡器。通过访问的 URL，把请求转发给不同的后端 Service。 这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。可以说是 Service 的“Service”。 举个例子，假如我现在有这样一个站点：https://cafe.example.com。其中， https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。 https://cafe.example.com/tea，对应的则是“茶水点餐系统”。 这两个系统，分别由名叫 coffee 和 tea 这样两个 Deployment 来提供服务 如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？ 在 Kubernetes 里就需要通过 Ingress 对象来描述，如下所示： 123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: cafe-ingressspec: tls: - hosts: - cafe.example.com secretName: cafe-secret rules: - host: cafe.example.com http: paths: - path: /tea backend: serviceName: tea-svc servicePort: 80 - path: /coffee backend: serviceName: coffee-svc servicePort: 80 在 Kubernetes 里，文件的 rules 字段叫作：IngressRule IngressRule 的 Key 就叫做：host。它必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。 而 host 字段定义的值就是这个 Ingress 的入口。当用户访问 cafe.example.com 的时候，实际上访问到的是这个 Ingress 对象。这样，Kubernetes 就能使用 IngressRule 来对你的请求进行下一步转发。 接下来 IngressRule 规则的定义，则依赖于 path 字段。你可以简单地理解为，这里的每一个 path 都对应一个后端 Service。所以在我们的例子里，我定义了两个 path，它们分别对应 coffee 和 tea 这两个 Deployment 的 Service（即：coffee-svc 和 tea-svc） 所以所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。而这个代理服务对应的转发规则，就是 IngressRule。跟 Nginx、HAproxy 等项目的配置文件的写法是一致的。Kubernetes 的用户就无需关心 Ingress 的具体细节了。选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可 这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。 以最常用的 Nginx Ingress Controller 为例，在用 kubeadm 部署的 Bare-metal 环境中，实践 Ingress 机制的使用过程。 部署 Nginx Ingress Controller 的方法非常简单，如下所示： 1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml 在mandatory.yaml这个文件里，正是 Nginx 官方为你维护的 Ingress Controller 的定义。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: ... spec: serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE - name: http valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 上述是使用 nginx-ingress-controller 镜像的 Pod。 注意: 这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数{POD_NAMESPACE}。而这个信息，当然是通过 Downward API 拿到的，即：Pod 的 env 字段里的定义（env.valueFrom.fieldRef.fieldPath） 而这个 Pod 本身，就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。 而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过Nginx Lua方案实现了 Nginx Upstream 的动态配置 此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。可以看到，一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门12-容器网络之Service调试访问","slug":"K8S/k8s入门12-容器网络之Service调试访问","date":"2022-02-20T16:34:52.000Z","updated":"2022-02-20T16:36:17.744Z","comments":true,"path":"2022/02/21/K8S/k8s入门12-容器网络之Service调试访问/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A812-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8BService%E8%B0%83%E8%AF%95%E8%AE%BF%E9%97%AE/","excerpt":"","text":"Service 的访问入口，其实就是每台宿主机上由 kube-proxy 生成的 iptables 规则，以及 kube-dns 生成的 DNS 记录。Service 的访问信息在 Kubernetes 集群之外是无效的 如何从外部(Kubernetes 集群之外)，访问到 Kubernetes 里创建的 Service 外部访问Service 最常用的办法：NodePort 123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: my-nginx labels: run: my-nginxspec: type: NodePort ports: - nodePort: 8080 targetPort: 80 protocol: TCP name: http - nodePort: 443 protocol: TCP name: https selector: run: my-nginx 在 Service 定义中声明它的类型是，type=NodePort。然后，在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口 如果不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，可通过 kube-apiserver 的–service-node-port-range 参数来修改它 这时候，要访问这个 Service，你只需要访问 1&lt;任何一台宿主机的IP地址&gt;:8080 就可以访问到某一个被代理的 Pod 的 80 端口了。NodePort 模式下，kube-proxy 要做的就是在每台宿主机上生成这样一条 iptables 规则： 1-A KUBE-NODEPORTS -p tcp -m comment --comment \"default/my-nginx: nodePort\" -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了。需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示： 1-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE 可以看到，这条规则设置在 POSTROUTING 检查点，也就是说，它给即将离开这台主机的 IP 包，进行了一次 SNAT 操作，将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。 当然，这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行（否则普通的 IP 包就被影响了）。而 iptables 做这个判断的依据，就是查看该 IP 包是否有一个“0x4000”的“标志”。你应该还记得，这个标志正是在 IP 包被执行 DNAT 操作之前被打上去的。 可是，为什么一定要对流出的包做 SNAT操作呢？ 这里的原理其实很简单，如下所示： 123456789 client \\ ^ \\ \\ v \\ node 1 &lt;--- node 2 | ^ SNAT | | ---&gt; v |endpoint 当一个外部的 client 通过 node 2 的地址访问一个 Service 的时候，node 2 上的负载均衡规则，就可能把这个 IP 包转发给一个在 node 1 上的 Pod。这里没有任何问题。 而当 node 1 上的这个 Pod 处理完请求之后，它就会按照这个 IP 包的源地址发出回复。 可是，如果没有做 SNAT 操作的话，这时候，被转发来的 IP 包的源地址就是 client 的 IP 地址。所以此时，Pod 就会直接将回复发给client。对于 client 来说，它的请求明明发给了 node 2，收到的回复却来自 node 1，这个 client 很可能会报错。 所以，在上图中，当 IP 包离开 node 2 之后，它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址。这样，Pod 在处理完成之后就会先回复给 node 2（而不是 client），然后再由 node 2 发送给 client。 当然，这也就意味着这个 Pod 只知道该 IP 包来自于 node 2，而不是外部的 client。对于 Pod 需要明确知道所有请求来源的场景来说，这是不可以的。 所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。 而这个机制的实现原理也非常简单：这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示： 123456789 client ^ / \\ / / \\ / v X node 1 node 2 ^ | | | | vendpoint 当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。 从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 LoadBalancer 类型的 Service，如下所示： 123456789101112---kind: ServiceapiVersion: v1metadata: name: example-servicespec: ports: - port: 8765 targetPort: 9376 selector: app: example type: LoadBalancer 在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。 而第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 ExternalName。举个例子： 1234567kind: ServiceapiVersion: v1metadata: name: my-servicespec: type: ExternalName externalName: my.database.example.com 在上述 Service 的 YAML 文件中，我指定了一个 externalName=my.database.example.com 的字段。而且你应该会注意到，这个 YAML 文件里不需要指定 selector。 这时候，当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。 此外，Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址，比如下面这个例子： 1234567891011121314kind: ServiceapiVersion: v1metadata: name: my-servicespec: selector: app: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 9376 externalIPs: - 80.11.12.10 在上述 Service 中，我为它指定的 externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。不过，在这里 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点。你可以想一想这是为什么。 实际上，在理解了 Kubernetes Service 机制的工作原理之后，很多与 Service 相关的问题，其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。 比如，当你的 Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常： 1234567# 在一个Pod里执行$ nslookup kubernetes.defaultServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local 如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。 而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints： 123$ kubectl get endpoints hostnamesNAME ENDPOINTShostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 需要注意的是，如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。 而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示： 12345678910I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container \"/kube-proxy\"I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier.I1027 22:14:53.999055 5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns-tcp\" to [10.244.1.3:53]I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns\" to [10.244.1.3:53]I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for \"default/kubernetes:https\" to [10.240.0.2:443]I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from masterI1027 22:14:54.040048 5063 proxier.go:294] Adding new service \"default/kubernetes:https\" at 10.0.0.1:443/TCPI1027 22:14:54.040154 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns\" at 10.0.0.10:53/UDPI1027 22:14:54.040223 5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns-tcp\" at 10.0.0.10:53/TCP 如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。而一个 iptables 模式的 Service 对应的规则，我在上一篇以及这一篇文章里已经全部介绍到了，它们包括： KUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应； KUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应； KUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致； 如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。 通过查看这些链的数量、转发目的地址、端口、过滤条件等信息，你就能很容易发现一些异常的蛛丝马迹。 当然，还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己。这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。关于 Hairpin 的原理我在前面已经介绍过，这里就不再赘述了。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。 其中，在 hairpin-veth 模式下，你应该能看到 CNI 网桥对应的各个 VETH 设备，都将 Hairpin 模式设置为了 1，如下所示： 123$ for d in &#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;veth*&#x2F;hairpin_mode; do echo &quot;$d &#x3D; $(cat $d)&quot;; done&#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;veth4bfbfe74&#x2F;hairpin_mode &#x3D; 1&#x2F;sys&#x2F;devices&#x2F;virtual&#x2F;net&#x2F;cni0&#x2F;brif&#x2F;vethfc2a18c5&#x2F;hairpin_mode &#x3D; 1 而如果是 promiscuous-bridge 模式的话，你应该看到 CNI 网桥的混杂模式（PROMISC）被开启，如下所示： 12$ ifconfig cni0 |grep PROMISCUP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1 总结： 从外部访问Service的三种方式 NodePort LoadBalancer External Name 所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护 Kubernetes 里面的 Service 和 DNS 机制，也都不具备强多租户能力。比如，在多租户情况下，每个租户应该拥有一套独立的 Service 规则（Service 只应该看到和代理同一个租户下的 Pod）。再比如 DNS，在多租户情况下，每个租户应该拥有自己的 kube-dns（kube-dns 只应该为同一个租户下的 Service 和 Pod 创建 DNS Entry） 为什么Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点？ 因为k8s只是在集群中的每个节点上创建了一个 externalIPs 与kube-ipvs0网卡的绑定关系. 若流量都无法路由到任意的一个k8s节点,那自然无法将流量转给具体的service 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门11-容器网络之服务发现","slug":"K8S/k8s入门11-容器网络之服务发现","date":"2022-02-20T16:34:37.000Z","updated":"2022-02-20T16:36:23.455Z","comments":true,"path":"2022/02/21/K8S/k8s入门11-容器网络之服务发现/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A811-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。 一个Service例子： 123456789101112apiVersion: v1kind: Servicemetadata: name: hostnamesspec: selector: app: hostnames ports: - name: default protocol: TCP port: 80 targetPort: 9376 使用 selector 字段来声明这个 Service 只代理携带了 app=hostnames 标签的 Pod。Service 的 80 端口，代理的是 Pod 的 9376 端口 然后应用的Deployment为： 1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: hostnamesspec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: - name: hostnames image: k8s.gcr.io/serve_hostname ports: - containerPort: 9376 protocol: TCP 这个应用的作用就是每次访问 9376 端口时，返回它自己的 hostname。 被 selector 选中的 Pod，称为 Service 的 Endpoints，可使用 kubectl get ep 命令看到它们，如下所示： 123$ kubectl get endpoints hostnamesNAME ENDPOINTShostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376 只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉 通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod ： 12345678910111213$ kubectl get svc hostnamesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEhostnames ClusterIP 10.0.1.175 &lt;none&gt; 80/TCP 5s$ curl 10.0.1.175:80hostnames-0uton$ curl 10.0.1.175:80hostnames-yp2kp$ curl 10.0.1.175:80hostnames-bvc05 这个 VIP 地址是 Kubernetes 自动为 Service 分配的。 通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。这种方式称为：ClusterIP 模式的 Service Service 是由 kube-proxy 组件，加上 iptables 来共同实现的 举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示： 1-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \"default/hostnames: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。 10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。 KUBE-SVC-NWV5X2332I4OT4T3 规则，实际上是一组规则的集合，如下所示： 123-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -j KUBE-SEP-57KPRZ3JQVENLNBR 这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。 而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。 这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。 通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示 123456789-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.3.6:9376-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.1.7:9376-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.2.3:9376 可以看到，这三条链，其实是三条 DNAT 规则。但在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。而 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。 这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的 kube-proxy - IPVS模式 kube-proxy 通过 iptables 处理 Service 的过程，需要在宿主机上设置相当多的 iptables 规则，并在控制循环里不断地刷新这些规则来确保它们始终是正确的 当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。 解决办法：IPVS模式的Service IPVS 模式的工作原理：当创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示： 123456# ip addr ... 73：kube-ipvs0：&lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 1000 link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff inet 10.0.1.175/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示： 12345678# ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.102.128.4:80 rr -&gt; 10.244.3.6:9376 Masq 1 0 0 -&gt; 10.244.1.7:9376 Masq 1 0 0 -&gt; 10.244.2.3:9376 Masq 1 0 0 三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。 相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。 不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。所以，在大规模集群里，建议为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。 service 与 DNS 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。 对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：…svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。 而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：…svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。 此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：…pod.cluster.local。这条记录指向 Pod 的 IP 地址。而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：…svc.cluster.local。这条记录也指向 Pod 的 IP 地址。 但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain 字段，那么这时候 Pod 的 A 记录就会变成：…svc.cluster.local，比如： 12345678910111213141516171819202122232425262728apiVersion: v1kind: Servicemetadata: name: default-subdomainspec: selector: name: busybox clusterIP: None ports: - name: foo port: 1234 targetPort: 1234---apiVersion: v1kind: Podmetadata: name: busybox1 labels: name: busyboxspec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - \"3600\" name: busybox 在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。 在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。 总结： 当服务（Pod）的 IP 地址是不固定的且没办法提前获知时，该如何通过一个固定的方式访问到这个 Pod 呢？ ClusterIP 模式的 Service 为你提供的，就是一个 Pod 的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过 Label 确定的。 Headless Service 提供的是一个 Pod 的稳定的 DNS 名字，并且名字是可以通过 Pod 名字和 Service 名字拼接出来的。 参考链接 《极客时间-深入剖析Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"k8s入门10-容器网络之多租户","slug":"K8S/k8s入门10-容器网络之多租户","date":"2022-02-20T16:34:25.000Z","updated":"2022-02-20T16:36:30.059Z","comments":true,"path":"2022/02/21/K8S/k8s入门10-容器网络之多租户/","link":"","permalink":"http://xboom.github.io/2022/02/21/K8S/k8s%E5%85%A5%E9%97%A810-%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%A4%9A%E7%A7%9F%E6%88%B7/","excerpt":"","text":"什么是多租户：一种软件架构规范，运行在服务器上的单一实例，可服务多个客户或组织（租户）,一个满足多租户规范的软件应用需要对数据和配置进行隔离，每一个租户都有自己虚拟的实例 Kubernetes 的网络模型，以及前面这些网络方案的实现，都只关注网络的“连通”，却不关心“隔离”，那么 Kubernetes 的网络方案对“隔离”是如何考虑的？ NetworkPolicy 在 Kubernetes 里，网络隔离能力的定义，是依靠一种专门的 API 对象来描述的，即：NetworkPolicy 12345678910111213141516171819202122232425262728293031323334apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db policyTypes: - Ingress - Egress ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 Kubernetes 里的 Pod 默认都是“允许所有”（Accept All）的，即：Pod 可以接收来自任何发送方的请求；或者，向任何接收方发送请求。而如果你要对这个情况作出限制，就必须通过 NetworkPolicy 对象来指定 而在上面这个例子里，你首先会看到 podSelector 字段。它的作用，就是定义这个 NetworkPolicy 的限制范围，比如：当前 Namespace 里携带了 role=db 标签的 Pod 而如果你把 podSelector 字段留空： 12spec: podSelector: &#123;&#125; 那么这个 NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod。而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。而 NetworkPolicy 定义的规则，其实就是“白名单”。 例如，在我们上面这个例子里，我在 policyTypes 字段，定义了这个 NetworkPolicy 的类型是 ingress 和 egress，即：它既会影响流入（ingress）请求，也会影响流出（egress）请求。 然后，在 ingress 字段里，我定义了 from 和 ports，即：允许流入的“白名单”和端口。其中，这个允许流入的“白名单”里，我指定了三种并列的情况，分别是：ipBlock、namespaceSelector 和 podSelector。 而在 egress 字段里，我则定义了 to 和 ports，即：允许流出的“白名单”和端口。这里允许流出的“白名单”的定义方法与 ingress 类似。只不过，这一次 ipblock 字段指定的，是目的地址的网段。 综上所述，这个 NetworkPolicy 对象，指定的隔离规则如下所示： 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。 Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：a. default Namespace 里的，携带了 role=fronted 标签的 Pod；b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。 Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。 需要注意的是，定义一个 NetworkPolicy 对象的过程，容易犯错的是“白名单”部分（from 和 to 字段） 12345678910... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ... 像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。 而下面这个例子，虽然看起来类似，但是它定义的规则却完全不同： 12345678910... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... 注意看，这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。 此外，如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用，你的 CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的。 在具体实现上，凡是支持 NetworkPolicy 的 CNI 网络插件，都维护着一个 NetworkPolicy Controller，通过控制循环的方式对 NetworkPolicy 对象的增删改查做出响应，然后在宿主机上完成 iptables 规则的配置工作。 在 Kubernetes 生态里，目前已经实现了 NetworkPolicy 的网络插件包括 Calico、Weave 和 kube-router 等多个项目，但是并不包括 Flannel 项目。 所以说，如果想要在使用 Flannel 的同时还使用 NetworkPolicy 的话，你就需要再额外安装一个网络插件，比如 Calico 项目，来负责执行 NetworkPolicy。 安装 Flannel + Calico 的流程非常简单，你直接参考这个文档一键安装即可 网络隔离 以三层网络插件为例(比如 Calico 和 kube-router)，分析一下这部分的原理。简单的NetworkPolicy 策略 1234567891011121314151617181920apiVersion: extensions/v1beta1kind: NetworkPolicymetadata: name: test-network-policy namespace: defaultspec: podSelector: matchLabels: role: db ingress: - from: - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: tcp port: 6379 可以看到，我们指定的 ingress“白名单”，是任何 Namespace 里，携带 project=myproject 标签的 Namespace 里的 Pod；以及 default Namespace 里，携带了 role=frontend 标签的 Pod。允许被访问的端口是：6379。而被隔离的对象，是所有携带了 role=db 标签的 Pod。 那么这个时候，Kubernetes 的网络插件就会使用这个 NetworkPolicy 的定义，在宿主机上生成 iptables 规则。这个过程，我可以通过如下所示的一段 Go 语言风格的伪代码来为你描述： 1234567for dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址 for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址 for port, protocol := range ingress.ports &#123; iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT &#125; &#125;&#125; 可以看到，这是一条最基本的、通过匹配条件决定下一步动作的 iptables 规则。 这条规则的名字是 KUBE-NWPLCY-CHAIN，含义是：当 IP 包的源地址是 srcIP、目的地址是 dstIP、协议是 protocol、目的端口是 port 的时候，就允许它通过（ACCEPT）。 而正如这段伪代码所示，匹配这条规则所需的这四个参数，都是从 NetworkPolicy 对象里读取出来的 Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。 在设置好上述“隔离”规则之后，网络插件还需要想办法，将所有对被隔离 Pod 的访问请求，都转发到上述 KUBE-NWPLCY-CHAIN 规则上去进行匹配。并且，如果匹配不通过，这个请求应该被“拒绝”。 在 CNI 网络插件中，上述需求可以通过设置两组 iptables 规则来实现。 第一组规则，负责“拦截”对被隔离 Pod 的访问请求。生成这一组规则的伪代码，如下所示： 1234567for pod := range 该Node上的所有Pod &#123; if pod是networkpolicy.spec.podSelector选中的 &#123; iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN ... &#125;&#125; IPTABLE iptables 规则使用到了内置链：FORWARD。它是什么意思呢？说到这里，我就得为你稍微普及一下 iptables 的知识了。实际上，iptables 只是一个操作 Linux 内核 Netfilter 子系统的“界面”。顾名思义，Netfilter 子系统的作用，就是 Linux 内核里挡在“网卡”和“用户态进程”之间的一道“防火墙”。它们的关系，可以用如下的示意图来表示： 可以看到，这幅示意图中，IP 包“一进一出”的两条路径上，有几个关键的“检查点”，它们正是 Netfilter 设置“防火墙”的地方。在 iptables 中，这些“检查点”被称为：链（Chain）。这是因为这些“检查点”对应的 iptables 规则，是按照定义顺序依次进行匹配的。这些“检查点”的具体工作原理，可以用如下所示的示意图进行描述： 可以看到，当一个 IP 包通过网卡进入主机之后，它就进入了 Netfilter 定义的流入路径（Input Path）里。在这个路径中，IP 包要经过路由表路由来决定下一步的去向。而在这次路由之前，Netfilter 设置了一个名叫 PREROUTING 的“检查点”。在 Linux 内核的实现里，所谓“检查点”实际上就是内核网络协议栈代码里的 Hook（比如，在执行路由判断的代码之前，内核会先调用 PREROUTING 的 Hook）。 而在经过路由之后，IP 包的去向就分为了两种： 第一种，继续在本机处理； 第二种，被转发到其他目的地 我们先说一下 IP 包的第一种去向。这时候，IP 包将继续向上层协议栈流动。在它进入传输层之前，Netfilter 会设置一个名叫 INPUT 的“检查点”。到这里，IP 包流入路径（Input Path）结束。 接下来，这个 IP 包通过传输层进入用户空间，交给用户进程处理。而处理完成后，用户进程会通过本机发出返回的 IP 包。这时候，这个 IP 包就进入了流出路径（Output Path）。此时，IP 包首先还是会经过主机的路由表进行路由。 路由结束后，Netfilter 就会设置一个名叫 OUTPUT 的“检查点”。然后，在 OUTPUT 之后，再设置一个名叫 POSTROUTING“检查点”。你可能会觉得奇怪，为什么在流出路径结束后，Netfilter 会连着设置两个“检查点”呢？ 这就要说到在流入路径里，路由判断后的第二种去向了。在这种情况下，这个 IP 包不会进入传输层，而是会继续在网络层流动，从而进入到转发路径（Forward Path）。在转发路径中，Netfilter 会设置一个名叫 FORWARD 的“检查点”。而在 FORWARD“检查点”完成后，IP 包就会来到流出路径。而转发的 IP 包由于目的地已经确定，它就不会再经过路由，也自然不会经过 OUTPUT，而是会直接来到 POSTROUTING“检查点”。所以说，POSTROUTING 的作用，其实就是上述两条路径，最终汇聚在一起的“最终检查点”。 需要注意的是，在有网桥参与的情况下，上述 Netfilter 设置“检查点”的流程，实际上也会出现在链路层（二层），并且会跟我在上面讲述的网络层（三层）的流程有交互。 这些链路层的“检查点”对应的操作界面叫作 ebtables。所以，准确地说，数据包在 Linux Netfilter 子系统里完整的流动过程，其实应该如下所示（这是一幅来自Netfilter 官方的原理图，建议你点击图片以查看大图）： 可以看到，我前面为你讲述的，正是上图中绿色部分，也就是网络层的 iptables 链的工作流程。 另外，你应该还能看到，每一个白色的“检查点”上，还有一个绿色的“标签”，比如：raw、nat、filter 等等。 在 iptables 里，这些标签叫作：表。比如，同样是 OUTPUT 这个“检查点”，filter Output 和 nat Output 在 iptables 里的语法和参数，就完全不一样，实现的功能也完全不同。 所以说，iptables 表的作用，就是在某个具体的“检查点”（比如 Output）上，按顺序执行几个不同的检查动作（比如，先执行 nat，再执行 filter）。 在理解了 iptables 的工作原理之后，我们再回到 NetworkPolicy 上来。这时候，前面由网络插件设置的、负责“拦截”进入 Pod 的请求的三条 iptables 规则，就很容易读懂了： 123iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAINiptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN... 其中，第一条 FORWARD 链“拦截”的是一种特殊情况：它对应的是同一台宿主机上容器之间经过 CNI 网桥进行通信的流入数据包。其中，–physdev-is-bridged 的意思就是，这个 FORWARD 链匹配的是，通过本机上的网桥设备，发往目的地址是 podIP 的 IP 包。 kube-router 其实是一个简化版的 Calico，它也使用 BGP 来维护路由信息，但是使用 CNI bridge 插件负责跟 Kubernetes 进行交互。 而第二条 FORWARD 链“拦截”的则是最普遍的情况，即：容器跨主通信。这时候，流入容器的数据包都是经过路由转发（FORWARD 检查点）来的。 不难看到，这些规则最后都跳转（即：-j）到了名叫 KUBE-POD-SPECIFIC-FW-CHAIN 的规则上。它正是网络插件为 NetworkPolicy 设置的第二组规则。而这个 KUBE-POD-SPECIFIC-FW-CHAIN 的作用，就是做出“允许”或者“拒绝”的判断。这部分功能的实现，可以简单描述为下面这样的 iptables 规则： 12iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAINiptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable 可以看到，首先在第一条规则里，我们会把 IP 包转交给前面定义的 KUBE-NWPLCY-CHAIN 规则去进行匹配。按照我们之前的讲述，如果匹配成功，那么 IP 包就会被“允许通过”。 而如果匹配失败，IP 包就会来到第二条规则上。可以看到，它是一条 REJECT 规则。通过这条规则，不满足 NetworkPolicy 定义的请求就会被拒绝掉，从而实现了对该容器的“隔离”。 以上，就是 CNI 网络插件实现 NetworkPolicy 的基本方法了。当然，对于不同的插件来说，上述实现过程可能有不同的手段，但根本原理是不变的。 NetworkPolicy 实际上只是宿主机上的一系列 iptables 规则，Kubernetes 负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力 它使得指定的 Namespace（比如 my-namespace）里的所有 Pod，都不能接收任何 Ingress 请求。 job，cronjob这类计算型pod不需要也不应该对外提供服务，可以拒绝所有流入流量，提高系统安全。 参考链接 《极客时间-深入剖析 Kubernetes》","categories":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"}]},{"title":"协议5-TLS","slug":"Protocol/协议5-TLS","date":"2022-02-20T16:27:19.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2022/02/21/Protocol/协议5-TLS/","link":"","permalink":"http://xboom.github.io/2022/02/21/Protocol/%E5%8D%8F%E8%AE%AE5-TLS/","excerpt":"","text":"引言 HTTPS是建立在HTTP的基础上添加SSL加密层，通过TLS/SSL具有的身份验证、信息加密和完整性校验功能保证数据安全。 HTTP与HTTPS的区别： HTTPS是加密传输协议，HTTP是名文传输协议; HTTPS需要用到SSL证书，而HTTP不用; HTTPS比HTTP更加安全，对搜索引擎更友好，利于SEO HTTPS标准端口443，HTTP标准端口80; HTTPS基于传输层，HTTP基于应用层; HTTPS在浏览器显示绿色安全锁，HTTP没有显示; 握手与密钥协商过程(报文分析) 基于RSA握手和密钥交换的TLS/SSL握手过程 未完待续。。。 参考链接 SSL/TLS握手过程 SSL/TLS详解","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"TLS","slug":"TLS","permalink":"http://xboom.github.io/tags/TLS/"}]},{"title":"算法实践1-令牌桶算法","slug":"Algorithm Applied/算法实践1-令牌桶算法","date":"2022-02-20T16:22:31.000Z","updated":"2022-06-25T07:00:00.218Z","comments":true,"path":"2022/02/21/Algorithm Applied/算法实践1-令牌桶算法/","link":"","permalink":"http://xboom.github.io/2022/02/21/Algorithm%20Applied/%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B51-%E4%BB%A4%E7%89%8C%E6%A1%B6%E7%AE%97%E6%B3%95/","excerpt":"","text":"在高并发系统中有三把利器用来保护系统：缓存、降级和限流 其中常用的限流算法有 **漏桶算法 **和 令牌桶算法 漏桶算法 把请求比作是水，水来了都先放进桶里，并以限定的速度出水，当水来得过猛而出水不够快时就会导致水直接溢出，即拒绝服务 漏斗有一个进水口 和 一个出水口，出水口以一定速率出水，并且有一个最大出水速率： 在漏斗中没有水的时候， 如果进水速率小于等于最大出水速率，那么，出水速率等于进水速率，此时，不会积水 如果进水速率大于最大出水速率，那么，漏斗以最大速率出水，此时，多余的水会积在漏斗中 在漏斗中有水的时候 出水口以最大速率出水 如果漏斗未满，且有进水的话，那么这些水会积在漏斗中 如果漏斗已满，且有进水的话，那么这些水会溢出到漏斗之外 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package ratelimit // import \"go.uber.org/ratelimit\"import ( \"time\" \"sync/atomic\" \"unsafe\")type state struct &#123; last time.Time sleepFor time.Duration&#125;type atomicLimiter struct &#123; state unsafe.Pointer //用于存储上一次执行的时间以及需要sleep的时间 padding [56]byte //是一个无意义的填充数据，为了提高性能，避免 cpu 缓存的 false sharing perRequest time.Duration //只单位，默认为秒 maxSlack time.Duration //松弛时间，也就是可以允许的突发流量的大小, 默认是 Pre/10 clock Clock //时钟，用于在测试的时候可以 mock 掉不使用真实的时间&#125;// newAtomicBased returns a new atomic based limiter.func newAtomicBased(rate int, opts ...Option) *atomicLimiter &#123; // TODO consider moving config building to the implementation // independent code. config := buildConfig(opts) perRequest := config.per / time.Duration(rate) l := &amp;atomicLimiter&#123; perRequest: perRequest, maxSlack: -1 * time.Duration(config.slack) * perRequest, clock: config.clock, &#125; initialState := state&#123; last: time.Time&#123;&#125;, sleepFor: 0, &#125; atomic.StorePointer(&amp;l.state, unsafe.Pointer(&amp;initialState)) return l&#125;// Take blocks to ensure that the time spent between multiple// Take calls is on average time.Second/rate.func (t *atomicLimiter) Take() time.Time &#123; var ( newState state // 状态 taken bool // 用于表示原子操作是否成功 interval time.Duration // 需要 sleep 的时间 ) for !taken &#123; // 如果 CAS 操作不成功就一直尝试 now := t.clock.Now() //获取当前时间 previousStatePointer := atomic.LoadPointer(&amp;t.state) // load 出上一次调用的时间 oldState := (*state)(previousStatePointer) newState = state&#123; last: now, sleepFor: oldState.sleepFor, &#125; // 如果 last 是零值的话，表示之前就没用过，直接保存返回即可 if oldState.last.IsZero() &#123; taken = atomic.CompareAndSwapPointer(&amp;t.state, previousStatePointer, unsafe.Pointer(&amp;newState)) continue &#125; // sleepFor 是需要睡眠的时间，由于引入了松弛时间，所以 sleepFor 可能是一个 // maxSlack ~ 0 之间的一个值，所以这里需要将现在的需要 sleep 的时间和上一次 // sleepFor 的值相加 newState.sleepFor += t.perRequest - now.Sub(oldState.last) // 如果距离上一次调用已经很久了，sleepFor 可能会是一个很小的值 // 最小值只能是 maxSlack 的大小 if newState.sleepFor &lt; t.maxSlack &#123; newState.sleepFor = t.maxSlack &#125; // 如果 sleepFor 大于 0 的话，计算出需要 sleep 的时间 // 然后将 state.sleepFor 置零 if newState.sleepFor &gt; 0 &#123; newState.last = newState.last.Add(newState.sleepFor) interval, newState.sleepFor = newState.sleepFor, 0 &#125; // 保存状态 taken = atomic.CompareAndSwapPointer(&amp;t.state, previousStatePointer, unsafe.Pointer(&amp;newState)) &#125; t.clock.Sleep(interval) return newState.last&#125; 令牌桶算法 令牌桶算法的原理是系统以恒定的速率产生令牌，然后把令牌放到令牌桶中，令牌桶有一个容量，当令牌桶满了的时候，再向其中放令牌，那么多余的令牌会被丢弃；当想要处理一个请求的时候，需要从令牌桶中取出一个令牌，如果此时令牌桶中没有令牌，那么则拒绝该请求 源码：https://github.com/beefsack/go-rate/blob/master/rate.go 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package rateimport ( \"container/list\" \"sync\" \"time\")// A RateLimiter limits the rate at which an action can be performed. It// applies neither smoothing (like one could achieve in a token bucket system)// nor does it offer any conception of warmup, wherein the rate of actions// granted are steadily increased until a steady throughput equilibrium is// reached.type RateLimiter struct &#123; limit int interval time.Duration mtx sync.Mutex times list.List //双向链表&#125;// New creates a new rate limiter for the limit and interval.func New(limit int, interval time.Duration) *RateLimiter &#123; lim := &amp;RateLimiter&#123; limit: limit, interval: interval, &#125; lim.times.Init() return lim&#125;// Wait blocks if the rate limit has been reached. Wait offers no guarantees// of fairness for multiple actors if the allowed rate has been temporarily// exhausted.func (r *RateLimiter) Wait() &#123; for &#123; ok, remaining := r.Try() if ok &#123; break &#125; time.Sleep(remaining) &#125;&#125;// Try returns true if under the rate limit, or false if over and the// remaining time before the rate limit expires.func (r *RateLimiter) Try() (ok bool, remaining time.Duration) &#123; r.mtx.Lock() defer r.mtx.Unlock() now := time.Now() if l := r.times.Len(); l &lt; r.limit &#123; r.times.PushBack(now) return true, 0 &#125; frnt := r.times.Front() if diff := now.Sub(frnt.Value.(time.Time)); diff &lt; r.interval &#123; return false, r.interval - diff &#125; frnt.Value = now r.times.MoveToBack(frnt) return true, 0&#125; 漏桶VS令牌桶 漏桶算法 能够强行限制数据的传输速率， 令牌桶算法 在能够限制数据的平均传输速率外，只要桶中存在令牌，就允许突发地传输数据直到达到用户配置的门限，所以也适合于具有突发特性的流量 参考链接 https://www.cnblogs.com/xuwc/p/9123078.html https://www.jianshu.com/p/d6250493308b https://segmentfault.com/a/1190000015967922","categories":[{"name":"Algorithm Applied","slug":"Algorithm-Applied","permalink":"http://xboom.github.io/categories/Algorithm-Applied/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"Acwing-剑指","slug":"Acwing/Test","date":"2022-02-14T16:27:12.000Z","updated":"2022-06-28T14:44:37.504Z","comments":true,"path":"2022/02/15/Acwing/Test/","link":"","permalink":"http://xboom.github.io/2022/02/15/Acwing/Test/","excerpt":"","text":"本人学历和眼界有限，以下内容仅供参考： 不建议进入该专业的原因： 内卷严重：大量人员涌入，比较内卷 频繁裁员：当前互联网公司也在频繁裁员(可能和当前经济环境有关，这样的环境会持续好久 性别因素：公司在招聘研发岗位可能会更多的考虑男孩子(研究岗位不清楚，不评论) 技术更新快：需要不断学习，技术更新迭代很快 进入该专业的原因： 社会需求大：社会各单位都需要计算机人才 前景巨大：计算机需求的趋势不变，不管是新基建、数字乡村的国家政策还是万物互联、数字化等社会需求 找工作容易：对于考公务员、进入大厂或从事计算机相关工作，有计算机背景会有更多的选择 是怎么选自己的专业 社会需求 当前需求 未来需求 发展前景 薪资 工作强度 工作强度主要跟工作类型区分","categories":[{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/categories/Acwing/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/tags/Acwing/"}]},{"title":"Acwing-剑指","slug":"Acwing/Acwing-剑指","date":"2022-02-14T16:27:12.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2022/02/15/Acwing/Acwing-剑指/","link":"","permalink":"http://xboom.github.io/2022/02/15/Acwing/Acwing-%E5%89%91%E6%8C%87/","excerpt":"","text":"数据结构 链表 从尾到头打印链表 题目 1234567输入一个链表的头节点，按链表从尾到头的顺序返回每个节点的值（用数组返回）。如输入&#123;1,2,3&#125;的链表如下图:返回一个数组为[3,2,1]0 &lt;= 链表长度 &lt;= 10000 解答 123456789101112131415161718192021/*** struct ListNode &#123;* int val;* struct ListNode *next;* ListNode(int x) :* val(x), next(NULL) &#123;* &#125;* &#125;;*/class Solution &#123;public: vector&lt;int&gt; printListFromTailToHead(ListNode* head) &#123; vector&lt;int&gt; result; while(head) &#123; result.push_back(head-&gt;val); head = head-&gt;next; &#125; reverse(result.begin(), result.end()); return result; &#125;&#125;; 反转链表 题目 给定一个单链表的头结点pHead(该头节点是有值的，比如在下图，它的val是1)，长度为n，反转该链表后，返回新链表的表头。 数据范围： 0\\leq n\\leq10000≤n≤1000 要求：空间复杂度 O(1)O(1) ，时间复杂度 O(n)O(n) 。 如当输入链表{1,2,3}时， 经反转后，原链表变为{3,2,1}，所以对应的输出为{3,2,1}。 以上转换过程如下图所示： 解答 1234567891011121314151617181920212223/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* ReverseList(ListNode* pHead) &#123; ListNode *pre = nullptr; ListNode *cur = pHead; ListNode *nex = nullptr; // 这里可以指向nullptr，循环里面要重新指向 while (cur) &#123; nex = cur-&gt;next; cur-&gt;next = pre; pre = cur; cur = nex; &#125; return pre; &#125;&#125;; 合并两个排序的链表 题目 输入两个递增的链表，单个链表的长度为n，合并这两个链表并使新链表中的节点仍然是递增排序的。 数据范围： 0 \\le n \\le 10000≤n≤1000，-1000 \\le 节点值 \\le 1000−1000≤节点值≤1000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 如输入{1,3,5},{2,4,6}时，合并后的链表为{1,2,3,4,5,6}，所以对应的输出为{1,2,3,4,5,6} 解答 12345678910111213141516171819202122232425262728/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) &#123; ListNode * pre = new ListNode(-1); ListNode * cur = pre; while(pHead1 &amp;&amp; pHead2) &#123; if(pHead1-&gt;val &lt; pHead2-&gt;val) &#123; cur-&gt;next = pHead1; pHead1 = pHead1-&gt;next; &#125; else &#123; cur-&gt;next = pHead2; pHead2 = pHead2-&gt;next; &#125; cur = cur-&gt;next; &#125; if(!pHead1) cur-&gt;next = pHead2; if(!pHead2) cur-&gt;next = pHead1; return pre-&gt;next; &#125;&#125;; 两个链表的第一个公共结点 题目 输入两个无环的单向链表，找出它们的第一个公共结点，如果没有公共节点则返回空。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的） 数据范围： n \\le 1000n≤1000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如，输入{1,2,3},{4,5},{6,7}时，两个无环的单向链表的结构如下图所示： 输入分为是3段，第一段是第一个链表的非公共部分，第二段是第二个链表的非公共部分，第三段是第一个链表和二个链表的公共部分。 后台会将这3个参数组装为两个链表，并将这两个链表对应的头节点传入到函数FindFirstCommonNode里面，用户得到的输入只有pHead1和pHead2 解答 1234567891011121314151617181920/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) &#123; ListNode *ta = pHead1; ListNode *tb = pHead2; while (ta != tb) &#123; ta = ta ? ta-&gt;next : pHead2; tb = tb ? tb-&gt;next : pHead1; &#125; return ta; &#125;&#125;; 链表中环的入口结点 题目 给一个长度为n链表，若其中包含环，请找出该链表的环的入口结点，否则，返回null。 数据范围： n\\le10000n≤10000，1&lt;=结点值&lt;=100001&lt;=结点值&lt;=10000 要求：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如，输入{1,2},{3,4,5}时，对应的环形链表如下图所示： 12345678910111213141516171819202122232425262728/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* EntryNodeOfLoop(ListNode* pHead) &#123; ListNode *fast = pHead; ListNode *slow = pHead; while(fast &amp;&amp; fast-&gt;next) &#123; fast = fast-&gt;next-&gt;next; slow = slow-&gt;next; if(fast == slow) break; &#125; if(!fast || !fast-&gt;next) return nullptr; fast = pHead; while(fast != slow) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return fast; &#125;&#125;; 链表中倒数最后k个结点 题目 输入一个长度为 n 的链表，设链表中的元素的值为 ai ，返回该链表中倒数第k个节点。 如果该链表长度小于k，请返回一个长度为 0 的链表。 数据范围：0 \\leq n \\leq 10^50≤n≤105，0 \\leq a_i \\leq 10^90≤a**i≤109，0 \\leq k \\leq 10^90≤k≤109 要求：空间复杂度 O(n)O(n)，时间复杂度 O(n)O(n) 进阶：空间复杂度 O(1)O(1)，时间复杂度 O(n)O(n) 例如输入{1,2,3,4,5},2时，对应的链表结构如下图所示： 其中蓝色部分为该链表的最后2个结点，所以返回倒数第2个结点（也即结点值为4的结点）即可，系统会打印后面所有的节点来比较。 解答 12345678910111213141516171819202122232425262728/** * struct ListNode &#123; * int val; * struct ListNode *next; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * &#125;; */class Solution &#123;public: /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param pHead ListNode类 * @param k int整型 * @return ListNode类 */ ListNode* FindKthToTail(ListNode* pHead, int k) &#123; ListNode* r = pHead; while (k-- &amp;&amp; r) r = r-&gt;next; // 移动右侧指针造成 k 的距离差 if (k &gt;= 0) return nullptr; // 此时说明 k 比链表长度长 ListNode* l = pHead; while (r) r = r-&gt;next, l = l-&gt;next; // 两个指针一起移动找到倒数第 k 个节点 return l; &#125;&#125;; 删除链表中重复的节点 题目 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表 1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 数据范围：链表长度满足 0 \\le n \\le 1000 \\0≤n≤1000 ，链表中的值满足 1 \\le val \\le 1000 \\1≤val≤1000 进阶：空间复杂度 O(n)*O*(n) ，时间复杂度 O(n) *O*(n) 例如输入{1,2,3,3,4,4,5}时，对应的输出为{1,2,5}，对应的输入输出链表如下图所示： 例如 输入：{1,2,3,3,4,4,5},输出：{1,2,5} 输入：{1,1,1,8}，输出：{8} 解答 123456789101112131415161718192021222324252627282930313233343536/*struct ListNode &#123; int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) &#123; &#125;&#125;;*/class Solution &#123;public: ListNode* deleteDuplication(ListNode* pHead) &#123; if (!pHead) return NULL; ListNode* slow = new ListNode(-1), *fast = new ListNode(-1), *dummy = new ListNode(-1); dummy-&gt;next = pHead; // 初始化两个指针 slow = dummy; //虚拟指针 fast = dummy-&gt;next; //头指针 while (fast) &#123; // 遇到重复则只更新fast节点 while (fast-&gt;next &amp;&amp; fast-&gt;val == fast-&gt;next-&gt;val) &#123; fast = fast-&gt;next; &#125; // 遇到重复 if (slow-&gt;next != fast) &#123; //如果不相等，说明中间slow 与 fast有相同值的节点 slow-&gt;next = fast-&gt;next; fast = slow-&gt;next; &#125; else &#123; // 没有重复 fast = fast-&gt;next; slow = slow-&gt;next; &#125; &#125; return dummy-&gt;next; &#125;&#125;; 参考链接 https://www.nowcoder.com/ta/coding-interviews","categories":[{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/categories/Acwing/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/tags/Acwing/"}]},{"title":"MySql入门9-温故而知新","slug":"MySql/MySql入门9-温故而知新","date":"2022-02-11T07:51:12.000Z","updated":"2022-02-11T08:17:48.565Z","comments":true,"path":"2022/02/11/MySql/MySql入门9-温故而知新/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql%E5%85%A5%E9%97%A89-%E6%B8%A9%E6%95%85%E8%80%8C%E7%9F%A5%E6%96%B0/","excerpt":"","text":"数据库基础知识","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门7-表","slug":"MySql/MySql入门7-表","date":"2022-02-11T07:49:55.000Z","updated":"2022-02-11T07:50:14.306Z","comments":true,"path":"2022/02/11/MySql/MySql入门7-表/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql%E5%85%A5%E9%97%A87-%E8%A1%A8/","excerpt":"","text":"","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门6-集群","slug":"MySql/MySql入门6-集群","date":"2022-02-11T07:47:24.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2022/02/11/MySql/MySql入门6-集群/","link":"","permalink":"http://xboom.github.io/2022/02/11/MySql/MySql%E5%85%A5%E9%97%A86-%E9%9B%86%E7%BE%A4/","excerpt":"","text":"高可用方案 共享存储：SAN/NAS 操作系统实时数据块恢复：DRBD架构(MySQL+DRBD+Heartbeat) 主从复制架构 主从复制(一主多从) MMM架构(双主多从) MHA架构(多主多从) 数据库高可用架构 GRT(MySQL Group Replication) Galera MySQL Cluster 和 PXC MySQL Cluster(ndb存储引擎比较复杂，并没有大规模使用) PXC(Percona XtraDB Cluster) 常见方案介绍 方案1 主从架构 主从复制的模式有哪些？https://zhuanlan.zhihu.com/p/307288925 怎么保证数据库一致性 方案2 MHA架构 MHA: Master Hight Availability Manager and Toolsfor MySQL 生产环境MySQL数据库集群MHA上线实施方案 MHA(Master High Availability Manager and Toolsfor MySQL)目前在Mysql高可用方面是一个相对成熟的解决方案。它是日本的一位MySQL专家采用Perl语言编写的一个脚本管理工具，该工具仅适用于MySQLReplication 环境，目的在于维持Master主库的高可用性。 MHA是基于标准的MySQL复制(异步/半同步)。 MHA是由管理节点(MHA Manager)和数据节点(MHA Node)两部分组成。 MHA Manager可以单独部署在一台独立机器,也可以部署在一台slave上。 方案3 MMM架构 MySQL-MMM实现MySQL高可用 MMM，全称为Master-Master replication manager for Mysql，是一套支持双主故障切换和双主日常管理的脚本程序，MMM使用Perl语言开发。主要用来监控和管理MySQL Master-Master(双)复制。特别适合DBA做维护等需要主从复制的场景，通过双主架构避免了重复搭建从库的麻烦。虽然叫做双主复制，但是业务上同一时刻只允许对一个主进行写入，另一台备选主上提供部分读服务，以加速在主主切换时备选主的预热 MMM优缺点 优点：高可用性，扩展性好，出现故障自动切换，对于主主同步，在同一时间只提供一台数据库写操作，保证的数据的一致性。 缺点：Monitor节点是单点，可以结合Keepalived实现高可用 方案4 DRBD架构 https://www.linbit.com/en/drbd-community/drbd-download/ 读写分离方案 客户端解决方案(应用层) 常用：DDL、 Sharding-Jdbc (常用shardding-jdbc) 优点： 程序自动完成，数据源方便管理 不需要维护，因为没用中间件 理论支持任何数据库 （sql标准） 缺点： 增加了开发成本、代码有入侵 不能做到动态增加数据源 程序员开发完成，运维参与不了。 中间件解决方案（代理层） 常用：mysql proxy、mycat、altas (常用mycat) 优点： 数据增加了都程序没用任何影响 应用层（程序）不需要管数据库方面的事情 增加数据源不需要重启程序 缺点： 程序依赖中间件，导致切换数据库变的困难 增加了proxy 性能下降 增加了维护工作、高可用问题 主从复制 Mysql为了保证数据库一致性，引入了集中复制类型，分别是 异步复制 半同步复制 全同步复制 主从复制整理分为以下三个步骤 主库将数据库的变更操作记录到Binlog日志文件(Master服务器对数据库更改操作记录在Binlog中，BinlogDump Thread接到写入请求后，读取Binlog信息推送给Slave的I/O Thread) 从库读取主库中的Binlog日志文件信息写入到从库的Relay Log中继日志中(Slave的I/O Thread将读取到的Binlog信息写入到本地Relay Log中) 从库读取中继日志信息在从库中进行Replay,更新从库数据信息(Slave的SQL Thread检测到Relay Log的变更请求，解析relay log中内容在从库上执行) 主库上并发的修改操作在从库上只能串行化执行，因为只有一个SQL线程来重放中继日志，这也是很多工作负载的性能瓶颈所在 异步复制 mysql主从复制存在的问题： 主库宕机后，数据可能丢失 从库只有一个SQL Thread，主库写压力大，复制很可能延时 半同步复制 为了提升数据安全，MySQL让Master在某一个时间点等待Slave节点的 ACK（Acknowledgecharacter）消息，接收到ACK消息后才进行事务提交，这也是半同步复制的基础，MySQL从5.5版本开始引入了半同步复制机制来降低数据丢失的概率。 介绍半同步复制之前先快速过一下 MySQL 事务写入碰到主从复制时的完整过程，主库事务写入分为 4个步骤： InnoDB Redo File Write (Prepare Write) Binlog File Flush &amp; Sync to Binlog File InnoDB Redo File Commit（Commit Write） Send Binlog to Slave 当Master不需要关注Slave是否接受到Binlog Event时，即为传统的主从复制。 当Master需要在第三步等待Slave返回ACK时，即为 after-commit（Master先提交，等Slave ACK以后再，回复客户端），半同步复制（MySQL 5.5引入）。 当Master需要在第二步等待 Slave 返回 ACK 时，即为 after-sync（Master先写binlog，等Slave ACK以后再提交），增强半同步（MySQL 5.7引入）。 下图是 MySQL 官方对于半同步复制的时序图，主库等待从库写入 relay log 并返回 ACK 后才进行Engine Commit。 MySQL Group Replication MySQL Group Replication是建立在已有MySQL复制框架的基础之上，通过新增Group Replication Protocol协议及Paxos协议的实现，形成的整体高可用解决方案 与原有复制方式相比，MGR主要增加了状态机一致性顺序复制和certify这两个环境 MySQL事务通过before_commit钩子进入MGR，before_commit位于MYSQL_BIN_LOG::commit()函数中，具体是在进入事务组提交MYSQL_BIN_LOG::ordered_commit()之前，这就意味着执行到before_commit这个钩子时，事务还未提交，产生的Binlog还未写入Binlog文件中，事务GTID还未产生 分布式数据库事务 分布式数据库实现分布式事务的主流方法还是2PC 如上图所示，当分布式事务提交时，会选择其中的一个数据分片作为协调者在所有数据分片上执行两阶段提交协议。由于所有数据分片都是通过 Paxos 复制日志实现多副本高可用的，当主副本发生宕机后，会由同一数据分片的备副本转换为新的主副本继续提供服务，所以可以认为参与者和协调者都是保证高可用不宕机的（多数派存活），绕开了协调者宕机的问题。 在参与者高可用的实现前提下，可以对协调者进行了“无状态”的优化。在标准的两阶段提交中，协调者要通过记录日志的方法持久化自己的状态，否则如果协调者和参与者同时宕机，协调者恢复后可能会导致事务提交状态不一致。但是如果我们认为参与者不会宕机，那么协调者并不需要写日志记录自己的状态。 所以在第一阶段所有参与者都回复prepare完成以后，即可以反馈事务提交成功，提升了2PC的效率 由于存在多副本，只要保证在prepare阶段，验证事务执行没有错误，协调者发出commit指令后，就可以乐观的认为，事务执行成功并反馈给事务发起者。相信commit消息会被多数副本收到，多数副本收到消息以后，剩下的就交给他们自己同步 在上图中（绿色部分表示写日志的动作），左侧为标准两阶段提交协议，用户感知到的提交时延是4次写日志耗时以及2次 RPC 的往返耗时；由于少了协调者的写日志耗时以及提前了应答客户端的时机，用户感知到的提交时延是1次写日志耗时以及1次 RPC 的往返耗时 参考链接 MySQL高可用集群方案 MySQL 主从复制","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"协议3-Quic","slug":"Protocol/协议3-Quic","date":"2022-02-09T07:20:49.000Z","updated":"2022-06-25T07:00:00.116Z","comments":true,"path":"2022/02/09/Protocol/协议3-Quic/","link":"","permalink":"http://xboom.github.io/2022/02/09/Protocol/%E5%8D%8F%E8%AE%AE3-Quic/","excerpt":"","text":"TCP协议在创建链接前会进行三次握手。如果增加传输层协议(TLS)，则握手次数更多 QUIC协议 可以在1到2个数据包(取决于连接服务是未知还是已知)内，完成连接的创建(包括TLS) QUIC 非常类似于在 UDP 上实现的 TCP + TLS + HTTP/2。相比于 TCP，流控功能在用户空间而不在内核空间，可以不受限于 CUBIC 或是 BBR，而是可以自由选择甚至根据应用场景自由调整优化 为什么不修改TCP协议? TCP 是在操作系统内核和中间件固件中实现的 这么好为什么没有大规模使用？ 可能会被路由封杀UDP 443端口（ 这正是QUIC 部署的端口）； UDP包过多，由于QS限定，会被服务商误认为是攻击，UDP包被丢弃； 无论是路由器还是防火墙目前对QUIC都还没有做好准备。 QUIC 优点 QUIC 与现有 TCP + TLS + HTTP/2 方案相比，有以下几点主要特征： 利用缓存，显著减少连接建立时间； 改善拥塞控制，拥塞控制从内核空间到用户空间； 没有 head of line 阻塞的多路复用； 前向纠错，减少重传； 连接平滑迁移，网络状态的变更不会影响连接断线 TCP 的拥塞控制实际上包含了四个算法：慢启动，拥塞避免，快速重传，快速恢复 QUIC 协议当前默认使用了 TCP 协议的 Cubic 拥塞控制算法，同时也支持 CubicBytes, Reno, RenoBytes, BBR, PCC 等拥塞控制算法。 从拥塞算法本身来看，QUIC 只是按照 TCP 协议重新实现了一遍，那么 QUIC 协议到底改进在哪些方面呢？ 可插拔 就是能够非常灵活地生效，变更和停止 应用程序层面就能实现不同的拥塞控制算法，不需要操作系统，不需要内核支持。而传统的 TCP 拥塞控制，必须要端到端的网络协议栈支持，才能实现控制效果。 即使是单个应用程序的不同连接也能支持配置不同的拥塞控制。能提供更加有效的拥塞控制。比如 BBR 适合，Cubic 适合； 程序不需要停机和升级就能实现拥塞控制的变更 单调递增的 Packet Number TCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达 QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 sequence number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。而 TCP 呢，重传 segment 的 sequence number 和原始的 segment 的 Sequence Number 保持不变，也正是由于这个特性，引入了 Tcp 重传的歧义问题 如上图，超时事件 RTO 发生后，客户端发起重传，然后接收到了 Ack 数据。由于序列号一样，这个 Ack 数据到底是原始请求的响应还是重传请求的响应呢？不好判断。 如果算成原始请求的响应，但实际上是重传请求的响应（上图左），会导致采样 RTT 变大。如果算成重传请求的响应，但实际上是原始请求的响应，又很容易导致采样 RTT 过小。 由于 Quic 重传的 Packet 和原始 Packet 的 Pakcet Number 是严格递增的，所以很容易就解决了这个问题 如上图所示，RTO 发生后，根据重传的 Packet Number 就能确定精确的 RTT 计算。如果 Ack 的 Packet Number 是 N+M，就根据重传请求计算采样 RTT。如果 Ack 的 Pakcet Number 是 N，就根据原始请求的时间计算采样 RTT，没有歧义性。 但是单纯依靠严格递增的 Packet Number 肯定是无法保证数据的顺序性和可靠性。QUIC 又引入了一个 Stream Offset 的概念。 即一个 Stream 可以经过多个 Packet 传输，Packet Number 严格递增，没有依赖。但是 Packet 里的 Payload 如果是 Stream 的话，就需要依靠 Stream 的 Offset 来保证应用数据的顺序。 假设 Packet N 丢失了，发起重传，重传的 Packet Number 是 N+2，但是它的 Stream 的 Offset 依然是 x，这样就算 Packet N + 2 是后到的，依然可以将 Stream x 和 Stream x+y 按照顺序组织起来，交给应用程序处理。 不允许 Reneging Reneging: 接收方丢弃已经接收并且上报给 SACK 选项的内容。TCP 协议不鼓励这种行为，但是协议层面允许这样的行为。主要是考虑到服务器资源有限，比如 Buffer 溢出，内存不够等情况。 Reneging 对数据重传会产生很大的干扰。因为 Sack 都已经表明接收到了，但是接收端事实上丢弃了该数据。 QUIC 在协议层面禁止 Reneging，一个 Packet 只要被 Ack，就认为它一定被正确接收，减少了这种干扰。 更多的ACK块 TCP 的 Sack 选项能够告诉发送方已经接收到的连续 Segment 的范围，方便发送方进行选择性重传。 由于 TCP 头部最大只有 60 个字节，标准头部占用了 20 字节，所以 Tcp Option 最大长度只有 40 字节，再加上 Tcp Timestamp option 占用了 10 个字节 [25]，所以留给 Sack 选项的只有 30 个字节。每一个 Sack Block 的长度是 8 个，加上 Sack Option 头部 2 个字节，也就意味着 Tcp Sack Option 最大只能提供 3 个 Block。 Quic Ack Frame 可同时提供 256 个 Ack Block，在丢包率比较高的网络下，更多的 Sack Block 可以提升网络的恢复速度，减少重传量。 Ack Delay 时间 Tcp 的 Timestamp 选项存在一个问题：只回显发送方的时间戳，但没有计算接收端接收到 segment 到发送 Ack 该 segment 的时间。这个时间可以简称为 Ack Delay 这样就会导致 RTT 计算误差。如下图： TCP 的 RTT 计算：RTT = timestamp2 - timestamp1 Quic 的RTT 计算：RTT = timestamp2 - timestamp1 - Ack Delay 当然RTT的具体计算需要采样，参考历史数据平滑计算 SRTT = SRTT + α(RTT - SRTT) RTO = β * SRTT + α * DevRTT 基于 stream 和 connection 级别的流量控制 QUIC 的流量控制类似 HTTP2，即在 Connection 和 Stream 级别提供了两种流量控制 Connection 可以类比一条 TCP 连接，Stream 可以认为就是一条 HTTP 请求。多路复用意味着在一条 Connetion 上会同时存在多条 Stream。既需要对单个 Stream 进行控制，又需要针对所有 Stream 进行总体控制。 QUIC 实现流量控制的原理比较简单： 通过 window_update 帧告诉对端自己可以接收的字节数，这样发送方就不会发送超过这个数量的数据。 通过 BlockFrame 告诉对端由于流量控制被阻塞了，无法发送数据。 QUIC 的流量控制和 TCP 有点区别，TCP 为了保证可靠性，窗口左边沿向右滑动时的长度取决于已经确认的字节数。如果中间出现丢包，就算接收到了更大序号的 Segment，窗口也无法超过这个序列号。 但 QUIC 不同，就算此前有些 packet 没有接收到，它的滑动只取决于接收到的最大偏移字节数 针对Stream: 可用窗口 = 最大窗口数 - 接收到的最大偏移数 针对Connection: 可用窗口 = stream1 可用窗口 + stream2 可用窗口+ …… + streamN 可用窗口 STGW 也在连接和 Stream 级别设置了不同的窗口数。可以在内存不足或者上游处理性能出现问题时，通过流量控制来限制传输速率，保障服务可用性 没有对头阻塞的多路复用 QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势，很大程度上缓解甚至消除了队头阻塞的影响。 QUIC 一个连接的多个 stream 之间没有依赖。假如 stream2 丢了一个 udp packet，不会影响 其他stream 的处理。 多路复用是 HTTP2 最强大的特性，能够将多条请求在一条 TCP 连接上同时发出去。但也恶化了 TCP 的一个问题，队头阻塞。 HTTP2 在一个 TCP 连接上同时发送 4 个 Stream。其中 Stream1 已经正确到达，并被应用层读取。但是 Stream2 的第三个 tcp segment 丢失了，TCP 为了保证数据的可靠性，需要发送端重传第 3 个 segment 才能通知应用层读取接下去的数据，虽然这个时候 Stream3 和 Stream4 的全部数据已经到达了接收端，但都被阻塞住了 不仅如此，由于 HTTP2 强制使用 TLS，还存在一个 TLS 协议层面的队头阻塞 上面两段存在歧义TODO ??? gRPC 基于 HTTP2 但是并没有TLS Record 是 TLS 协议处理的最小单位，最大不超过 16K，Nginx 默认的大小就是 16K。由于一个 record 必须经过数据一致性校验才能进行加解密，所以一个 16K 的 record，就算丢了一个字节，也会导致已接收的 15.99K 数据无法处理，因为不完整 那 QUIC 多路复用为什么能避免上述问题呢？ QUIC 最基本的传输单元是 Packet，不会超过 MTU 的大小，整个加密和认证过程都是基于 Packet 的，不会跨越多个 Packet。这样就能避免 TLS 协议存在的队头阻塞； Stream 之间相互独立，比如 Stream2 丢了一个 Pakcet，不会影响 Stream3 和 Stream4。不存在 TCP 队头阻塞 当然，并不是所有的 QUIC 数据都不会受到队头阻塞的影响，比如 QUIC 当前也是使用 Hpack 压缩算法 [10]，由于算法的限制，丢失一个头部数据时，可能遇到队头阻塞。 总体来说，QUIC 在传输大量数据时，比如视频，受到队头阻塞的影响很小。 为什么压缩之后就出现对头阻塞了？ TODO 加密认证的报文 TCP 协议头部没有经过任何加密和认证，所以在传输过程中很容易被中间网络设备篡改，注入和窃听。比如修改序列号、滑动窗口。这些行为有可能是出于性能优化，也有可能是主动攻击。 但是 QUIC 的 packet 可以说是武装到了牙齿。除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。 这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。 如下图所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。 连接迁移 一条 TCP 连接是由四元组标识的（源 IP，源端口，目的 IP，目的端口）。什么叫连接迁移呢？就是当其中任何一个元素发生变化时，这条连接依然维持着，能够保持业务逻辑不中断。当然这里面主要关注的是客户端的变化，因为客户端不可控并且网络环境经常发生变化，而服务端的 IP 和端口一般都是固定的。 比如大家使用手机在 WIFI 和 4G 移动网络切换时，客户端的 IP 肯定会发生变化，需要重新建立和服务端的 TCP 连接。 又比如大家使用公共 NAT 出口时，有些连接竞争时需要重新绑定端口，导致客户端的端口发生变化，同样需要重新建立 TCP 连接。 那 QUIC 是如何做到连接迁移呢？任何一条 QUIC 连接不再以 IP 及端口四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。 由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。 其他 此外，QUIC 还能实现前向冗余纠错，在重要的包比如握手消息发生丢失时，能够根据冗余信息还原出握手消息。 QUIC 还能实现证书压缩，减少证书传输量，针对包头进行验证等 QUIC原理 代码实现：https://github.com/lucas-clemente/quic-go 参考链接 http://www.52im.net/thread-2816-1-1.html https://docs.google.com/document/d/1F2YfdDXKpy20WVKJueEf4abn_LVZHhMUMS5gX6Pgjl4/edit# 网络编程懒人入门(十)：一泡尿的时间，快速读懂QUIC协议 让互联网更快：新一代QUIC协议在腾讯的技术实践分享 七牛云技术分享：使用QUIC协议实现实时视频直播0卡顿！ https://hungryturbo.com/HTTP3-explained/quic/为什么需要QUIC.html#回顾http-2 https://zhuanlan.zhihu.com/p/32553477","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"Quic","slug":"Quic","permalink":"http://xboom.github.io/tags/Quic/"}]},{"title":"Acwing-背包问题","slug":"Acwing/Acwing-背包问题","date":"2022-01-11T16:06:29.000Z","updated":"2022-01-15T11:58:29.366Z","comments":true,"path":"2022/01/12/Acwing/Acwing-背包问题/","link":"","permalink":"http://xboom.github.io/2022/01/12/Acwing/Acwing-%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/","excerpt":"","text":"01背包 题目 https://www.acwing.com/problem/content/description/2/ 1234567891011121314151617181920212223242526有 N 件物品和一个容量是 V 的背包。每件物品只能使用一次。第 i 件物品的体积是 vi，价值是 wi。求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。输出最大价值。输入格式第一行两个整数，N，V，用空格隔开，分别表示物品数量和背包容积。接下来有 N 行，每行两个整数 vi,wi，用空格隔开，分别表示第 i 件物品的体积和价值。输出格式输出一个整数，表示最大价值。数据范围0&lt;N,V≤10000&lt;vi,wi≤1000输入样例4 51 22 43 44 5输出样例：8 讲解 2.1 版本1 二维 12345678910111213141516171819202122#include&lt;bits/stdc++.h&gt;using namespace std;const int MAXN = 1005;int f[MAXN]; // int main() &#123; int n, m; cin &gt;&gt; n &gt;&gt; m; for(int i = 1; i &lt;= n; i++) &#123; int v, w; cin &gt;&gt; v &gt;&gt; w; // 边输入边处理 for(int j = m; j &gt;= v; j--) f[j] = max(f[j], f[j - v] + w); &#125; cout &lt;&lt; f[m] &lt;&lt; endl; return 0;&#125; 参考链接 https://www.acwing.com/solution/content/1374/","categories":[{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/categories/Acwing/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/tags/Acwing/"}]},{"title":"Acwing-链表问题","slug":"Acwing/Acwing-链表问题","date":"2022-01-11T16:05:08.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2022/01/12/Acwing/Acwing-链表问题/","link":"","permalink":"http://xboom.github.io/2022/01/12/Acwing/Acwing-%E9%93%BE%E8%A1%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"17.从尾到头打印链表 https://www.acwing.com/problem/content/18/ 题目 12345678910输入一个链表的头结点，按照 从尾到头 的顺序返回节点的值。返回的结果用数组存储。数据范围0≤ 链表长度 ≤1000。样例输入：[2, 3, 5]返回：[5, 3, 2] 解答 12345678910111213141516171819&#x2F;** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; *&#x2F;class Solution &#123;public: vector&lt;int&gt; printListReversingly(ListNode* head) &#123; vector&lt;int&gt; result; while(head) &#123; result.push_back(head-&gt;val); head &#x3D; head-&gt;next; &#125; return vector&lt;int&gt;(result.rbegin(), result.rend()); &#125;&#125;; 相关知识 - 反向迭代器 begin() 返回一个迭代器，它指向容器的第一个元素 end() 返回一个迭代器，它指向容器的最后一个元素的下一个位置 rbegin() 返回一个逆序迭代器，它指向容器的最后一个元素 rend() 返回一个逆序迭代器，它指向容器的第一个元素前面的位置 应用场景： 1234&#x2F;&#x2F;sorts v in &quot;normal&quot; ordersort(v.begin(), v.end());&#x2F;&#x2F;sorts in reversesort(v.rbegin(), v.rend()); 28.O(1)删除链表结点 题目 https://www.acwing.com/problem/content/85/ 123456789101112给定单向链表的一个节点指针，定义一个函数在O(1)时间删除该结点。假设链表一定存在，并且该节点一定不是尾节点。数据范围链表长度 [1,500]。样例输入：链表 1-&gt;4-&gt;6-&gt;8 删掉节点：第2个节点即6（头节点为第0个节点）输出：新链表 1-&gt;4-&gt;8 解答 123456789101112131415161718&#x2F;** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; *&#x2F;class Solution &#123;public: void deleteNode(ListNode* node) &#123; ListNode* nxt &#x3D; node-&gt;next; node-&gt;val &#x3D; nxt-&gt;val; node-&gt;next &#x3D; nxt-&gt;next; delete nxt; &#125;&#125;; 相关知识 12//1. auto: 在声明变量的时候可根据变量初始值的数据类型自动为该变量选择与之匹配的数据类型//2. delete TODO 29.删除链表重复节点 题目 1234567891011121314在一个排序的链表中，存在重复的节点，请删除该链表中重复的节点，重复的节点不保留。数据范围链表中节点 val 值取值范围 [0,100]。链表长度 [0,100]。样例1输入：1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5输出：1-&gt;2-&gt;5样例2输入：1-&gt;1-&gt;1-&gt;2-&gt;3输出：2-&gt;3 注意重复的节点也不保留 解答 123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplication(ListNode* head) &#123; ListNode * dummy = new ListNode(-1); //定义一个头指针(防止head为空) dummy-&gt;next = head; ListNode * pre = dummy; //双指针前指针 while(pre-&gt;next) &#123; ListNode * nxt = pre-&gt;next; //双指针尾指针 //一直循环，直到头指针与尾指针值不一致 //注意这里pre是从dummy开头的，而这题是要删除所有重复的节点，所以有时候pre—&gt;next需要一起删除 //所以这里每次都是比较 pre-&gt;next 与 nxt while(nxt &amp;&amp; pre-&gt;next-&gt;val == nxt-&gt;val) nxt = nxt-&gt;next; //经过上述流转出现 pre-&gt;next-&gt;val != nxt-&gt;val //1. 如果两者相邻，那么pre向后移动一个节点 //2. 如果两者不相邻，那么pre与nxt之间都是重复节点 if (pre-&gt;next-&gt;next == nxt) pre = pre-&gt;next; else pre-&gt;next = nxt; &#125; return dummy-&gt;next; &#125;&#125;; 重点：是每次使用 pre-&gt;next 与 nxt进行比较，那样即使删除也是删除pre-&gt;next 与 nxt之间的结点 33.链表中倒数第N个节点 题目 12345678910111213输入一个链表，输出该链表中倒数第 k 个结点。注意：k &gt;= 1;如果 k 大于链表长度，则返回 NULL;数据范围链表长度 [0,30]。样例输入：链表：1-&gt;2-&gt;3-&gt;4-&gt;5 ，k=2输出：4 解答 123456789101112131415161718192021222324252627/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* findKthToTail(ListNode* pListHead, int k) &#123; //使用快慢指针 ListNode *fast = pListHead; ListNode *slow = pListHead; //快指针先走k步，如果fast为空或者为空都没有走到k个，说明链表数目小于k for (int i = 0; i &lt; k; i++) &#123; if (fast) fast = fast-&gt;next; //这里k-1 是因为 i = 0 即为第一个节点 fast 需要走 i = k - 1 步 if (!fast &amp;&amp; i &lt; k - 1) return NULL; &#125; while (fast) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return slow; &#125;&#125;; 重点：快慢指针 34.链表中环的入口节点 题目 https://www.acwing.com/problem/content/86/ 12345678910111213141516给定一个链表，若其中包含环，则输出环的入口节点。若其中不包含环，则输出null。数据范围节点 val 值取值范围 [1,1000]。链表长度 [0,500]。样例如下图给定如上所示的链表：[1, 2, 3, 4, 5, 6]2注意，这里的2表示编号是2的节点，节点编号从0开始。所以编号是2的节点就是val等于3的节点。则输出环的入口节点3. 证明： 如上图所示，a 是起点，b 是环的入口，c 是两个指针的第一次相遇点，ab 距离 x，bc 距离是 y 存在快慢指针 first 与 second，其中second的速度是first的2倍 想法1： 当 first 走到 b 时， second 已经从 b 开始在环上走了 x 步，可能多余n圈，距离 b 还差 y 步 所以 second 走的路程 2x + y = x + n 圈 ==&gt; x + y = n 圈，那么从 c 走 x 步 也能到达b 想法2： 用 z 表示从 c 点顺时针走到 b 的距离。则第一次相遇时 second 所走的距离是 x+(y+z)∗n+y，其中n表示圈数，所以 12x+(y+z)∗n+y = 2(x+y) //second是 first路程的两倍x = (n-1) * (y+z) + z 所以其实 x == z ，相遇之后节点距离入环扣与链表头到入环扣距离相等 解答 12345678910111213141516171819202122232425262728293031323334/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *entryNodeOfLoop(ListNode *head) &#123; if (!head || !head-&gt;next) return NULL; //初始节点判断 //快慢指针 ListNode * first = head; ListNode * second = head; while (first &amp;&amp; second)&#123; //外循环防止节点为空，找到第一个相遇的地方 first = first-&gt;next; second = second-&gt;next; if(second) second = second-&gt;next; else return NULL; if (first == second) &#123; //当相遇则再次相遇说走的路程就是换入口处 first = head; while (first != second) &#123; first = first-&gt;next; second = second-&gt;next; &#125; return first; &#125; &#125; return NULL; &#125;&#125;; 重点是：从第一次相遇的地方走x步就能到达入环扣 35.反转链表 题目 123456789101112定义一个函数，输入一个链表的头结点，反转该链表并输出反转后链表的头结点。思考题：请同时实现迭代版本和递归版本。数据范围链表长度 [0,30]。样例输入:1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出:5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 解答 非递归 12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; ListNode * second = head; ListNode * first = nullptr; //头部空指针 while(second) &#123; ListNode * pre = second-&gt;next; second-&gt;next = first; first = second; second = pre; &#125; return first; &#125;&#125;; 递归 12345678910111213141516171819202122/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if(!head || !head-&gt;next ) return head; //1 - 2 - 3 - 4 ListNode *tail = reverseList(head-&gt;next); //反转head-&gt;next 链表, 原链表的尾节点tail //为什么不用记录前一个节点进行转换， //因为 递归中 head-&gt;next 已经经过了反转，后面只需要反转 head 与 head-&gt;next的关系就可以了 head-&gt;next-&gt;next = head; //将 这里直接使用 head-&gt;next-&gt;next 不用再声明变量 head-&gt;next = nullptr; // nullptr &lt;- 1 &lt;- 2 return tail; //其实这里一直返回的是原链表的尾节点 &#125;&#125;; 36.合并两个排序的链表 题目 https://www.acwing.com/problem/content/34/ 123456789输入两个递增排序的链表，合并这两个链表并使新链表中的结点仍然是按照递增排序的。数据范围链表长度 [0,500]。样例输入：1-&gt;3-&gt;5 , 2-&gt;4-&gt;5输出：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;5 解答 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* merge(ListNode* l1, ListNode* l2) &#123; ListNode * dummy = new ListNode(-1); //新建头部的保护结点dumm ListNode * cur = dummy; //当前指针 while(l1 &amp;&amp; l2) &#123; if(l1-&gt;val &gt; l2-&gt;val) &#123; cur-&gt;next = l2; l2 = l2-&gt;next; &#125;else&#123; cur-&gt;next = l1; l1 = l1-&gt;next; &#125; cur = cur-&gt;next; &#125; //二元判断，防止两个链表数目不一致 cur-&gt;next = (l1 != nullptr ? l1 : l2); return dummy-&gt;next; &#125;&#125;; 两个链表各遍历一次，所以时间复杂度为O(n) 48. 复杂链表的复刻 题目 https://www.acwing.com/problem/content/89/ 123456789请实现一个函数可以复制一个复杂链表。在复杂链表中，每个结点除了有一个指针指向下一个结点外，还有一个额外的指针指向链表中的任意结点或者null。注意：函数结束后原链表要与输入时保持一致。数据范围链表长度 [0,500]。 解答 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Definition for singly-linked list with a random pointer. * struct ListNode &#123; * int val; * ListNode *next, *random; * ListNode(int x) : val(x), next(NULL), random(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *copyRandomList(ListNode *head) &#123; //开始遍历添加cur的复制点np ListNode * p = head; while(p) &#123; //1 - 2 --&gt; 1 - 1 - 2 复制节点并添加到节点后面 auto * np = new ListNode(p-&gt;val); auto * nxt = p-&gt;next; p-&gt;next = np; np-&gt;next = nxt; p = nxt; &#125; //如果存在 cur 有 random 则 p-&gt;next 是被复制的节点 p = head; while (p) &#123; if(p-&gt;random) //p 是旧节点,则 p-&gt;next 是新节点 p-&gt;next-&gt;random = p-&gt;random-&gt;next; //这里也是随机指定，和原来的不一定相等 p = p-&gt;next-&gt;next; &#125; ListNode * dummy = new ListNode(-1); //保护头节点 ListNode * cur = dummy; p = head; while(p) &#123; //1 - 1 - 2 - 2 - 3 - 3 cur-&gt;next = p-&gt;next; //新链表 cur = cur-&gt;next; p-&gt;next = p-&gt;next-&gt;next; //恢复旧指针 p = p-&gt;next; &#125; return dummy-&gt;next; &#125;&#125;; 重点是：p-&gt;next-&gt;random = p-&gt;random-&gt;next; //如果有random，则它是原节点。将新节点指一个random 49.二叉搜索树与双向链表 https://www.acwing.com/problem/content/87/ 题目 1234567891011输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。注意：需要返回双向链表最左侧的节点。例如，输入上图中左边的二叉搜索树，则输出右边的排序双向链表。数据范围树中节点数量 [0,500]。 解答 在中序递归遍历的基础，用一个pre指针保存中序遍历的前一个结点。遍历顺序就是双线链表的建立顺序； 每一个结点访问时它的左子树肯定被访问过了，所以放心大胆的改它的left指针，不怕树断掉； 同理，pre指向的结点保存的数肯定小于当前结点，所以其左右子树肯定都访问过了，所以其right指针也可以直接改。 最后需要一直向左找到双向链表的头结点。 1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode * pre = NULL; //pre记录当前节点指针 TreeNode* convert(TreeNode* root) &#123; dfs(root); while(root &amp;&amp; root-&gt;left) root = root-&gt;left; //遍历找到第一个节点 return root; &#125; //深度优先遍历 void dfs(TreeNode* root) &#123; if(!root) return; dfs(root-&gt;left); root-&gt;left = pre; //左节点就是前节点 if(pre) pre-&gt;right = root; //前节点的后继节点就是自己 pre = root; dfs(root-&gt;right); &#125;&#125;; 重点是：利用深度优先遍历 + pre节点记录 66. 两个链表的第一个公共结点 题目 https://www.acwing.com/problem/content/62/ 12345678910111213141516输入两个链表，找出它们的第一个公共结点。当不存在公共节点时，返回空节点。数据范围链表长度 [1,2000]。样例给出两个链表如下所示：A： a1 → a2 ↘ c1 → c2 → c3 ↗ B: b1 → b2 → b3输出第一个公共节点c1 不要用两层循环暴力破解！！！ 解答 不同部分为a和b，公共部分为c；a + c + b = b + c + a;让两个一起走，a走到头就转向b， b走到头转向a，则在公共部分相遇 12345678910111213141516171819202122232425/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode *findFirstCommonNode(ListNode *headA, ListNode *headB) &#123; ListNode * A = headA; ListNode * B = headB; while(A != B) &#123; //A遍历完开始走到B if(A) A = A-&gt;next; else A = headB; //B遍历完开始走到A if(B) B = B-&gt;next; else B = headA; &#125; return A; &#125;&#125;; 826. 单链表 题目 1234567891011121314151617181920212223242526272829303132333435363738实现一个单链表，链表初始为空，支持三种操作：向链表头插入一个数；删除第 k 个插入的数后面的数；在第 k 个插入的数后插入一个数。现在要对该链表进行 M 次操作，进行完所有操作后，从头到尾输出整个链表。注意:题目中第 k 个插入的数并不是指当前链表的第 k 个数。例如操作过程中一共插入了 n 个数，则按照插入的时间顺序，这 n 个数依次为：第 1 个插入的数，第 2 个插入的数，…第 n 个插入的数。输入格式第一行包含整数 M，表示操作次数。接下来 M 行，每行包含一个操作命令，操作命令可能为以下几种：H x，表示向链表头插入一个数 x。D k，表示删除第 k 个插入的数后面的数（当 k 为 0 时，表示删除头结点）。I k x，表示在第 k 个插入的数后面插入一个数 x（此操作中 k 均大于 0）。输出格式共一行，将整个链表从头到尾输出。数据范围1≤M≤100000所有操作保证合法。输入样例：10H 9I 1 1D 1D 0H 6I 3 6I 4 5I 4 5I 3 4D 6输出样例：6 4 6 5 解答 静态链表 1451. 单链表快速排序 题目 12345678910111213给定一个单链表，请使用快速排序算法对其排序。要求：期望平均时间复杂度为 O(nlogn)，期望额外空间复杂度为 O(logn)。思考题： 如果只能改变链表结构，不能修改每个节点的val值该如何做呢？数据范围链表中的所有数大小均在 int 范围内，链表长度在 [0,10000]。输入样例：[5, 3, 2]输出样例：[2, 3, 5] 解答 首先复习以下快速排序 123456789101112131415161718192021222324252627282930313233343536//用于默写test中国暖的算法#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e6 + 1;int nums[N]; //用于存储数据int n;void quick_sort(int nums[], int l, int r)&#123; if(l &gt;= r) return; int mid = nums[l + r &gt;&gt; 1]; int i = l - 1; int j = r + 1; while(i &lt; j) &#123; do i++; while (nums[i] &lt; mid); do j--; while (nums[j] &gt; mid); if (i &lt; j) swap(nums[i], nums[j]); //表示num[i] 在左边但是数据却比 num[j] 大 &#125; quick_sort(nums, l, i); quick_sort(nums, i + 1, r); return;&#125;int main()&#123; scanf(\"%d\", &amp;n); for(int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;nums[i]); quick_sort(nums, 0, n - 1); for(int i = 0; i &lt; n; i++) printf(\"%d \", nums[i]); return 0;&#125; 则链表排序为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e5 + 1;int n;struct ListNode&#123; int val; ListNode *next; ListNode(int x) : val(x), next(NULL) &#123;&#125;&#125;;void quickSort(ListNode *head, ListNode *tail)&#123; if (head != tail) &#123; int key = head-&gt;val; ListNode *p = head, *q = p-&gt;next; while (q != tail) &#123; if (q-&gt;val &lt; key) &#123; p = p-&gt;next; swap(p-&gt;val, q-&gt;val); &#125; q = q-&gt;next; &#125; if (p != head) swap(head-&gt;val, p-&gt;val); quickSort(head, p); quickSort(p-&gt;next, tail); &#125;&#125;ListNode *quickSortList(ListNode *head)&#123; if (!head) return head; quickSort(head, NULL); return head;&#125;int main()&#123; ListNode * head = new ListNode(-1); ListNode * cur = head; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; int val; scanf(\"%d\", &amp;val); ListNode * node = new ListNode(val); cur-&gt;next = node; cur = cur-&gt;next; &#125; ListNode * result = quickSortList(head-&gt;next); while (result) &#123; printf(\"%d\", result-&gt;val); result = result-&gt;next; &#125; return 0;&#125; 1560. 反转链表 题目 https://www.acwing.com/problem/content/1562/ 1234567891011121314151617181920212223242526272829303132333435363738394041给定一个常数 K 和一个单链表 L，请你在单链表上每 K 个元素做一次反转，并输出反转完成后的链表。如果链表最后一部分不足 K 个元素，则最后一部分不翻转。例如，假设 L 为 1→2→3→4→5→6，如果 K=3，则你应该输出 3→2→1→6→5→4；如果 K=4，则你应该输出 4→3→2→1→5→6。### 补充1、本题中可能包含不在链表中的节点，这些节点无需考虑。### 输入格式第一行包含头节点地址，总节点数量 N 以及常数 K。节点地址用一个 5 位非负整数表示（可能有前导 0），NULL 用 −1 表示。接下来 N 行，每行描述一个节点的信息，格式如下：`Address Data Next`其中 Address 是节点地址，Data 是一个整数，Next 是下一个节点的地址。### 输出格式将重新排好序的链表，从头节点点开始，依次输出每个节点的信息，格式与输入相同。### 数据范围1≤N≤105,1≤K≤N### 输入样例：00100 6 400000 4 9999900100 1 1230968237 6 -133218 3 0000099999 5 6823712309 2 33218### 输出样例：00000 4 3321833218 3 1230912309 2 0010000100 1 9999999999 5 6823768237 6 -1 参考链接 https://www.acwing.com/problem/","categories":[{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/categories/Acwing/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/tags/Acwing/"}]},{"title":"MQ-Kafka4-日志存储","slug":"MQ/MQ-Kafka4-日志存储","date":"2022-01-06T15:35:06.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka4-日志存储/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka4-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/","excerpt":"","text":"文件目录 为了防止Log过大，Kafka映日了日志分段(LogSegment)的概念，将Log 切分为多个LogSegment。 Log在物理上只已文件夹的形式存储，而每个LogSegment对应于磁盘上的一个日志文件和两个索引文件，以及可能的其他文件(比如以&quot;.txnindex&quot;为后缀的事务索引文件) Log对应的是一个命名形式为 &lt;topic&gt;-&lt;partition&gt;的文件夹，假设有一个名为 &quot;topic-log&quot;的主题，此主题具有4个分区，那么实际物理存储上表现为：“topic-log-0”、“topic-log-1”、“topic-log-2”、“topic-log-3” 向Log 中追加消息时是顺序写入的，只有最后一个LogSegment 才能执行写入操作，在此之前所有的LogSegment 都不能写入数据 为了方便描述，将最后一个LogSegment 称为activeSegment ，表示当前活跃的日志分段。随着消息的不断写入，当 activeSegment 满足一定的条件时，就创建新的activeSegment，并将消息追加到新的activeSegment 为了便于消息的检索，每个LogSegment 中的日志文件（以“ .log ”为文件后缀）都有对应的两个索引文件： 偏移量索引文件，以“ .index ”为文件后缀 时间戳索引文件，以“ .timeindex ”为文件后缀 每个LogSegment 都有一个基准偏移量baseOffset，用来表示当前LogSegment中第一条消息的offset 。偏移量是一个64 位的长整型数，日志文件和两个索引文件都是根据基准偏移量（ baseOffset ）命名的，名称固定为20 位数字，没有达到的位数则用0 填充。比如第一个LogSegment 的基准偏移量为0 ，对应的日志文件为00000000000000000000.log 第2 个 LogSegment 对应的基准位移是133 ，也说明了该LogSegment 中的第一条消息的偏移量为133 ＇同时可以反映出第一个LogSegment 中共有133 条消息(偏移量从0 至132的消息） 消费者提交的位移是保存在Kafka 内部的主题consumer offsets中的，初始情况下这个主题并不存在，当第一次有消费者消费消息时会自动创建这个主题 Kafka文件目录布局 每一个根目录都会包含最基本的4个检查点文件（ xxx-checkpoint ）和meta.propties 文件。在创建主题的时候，如果当前broker中不止配置了一个根目录，那么会挑选分区数最少的那个根目录来完成本次创建任务 日志格式 消息集称为 Record Batch，其内部可包含一条或多条消息 在消息压缩的情形下， Record Batch Header 部分（参见图5-7 左部， 从first offset 到 records count 字段）是不被压缩的，而被压缩的是records 字段中的所有内容。生产者客户端中的ProducerBatch 对应这里的RecordBatch,而ProducerRecord 对应这里的Record Record Record包含： length ：消息总长度。 attributes ： 弃用，但还是在消息格式中占据1B 的大小， 以备未来的格式扩展。 timestamp delta ： 时间戳增量。通常一个time stamp 需要占用8 个字节，如果像这里一样保存与RecordBatch 的起始时间戳的差值，则可以进一步节省占用的字节数。 offset delta ： 位移增量。保存与RecordBatc h 起始位移的差值，可以节省占用的字节数 headers ：这个字段用来支持应用级别的扩展，包含key和value ，一个Record 里面可以包含0 至多个Header RecodeBatch包含： first offset ：表示当前RecordBatch 的起始位移。 length ：计算从partition leader epoeh 字段开始到末尾的长度。 partition leader epoeh ：分区leader 纪元，可以看作分区leader 的版本号或更新次数 magic ：消息格式的版本号，对v2 版本而言， magie 等于2 。 attributes ：消息属性(2B) 低 3 位表示压缩格式， 第4 位表示时间戳类型； 第5 位表示此RecordBatch 是否处于事务中，0 表示非事务， l 表示事务。 第6 位表示是否是控制消息(ControlBatch)。0 表示非控制消息，而 1 表示是控制消息，控制消息用来支持事务功能。 last offset delta: RecordBatch 中最后一个Record 的offset 与自rst offset 的差值。主要被broker 用来确保RecordBatch 中Record 组装的正确性。 first timestamp: RecordBatch 中第一条Record 的时间戳。 max timestamp: RecordBatch 中最大的时间戳， 一般情况下是指最后一个Record的时间戳，和last offset delta 的作用一样，用来确保消息组装的正确性。 produeer id: PID ，用来支持幂等和事务 日志索引 每个日志分段文件对应了两个索引文件，主要用来提高查找消息的效率。 偏移量索引文件用来建立消息偏移量（ offset ）到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置； 时间戳索引文件则根据指定的时间戳（ timestamp ）来查找对应的偏移量信息 Kafka 中的索引文件以 稀疏索引(sparse index)的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引页。每当写入一定量（由broker 端参数log.index.interval.bytes 指定，默认值为4096 ，即4KB ）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小log.index.interval.bytes的值，对应地可以增加或缩小索引项的密度。 稀疏索引通过 MappedByteBuffer 将索引文件映射到内存中，以加快索引的查询速度。 偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。 时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。 当日志分段达到一定条件则创建新的日志分段，一定条件包括： 当前日志分段文件的大小超过了broker 端参数log.segment.bytes 配置的值，默认值1GB 当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.hours参数配置的值。如果同时配置了log.roll.ms 和log.roll.hours 参数，那么log.roll.ms 的优先级高。默认情况下，只配置了log.roll.hours 参数，其值为168，即7天。 偏移量索引文件或时间戳索引文件的大小达到broker 端参数 log.index.size.max.bytes，默认10M 追加的消息的偏移量与当前日志分段的偏移量之间的差值大于Integer.MAX_VALUE,即offset - baseOffset &gt; Integer.MAX_VALUE 对非当前活跃的日志分段而言，其对应的索引文件内容己经固定而不需要再写入索引项，所以会被设定为只读,而对当前活跃的日志分段C activeSegment ）而言，索引文件还会追加更多的索引项，所以被设定为可读写 在索引文件切分的时候， Kafka 会关闭当前正在写入的索引文件并置为只读模式，同时以可读写的模式创建新的索引文件 Kafka 在创建索引文件的时候会为其预分配log.index.size.max.bytes 大小的空间，注意这一点与日志分段文件不同，只有当索引文件进行切分的时候， Kafka 才会把该索引文件裁剪到实际的数据大小。也就是说，与当前活跃的日志分段对应的索引文件的大小固定为log.index.size.max.bytes，而其余日志分段对应的索引文件的大小为实际的占用空间。 偏移量索引 偏移量索引项的格式如下图，每个索引项占用8 个字节，分为两个部分。 relativeOffset：相对偏移量(4B)，表示消息相对于baseOffset 的偏移量，当前索引文件的文件名即为baseOffset 的值。 position：物理地址(4B)，也就是消息在日志分段文件中对应的物理位置 消息的偏移量(offset)占用8 个字节，也称绝对偏移量。索引项中没有直接使用绝对偏移量而改为只占用4 个字节的相对偏移量CrelativeOffset =offset - baseOffset，这样可以减小索引文件占用的空间。 举个例子， 一个日志分段的baseOffset 为32 ，则文件名是00000000000000000032.log，offset 为35 的消息在索引文件中的relativeOffset 的值为35 - 32=3 如果我们要查找偏移量为23 的消息，那么应该怎么做呢？ 首先通过二分法在偏移量索引文件中找到不大于23 的最大索引项，即［ 22 , 656 ］， 然后从日志分段文件中的物理位置656 开始顺序查找偏移量为23 的消息。 那么用户又是如何查找日志分段的呢？ 并不是顺序查找，而是使用跳跃表的结构。Kafka的每个日志对象中使用了 ConcurrentSkipListMap来保存各个日志分段，每个日志分段的baseOffset 作为key,这样可以根据指定偏移量快速定位到消息所在的日志分段 Kafka 强制要求索引文件大小必须是索引项大小的整数倍，对偏移量索引文件而言，必须为8 的整数倍 时间戳索引 每个索引项占用12 个字节，分为两个部分 timestamp ： 当前日志分段最大的时间戳。 relativeOffset ：时间戳所对应的消息的相对偏移量 时间戳索引文件中包含若干时间戳索引项， 每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的timestamp ，否则不予追加 与偏移量索引文件相似，时间戳索引文件大小必须是索引项大小(12B)的整数倍 会在偏移量索引文件和时间戳索引文件中分别增加一个偏移量索引项和时间戳索引项。两个文件增加索引项的操作是同时进行的，但并不意味着偏移量索引中的relativeOffset 和时间戳索引项中的relativeOffset 是同一个值 如果一个失败了怎么办？ 为什么会出现不是同一个值的情况 如果要查找指定时间戳 targetTimeStamp = 1526384718288 开始的消息，首先是找到不小于 指定时间戳的日志分段。这里就无法使用跳跃表来快速定位到相应的日志分段了， 需要分以下 几个步骤来完成。 步骤1 ： 将 targetTimeStamp 和每个日志分段中的最大时间戳 largestTimeStamp 逐一对比，直到找到不小于targetTimeStamp 的 largestTimeStamp 所对应的日志分段。日志分段中的 largestTimeStamp 的计算是先查询该日志分段所对应的时间戳索引文件，找到最后一条索引项，若最后一条索引项的时间戳字段值大于0，则取其值，否则取该日志分段的最近修改时间。 步骤2 ： 找到相应的日志分段之后，在时间戳索引文件中使用二分查找算法查找到不大于targetTimeStamp 的最大索引项，即［152638478283, 28］，如此便找到了一个相对偏移量28 。 步骤3 ： 在偏移量索引文件中使用二分算法查找到不大于28 的最大索引项，即［26, 838 ] 步骤4 ：从步骤1中找到日志分段文件中的838 的物理位置开始查找不小于targetTimeStamp的消息 日志清理 Kafka 提供了两种日志清理策略 日志删除(Log Retention)：按照一定的保留策略直接删除不符合条件的日志分段 日志压缩(Log Compaction)：针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本 通过broker端参数 log.cleanup.policy 来设置日志清理策略 “delete”：即采用日志删除的清理策略 “compact”: 即采用日志压缩的清理策略 “delete,compact”：同时迟滞日志删除和日志压缩两种策略 日志清理 在Kafka的日志管理器中有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过 broker 端参数 log.retention.check.interval.ms配置，默认5 分钟。当前日志分段的保留策略有3 种： 基于时间的保留策略 基于日志大小的保留策略 基于日志起始偏移量的保留策略 基于时间 日志删除任务 检查日志文件中是否有保留时间超过设定的阀值(retentionMs)来寻找可删除的日志分段文件集合(deletableSegments)，retentionMs 可以通过broker端参数log.retention.hours、log.retention.minutes 和log.retention.ms 来配置，默认7 天 查找过期的日志分段文件，并不是简单地根据日志分段的最近修改时间lastModifiedTime来计算的， 而是通过日志分段对应的时间戳索引文件，找出最后一条索引项(如果不大于0，则取最近修改时间lastModifiedTime) 如果所有的日志分段都己过期， 但该日志文件中还要有一个日志分段用于接收消息的写入，即必须要保证有一个活跃的日志分段acti veSegment ，在此种情况下，会先切分出一个新的日志分段作为activeSegment ， 然后执行删除操作 删除日志分段步骤 首先会从Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。 然后将日志分段所对应的所有文件添加上 .deleted 的后缀（当然也包括对应的索引文件） 。 最后交由一个以 delete-file 命名的延迟任务来删除这些以 .deleted为后缀的文件，这个任务的延迟执行时间可以通过 file.delete.delay.ms 参数默认1 分钟 基于日志大小 日志删除任务会检查当前日志的大小是否超过设定的阔值(retentionSize)来寻找可删除的日志分段的文件集合(deletableSegments)，retentionSize 可以通过broker 端参数log.retention.bytes 来配置，默认值为 -1，表示无穷大 注意 log.retention.bytes 配置的是 Log 中所有日志文件的总大小，而不是单个日志分段（确切地说应该为.log 日志文件）的大小。单个日志分段的大小由broker 端参数 log.segment.bytes 来限制，默认值为1GB 基于日志大小的保留策略与基于时间的保留策略类似 基于日志起始偏移量 基于日志起始偏移量的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量 baseOffset 是否小于等于logStartOffset，若是，则可以删除此日志分段。 如图，假设 logStartOffset 等于25，日志分段 1 的起始偏移量为0，日志分段2 的起始偏移量为11，日志分 段3 的起始偏移量为23 ，通过如下动作收集可删除的日志分段的文件集合deletableSegments : 从头开始遍历每个日志分段，日志分段 1 的下一个日志分段的起始偏移量为11 ，小于 logStartOffset 的大小，将日志分段 1 加入deletableSegments 。 日志分段2 的下一个日志偏移量的起始偏移量为23 ，也小于logStartOffset 的大小，将日志分段2 页加入deletableSegments 日志分段3 的下一个日志偏移量在 logStartOffset 的右侧，故从日志分段3 开始的所有日志分段都不会加deletableSegments logStartOffset是怎么来的？ 一般情况下：日志文件的起始偏移量 logStartOffset 等于第一个日志分段的baseOffset，但可以通过脚本或请求进行修改 日志压缩 如果只关心 key 对应的最新 value 值，则可以开启Kafka 的日志清理功能，Kafka 会定期将相同 key 的消息进行合井，只保留最新的value值 注意区分日志压缩与消息压缩 Log Compaction 执行前后，日志分段中的每条消息的偏移量和写入时的偏移量保持一致。Log Compaction 会生成新的日志分段文件，日志分段中每条消息的物理位置会重新按照新文件来组织 拉取状态是客户端保存的，这个时候如果进行了日志压缩，是否导致乱序？ 如何对日志文件中消息的Key进行筛选操作？ 每个日志清理线程都会使用 SkimpyOffsetMap的对象来构建key与offset的映射关系的哈希表 日志清理需要遍历两次日志文件 第一次：遍历把每个key的哈希值和最后出现的offset都保存在SkimpyOffsetMap中 第二次：检查每个消息的偏移量在Map中是否一样，否则就清理 墓碑消息是什么？ 执行日志压缩之后，日志分段的大小会比原来小，如何防止出现大量小文件？ 清理过程中并不对单个的日志分段进行单独清理，而是将日志文件中 offset 从 0 - firstUncleanableOffset的所有日志进行分组。每组中日志分段占用空间大小之和不超过 segmentSize( log.segment.bytes)，清理后生成一个新的日志分段 磁盘存储 在印象中，磁盘的速率要远低于内存，其实这要看我们怎么样使用磁盘。顺序写盘的速度不仅比随机写盘的速度快，而且也比随机写内存的速度快 Kafka在设计时候采用了文件追击的方式来写入消息，只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作 日志压缩是不是破坏了顺序写盘？ 页缓存 页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘I/O 的操作。其实是将磁盘中的数据缓存到内存中，将对磁盘的访问变为内存的访问 当进程准备读取磁盘上的文件时，操作系统会先查看待读取的数据所在的页(page)是否在页缓存(pagecache)中 如果存在(命中)则直接返回数据 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。 同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在， 则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性 Linux 操作系统中的vm.dirty background ratio 参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页， 一般设置为小于10 的值即可 Kafka 中大量使用了页缓存，这是Kafka 实现高吞吐的重要因素之一。 Kafka 中也提供了同步刷盘及间断性强制刷盘(fsync)的功能，这些功能可通过log.flush.interval.messages 、log.flush.int erval.ms 等参数来控制，但不建议使用，会严重影响性能，消息的可靠性应该由多副本机制保证 另外Linux会使用磁盘的一部分做为swap分区，非活跃进行调入swap分区，把进程空出来让活跃的进程使用，Kafka也应该尽量避免使用 vm.swappiness vm.swappiness 100 表示积极使用 0 表示任何时候都不要发生交换 磁盘I/O流程 磁盘IO四种场景如下： 用户调用IO操作接口，数据流为：应用程序buffer -&gt; C 库标准IObuffer -&gt; 文件系统页缓存 -&gt; 通过具体文件系统到磁盘 用户调用文件I/O，数据流为：应用程序buffer -&gt; 文件系统页缓存 -&gt; 通过具体文件系统到磁盘 用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘 用户使用类似dd工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘。 零拷贝 参考链接 《深入理解Kafka核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka3-主题与分区","slug":"MQ/MQ-Kafka3-主题与分区","date":"2022-01-06T12:43:02.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka3-主题与分区/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka3-%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA/","excerpt":"","text":"主题作为消息的归类，可以再细分为一个或多个分区，分区则可看作对消息的二次归类。 分区的划分不仅为Kafka 提供了可伸缩性、水平扩展的功能，还通过多副本机制提高数据可靠性 主题与分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段(LogSegment)，每个日志分段还可以细分为索引文件、日志存储文件和快照文件等 分区的管理 优先副本的选举 分区使用多副本机制来提升可靠性，但只有leader 副本对外提供读写服务，而follower 副本只负责在内部进行消息的同步 对同一个分区而言， 同一个broker 节点中不可能出现它的多个副本， 即Kafka 集群的一个broker 中最多只能有它的一个副本， 可以将leader 副本所在的broker 节点叫作分区的leader 节点，而follower副本所在的broker 节点叫作分区的follower 节点。 在创建主题的时候，该主题的分区及副本会尽可能均匀地分布到Kafka 集群的各个broker节点上，对应的leader 副本的分配也比较均匀。 将leader 副本所在的broker 节点叫作分区的leader 节点，而follower副本所在的broker 节点叫作分区的follower 节点。如果Kafka中leader副本过于集中在同一个节点上，集群会出现负载失衡的情况。 为此，Kafka 引入了优先副本(preferred replica) : 指在AR 集合列表中的第一个副本。也可以称之为preferred leader 。Kafka 要确保所有主题的优先副本在Kafka 集群中均匀分布，这样就保证了所有分区的leader 均衡分布。 所谓的优先副本的选举是指通过一定的方式促使优先副本选举为leader 副本，以此来促进集群的负载均衡， 这一行为也可以称为“分区平衡” 分区平衡并不意味着Kafka 集群的负载均衡！！！ 因为还要考虑集群中的分区分配是否均衡。每个分区的leader 副本的负载也是各不相同的 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的broker节点，计算每个broker 节点的分区不平衡率（ broker 中的不平衡率＝非优先副本的leader 个数／分区总数）是否超过leader .mbalance.per.broker.percentage 参数配置的比值，默认值为10% ，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。执行周期由参数leader.Imbalance.check .interval . seconds 控制，默认值为3 00 秒 分区重分配 当集群中的一个节点突然若机下线时， 如果节点上的分区是单副本的，这些分区就变得不可用了，在节点恢复前，相应的数据也就处于丢失状态； 如果节点上的分区是多副本的，位于这个节点上的 leader 副本的角色会转交到集群的其他follower副本中。 总而言之，这个节点上的分区副本都已经处于功能失效的状态， Kafka 并不会将这些失效的分区副本自动地迁移到集群中剩余的可用broker 节点上，如果放任不管，则不仅会影响整个集群的均衡负载，还会影响整体服务的可用性和可靠性 分区重分配的基本原理是 第一步：添加新副本(增加副本因子) 第二步：新副本将从分区的 leader 副本那里复制所有的数据。根据分区的大小不同，复制过程可能需要花一些时间，因为数据是通过网络复制到新副本上的。 第三步：在复制完成之后，控制器将旧副本从副本清单里移除(恢复为原先的副本因子数)注意在重分配的过程中要确保有足够的空间。 分区重分配的量如果太大必然会严重影响整体的性能，对副本间的复制流量加以限制来保证重分配期间整体服务不会受太大的影响，复制限流有两种实现方式： kafka-config. sh 脚本和kafka-reassign-partitions .sh 脚本 参考链接 《深入理解Kafka 核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka2-生产者与消费者","slug":"MQ/MQ-Kafka2-生产者与消费者","date":"2022-01-05T17:03:04.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka2-生产者与消费者/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka2-%E7%94%9F%E4%BA%A7%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E8%80%85/","excerpt":"","text":"生产者 如下图是生产者客户端的整体架构 整个生产者客户端由两个线程协调运行，这两个线程分别为 主线程：由 Producer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（ RecordAccumulator ，也称为消息收集器〉中 Sender 线程（发送线程）：负责从RecordAccumulator 中获取消息并将其发送到Kafka 中 RecordAccumulator 主要用来缓存消息以便批量发送，进而减少网络传输的资源消耗。RecordAccumulator 缓存的大小可通过客户端参数 buffer.memory 配置，默认32MB。 当生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候KafkaProducer 的send()方法调用要么被阻塞，要么抛出异常，这个取决于参数 max.block.ms 的配置，此参数的默认值60 秒 重要的参数 acks(字符串类型)：用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的 acks = 1(默认)：生产者发送消息之后，只要分区的leader 副本成功写入消息，那么它就会收到来自服务端的成功响应。 如果消息写入leader 副本并返回成功响应给生产者，且在被其他fo llower 副本拉取之前leader 副本崩溃，那么此时消息还是会丢失，因为新选举的leader 副本中并没有这条对应的消息。 Acks = 0：生产者发送消息之后不需要等待任何服务端的响应 Acks = -1：生产者在消息发送之后，需要等待ISR 中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应 max.request.size：限制生产者客户端能发送的消息的最大值，默认值为1048576B ，即1MB retries 和retry. backoff.ms：retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作，retry. backoff.ms 则用来设置两次重试之间的间隔 compression.type：压缩方式 connections.max.idle.ms：指定在多久之后关闭闲置的连接，默认值是540000 ( ms ） ，即9 分钟 linger.ms：指定生产者发送ProducerBatch 之前等待更多消息（ ProducerRecord ）加入Producer Batch 的时间，默认值为0，生产者客户端会在ProducerBatch 被填满或等待时间超过 linger.ms 值时发迭出去 recvive.buffer.bytes：设置Socket接收消息的缓冲区(SO_RECBUF)的大小，默认值32KB send.buffer.bytes：设置Socket发送消息的缓冲区(SO_RECBUF)的大小，默认值128KB request.time.ms：配置Producer等待请求响应的最长时间，默认值为3000(ms) 消费者 消费者(Consumer)负责订阅Kafka 中的主题(Topic)，并从订阅的主题上拉取消息，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 主题中共有4 个分区(Partition) : PO 、Pl 、P2 、P3 。有两个消费组A和B 都订阅了这个主题，消费组A 中有4 个消费者(CO 、Cl 、C2 和C3)，消费组B 中有2个消费者(C4 和CS ） 。按照Kafka 默认的规则，最后的分配结果是消费组A 中的每一个消费者分配到l1个分区，消费组B 中的每一个消费者分配到2 个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。即每一个分区只能被一个消费组中的一个消费者所消费 假设目前某消费组内只有一个消费者C0 ，订阅了一个主题，这个主题包含7 个分区： PO 、Pl 、P2 、P3 、P4 、 PS 、P6 也就是说，这个消费者C0 订阅了7 个分区。消费组内又加入了一个新的消费者C1，按照既定的逻辑，需要将原来消费者C0 的部分分区分配给消费者C1消费， 彼此之间并无逻辑上的干扰 此时又加入了消费者C3，则按照上述规则继续分配。一昧地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况就会有消费者分配不到任何分区 通过以上方式 Kafka支持两种投递方式： 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布／订阅模式的应用 如何实现多副本的发布订阅与广播 每一个消费者只隶属于一个消费组 ！！！！！ 一个消费者可以订阅一个或多个主题 每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id 来配置，默认值为空宇符串 消息消费 Kafka 中的消费是基于拉模式的，Kafka 中的消息消费是一个不断轮询的过程 在默认的方式下，消费者每隔5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在拉取的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 在Kafka 消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用）。可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。 再平衡 再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，在再均衡发生期间，消费组内的消费者是无法读取消息的，即一段时间消费组会变得不可用。 另外，当一个分区被重新分配给另一个消费者时， 消费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作， 之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生 参考链接 《深入理解Kafka 核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"MQ-Kafka1-初识Kafka","slug":"MQ/MQ-Kafka1-初识Kafka","date":"2022-01-05T16:15:26.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2022/01/06/MQ/MQ-Kafka1-初识Kafka/","link":"","permalink":"http://xboom.github.io/2022/01/06/MQ/MQ-Kafka1-%E5%88%9D%E8%AF%86Kafka/","excerpt":"","text":"Kafka一个采用Scala语言(0.9.0版本有java版本)开发的多分区、多副本且基于ZooKeeper协调的分布式消息系统，主要作用有： 消息系统：Kafka除了传统的消息中间件都有的系统解耦、流量削峰、异步通信的功能功能，还提供了消息顺序性消费保障和回溯消费的功能 存储系统：Kafka可以将消息持久化到磁盘。通过Kafka的消息持久化功能和多副本机制，可以将Kafka做长期的存储系统使用(数据可以&quot;永久&quot;保存、以及 主题日志压缩) 流式处理平台：Kafka不仅为流式处理框架提供可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、链接、变换和聚合等操作 基本概念 一个典型的Kafka系统架构 Producer：生产者，也就是消息发送方。负责创建消息，并将其投递给Kafka Broker: 服务代理节点。对于Kafka而言，Broker可以简单地看做一个独立的Kafka服务节点或Kafka服务实例。一个或多个Broker组成一个Kafka集群 Consumer：消费者，消息接收方。消费者连接到Kafka上并接收消息，进行相关的业务处理 Zookeeper：负责集群元数据的管理、控制器的选举等操作 主题与分区 主题(Topic)：主题是一个逻辑概念，Kafka的消息以主题为单位进行归类，生产者负责讲消息发送到特定的主题(发送到kafka集群中的每一条消息都要指定一个主题)，而消费者负责订阅主题并进行消费 分区(Partition): 每个主题可以分为多个分区，一个分区只属于单个主题，也叫主题分区(Tppic-Partition)。分区在存储层面可以看作是一个可追加的日志(Log)文件，消息被追加到分区日志文件的时候都会分配一个特定的偏移量(offset)。offset是消息在分区中的唯一标识，Kafka通过 offset 来保证消息在分区的顺序性。不过offset并不跨分区，也就是说，Kafka保证的是分区有序而不是主题有序 如下图，主题有四个分区，消息被顺序追加到每个分区日志文件的尾部 Kafka中的分区可以分布在不同的服务器(broker)上，也就是说，一个主题可以横跨多个broker。以此来提供比单个broker更强大的性能。 创建主题时候可以通过指定参数来设置主题的分区个数；也可以在创建完成之后去修改分区的数量，实现水平拓展 每一个消息被发送到broker之前，会根据分区规则选择存储到哪个具体的分区 问题1：分区规则是什么？？ TODO 分区容灾 Kafka分区引入了多副本(Replica)机制，同一个分区的不同副本中保存的是相同的消息(在同一时刻，副本之间并非完全相同)，副本之间是&quot;一主多从&quot;的关系，其中leader副本负责处理读写请求，follwer副本只负责与leader副本进行消息同步。 通过设置副本因子来指定分区的副本数量 follower 不支持读请求吗？ 生产者与消费者只与leader副本进行交互，follower副本只负责消息的同步 容灾概念： AR(Assigned Replicas): 分区中的所有副本统称 ISR(In-Sync Replicas): 所有与Leader副本保持一定程度同步的副本(包括leader副本在内)组成 OSR(Out-of-Sync Replicas): 与leader副本同步滞后过多的副本(不包括leader副本)组成 一定程度可以通过参数控制 leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从ISR集合中剔除，如果OSR 集合中有follower 副本&quot;追上&quot;了leader 副本，那么leader 副本会把它从OSR 集合转移至ISR 集合 默认情况下，当leader副本出现故障，只有在ISR集合中的副本才有资格被选举为新的leader 以上选举规则也是可以修改的 问题1：如果所有副本都处于OSR怎么办 TODO 消息拉取 HW(High Watermark):高水位，标识一个特定的消息偏移量(offset)，消费者智能拉取到这个offset之前的消息 LEO(Log End Offset)的缩写，标识当前日志文件中下一条待写入消息的offset 上图代表一个日志文件，这个日志文件中有9条消息，第一条消息的offset(LogStartOffset)为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0-5之间的消息，而offset为6的消息对消费者而言是不可见的。 分区ISR集合中的每个副本都会维护自身的LEO，而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息 注意上图所示 HW 为5 ，LEO 为 9 消费者容灾 Consumer使用拉(Pull)模式从服务端拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时，可以根据之前保存的消费者位置重新拉取需要的消息进行消费，保证消息不会丢失 参考链接 《深入理解Kafka核心设计与实践原理》","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"}]},{"title":"算法1-十大算法","slug":"Algorithm/算法1-十大算法","date":"2022-01-05T15:39:25.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2022/01/05/Algorithm/算法1-十大算法/","link":"","permalink":"http://xboom.github.io/2022/01/05/Algorithm/%E7%AE%97%E6%B3%951-%E5%8D%81%E5%A4%A7%E7%AE%97%E6%B3%95/","excerpt":"","text":"算法概述 十种常见算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序 算法复杂度： 相关概念： 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 **空间复杂度：**是指算法在计算机内执行时所需存储空间的度量，它也是数据规模n的函数 冒泡排序(Bubble Sort) 冒泡排序(Bubble Sort)重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 1234567891011121314//冒泡排序func bubbleSort(src []int) []int &#123; length := len(src) for i := 0; i &lt; length; i++ &#123; for j := 0; j &lt; length-1-i; j++ &#123; //length-1-i 表示最后i个已经排好了 if src[j] &gt; src[j+1] &#123; tmp := src[j+1] src[j+1] = src[j] src[j] = tmp &#125; &#125; &#125; return src&#125; 选择排序(Selection Sort) 选择排序(selection sort)首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕 初始状态：无序区为R[1…n]，有序区为空 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1…i-1]和R(i…n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1…i]和R[i+1…n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区 n-1趟结束，数组有序化了 12345678910111213141516171819//选择排序func selectSort(src []int) []int &#123; length := len(src) var minIndex = 0 var temp = 0 for i := 0; i &lt; length-1; i++ &#123; minIndex = i for j := i; j &lt; length-1; j++ &#123; if src[j] &lt; src[minIndex] &#123; minIndex = j &#125; &#125; temp = src[minIndex] src[minIndex] = src[i] src[i] = temp &#125; return src&#125; 插入排序(Insertion Sort) 插入排序（Insertion-Sort）工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5 12345678910111213141516//插入排序func insertionSort(src []int) []int &#123; length := len(src) var preIndex = 0 var current = 0 for i := 1; i &lt; length; i++ &#123; preIndex = i - 1 current = src[i] for preIndex &gt;= 0 &amp;&amp; src[preIndex] &gt; current &#123; src[preIndex+1] = src[preIndex] preIndex-- &#125; src[preIndex+1] = current &#125; return src&#125; 希尔排序(Shell Sort) 希尔排序又叫缩小增量排序 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度 1234567891011121314151617181920func shellSort(src []int) []int &#123; length := len(src) gap := 1 for gap &lt; gap/3 &#123; gap = gap*3 + 1 &#125; for gap &gt; 0 &#123; for i := gap; i &lt; length; i++ &#123; temp := src[i] j := i - gap for j &gt;= 0 &amp;&amp; src[j] &gt; temp &#123; src[j+gap] = src[j] j -= gap &#125; src[j+gap] = temp &#125; gap = gap / 3 &#125; return src&#125; 归并排序 归并排序是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 1234567891011121314151617181920212223242526272829303132333435func mergeSort(arr []int) []int &#123; length := len(arr) if length &lt; 2 &#123; return arr &#125; middle := length / 2 left := arr[0:middle] right := arr[middle:] return merge(mergeSort(left), mergeSort(right))&#125;func merge(left []int, right []int) []int &#123; var result []int for len(left) != 0 &amp;&amp; len(right) != 0 &#123; if left[0] &lt;= right[0] &#123; result = append(result, left[0]) left = left[1:] &#125; else &#123; result = append(result, right[0]) right = right[1:] &#125; &#125; for len(left) != 0 &#123; result = append(result, left[0]) left = left[1:] &#125; for len(right) != 0 &#123; result = append(result, right[0]) right = right[1:] &#125; return result&#125; 快速排序 快速排序：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序 12345678910111213141516171819202122232425262728293031#include &lt;bits/stdc++.h&gt;using namespace std;const int N = 1e6 + 10;int n;int q[N];void quick_sort(int q[], int l, int r)&#123; if (l &gt;= r) return; //如果只有一个数或者没有数目，那么直接返回 //第一次 x 取最左边的值 int x = q[l + r &gt;&gt; 1], i = l - 1, j = r + 1; while (i &lt; j) &#123; do i++; while(q[i] &lt; x); do j--; while(q[j] &gt; x); if (i &lt; j) swap(q[i], q[j]); &#125; quick_sort(q, l, j); quick_sort(q, j + 1, r);&#125;int main()&#123; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) scanf(\"%d\", &amp;q[i]); quick_sort(q, 0, n - 1); for (int i = 0; i &lt; n; i++) printf(\"%d \", q[i]); return 0;&#125; 堆排序(Heapsort) 堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成 123456789101112131415161718192021222324252627282930313233343536func heapSort(arr []int) []int &#123; arrLen := len(arr) buildMaxHeap(arr, arrLen) for i := arrLen - 1; i &gt;= 0; i-- &#123; swap(arr, 0, i) arrLen -= 1 heapify(arr, 0, arrLen) &#125; return arr&#125;func buildMaxHeap(arr []int, arrLen int) &#123; for i := arrLen / 2; i &gt;= 0; i-- &#123; heapify(arr, i, arrLen) &#125;&#125;func heapify(arr []int, i, arrLen int) &#123; left := 2*i + 1 right := 2*i + 2 largest := i if left &lt; arrLen &amp;&amp; arr[left] &gt; arr[largest] &#123; largest = left &#125; if right &lt; arrLen &amp;&amp; arr[right] &gt; arr[largest] &#123; largest = right &#125; if largest != i &#123; swap(arr, i, largest) heapify(arr, largest, arrLen) &#125;&#125;func swap(arr []int, i, j int) &#123; arr[i], arr[j] = arr[j], arr[i]&#125; 计数排序 计数排序核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 123456789101112131415161718192021func countingSort(arr []int, maxValue int) []int &#123; bucketLen := maxValue + 1 bucket := make([]int, bucketLen) // 初始为0的数组 sortedIndex := 0 length := len(arr) for i := 0; i &lt; length; i++ &#123; bucket[arr[i]] += 1 &#125; for j := 0; j &lt; bucketLen; j++ &#123; for bucket[j] &gt; 0 &#123; arr[sortedIndex] = j sortedIndex += 1 bucket[j] -= 1 &#125; &#125; return arr&#125; 桶排序(Bucket sort) 桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排） 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。 元素分布在桶中 然后，元素在每个桶中排序 基数排序（Radix Sort） 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点） 参考链接 https://www.cnblogs.com/onepixel/articles/7674659.html","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"MQ-Redis","slug":"MQ/MQ-Redis","date":"2022-01-04T16:08:38.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2022/01/05/MQ/MQ-Redis/","link":"","permalink":"http://xboom.github.io/2022/01/05/MQ/MQ-Redis/","excerpt":"","text":"在进行消息队列预研的时候，发现Redis也能做为消息队列，看看Redis做为消息队列是如何实现的 Redis做为消息队列有三种方案 List Streams Pub/Sub List Redis的List是简单的字符串列表，底层由 quicklist 实现。 List消息队列原理 命令 用法 描述 LPUSH LPUSH key value [value …] 将一个或多个值value插入到列表key的表头，如果有多个value值，那么各个value值按从左到右的顺序依次插入到表头 RPUSH RPUSH key value [value …] 将一个或多个值value插入到列表key的表尾(最右边) LPOP LPOP key [count] 移除并返回列表key的头元素(count 指定出队列数目) BLPOP BLPOP key [key …] timeout 移除并获取列表的第一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 RPOP PROP key 移除并返回列表key的尾元素 BRPOP BRPOP key [key …] timeout 移除并获取列表的最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 BRPRPLPUSH BRPOPLPUSH source destination timeout 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它；如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止 RPOPLPUSH RPOPLPUSH source destination 命令RPOPLPUSH 在一个原子时间内，执行以下两个动作：将列表source中的最后一个元素(尾元素)弹出，并返回给客户端。将source弹出的元素插入到猎豹destination,做为destination列表的头元素 LLEN LLEN key 返回列表key的长度。如果key不存在，则key被解释为一个空列表，返回0。如果key不是列表类型，返回一个错误 LRANGE LRANGE key start stop 返回列表key中指定区间内的元素，区间以偏移量 start 和 stop 指定 使用命令组合即可实现消息的出队入队 LPUSH、RPOP 左进右出 RPUSH、LPOP 右进左出 通过LPUSH、RPOP这样的方式，会存在一个性能风险点： 消费者要即使的处理数据，类似要在消费端添加类似 while(true) 的逻辑，不停的调用RPOP或LPOP命令，这样就会给消费者程序带来不必要的性能损失，于是 --&gt; Redis 提供了BLPOP、BRPOP这样阻塞式读取的命令(带B-Bloking的都是阻塞式)，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据(节省不必要要的CPU开销) LPUSH、BRPOP左进右阻塞出 RPUSH、BLPOP右进左阻塞出 因为 Redis 单线程的特点，所以在消费数据时，同一个消息不会同时被多个 consumer 消费掉，但是需要我们考虑消费不成功的情况 可靠队列模式 List 队列中的消息一经发送出去，便从队列里删除。如果由于网络原因消费者没有收到消息，或者消费者在处理这条消息的过程中崩溃了，就再也无法还原出这条消息。究其原因，就是缺少消息确认机制。 为了保证消息的可靠性，消息队列都会有完善的消息确认机制(Acknowledge)，即消费者向队列报告消息已收到或已处理的机制 RPOPLPUSH、BRPOPLPUSH (阻塞)从一个 list 中获取消息的同时把这条消息复制到另一个 list 里(可以当做备份)，而且这个过程是原子的 数据标识从一个 List 取出后放入另一个 List，业务操作安全执行完成后，再去删除 List 中的数据，如果有问题的话，很好回滚 延时消息 通过 zset 来实现延时消息队列，原理就是将消息加到 zset 结构后，将要被消费的时间戳设置为对应的 score 即可，只要业务数据不会是重复数据就可以 Pub/Sub 消息模型包含 点对点：Point-to-Point(P2P) 发布订阅：Publish/Subscribe(Pub/Sub) List 实现方式就是点对点模式，Redis的发布订阅模式(消息多播)就是真正的Redis MQ &quot;发布/订阅&quot;模式包含两种角色，分别是发布者和订阅者。订阅者可以订阅一个或者多个频道(channel)，而发布者可以向指定的频道(channel)发送消息，所有订阅此频道的订阅者都会收到此消息 Redis 通过 PUBLISH 、 SUBSCRIBE 等命令实现了订阅与发布模式， 这个功能提供两种信息机制 订阅/发布到频道 订阅/发布到模式 频道可以先理解为是个 Redis 的 key 值，而模式则是一个类似正则匹配的 Key，只是个可以匹配给定模式的频道。这样就不需要显式的去订阅多个名称了，可以通过模式订阅这种方式，一次性关注多个频道 Pub/Sub常用命令 命令 用法 描述 PSUBSCRIBE PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道 PUBSUB PUBSUB subcommand [argument [argument …]] 查看订阅与发布系统状态 PUBLISH PUBLISH channel message 将信息发送到指定的频道 PUNSUBSCRIBE PUNSUBSCRIBE [pattern [pattern …]] 退订所有给定模式的频道 SUBSCRIBE SUBSCRIBE channel [channel …] 订阅给定的一个或多个频道的信息 UNSUBSCRIBE UNSUBSCRIBE [channel [channel …]] 指退订给定的频道 频道 如上创建一个生产者和两个消费者，消费者1 subscribe channel1 channel2，消费者2subscribe channel1。当生产者使用命令PUBBLISH channel message 向 隧道channel1发送消息时，两个消费者都能。向隧道channel2发送消息时，只有消费者1能够收到消息 其中消费者每次都可以收到3个参数的消息 消息的种类 频道的名称 实际的消息 模式 订阅符合给定模式的频道，命令是 PSUBSCRIBE 如上创建一个生产者和两个消费者，一个使用 SUBSCRIBE channel1，另外一个使用 PSUBSCRIBE chann*,当生产者使用PUBLISH channel1 msg3,两个消费者都能收到消息 PSUBSCRIBE 更像是支持匹配模式的消费者 Redis 发布订阅 (pub/sub) 有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。而且也没有 Ack 机制来保证数据的可靠性，假设一个消费者都没有，那消息就直接被丢弃了。 Streams Redis 5.0 版本新增了一个更强大的数据结构——Stream。它提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 像是个仅追加内容的消息链表，把所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容。而且消息是持久化的 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 Streams 是 Redis 专门为消息队列设计的数据类型，所以提供了丰富的消息队列操作命令 Stream常用命令 描述 命令 添加消息到末尾，保证有序，可以自动生成唯一ID XADD key ID field value [field value …] 对流进行修剪，限制长度 XTRIM key MAXLEN [~] count 删除消息 XDEL key ID [ID …] 获取流包含的元素数量，即消息长度 XLEN key 获取消息列表，会自动过滤已经删除的消息 XRANGE key start end [COUNT count] 以阻塞或非阻塞方式获取消息列表 XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key …] id [id …] 创建消费者组 XGROUP [CREATE key groupname id-or-] [DESTROY key groupname] [DELCONSUMER key groupname consumername] 读取消费者组中的消息 XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key …] ID [ID …] 将消息标记为&quot;已处理&quot; XACK key group ID [ID …] 为消费者组设置新的最后递送消息ID XGROUP SETID [CREATE key groupname id-or-] [DESTROY key groupname] 删除消费者 XGROUP DELCONSUMER [CREATE key groupname id-or-] [DESTROY key groupname] 删除消费者组 XGROUP DESTROY [CREATE key groupname id-or-] [DESTROY key groupname] [DEL 显示待处理消息的相关信息 XPENDING key group [start end count] [consumer] 查看流和消费者组的相关信息 XINFO [CONSUMERS key groupname] [GROUPS key] [STREAM key] [HELP] 打印流信息 XINFO STREAM [CONSUMERS key groupname] [GROUPS key] [STREAM key] [HELP] * 号表示服务器自动生成 ID，后面顺序跟着一堆 key/value 消息ID 必须要比上个 ID 大 -表示最小值 , + 表示最大值,也可以指定最大消息ID，或最小消息ID，配合 -、+ 使用 独立消费 xread 以阻塞或非阻塞方式获取消息列表，指定BLOCK选项即表示阻塞，超过时间0ms(意味用不超时) 阻塞的从尾部读取流，开启新的客户端xadd后发现这里就读到了,block 0 表示永久阻塞 没有给流 mystream 传入一个常规的 ID，而是传入了一个特殊的 ID $ $ 意思是: XREAD 应该使用流 streamtest 已经存储的最大 ID 作为最后一个 ID 当然，也可以指定任意有效的 ID。 而且， XREAD 的阻塞形式还可以同时监听多个 Strem，只需要指定多个键名即可 1127.0.0.1:6379&gt; xread block 0 streams mystream yourstream $ $ 多个客户端监听相同的stream，那么它们都会收到消息！！！，如果想多个客户端监听同一个流怎么办呢？便是创建消费者组 创建消费者组 上述 xread 虽然分发到 N 个客户端，如果想要做的不是向许多客户端提供相同的消息流，而是从同一流向许多客户端提供不同的消息子集。比如下图这样，三个消费者按轮训的方式去消费一个 Stream Redis Stream 借鉴了很多 Kafka 的设计。 Consumer Group：有了消费组的概念，每个消费组状态独立，互不影响，一个消费组可以有多个消费者 last_delivered_id ：每个消费组会有个游标 last_delivered_id 在数组之上往前移动，表示当前消费组已经消费到哪条消息了 pending_ids ：消费者的状态变量，作用是维护消费者的未确认的 id。pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack。如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少。这个 pending_ids 变量在 Redis 官方被称之为 PEL，也就是 Pending Entries List，这是一个很核心的数据结构，它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理。 Stream 不像 Kafak 那样有分区的概念，如果想实现类似分区的功能，就要在客户端使用一定的策略将消息写到不同的 Stream。 xgroup create：创建消费者组 xgreadgroup：读取消费组中的消息 xack：ack 掉指定消息 按消费组消费 Stream 提供了 xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息 ID。它同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除。 参考链接 https://stor.51cto.com/art/202101/640335.htm http://xiaorui.cc/archives/5285","categories":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Docker入门1-Dockerfile","slug":"Docker/Docker入门1-Dockerfile","date":"2021-11-16T15:36:01.000Z","updated":"2021-11-16T15:36:31.832Z","comments":true,"path":"2021/11/16/Docker/Docker入门1-Dockerfile/","link":"","permalink":"http://xboom.github.io/2021/11/16/Docker/Docker%E5%85%A5%E9%97%A81-Dockerfile/","excerpt":"","text":"Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明 指令详解 FROM: 定制的镜像都是基于 FROM 的镜像，即基础镜像 RUN：用于执行后面跟着的命令行命令 12345678#shell 格式RUN &lt;命令行命令&gt;# &lt;命令行命令&gt; 等同于，在终端操作的 shell 命令。#exec 格式RUN [\"可执行文件\", \"参数1\", \"参数2\"]# 例如：# RUN [\"./test.php\", \"dev\", \"offline\"] 等价于 RUN ./test.php dev offline COPY: 复制指令，从上下文目录中复制文件或者目录到容器里指定路径 123456789#格式COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径1&gt;... &lt;目标路径&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [\"&lt;源路径1&gt;\",... \"&lt;目标路径&gt;\"]#[--chown=&lt;user&gt;:&lt;group&gt;]：可选参数，用户改变复制到容器内文件的拥有者和属组。#&lt;源路径&gt;：源文件或者源目录，可以是通配符表达式，例如：#&lt;目标路径&gt;：容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。COPY hom* /mydir/COPY hom?.txt /mydir/ ADD：ADD 指令和 COPY 的使用格类似（同样需求下，官方推荐使用 COPY） ADD 的优点：在执行 &lt;源文件&gt; 为 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，会自动复制并解压到 &lt;目标路径&gt;。 ADD 的缺点：在不解压的前提下，无法复制 tar 压缩文件。会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。具体是否使用，可以根据是否需要自动解压来决定。 CMD: 为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束。类似于 RUN 指令，但二者运行的时间点不同: CMD 在docker run 时运行 RUN 是在 docker build 注意： 如果 Dockerfile 中如果存在多个 CMD 指令，仅最后一个生效。 CMD 指令指定的程序可被 docker run 命令行参数中指定要运行的程序所覆盖。 123CMD &lt;shell 命令&gt; CMD [\"&lt;可执行文件或命令&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] CMD [\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] # 该写法是为 ENTRYPOINT 指令指定的程序提供默认参数 推荐使用第二种格式，执行过程比较明确。第一种格式实际上在运行的过程中也会自动转换成第二种格式运行，并且默认可执行文件是 sh ENTRYPOINT: 类似于 CMD 指令，但其不会被 docker run 的命令行参数指定的指令所覆盖，而且这些命令行参数会被当作参数送给 ENTRYPOINT 指令指定的程序 优点：在执行 docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。 注意：如果 Dockerfile 中如果存在多个 ENTRYPOINT 指令，仅最后一个生效。 1ENTRYPOINT [\"&lt;executeable&gt;\",\"&lt;param1&gt;\",\"&lt;param2&gt;\",...] 可以搭配 CMD 命令使用：一般是变参才会使用 CMD ，这里的 CMD 等于是在给 ENTRYPOINT 传参 1234FROM nginxENTRYPOINT [\"nginx\", \"-c\"] # 定参CMD [\"/etc/nginx/nginx.conf\"] # 变参 不传参 123$ docker run nginx:test#容器内会默认运行以下命令，启动主进程。#nginx -c /etc/nginx/nginx.conf 传参 123docker run nginx:test -c /etc/nginx/new.conf#容器内会默认运行以下命令，启动主进程(/etc/nginx/new.conf:假设容器内已有此文件)nginx -c /etc/nginx/new.conf ENV: 环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量 1234567ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;...#例如ENV NODE_VERSION 7.2.0RUN curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.xz\" \\ &amp;&amp; curl -SLO \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" ARG: 构建参数，与 ENV 作用一致。不过作用域不一样。ARG 设置的环境变量仅对 Dockerfile 内有效，也就是说只有 docker build 的过程中有效，构建好的镜像内不存在此环境变量 1ARG &lt;参数名&gt;[=&lt;默认值&gt;] VOLUME: 定义匿名数据卷。在启动容器时忘记挂载数据卷，会自动挂载到匿名卷。 避免重要的数据，因容器重启而丢失，这是非常致命的。 避免容器不断变大 12VOLUME [\"&lt;路径1&gt;\", \"&lt;路径2&gt;\"...]VOLUME &lt;路径&gt; 在启动容器 docker run 的时候，我们可以通过 -v 参数修改挂载点 EXPOSE: 仅仅只是声明端口 帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射。 在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口 1EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] WORKDIR: 用 WORKDIR 指定的工作目录，会在构建镜像的每一层中都存在 1WORKDIR &lt;工作目录路径&gt; USER: 指定执行后续命令的用户和用户组，只是切换后续命令执行的用户(用户和用户组必须提前已经存在） 1USER &lt;用户名&gt;[:&lt;用户组&gt;] HEALTHCHECK: 指定某个程序或者指令来监控 docker 容器服务的运行状态 1234HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令HEALTHCHECK [选项] CMD &lt;命令&gt; : 这边 CMD 后面跟随的命令使用，可以参考 CMD 的用法。 ONBUILD: 用于延迟构建命令的执行。 简单的说，就是 Dockerfile 里用 ONBUILD 指定的命令，在本次构建镜像的过程中不会执行（假设镜像为 test-build）。当有新的 Dockerfile 使用了之前构建的镜像 FROM test-build ，这时执行新镜像的 Dockerfile 构建时候，会执行 test-build 的 Dockerfile 里的 ONBUILD 指定的命令 1ONBUILD &lt;其它指令&gt; LABEL: LABEL 指令用来给镜像添加一些元数据（metadata），以键值对的形式，语法格式如下： 123LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...#比如可以添加镜像的作者LABEL org.opencontainers.image.authors=\"runoob\" 构建镜像 12FROM nginxRUN echo '这是一个本地构建的nginx镜像' &gt; /usr/share/nginx/html/index.html docker build -t nginx:v3 . 已当前文件为上下文，构建nginx:v3镜像 注意 Dockerfile 的指令每执行一次都会在 docker 上新建一层 1234FROM centosRUN yum -y install wgetRUN wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\"RUN tar -xvf redis.tar.gz 以上执行会创建 3 层镜像，可简化为以下格式, 以 &amp;&amp; 符号连接命令，只会创建 1 层镜像。 1234FROM centosRUN yum -y install wget \\ &amp;&amp; wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\ &amp;&amp; tar -xvf redis.tar.gz 参考链接 https://www.runoob.com/docker/docker-dockerfile.html","categories":[{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/tags/Docker/"}]},{"title":"协议2-HTTP2","slug":"Protocol/协议2-HTTP2","date":"2021-11-01T23:25:03.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/11/02/Protocol/协议2-HTTP2/","link":"","permalink":"http://xboom.github.io/2021/11/02/Protocol/%E5%8D%8F%E8%AE%AE2-HTTP2/","excerpt":"","text":"学习gRPC的过程中，发现gRPC是基于HTTP/2实现的，什么是HTTP2? HTTP/3又是因为什么而被推出 HTTP/1.1 随着网络的发展，单个页面为了显示和渲染需要的资源越来越多，这其中存在着一些问题 TCP连接数限制 对于同一个域名，浏览器最多只能同时创建 6~8 个 TCP 连接 (不同浏览器不一样)。为了解决数量限制，出现了 域名分片 技术，其实就是资源分域，将资源放在不同域名下 (比如二级子域名下)，这样就可以针对不同域名创建连接并请求，以一种讨巧的方式突破限制，但是滥用此技术也会造成很多问题，比如每个 TCP 连接本身需要经过 DNS 查询、三步握手、慢启动等，还占用额外的 CPU 和内存，对于服务器来说过多连接也容易造成网络拥挤、交通阻塞等，对于移动端来说问题更明显 在图中可以看到新建了六个 TCP 连接，每次新建连接 DNS 解析需要时间(几 ms 到几百 ms 不等)、TCP 慢启动也需要时间、TLS 握手又要时间，而且后续请求都要等待队列调度 队头阻塞(Head-Of-Line Blocking) 每个 TCP 连接同时只能处理一个请求 - 响应，浏览器按 FIFO 原则处理请求，如果上一个响应没返回，后续请求 - 响应都会受阻。 针对队头阻塞,有以下办法来解决: HTTP 管线化(HTTP pipelining)是将多个请求（request）整批提交的技术，而在发送过程中不需先等待服务器的回应。缺点是：第一个响应慢还是会阻塞后续响应、服务器为了按序返回相应需要缓存多个响应占用更多资源、浏览器中途断连重试服务器可能得重新处理多个请求、还有必须客户端 - 代理 - 服务器都支持管线化 将同一页面的资源分散到不同域名下，提升连接上限。对于同一个域名，Chrome默认允许同时建立 6 个 TCP持久连接，使用持久连接时。虽然能公用一个TCP管道，但一个管道同一时刻只能处理一个请求，在当前请求没有结束之前，其他的请求只能处于阻塞状态。另外如果在同一个域名下同时有10个请求发生，那么其中4个请求会进入排队等待状态，直至进行中的请求完成。 雪碧图：合并多张小图为一张大图,再用JavaScript或者CSS将小图重新“切割”出来的技术。 内联(Inlining)是另外一种防止发送很多小图请求的技巧，将图片的原始数据嵌入在CSS文件里面的URL里，减少网络请求次数 使用 quic 协议，由于使用的UDP协议，所以可以避免因为TCP自身机制而产生的对头阻塞问题 使用 SCTP 流控制传输协议 无状态特性–带来的巨大HTTP头部 报文Header一般会携带&quot;User Agent&quot;“Cookie”&quot;Accept&quot;等许多固定的头字段，存在大量重复的字段值，增加了传输的成本 明文传输–带来的不安全性 HTTP/1.1在传输数据时，所有传输的内容都是明文 不支持服务器推送消息 HTTP/2 HTTP/2由两个规范（Specification）组成： Hypertext Transfer Protocol version 2 - RFC7540 HPACK - Header Compression for HTTP/2 - RFC7541 报文解析 执行命令：curl --http2 -v nghttp2.org/robots.txt nghttp2.org/humans.txt 分析详情见：gRPC入门2-gRPC交互 HTTP优点 二进制传输 HTTP/2 采用二进制格式传输数据，而非HTTP/1.x 里纯文本形式的报文，二进制协议解析起来更高效。HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。 它把TCP协议的部分特性挪到了应用层，把原来的&quot;Header+Body&quot;的消息&quot;打散&quot;为数个小片的二进制&quot;帧&quot;(Frame),用&quot;HEADERS&quot;帧存放头数据、“DATA&quot;帧存放实体数据。HTTP/2数据分帧后&quot;Header+Body&quot;的报文结构就完全消失了，协议看到的只是一个个的&quot;碎片” HTTP/2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装 Header压缩 HTTP/2并没有使用传统的压缩算法，而是开发了专门的&quot;HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还采用哈夫曼编码来压缩整数和字符串，可以达到50%~90%的高压缩率。 具体来说: 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键-值对，对于相同的数据，不再通过每次请求和响应发送； 首部表在HTTP/2的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键-值对要么被追加到当前表的末尾，要么替换表中之前的值 例如下图中的两个请求， 请求一发送了所有的头部字段，第二个请求则只需要发送差异数据，这样可以减少冗余数据，降低开销 多路复用 在 HTTP/2 中引入了多路复用的技术。多路复用很好的解决了浏览器限制同一个域名下的请求数量的问题，同时也接更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度 在 HTTP/2 中，有了二进制分帧之后，HTTP /2 不再依赖 TCP 链接去实现多流并行了，在 HTTP/2中, 同域名下所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流。 数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。 这一特性，使性能有了极大提升： 同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应,这样整个页面资源的下载过程只需要一次慢启动，同时也避免了多个TCP连接竞争带宽所带来的问题。 并行交错地发送多个请求/响应，请求/响应之间互不影响。 在HTTP/2中，每个请求都可以带一个31bit的优先值，0表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。 Server Push HTTP2还在一定程度上改变了传统的“请求-应答”工作模式，服务器不再是完全被动地响应请求，也可以新建“流”主动向客户端发送消息。比如，在浏览器刚请求HTML的时候就提前把可能会用到的JS、CSS文件发给客户端，减少等待的延迟，这被称为&quot;服务器推送&quot;（ Server Push，也叫 Cache push） 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。主动推送也遵守同源策略，换句话说，服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行(怎么确认的？) 安全性 HTTP/2可以使用明文传输数据，不强制使用加密通信，不过格式还是二进制，只是不需要解密。 但由于HTTPS已是大势所趋，主流浏览器Chrome、Firefox等都公开宣布只支持加密的HTTP/2，所以“事实上”的HTTP/2是加密的。也就是说，互联网上通常所能见到的HTTP/2都是使用&quot;https”协议名，跑在TLS上面。HTTP/2协议定义了两个字符串标识符：“h2&quot;表示加密的HTTP/2，“h2c”表示明文的HTTP/2 应用层的重置连接 对于 HTTP/1 来说，是通过设置 tcp segment 里的 reset flag 来通知对端关闭连接的。这种方式会直接断开连接，下次再发请求就必须重新建立连接。HTTP/2 引入 RST_STREAM 类型的 frame，可以在不断开连接的前提下取消某个 request 的 stream，表现更好 请求优先级设置 HTTP/2 里的每个 stream 都可以设置依赖 (Dependency) 和权重，可以按依赖树分配优先级，解决了关键请求被阻塞的问题 流量控制 每个 http2 流都拥有自己的公示的流量窗口，它可以限制另一端发送数据。对于每个流来说，两端都必须告诉对方自己还有足够的空间来处理新的数据，而在该窗口被扩大前，另一端只被允许发送这么多数据 帧 - Frame 所有帧都是一个固定的 9 字节头部 (payload 之前) 跟一个指定长度的负载 (payload) Length 代表整个 frame 的长度，用一个 24 位无符号整数表示。除非接收者在 SETTINGS_MAX_FRAME_SIZE 设置了更大的值 (大小可以是 2^14(16384) 字节到 2^24-1(16777215) 字节之间的任意值)，否则数据长度不应超过 2^14(16384) 字节。头部的 9 字节不算在这个长度里 Type 定义 frame 的类型，用 8 bits 表示。帧类型决定了帧主体的格式和语义，如果 type 为 unknown 应该忽略或抛弃。 Flags 是为帧类型相关而预留的布尔标识。标识对于不同的帧类型赋予了不同的语义。如果该标识对于某种帧类型没有定义语义，则它必须被忽略且发送的时候应该赋值为 (0x0) R 是一个保留的比特位。这个比特的语义没有定义，发送时它必须被设置为 (0x0), 接收时需要忽略。 Stream Identifier 用作流控制，用 31 位无符号整数表示。客户端建立的 sid 必须为奇数，服务端建立的 sid 必须为偶数，值 (0x0) 保留给与整个连接相关联的帧 (连接控制消息)，而不是单个流 Frame Payload 是主体内容，由帧类型决定 共分为十种类型的帧: HEADERS: 报头帧 (type=0x1)，用来打开一个流或者携带一个首部块片段 DATA: 数据帧 (type=0x0)，装填主体信息，可以用一个或多个 DATA 帧来返回一个请求的响应主体 PRIORITY: 优先级帧 (type=0x2)，指定发送者建议的流优先级，可以在任何流状态下发送 PRIORITY 帧，包括空闲 (idle) 和关闭 (closed) 的流 RST_STREAM: 流终止帧 (type=0x3)，用来请求取消一个流，或者表示发生了一个错误，payload 带有一个 32 位无符号整数的错误码 (Error Codes)，不能在处于空闲 (idle) 状态的流上发送 RST_STREAM 帧 SETTINGS: 设置帧 (type=0x4)，设置此 连接 的参数，作用于整个连接 PUSH_PROMISE: 推送帧 (type=0x5)，服务端推送，客户端可以返回一个 RST_STREAM 帧来选择拒绝推送的流 PING: PING 帧 (type=0x6)，判断一个空闲的连接是否仍然可用，也可以测量最小往返时间 (RTT) GOAWAY: GOWAY 帧 (type=0x7)，用于发起关闭连接的请求，或者警示严重错误。GOAWAY 会停止接收新流，并且关闭连接前会处理完先前建立的流 WINDOW_UPDATE: 窗口更新帧 (type=0x8)，用于执行流量控制功能，可以作用在单独某个流上 (指定具体 Stream Identifier) 也可以作用整个连接 (Stream Identifier 为 0x0)，只有 DATA 帧受流量控制影响。初始化流量窗口后，发送多少负载，流量窗口就减少多少，如果流量窗口不足就无法发送，WINDOW_UPDATE 帧可以增加流量窗口大小 CONTINUATION: 延续帧 (type=0x9)，用于继续传送首部块片段序列，见 首部的压缩与解压缩 HTTP/2缺点 HTTP/2的缺点是底层支撑的 TCP 协议造成的。HTTP/2的缺点主要有以下几点： TCP 以及 TCP+TLS建立连接的延时 HTTP/2都是使用TCP协议来传输的，而如果使用HTTPS的话，还需要使用TLS协议进行安全传输，而使用TLS也需要一个握手过程，这样就需要有两个握手延迟过程： ① 在建立TCP连接的时候，需要和服务器进行三次握手来确认连接成功，也就是说需要在消耗完1.5个RTT之后才能进行数据传输。 ② 进行TLS连接，TLS有两个版本——TLS1.2和TLS1.3，每个版本建立连接所花的时间不同，大致是需要1~2个RTT。 总之，在传输数据之前，我们需要花掉 3～4 个 RTT。 TCP的队头阻塞并没有彻底解决 在HTTP/2中，多个请求是跑在一个TCP管道中的。但当出现了丢包时，HTTP/2 的表现反倒不如 HTTP/1 了。因为TCP为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，HTTP/2出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该TCP连接中的所有请求。而对于 HTTP/1.1 来说，可以开启多个 TCP 连接，出现这种情况反到只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据。 HTTP3 因为HTTP/2的问题，HTTP/3诞生，一个基于 UDP 协议的“QUIC”协议，让HTTP跑在QUIC上而不是TCP上。而这个“HTTP over QUIC”就是HTTP协议的下一个大版本。 实现了类似TCP的流量控制、传输可靠性的功能 实现了快速握手功能 集成了TLS加密功能 多路复用，彻底解决TCP中队头阻塞的问题 参考链接 https://blog.csdn.net/howgod/article/details/102597450?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~default-1.no_search_link https://blog.wangriyu.wang/2018/05-HTTP2.html","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"http://xboom.github.io/tags/HTTP/"}]},{"title":"gRPC入门2-gRPC交互","slug":"gRPC/gRPC入门2-gRPC交互","date":"2021-10-31T11:25:42.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/10/31/gRPC/gRPC入门2-gRPC交互/","link":"","permalink":"http://xboom.github.io/2021/10/31/gRPC/gRPC%E5%85%A5%E9%97%A82-gRPC%E4%BA%A4%E4%BA%92/","excerpt":"","text":"gRCP有四种调用方式，通过抓包分析这四种的交互过程 一元RPC 客户端流RPC 服务端流RPC 双向流RPC 示例代码以及报文路径 服务端监听端口9001 一元RPC 参考1.pcap 1 - 3: client -&gt; server 客户端向服务器发起连接请求，完成三次握手 4 - 5: server -&gt; client 服务器向客户端发送 SETTINGS，在建立连接开始时双方都要发送 SETTINGS 帧以表明自己期许对方应做的配置，对方接收后同意配置参数便返回带有 ACK 标识的空 SETTINGS 帧表示确认，而且连接后任意时刻任意一方也都可能再发送 SETTINGS 帧调整，SETTINGS 帧中的参数会被最新接收到的参数覆盖 一个Settings帧的payload由N个参数组成，每个参数的格式如下 Identifier: 代表参数类型，比如Max frame size 是 5 Value：相应参数的值 SETTINGS 帧作用于整个连接，而不是某个流，而且 SETTINGS 帧的 stream identifier 必须是 0x0，否则接收方会认为错误 (PROTOCOL_ERROR) SETTINGS 帧包含以下参数: SETTINGS_HEADER_TABLE_SIZE (0x1): 用于解析 Header block 的 Header 压缩表的大小，初始值是 4096 字节 SETTINGS_ENABLE_PUSH (0x2): 可以关闭 Server Push，该值初始为 1，表示允许服务端推送功能 SETTINGS_MAX_CONCURRENT_STREAMS (0x3): 代表发送端允许接收端创建的最大流数目 SETTINGS_INITIAL_WINDOW_SIZE (0x4): 指明发送端所有流的流量控制窗口的初始大小，会影响所有流，该初始值是 2^16 - 1(65535) 字节，最大值是 2^31 - 1，如果超出最大值则会返回 FLOW_CONTROL_ERROR SETTINGS_MAX_FRAME_SIZE (0x5): 指明发送端允许接收的最大帧负载的字节数，初始值是 2^14(16384) 字节，如果该值不在初始值 (2^14) 和最大值 (2^24 - 1) 之间，返回 PROTOCOL_ERROR SETTINGS_MAX_HEADER_LIST_SIZE (0x6): 通知对端，发送端准备接收的首部列表大小的最大字节数。该值是基于未压缩的首部域大小，包括名称和值的字节长度，外加每个首部域的 32 字节的开销 SETTINGS 帧有以下标识 (flags): ACK: bit 0 设为 1 代表已接收到对方的 SETTINGS 请求并同意设置，设置此标志的 SETTINGS 帧 payload 必须为空 6 - 7: client -&gt; server, 客户端的前言Magic，包含一个内容为 PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n 的序列 发送完前言后双方都得向对方发送带有 ACK 标识的 SETTINGS 帧表示确认，对应上图中编号 29 和 31 的帧。 请求站点的全部帧序列，帧后面的数字代表所属流的 id，最后以 GOAWAY 帧关闭连接 8 - 9：client -&gt; server, 客户端 SETTING[0]，客户端发送前言Magic 再带上一个空的 前言 10 - 11: server -&gt; client, 服务端针对客户端前言的ACK 12 - 13: client -&gt; server , 客户端针对服务端前言的ACK 14 - 15: client -&gt; server, 客户端向服务器发起 RPC的 Headers 帧 headers 帧格式： Pad Length: 指定 Padding 长度，存在则代表 PADDING flag 被设置 E: 一个比特位声明流的依赖性是否是排他的，存在则代表 PRIORITY flag 被设置 Stream Dependency: 指定一个 stream identifier，代表当前流所依赖的流的 id，存在则代表 PRIORITY flag 被设置 Weight: 一个无符号 8 为整数，代表当前流的优先级权重值 (1~256)，存在则代表 PRIORITY flag 被设置 Header Block Fragment: header 块片段 Padding: 填充字节，没有具体语义，作用与 DATA 的 Padding 一样，存在则代表 PADDING flag 被设置 HEADERS 帧有以下标识 (flags): END_STREAM: bit 0 设为 1 代表当前 header 块是发送的最后一块，但是带有 END_STREAM 标识的 HEADERS 帧后面还可以跟 CONTINUATION 帧 (这里可以把 CONTINUATION 看作 HEADERS 的一部分) END_HEADERS: bit 2 设为 1 代表 header 块结束 PADDED: bit 3 设为 1 代表 Pad 被设置，存在 Pad Length 和 Padding PRIORITY: bit 5 设为 1 表示存在 Exclusive Flag (E), Stream Dependency, 和 Weight Unused 16 - 17: client -&gt; server, 客户端向服务器发送rpc data帧 Pad Length: ? 表示此字段的出现时有条件的，需要设置相应标识 (set flag)，指定 Padding 长度，存在则代表 PADDING flag 被设置 Data: 传递的数据，其长度上限等于帧的 payload 长度减去其他出现的字段长度 Padding: 填充字节，没有具体语义，发送时必须设为 0，作用是混淆报文长度，与 TLS 中 CBC 块加密类似， DATA 帧有如下标识 (flags): END_STREAM: bit 0 设为 1 代表当前流的最后一帧 PADDED: bit 3 设为 1 代表存在 Padding 18 - 19: server -&gt; client, 发送 控制帧 WINDOW_UPDATE， 以及PING 用来判断连接是否可用 20 - 21: server -&gt; client , 服务器发送rpc 响应 22 - 23: client -&gt; server, 客户端向服务器发送 PING 帧，判断连接是否可用 24 - 25: client -&gt; server, 客户端向服务器发送 WINDOWS_UPDATE, PING帧 26 - 27: server -&gt; client, 客户端向服务器发送 PING帧 28: 客户端发送完成 退出进程 客户端流RPC 1 - 3: 完成三次握手 4 - 5: client -&gt; server, 客户端向服务器发送http/2前言 6 - 7: client -&gt; server, 客户端向服务器发送前言的时候带上空 SETTINGS 8 - 9: server -&gt; client, 服务器向客户端发送前言，设置 MAX_FREAME_SIZE 16384 10 - 11: server -&gt; client, 服务器向客户端发送 前言ACK 12 - 13: client -&gt; server, SETTINGS(ACK) + HEADERS 可以看到在流中，多个消息通过一个报文发送 14 - 15: server -&gt; client, WINDOWS_UPDATE + PING 16 - 17: server -&gt; client, HEADERS + DATA 18 - 19: client -&gt; server, PING(ACK) + WINDOWS_UPDATE + PING 20 -21: server -&gt; client , PING(ACK) 服务端流RPC 1 - 3： client -&gt; server，客户端向服务器发起连接请求，完成三次握手 4 - 5： client -&gt; server，Magic 客户端向服务器发送HTTP/2 前言 6 - 7： client -&gt; server，SETTING 客户端前言带上的空 SETTING 8 - 9: server -&gt; client，SETTING 服务端向客户端发送 HTTP/2 前言 10-11: server -&gt; client，SETTING(ACK) 12-13: client -&gt; server, SETTING(ACK) + HEADERS 14-15: server -&gt; client, WINDOWS_UPDATE + PING + HEADERS + DATA 16-17: client -&gt; server, PING(ACK) 18-19: client -&gt; server, WINDOWS_UPDATE, PING 20-21: server -&gt; client, PING(ACK) 双向流RPC 1 - 3: client -&gt; server, 客户端向服务器发起连接请求，完成三次握手 4 - 5: server -&gt; client, SETTING 服务器向客户端发送HTTP/2 前言 6 - 7: client -&gt; server，Magic 客户端向服务器发送HTTP/2 前言 8 - 9: client -&gt; server，SETTING 客户端前言带上的空 SETTING 10, 12: client -&gt; server, SETTING(ACK) + HEADERS 11, 13: server -&gt; client, SETTING(ACK) 14-15: client -&gt; server, DATA 16-17: server -&gt; client, WINDOWS + PING 18-19: client -&gt; server, PING(ACK) 20-21: server -&gt; client, HEADERS + DATA + DATA 22-23: client -&gt; server, WINDOWS + PING 24-25: server -&gt; client, PING(ACK) 26-27: client -&gt; server, DATA + DATA 28,30: server -&gt; client, WINDOWS_UPDATE, PING 29-31: server -&gt; client, DATA + DATA 32-33: client -&gt; server, PING(ACK), WINDOWS_UPDATE, PING 34-35: server -&gt; client, PING(ACK) 参考链接 https://httpwg.org/specs/rfc7540.html https://blog.wangriyu.wang/2018/05-HTTP2.html","categories":[{"name":"gRPC","slug":"gRPC","permalink":"http://xboom.github.io/categories/gRPC/"}],"tags":[{"name":"gRPC","slug":"gRPC","permalink":"http://xboom.github.io/tags/gRPC/"}]},{"title":"gRPC入门1-gRPC相关介绍","slug":"gRPC/gRPC入门1-gRPC介绍","date":"2021-10-31T07:25:42.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/10/31/gRPC/gRPC入门1-gRPC介绍/","link":"","permalink":"http://xboom.github.io/2021/10/31/gRPC/gRPC%E5%85%A5%E9%97%A81-gRPC%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"RPC 代指远程过程调用(Remote Procedure Call)，基于 HTTP 协议的。它的调用包含了传输协议和编码（对象序列号）协议等等。允许跨语言或跨设备通信，无需额外为这个交互编程。如果A服务器上的应用c 调用B服务器上的应用d，就跟本地调用一个函数一样 示例代码路径 RPC框架 一个完整的 RPC 框架，应包含负载均衡、服务注册和发现、服务治理等功能，并具有可拓展性便于流量监控系统等接入 有些较单一的 RPC 框架，通过组合多组件也能达到这个标准 常用的RPC框架 \\ 跨语言 多 IDL 服务治理 注册中心 服务管理 gRPC √ × × × × Thrift √ × × × × Rpcx × √ √ √ √ Dubbo × √ √ √ √ Protobuf Protocol Buffers 是一种与语言、平台无关，可扩展的序列化结构化数据的方法，常用于通信协议，数据存储等等。相较于 JSON、XML，它更小、更快、更简单。 123456789101112131415161718192021222324252627282930313233343536syntax = \"proto3\"; // proto版本option go_package = \"../proto\"; // 生成go文件目录package search; //生成的包名service SearchService &#123; rpc Search(SearchRequest) returns (SearchResponse) &#123;&#125; //一元RPC rpc List(StreamRequest) returns (stream StreamResponse) &#123;&#125;; //服务端流RPC rpc Record(stream StreamRequest) returns (StreamResponse) &#123;&#125;; //客户端流PRC rpc Route(stream StreamRequest) returns (stream StreamResponse) &#123;&#125;; //双向流RPC&#125;message SearchRequest &#123; string request = 1;&#125;message SearchResponse &#123; string response = 1;&#125;message StreamPoint &#123; string name = 1; int32 value = 2;&#125;message StreamRequest &#123; StreamPoint pt = 1;&#125;message StreamResponse &#123; StreamPoint pt = 1;&#125; 使用命令: protoc --go_out=. *.proto,生成的*.pb.go 详细语法：https://developers.google.com/protocol-buffers/docs/proto3 v2 和 v3 主要区别 删除原始值字段的字段存在逻辑 删除 required 字段 删除 optional 字段，默认就是 删除 default 字段 删除扩展特性，新增 Any 类型来替代它 删除 unknown 字段的支持 新增 JSON Mapping 新增 Map 类型的支持 修复 enum 的 unknown 类型 repeated 默认使用 packed 编码 引入了新的语言实现（C＃，JavaScript，Ruby，Objective-C） 相较Protobuf，为什么不用XML 更简单 数据描述只需原来的1/10 和 1/3 解析速度是原来的20倍至100倍 减少了二义性 生成了更易使用的数据访问类 gRPC gRPC 是一个 基于 HTTP/2 协议设计的 RPC 框架，采用了 Protobuf 作为 IDL(Interactive Data Language) 多语言支持：C++、C#、Dart、Go、Java、Node.js、Objective-C、PHP、Python、Ruby、C++ 特点：HTTP/2、Protobuf、客户端与服务器基于同一份IDL 架构 客户端（gRPC Sub）调用 A 方法，发起 RPC 调用 对请求信息使用 Protobuf 进行对象序列化压缩（IDL） 服务端（gRPC Server）接收到请求后，解码请求体，进行业务逻辑处理并返回 对响应结果使用 Protobuf 进行对象序列化压缩（IDL） 客户端接受到服务端响应，解码请求体。回调被调用的 A 方法，唤醒正在等待响应（阻塞）的客户端调用并返回响应结果 gRPC分为四种调用方式 一元RPC 客户端流RPC 服务端流RPC 双向RPC 构建服务端 123456789lis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))if err != nil &#123; log.Fatalf(\"failed to listen: %v\", err)&#125;grpcServer := grpc.NewServer()...pb.RegisterSearchServer(grpcServer, &amp;SearchServer&#123;&#125;)grpcServer.Serve(lis) 监听指定 TCP 端口，用于接受客户端请求 创建 gRPC Server 的实例对象 gRPC Server 内部服务和路由的注册 Serve() 调用服务器以执行阻塞等待，直到进程被终止或被 Stop() 调用 创建客户端 123456789var opts []grpc.DialOption...conn, err := grpc.Dial(*serverAddr, opts...)if err != nil &#123; log.Fatalf(\"fail to dial: %v\", err)&#125;defer conn.Close()client := pb.NewSearchClient(conn) 创建 gRPC Channel 与 gRPC Server 进行通信（需服务器地址和端口作为参数） 设置 DialOptions 凭证（例如，TLS，GCE 凭据，JWT 凭证） 创建 Search Client Stub 调用对应的服务方法 问题 什么场景下不适合使用 Protobuf，而适合使用 JSON、XML？ Protobuf 一节中提到的 packed 编码，是什么？ 参考链接 1.https://github.com/EDDYCJY/go-grpc-example","categories":[{"name":"gRPC","slug":"gRPC","permalink":"http://xboom.github.io/categories/gRPC/"}],"tags":[{"name":"gRPC","slug":"gRPC","permalink":"http://xboom.github.io/tags/gRPC/"}]},{"title":"Redis入门5-AOF","slug":"Redis/Redis入门5-AOF","date":"2021-10-10T12:42:44.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/10/10/Redis/Redis入门5-AOF/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A85-AOF/","excerpt":"","text":"除了RDB持久功能之外，Redis还提供了AOF(Append Only File)持久化功能。 RDB 持久化通过保存数据库中的键值对来记录数据库状态 AOF持久化是通过保存Redis 服务器锁执行的写命令来记录数据库状态 被写入AOF文件的命令是以Redis的命令请求协议格式保存的，Redis的命令请求协议是纯文本格式，直接打开AOF文件，观察内容 AOF 持久化的实现 AOF持久化功能的实现可以分为命令的追加(append)、文件写入、文件同步(sync)三个步骤 命令追加 当AOF持久化功能处于打开状态时，服务器在执行一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾 文件的写入与同步 这里的同步是操作系统在write 写入文件时,通常将数据保存在一个内存缓冲区，然后将缓冲区中的数据写入到磁盘的过程 Redis的服务器进程就是一个事件循环(loop)，这个循环中的文件事件负责接收客户端的命令请求以及回复，时间事件则负责执行像serverCron函数这样需要定时运行的函数 服务器在处理文件事件时可能会执行写命令，使得一些内容被追加到aof_buf缓冲区里面，所以每次结束一个事件循环之前，都会调用 flushAppendOnlyFile 函数，考虑是否将aof_buf缓冲区中的内容写入和保存到AOF文件里面 1234567891011def eventLoop(): while True: # 处理文件时间，接收命令请求以及发送命令回复 # 处理命令请求时可能会有新内容被追加到 aof_buf 缓冲区中 processFileEvents() # 处理时间事件 processTimeEvents() # 考虑是否将 aof_buf 中的内容写入和保存到 AOF 文件里面 flushAppendOnlyFile() 通过服务器配置的 appendfsync选型的值来决定AOF持久化行为 Appendfsync flushAppendOnlyFile 函数的行为 效率 安全 always 将aof_buf缓冲区中的所有内容写入并同步到AOF文件 最慢 最高 everysec 将aof_buf缓冲区的所有内容写入到AOF文件，如果上次同步AOF文件的时间距离现在超过1秒钟，那么子进程对AOF文件进行同步，并且这个同步操作是由换一个线程专门负责执行的 适中 适中 no 将aof_buf缓冲区的中的所有内容写入到AOF文件，但并不对AOF文件进行同步，何时同步由操作系统自己决定 最快 最低 为了提高文件的写入效率，在现代操作系统中，用户调用write函数，将一些数据写入到文件的时候，操作系统通常会将写入数据暂时保存在一个内存缓冲区里面，等到缓冲区的空间被填满，或者超过了指定的时限之后，才真正地将缓冲区中的数据写入到磁盘里面 导致当计算机发生停机的时候，内存缓冲区的写入数据可能会丢失。为此，系统提供了 fsync 和 fdatashync 两个同步函数，强制操作系统立即将缓冲区中的数据写入到硬盘 AOF文件的载入与数据还原 由于AOF 文件里包含了重建数据库状态所需的所有写命令，所以服务器只要读入并重新执行一遍AOF文件里面保存的写命令 Redis 读取AFO文件并还原数据库状态的详细步骤如下： 创建一个不带网络链接的伪客户端(fake client)：因为 Redis 的命令智能在客户端上下文中执行，而载入AOF文件时锁使用的命令直接来源于AOF 文件而不是网络连接，所以服务器使用一个伪客户端来执行AOF文件保存的写命令 从AOF文件中分析并读取出一条写命令 使用伪客户端执行被读出的谢命令 一直执行步骤2和步骤3，直到AOF文件中的所有写命令都被处理完毕为止 AOF 重写 因为AOF持久化是通过保存被执行的写命令来记录数据库状态，随着服务器运行时间的流逝，AOF文件中的内容会越来越多。为了解决AOF文件体积膨胀的问题，Redis提供的AOF文件重写(rewrite)功能 AOF文件重写的实现 Redis 将生成新AOF文件替换旧AOF文件的功能命令为 “AOF文件重写”。但实际上，AOF文件重写并不需要对现有的AOF文件进行任何读取、分析或写入操作，是通过读取服务器当前的数据库状态来实现的 例如： 保存当前list键(含有6个value)的状态，须在AOF文件中写六条命令，如果服务器想尽量少的命令来记录list键的状态，最简单高效的办法不是去读取和分析现有AOF文件，而是直接从数据库中读取键list的值，用 一条 RPUSH key value... 来代替保存在AOF文件中的六条命令 这就是 AOF 重写功能的实现原理，因为新AOF文件只包含还原当前数据库状态所必须的命令，所以AOF文件不会浪费任何硬盘空间 在实际执行过程中，为了避免执行命令时造成客户端输入缓冲区溢出，会检查键锁包含的元素数理那个，如果数据超过 redis.h/REDIS_AOF_REWRITE_ITEMS_PRE_CMD 敞亮的值，那么重写程序将使用多条命令来记录键的值，而不单单使用一条命令 AOF后台重写 AOF重写程序 aof_rewrite 函数可以很好地完成创建一个新AOF文件的任务，但这个函数进行了大量的写入操作，所以调用这个函数的线程将会被长时间阻塞。将其放到子进程里执行可以达到两个目的： 子进程进行AOF重写期间，服务器进程(父进程)可以继续处理命令请求 子进程带有服务器进程的数据副本，使用子进程而不是线程，避免使用锁的情况下，保证数据安全性 子进程在处理AOF重写期间，服务器还需要继续处理命令请求，可能导致新的数据库状态与AOF文件所保存的数据库状态不一致 如图：新的AOF文件只保存了k1一个键的数据，而服务器数据库现在却有k1、k2、k3、k4 四个键 为了解决这种数据不一致的问题，Redis服务器设置了一个AOF重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用 当Redis服务器执行完一个写命令之后，它会同时将这个写命令发送给AOF缓冲区和AOF重写缓冲区 步骤如下： 执行客户端发来的命令 将执行后的写命令追加到AOF缓冲区 将执行后的写命令追加到AOF重写缓冲区 这样可以保证： AOF缓冲区的内容讲定期被写入和同步到AOF文件，对现有AOF文件的处理工作正常进行 从创建子进程开始，服务器执行的所有写命令都会被记录到AOF重写缓冲区里面 当子进程完成AOF重写工作之后，它会向父进程发送一个信号，父进程在接到该信号之后，会调用一个信号处理函数，并执行以下工作： 将AOF重写缓冲区中的所有内容写入到新AOF文件中，这时新AOF文件所保存的数据库状态将和服务器当前的数据库状态一致 对新的AOF文件进行改名，原子地(atomic)覆盖现有的AOF文件，完成新旧两个AOF文件的替换 这个信号处理函数执行完毕之后，父进程就可以像往常一样接受命令请求 整个AOF后台重写过程，只有信号处理函数执行时会对服务器进程(父进程)造成阻塞，在其他时候，AOF后台重写都不会阻塞父进程 总结 AOF文件通过保存所有修改数据库的写命令请求来记录服务器的数据库状态 AOF文件中国暖的所有命令都以Redis命令请求协议的格式保存 命令请求会先保存到AOF缓冲区，之后再定期写入并同步到AOF文件 appendfsync选项的不同值对AOF持久化功能的安全性以及Redis服务器的性能有很大的影响 服务器只要载入并重新执行保存在AOF文件中的命令，就可以还原数据库状态 AOF重写可以产生一个新的AOF文件，新的AOF文件和原有的AOF文件所保存的数据库状态一样。但体积更小 AOF重写是有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无需对现有AOF文件进行任何读入、分析或写入操作 执行BGREWRITEAOF命令时，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，是的新旧两个AOF文件所保存的数据库状态一致。最后通过新的AOF替换旧的AOF文件，以此来完成AOF文件重写操作 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门4-RDB","slug":"Redis/Redis入门4-RDB","date":"2021-10-10T11:11:18.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/10/10/Redis/Redis入门4-RDB/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A84-RDB/","excerpt":"","text":"Redis 是一个键值对数据库服务器，服务器中通常包括含着任意个非空数据库，而每个非空数据库中又可以包含任意个键值对。将服务器中的非空数据库以及它们的键值对统称为数据库状态。下图展示包含三个非空数据库的Redis服务器。这三个数据库以及数据库中的键值对就是该服务器的数据库状态 Redis提供了RDB持久化功能，将某个时间点的数据库状态保存到一个RDB文件中。RDB文件是一个经过压缩的二进制文件，通过该文件可以还原生成RDB文件时的数据库状态 RDB文件的创建与载入 有两个Redis命令用于生成RDB文件 SAVE 命令会阻塞Redis服务器进程，直到RDB文件创建完毕为止，在服务器进程阻塞期间，服务器不处理任何命令请求 BGSAVE 命令会派生出一个子进程，然后由子进程负责创建RDB文件，服务器进程(父进程)继续处理命令请求 创建RDB文件实际工作有 rdb.c/rdbSave 函数完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667int rdbSave(char *filename, rdbSaveInfo *rsi) &#123; char tmpfile[256]; char cwd[MAXPATHLEN]; /* Current working dir path for error messages. */ FILE *fp = NULL; rio rdb; int error = 0; snprintf(tmpfile,256,\"temp-%d.rdb\", (int) getpid()); fp = fopen(tmpfile,\"w\"); if (!fp) &#123; char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \"Failed opening the RDB file %s (in server root dir %s) \" \"for saving: %s\", filename, cwdp ? cwdp : \"unknown\", strerror(errno)); return C_ERR; &#125; rioInitWithFile(&amp;rdb,fp); startSaving(RDBFLAGS_NONE); if (server.rdb_save_incremental_fsync) rioSetAutoSync(&amp;rdb,REDIS_AUTOSYNC_BYTES); if (rdbSaveRio(&amp;rdb,&amp;error,RDBFLAGS_NONE,rsi) == C_ERR) &#123; errno = error; goto werr; &#125; /* Make sure data will not remain on the OS's output buffers */ if (fflush(fp)) goto werr; if (fsync(fileno(fp))) goto werr; if (fclose(fp)) &#123; fp = NULL; goto werr; &#125; fp = NULL; /* Use RENAME to make sure the DB file is changed atomically only * if the generate DB file is ok. */ if (rename(tmpfile,filename) == -1) &#123; char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \"Error moving temp DB file %s on the final \" \"destination %s (in server root dir %s): %s\", tmpfile, filename, cwdp ? cwdp : \"unknown\", strerror(errno)); unlink(tmpfile); stopSaving(0); return C_ERR; &#125; serverLog(LL_NOTICE,\"DB saved on disk\"); server.dirty = 0; server.lastsave = time(NULL); server.lastbgsave_status = C_OK; stopSaving(1); return C_OK;werr: serverLog(LL_WARNING,\"Write error saving DB on disk: %s\", strerror(errno)); if (fp) fclose(fp); unlink(tmpfile); stopSaving(0); return C_ERR;&#125; RDB文件的载入工作是在服务器启动时自动执行的，所有Redis并没有专门用于载入RDB文件的命令，只要Redis服务器在启动时检测到RDB文件存在，就会自动载入RDB文件 因为AOF文件的更新频率通常比RDB文件的更新频率高，所以： 如果服务器开启了AOF持久化功能，那么服务器会优先使用AOF文件来还原数据库状态 只有在AOF持久性功能处于关闭状态时，服务器才会使用RDB文件来还原数据库状态 载入RDB文件 实际工作由 rdb.c/rdbLoad函数完成 由于使用BGSAVE命令是由子进程执行的，Redis服务器仍然可以继续处理客户端的命令请求。但在BGSAVE命令执行期间，服务器处理SAVE、BGSAVE、GBREWRITEAOF 三个命令的方式会和平时有所不同 在 BGSAVE 期间，客户端发送的 SAVE 和 BGSAVE 命令会被服务器拒绝，避免父进程(服务器进程)和子进程同时执行两个rdbSave调用，防止发生竞争条件 BGREWRITEAOF 和 BGSAVE 两个命令不能同时执行： 如果BGSAVE 命令正在执行，那么客户端发送的 BGREWRITEAOF 命令 会被延迟到 BGSAVE 命令执行完毕之后执行 如果 BGREWIRTEAOF 命令正在执行，那么客户端发送的 BGSAVE 命令会被服务器拒绝 虽然 BGREWRITEAOF 和 BGSAVE 两个命令的实际工作都是由子进程执行，不让同时执行考虑到都同时执行大量的磁盘写入操作 RDB文件载入时的服务器会一致处于阻塞状态，直到载入工作完成为止 自动间隔性保存 saveparams 属性是一个数组，数组中的每个元素都是一个saveparam结构。通过配置save选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令 除了 saveparams 数组之外，服务器状态还维持着一个dirty计数器，以及一个lastsave属性 dirty 计数器记录距离上一次成功执行 SAVE 命令或者 BGSAVE 命令之后，服务器对数据库状态(服务器中的所有数据库) 进行了多少次修改(包括写入、删除、更新操作) lastsave 属性 是一个 UNIX 时间戳，记录了服务器上一次成功执行 SAVE 命令或 BGSAVE 命令的时间 12345678910//向服务器提供以下配置，只要满足以下三个任意条件， BGSAVE 命令就会被执行// 以下也是save 的默认条件save 900 1 save 300 10save 60 10000/*1. 服务器在900秒之内，对数据库进行了至少1次修改2. 服务器在300秒之内，对数据库进行了至少10次修改3. 服务器在60秒之内，对数据库进行了至少10000次修改*/ Redis的服务器周期性操作函数 serverCron 默认每隔 100ms 就会执行一次，其中一项工作就是检查 save 选洗那个锁设置的保存条件是否满足，满足则执行 BGSAVE 命令 当时间来到1378271101(1378270800 + 300 = 1378271100)，服务器将自动执行一次 BGSAVE 命令，假设BGSAVE 在执行5s之后完成，那么服务器状态将更新，其中 dirty 计数器已经被重置为0，而 lastsave 属性也被更新为 1378271106 以上就是Redis 服务器根据 save 选项 锁设置的保存条件，自动执行 BGSAVE 命令，进行剑歌行数据保存的实现原理 RDB文件结构 一个完整的RDB文件所包含的各个部分 RDB文件最开头是REDIS 部分，这个部分的长度是5字节，保存着 “REDIS” 五个字符，通过这五个字符，在载入文件时快速判断是否是RDB文件 db_version 长度为4字节，它的值是一个字符串表示的整数，这个整数记录了RDB文件的版本号，比如&quot;0006&quot;表示RDB文件的版本为第六版本 database 部分包含着0个或任意多个数据库，以及各个数据库中的键值对数据 如果服务器的数据库状态为空(所有数据库都是空的)，那么这个部分也为空，长度为0字节 EOF 常量长度为1字节，标志着RDB文件的正文内容的结束，当读入程序遇到这个值的时候，它直到所有数据库的所有键值载入完毕 check_sum 是一个8字节长的无符号整数，保存着一个校验和，这个校验和是通过对 REDIS、db_version、databases、EOF四个部分计算出来的 一个RDB文件的databases部分可以保存任意多个非空数据库 每个非空数据在RDB文件中都保存为 SELECTDB、db_number、key_value_pairs三个部分 SELECTDB 常量长度为1字节，当读入程序遇到这个值，直到接下来要读入的是一个数据库号码 db_number 保存着一个数据库号码，根据号码大量的不同，这个部分的长度可以是1字节、2字节或者5字节。当程序读入db_number部分之后，服务器会调用SELECT命令，根据读入的数据库号码进行数据库切换 db_number 长度不固定，怎么判断db_number已经读完了？ key_value_pairs 部分保存了数据库中的所有键值对数(如果键值对带有过期时间，那么过期时间也会和键值对保存在一起) 不带过期时间的键值对在RDB文件中由 TYPE、key、value 三部分组成，TYPE 记录了value的类型，长度为1字节 Key 总是一个字符串常量，长度不固定 根据 TYPE 类型不同，保存的内容长度不同，value 的结构和长度也会有所不同 带有过期时间的RDB文件结构 EXPIRETIME_MS: 常量的长度为1字节，告知接下来要读入的将是一个以毫秒为单位的过期时间 ms 是一个8字节长的带符号整数，记录一个以毫秒为单位的 UNIX 时间戳，即键值对的过期时间 总结 RDB文件用于保存和还原Redis服务器所有数据库中的所有键值对数据 SAVE 命令由服务器进程执行会阻塞服务器，BGSAVE 由子进程执行保存操作，不会阻塞服务器 RDB文件是一个经过压缩的二进制文件，由多个部分组成 对于不同类型的键值对，RDB文件会使用不同的方式保存它们 BGSAVE 保存过程中，产生的新数据是如何保存的？ 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门3-数据库","slug":"Redis/Redis入门3-数据库","date":"2021-10-10T08:45:01.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/10/10/Redis/Redis入门3-数据库/","link":"","permalink":"http://xboom.github.io/2021/10/10/Redis/Redis%E5%85%A5%E9%97%A83-%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"服务器中的数据库 Redis服务器将所有数据库都保存在redis.h/redisServer 结构的 db数组 中，每个redisDb结构代表一个数据库 1234567891011121314151617181920212223//Redis服务结构struct redisServer &#123; //... //一个数组，保存着服务器中所有的数据库 redisDb *db; //初始化时候 根据dbnum 决定创建多少个数据库 int dbnum; //...&#125;//单个数据库结构typedef struct redisDb &#123; dict *dict; /* The keyspace for this DB */ dict *expires; /* Timeout of keys with a timeout set */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP)*/ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ int id; /* Database ID */ long long avg_ttl; /* Average TTL, just for stats */ unsigned long expires_cursor; /* Cursor of the active expire cycle. */ list *defrag_later; /* List of key names to attempt to defrag one by one, gradually. */&#125; redisDb; 默认情况下，会创建 dbnum=16 个数据库 每个Redis客户端都有自己的目标数据库，默认情况下，Redis客户端的目标数据库为0号数据库 不同的客户端是怎么选定目标数据库的？不同数据库之间又不会进行数据同步，都用同一个数据库其他不就没有用了 可以通过 SELECT N 来切换目标数据库 数据库健空间 redisDb结构的dict 字典保存了数据数据库中的所有键值对，称这个字典为键空间(key space) 键空间的键也就是数据库的键，每个键都是一个字符串对象 键空间的值也就是数据库的值，每个值可以是字符串对象、列表对象、哈希表对象、集合对象和有序集合对洗那个中的任意一种Redis对象 因为键空间是一个字段，所以所有针对数据库的实际都是对键空间字段进行操作来实现的 Redis 键空间是怎么解决Hash冲突的 除了读写还有一些维护操作 读取一个值之后，服务器会更新键的LRU(最近一次使用)时间，可以用来计算键的闲置时间。使用 Object idletime key 来查看键的空闲时间 如果服务器读取键已经过期，则会先删除这个键再进行其他操作 如果客户端使用WATCH 命令监视某个键，当键被修改，服务器会将键标记为脏(ditry)，从而让事务直到这个键被修改 服务器每次修改一个键之后，都会对脏(dirty)键计数器的值增1，这个计数器会触发服务器的持久化以及复制操作 如果服务器开启了数据库通知功能，那么对键修改后，会根据配置发送相应的数据库通知 键的生存时间与过期时间 redisDb结构的expires字典保存了数据库中所有键的过期时间，称为字典的过期字典 过期字典的键是一个指针，指针指向键空间中国暖的某个键对象(即是某个数据库键) 过期字典的值是一个long long 类型的整数，这个整数保存了键所有指向数据库键的过期时间–一个毫秒精度的unix 时间戳 过期键的判断 通过过期字典，程序可以通过以下步骤检查一个给定键是否过期 检查给顶键是否存在与过期字典；如果存在，那么取得键的过期时间 检查当前UNIX时间戳是否大于键的过期时间 过期键删除策略 数据键的过期时间都保存在过期字典中，如果一个键过期了，那么它是什么时候被删除的，可能存在三种不同的删除策略 定时删除：在设置键的过期时间的同时，创建一个定时器(timer)，让定时器在键的过期时间来临时，立即执行对键的删除操作 惰性删除：放任键过期不管，但每次从键空间中获取键时，都检查取得的键是否过期。过期删除，否则返回该键 定期删除：每隔一段时间，程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定 定时删除 定时删除策略对内存是最友好的：通过使用定时器，定时删除策略可以保证过期键会尽可能快的删除，并释放过期键锁占用的内存 缺点： 对CPU时间是最不友好的：在过期键比较多的情况下，删除过期键可能会占用相当一部分CPU时间，在内存不紧张但是CPU时间非常紧张的情况下，将CPU时间用在删除和当前任务无关的过期键上。五一对服务器的响应时间和吞吐量造成影响 创建一个定时器需要用到Redis服务器中的时间事件，而当前时间事件的实现方式–无序链表，查找一个事件的时间复杂度为O(N)–并不能高效地处理大量时间事件 惰性删除 惰性删除对CPU时间是最友好的：程序只会在取出键时才对键进行过期检查 缺点：对内存是最不友好的，如果一个键已经过期，而这个键又仍然保留在数据库中，那么只要这个过期键不被删除，它锁占用的内存就不会释放 定期删除 上面的定时删除和惰性删除在单一使用时都有明显的缺陷： 定时删除占用太多CPU时间，影响服务器的响应时间和吞吐量 惰性删除浪费太多内存，有内存泄露的危险 定期删除策略是前两种策略的一种整合和折中：每隔一段时间执行一次删除过期键的操作，并通过限制删除操作执行的时长和频率来减少删除操作对CPU时间的影响 问题是：如何设置定期删除的时间间隔，以及删除操作的执行时长 如果删除太频繁或者执行时间太长，定期删除策略就会退化成定时删除策略，以至于将CPU过多的浪费在删除过期键上 如果删除操作执行的太少或时间太短，定期删除策略又会和惰性删除策略一样，出现浪费内存的情况 Redis 的过期删除策略 Redis 实际使用的是 惰性删除和定期删除两种策略 惰性删除策略：所有读写的Redis命令在执行前都调用 expireIfNeeded 函数对键进行检查，如果键过期，函数将键从数据库删除 1234567891011121314151617181920212223int expireIfNeeded(redisDb *db, robj *key) &#123; if (!keyIsExpired(db,key)) return 0; /* If we are running in the context of a slave, instead of * evicting the expired key from the database, we return ASAP: * the slave key expiration is controlled by the master that will * send us synthesized DEL operations for expired keys. * * Still we try to return the right information to the caller, * that is, 0 if we think the key should be still valid, 1 if * we think the key is expired at this time. */ if (server.masterhost != NULL) return 1; /* If clients are paused, we keep the current dataset constant, * but return to the client what we believe is the right state. Typically, * at the end of the pause we will properly expire the key OR we will * have failed over and the new primary will send us the expire. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return 1; /* Delete the key */ deleteExpiredKeyAndPropagate(db,key); return 1;&#125; 注意上面的 server.masterhost != NULL 如果不为空，说明当前服务器为备机。那么即使当前键是过期的，也仅仅返回后状态1，而不会进行下面的删除操作 定时删除策略：每当Redis的服务器周期性操作 serverCron 函数执行时，activeExpireCycle 函数就会被调用，在规定时间内，分多次遍历服务器中各个数据库，从数据库的 expires 字典中随机检查一部分键的过期时间，并删除其中的过期键 函数每次运行时，都从一定数量的数据库中取出一定数量的随机键进行检查，并删除其中的过期键 全局变量 current_db记录当前 activeExpireCycle函数检查的进度，并在下一次 activeExpireCycle 函数调用时，接着上一次的进度进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206void activeExpireCycle(int type) &#123; /* Adjust the running parameters according to the configured expire * effort. The default effort is 1, and the maximum configurable effort * is 10. */ unsigned long effort = server.active_expire_effort-1, /* Rescale from 0 to 9. */ config_keys_per_loop = ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP + ACTIVE_EXPIRE_CYCLE_KEYS_PER_LOOP/4*effort, config_cycle_fast_duration = ACTIVE_EXPIRE_CYCLE_FAST_DURATION + ACTIVE_EXPIRE_CYCLE_FAST_DURATION/4*effort, config_cycle_slow_time_perc = ACTIVE_EXPIRE_CYCLE_SLOW_TIME_PERC + 2*effort, config_cycle_acceptable_stale = ACTIVE_EXPIRE_CYCLE_ACCEPTABLE_STALE- effort; /* This function has some global state in order to continue the work * incrementally across calls. */ static unsigned int current_db = 0; /* Next DB to test. */ static int timelimit_exit = 0; /* Time limit hit in previous call? */ static long long last_fast_cycle = 0; /* When last fast cycle ran. */ int j, iteration = 0; int dbs_per_call = CRON_DBS_PER_CALL; long long start = ustime(), timelimit, elapsed; /* When clients are paused the dataset should be static not just from the * POV of clients not being able to write, but also from the POV of * expires and evictions of keys not being performed. */ if (checkClientPauseTimeoutAndReturnIfPaused()) return; if (type == ACTIVE_EXPIRE_CYCLE_FAST) &#123; /* Don't start a fast cycle if the previous cycle did not exit * for time limit, unless the percentage of estimated stale keys is * too high. Also never repeat a fast cycle for the same period * as the fast cycle total duration itself. */ if (!timelimit_exit &amp;&amp; server.stat_expired_stale_perc &lt; config_cycle_acceptable_stale) return; if (start &lt; last_fast_cycle + (long long)config_cycle_fast_duration*2) return; last_fast_cycle = start; &#125; /* We usually should test CRON_DBS_PER_CALL per iteration, with * two exceptions: * * 1) Don't test more DBs than we have. * 2) If last time we hit the time limit, we want to scan all DBs * in this iteration, as there is work to do in some DB and we don't want * expired keys to use memory for too much time. */ if (dbs_per_call &gt; server.dbnum || timelimit_exit) dbs_per_call = server.dbnum; /* We can use at max 'config_cycle_slow_time_perc' percentage of CPU * time per iteration. Since this function gets called with a frequency of * server.hz times per second, the following is the max amount of * microseconds we can spend in this function. */ timelimit = config_cycle_slow_time_perc*1000000/server.hz/100; timelimit_exit = 0; if (timelimit &lt;= 0) timelimit = 1; if (type == ACTIVE_EXPIRE_CYCLE_FAST) timelimit = config_cycle_fast_duration; /* in microseconds. */ /* Accumulate some global stats as we expire keys, to have some idea * about the number of keys that are already logically expired, but still * existing inside the database. */ long total_sampled = 0; long total_expired = 0; for (j = 0; j &lt; dbs_per_call &amp;&amp; timelimit_exit == 0; j++) &#123; /* Expired and checked in a single loop. */ unsigned long expired, sampled; redisDb *db = server.db+(current_db % server.dbnum); /* Increment the DB now so we are sure if we run out of time * in the current DB we'll restart from the next. This allows to * distribute the time evenly across DBs. */ current_db++; /* Continue to expire if at the end of the cycle there are still * a big percentage of keys to expire, compared to the number of keys * we scanned. The percentage, stored in config_cycle_acceptable_stale * is not fixed, but depends on the Redis configured \"expire effort\". */ do &#123; unsigned long num, slots; long long now, ttl_sum; int ttl_samples; iteration++; /* If there is nothing to expire try next DB ASAP. */ if ((num = dictSize(db-&gt;expires)) == 0) &#123; db-&gt;avg_ttl = 0; break; &#125; slots = dictSlots(db-&gt;expires); now = mstime(); /* When there are less than 1% filled slots, sampling the key * space is expensive, so stop here waiting for better times... * The dictionary will be resized asap. */ if (slots &gt; DICT_HT_INITIAL_SIZE &amp;&amp; (num*100/slots &lt; 1)) break; /* The main collection cycle. Sample random keys among keys * with an expire set, checking for expired ones. */ expired = 0; sampled = 0; ttl_sum = 0; ttl_samples = 0; if (num &gt; config_keys_per_loop) num = config_keys_per_loop; /* Here we access the low level representation of the hash table * for speed concerns: this makes this code coupled with dict.c, * but it hardly changed in ten years. * * Note that certain places of the hash table may be empty, * so we want also a stop condition about the number of * buckets that we scanned. However scanning for free buckets * is very fast: we are in the cache line scanning a sequential * array of NULL pointers, so we can scan a lot more buckets * than keys in the same time. */ long max_buckets = num*20; long checked_buckets = 0; while (sampled &lt; num &amp;&amp; checked_buckets &lt; max_buckets) &#123; for (int table = 0; table &lt; 2; table++) &#123; if (table == 1 &amp;&amp; !dictIsRehashing(db-&gt;expires)) break; unsigned long idx = db-&gt;expires_cursor; idx &amp;= db-&gt;expires-&gt;ht[table].sizemask; dictEntry *de = db-&gt;expires-&gt;ht[table].table[idx]; long long ttl; /* Scan the current bucket of the current table. */ checked_buckets++; while(de) &#123; /* Get the next entry now since this entry may get * deleted. */ dictEntry *e = de; de = de-&gt;next; ttl = dictGetSignedIntegerVal(e)-now; if (activeExpireCycleTryExpire(db,e,now)) expired++; if (ttl &gt; 0) &#123; /* We want the average TTL of keys yet * not expired. */ ttl_sum += ttl; ttl_samples++; &#125; sampled++; &#125; &#125; db-&gt;expires_cursor++; &#125; total_expired += expired; total_sampled += sampled; /* Update the average TTL stats for this database. */ if (ttl_samples) &#123; long long avg_ttl = ttl_sum/ttl_samples; /* Do a simple running average with a few samples. * We just use the current estimate with a weight of 2% * and the previous estimate with a weight of 98%. */ if (db-&gt;avg_ttl == 0) db-&gt;avg_ttl = avg_ttl; db-&gt;avg_ttl = (db-&gt;avg_ttl/50)*49 + (avg_ttl/50); &#125; /* We can't block forever here even if there are many keys to * expire. So after a given amount of milliseconds return to the * caller waiting for the other active expire cycle. */ if ((iteration &amp; 0xf) == 0) &#123; /* check once every 16 iterations. */ elapsed = ustime()-start; if (elapsed &gt; timelimit) &#123; timelimit_exit = 1; server.stat_expired_time_cap_reached_count++; break; &#125; &#125; /* We don't repeat the cycle for the current database if there are * an acceptable amount of stale keys (logically expired but yet * not reclaimed). */ &#125; while (sampled == 0 || (expired*100/sampled) &gt; config_cycle_acceptable_stale); &#125; elapsed = ustime()-start; server.stat_expire_cycle_time_used += elapsed; latencyAddSampleIfNeeded(\"expire-cycle\",elapsed/1000); /* Update our estimate of keys existing but yet to be expired. * Running average with this sample accounting for 5%. */ double current_perc; if (total_sampled) &#123; current_perc = (double)total_expired/total_sampled; &#125; else current_perc = 0; server.stat_expired_stale_perc = (current_perc*0.05)+ (server.stat_expired_stale_perc*0.95);&#125; AOF、RDB和复制功能对过期键的处理 生成RDB文件：执行 SAVE 或 BGSAVE 创建一个新的RDB文件时，程序会对数据库中的键进行检查，已过期的键不会被保存到新创建的RDB文件中 载入RDB文件：在启动Redis服务器是，如果服务器开启了RDB功能，那么服务器将对RDB文件进行载入 如果服务器是以主服务器模式运行，那么载入RDB文件时，程序会对文件中保存键进行检查，过期键将被忽略。 如果服务器是已从服务器模式运行，那么载入RDB文件时，所有键不论是否过期，都将被载入到数据库中。因为主从服务器在进行数据同步时候，从服务器的数据库就会被清空，过期键对载入RDB文件的从服务器不会造成影响 AOF文件写入：当服务器以AOF持久化模式运行时，如果数据库中的某个键已经过期，但还没有被惰性或定期删除，AOF文件不会产生任何变化。如果被删除策略删除后，程序会向AOF文件追加AOF文件一个DEL命令，来显示的记录该键被删除 AOF重写：在执行AOF重写过程中，程序会对数据库中的键进行检查，已过期的键不会被保存到重写后的AOF文件中 复制：当服务器运行在复制模式下，从服务器的过期键删除动作由主服务器控制 主服务器删除一个过期键之后，会限制地向所有从服务器发送一个DEL命令，告知从服务器删除这个过期键 从服务器在执行客户端发送的读命令式，即使碰到过期键页不会将过期键删除，而是继续将处理未过期的键一样来处理过期键 从服务器只有在街道主服务器发来的DEL命令之后，才会删除过期键 12345678910111213141516171819202122232425262728293031robj *lookupKeyReadWithFlags(redisDb *db, robj *key, int flags) &#123; robj *val; if (expireIfNeeded(db,key) == 1) &#123; /* If we are in the context of a master, expireIfNeeded() returns 1 * when the key is no longer valid, so we can return NULL ASAP. */ if (server.masterhost == NULL) goto keymiss; //如果备机是只读模式，那么返回的就是空值 if (server.current_client &amp;&amp; server.current_client != server.master &amp;&amp; server.current_client-&gt;cmd &amp;&amp; server.current_client-&gt;cmd-&gt;flags &amp; CMD_READONLY) &#123; goto keymiss; &#125; &#125; val = lookupKey(db,key,flags); if (val == NULL) goto keymiss; server.stat_keyspace_hits++; return val;keymiss: if (!(flags &amp; LOOKUP_NONOTIFY)) &#123; notifyKeyspaceEvent(NOTIFY_KEY_MISS, \"keymiss\", key, db-&gt;id); &#125; server.stat_keyspace_misses++; return NULL;&#125; 如果从服务器是只读模式，那么返回的就是空值 否则即使键值已经过期，但是从服务器仍然能够返回该值 总结 Redis服务器所有数据库 都保存在 redisServer.db数组中，而数据库的数量则由 redisServer.dbnum属性保存 客户端通过修改目标数据库指针，让它指向redisServer.db数组中的不同元素来切换不同的数据库 数据库主要有dict 和 expires 两个字段构成，其中 dict字典负责保存键值对，而 expires 字典则负责保存键的过期时间 数据库的键总是一个字符串对象，而值则可以是任意一种Redis对象类型 expires 字典的键指向数据库中的某个键，而值则记录了数据库键的过期时间，过期时间是一个以毫秒为单位的UNIX时间戳 Redis 使用惰性删除和定期删除两种策略来删除过期的键 当一个过期键被删除之后，服务器会追加一条DEL命令到现在AOF文件的末尾，显示地删除过期键。从服务器即使发现过期键也不会主动删除它，而是等待主节点发来DEL命令 参考文献 《Redis 设计与实现》","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门2-结构对象","slug":"Redis/Redis入门2-结构对象","date":"2021-09-12T10:34:16.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/09/12/Redis/Redis入门2-结构对象/","link":"","permalink":"http://xboom.github.io/2021/09/12/Redis/Redis%E5%85%A5%E9%97%A82-%E7%BB%93%E6%9E%84%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"Redis 底层数据结构，比如简单动态字符串（SDS）、双端链表、字典、压缩列表、整数集合，等等。 Redis 并没有直接使用这些数据结构来实现键值对数据库，而是基于这些数据结构创建了一个对象系统，这个系统包含字符串对象、列表对象、哈希对象、集合对象和有序集合对象这五种类型的对象，每种对象都用到了至少一种前面所介绍的数据结构。 其中最新的版本中数据结构又进行了优化： Redis 的对象系统还实现了基于引用计数技术的内存回收机制：当程序不再使用某个对象的时候，对象所占用的内存就会被自动释放； Redis 还通过引用计数技术实现了对象共享机制，这一机制可以在适当的条件下，通过让多个数据库键共享同一个对象来节约内存。 最后，Redis 的对象带有访问时间记录信息，该信息可以用于计算数据库键的空转时长，在服务器启用了 maxmemory 功能的情况下，空转时长较大的那些键可能会优先被服务器删除。 对象的类型与编码 Redis 使用对象来表示数据库中的键和值，每当在 Redis 的数据库中新创建一个键值对时，至少会创建两个对象，一个对象用作键值对的键（键对象），另一个对象用作键值对的值（值对象）。 举个例子，以下 SET 命令在数据库中创建了一个新的键值对，其中键值对的键是一个包含了字符串值 &quot;msg&quot; 的对象，而键值对的值则是一个包含了字符串值 &quot;hello world&quot; 的对象： 12redis&gt; SET msg \"hello world\"OK Redis 中的每个对象都由一个 redisObject 结构表示，该结构中和保存数据有关的三个属性分别是 type 属性、 encoding 属性和 ptr 属性： 1234567891011121314typedef struct redisObject &#123; // 类型 unsigned type:4; // 编码 unsigned encoding:4; // 指向底层实现数据结构的指针 void *ptr; // ... &#125; robj; 类型 对象的 type 属性记录了对象的类型，这个属性的值可以是表中列出的常量的其中一个。 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 对于 Redis 数据库保存的键值对来说，键总是一个字符串对象，而值则可以是字符串对象、列表对象、哈希对象、集合对象或者有序集合对象的其中一种，因此： 当我们称呼一个数据库键为“字符串键”时，我们指的是“这个数据库键所对应的值为字符串对象”； 当我们称呼一个键为“列表键”时，我们指的是“这个数据库键所对应的值为列表对象”， 诸如此类。 TYPE 命令的实现也与此类似，对一个数据库键执行 TYPE 命令时，返回的结果为数据库键对应的值对象的类型，而不是键对象的类型： 12345678910111213141516171819202122232425262728293031323334# 键为字符串对象，值为字符串对象redis&gt; SET msg \"hello world\"OK redis&gt; TYPE msgstring # 键为字符串对象，值为列表对象redis&gt; RPUSH numbers 1 3 5(integer) 6 redis&gt; TYPE numberslist # 键为字符串对象，值为哈希对象redis&gt; HMSET profile name Tome age 25 career ProgrammerOK redis&gt; TYPE profilehash # 键为字符串对象，值为集合对象redis&gt; SADD fruits apple banana cherry(integer) 3 redis&gt; TYPE fruitsset # 键为字符串对象，值为有序集合对象redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 redis&gt; TYPE pricezset TYPE 命令在面对不同类型的值对象时所产生的输出: 对象 对象 type 属性的值 TYPE 命令的输出 字符串对象 REDIS_STRING &quot;string&quot; 列表对象 REDIS_LIST &quot;list&quot; 哈希对象 REDIS_HASH &quot;hash&quot; 集合对象 REDIS_SET &quot;set&quot; 有序集合对象 REDIS_ZSET &quot;zset&quot; 编码和底层实现 对象的 ptr 指针指向对象的底层实现数据结构，而这些数据结构由对象的 encoding 属性决定。 encoding 属性记录了对象所使用的编码，即这个对象使用了什么数据结构作为对象的底层实现，这个属性的值以下常量的其中一个。 编码常量 编码所对应的底层数据结构 REDIS_ENCODING_INT long 类型的整数 REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 每种类型的对象都至少使用了两种不同的编码，每种类型的对象可以使用的编码: REDIS_STRING REDIS_ENCODING_INT 使用整数值实现的字符串对象。 REDIS_STRING REDIS_ENCODING_EMBSTR 使用 embstr 编码的简单动态字符串实现的字符串对象。 REDIS_STRING REDIS_ENCODING_RAW 使用简单动态字符串实现的字符串对象。 REDIS_LIST REDIS_ENCODING_ZIPLIST 使用压缩列表实现的列表对象。 REDIS_LIST REDIS_ENCODING_LINKEDLIST 使用双端链表实现的列表对象。 REDIS_HASH REDIS_ENCODING_ZIPLIST 使用压缩列表实现的哈希对象。 REDIS_HASH REDIS_ENCODING_HT 使用字典实现的哈希对象。 REDIS_SET REDIS_ENCODING_INTSET 使用整数集合实现的集合对象。 REDIS_SET REDIS_ENCODING_HT 使用字典实现的集合对象。 REDIS_ZSET REDIS_ENCODING_ZIPLIST 使用压缩列表实现的有序集合对象。 REDIS_ZSET REDIS_ENCODING_SKIPLIST 使用跳跃表和字典实现的有序集合对象。 使用 OBJECT ENCODING 命令可以查看一个数据库键的值对象的编码： 1234567891011121314151617181920212223redis&gt; SET msg \"hello wrold\"OK redis&gt; OBJECT ENCODING msg\"embstr\" redis&gt; SET story \"long long long long long long ago ...\"OK redis&gt; OBJECT ENCODING story\"raw\" redis&gt; SADD numbers 1 3 5(integer) 3 redis&gt; OBJECT ENCODING numbers\"intset\" redis&gt; SADD numbers \"seven\"(integer) 1 redis&gt; OBJECT ENCODING numbers\"hashtable\" OBJECT ENCODING 对不同编码的输出 对象所使用的底层数据结构 编码常量 OBJECT ENCODING 命令输出 整数 REDIS_ENCODING_INT &quot;int&quot; embstr 编码的简单动态字符串（SDS） REDIS_ENCODING_EMBSTR &quot;embstr&quot; 简单动态字符串 REDIS_ENCODING_RAW &quot;raw&quot; 字典 REDIS_ENCODING_HT &quot;hashtable&quot; 双端链表 REDIS_ENCODING_LINKEDLIST &quot;linkedlist&quot; 压缩列表 REDIS_ENCODING_ZIPLIST &quot;ziplist&quot; 整数集合 REDIS_ENCODING_INTSET &quot;intset&quot; 跳跃表和字典 REDIS_ENCODING_SKIPLIST &quot;skiplist&quot; 字符串对象 字符串对象的编码可以是 int 、 raw 或者 embstr 。 如果一个字符串对象保存的是整数值，并且这个整数值可以用 long 类型来表示，那么字符串对象会将整数值保存在字符串对象结构的 ptr 属性里面（将 void* 转换成 long ），并将字符串对象的编码设置为 int 。 举个例子，如果执行以下 SET 命令，那么服务器将创建一个如图 8-1 所示的 int 编码的字符串对象作为 number 键的值： 12345redis&gt; SET number 10086OK redis&gt; OBJECT ENCODING number\"int\" 如果字符串对象保存的是一个字符串值，并且这个字符串值的长度大于 39 字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为 raw 。 举个例子，如果我们执行以下命令，那么服务器将创建一个如图 8-2 所示的 raw 编码的字符串对象作为 story 键的值： 12345678redis&gt; SET story \"Long, long, long ago there lived a king ...\"OK redis&gt; STRLEN story(integer) 43 redis&gt; OBJECT ENCODING story\"raw\" 如果字符串对象保存的是一个字符串值，且这个字符串值的长度小于等于 39 字节，字符串对象将使用 embstr 编码的方式来保存这个字符串值。 embstr 编码是专门用于保存短字符串的一种优化编码方式，这种编码和 raw 编码一样，都使用 redisObject 结构和 sdshdr 结构来表示字符串对象，但 raw 编码会调用两次内存分配函数来分别创建 redisObject 结构和 sdshdr 结构，而 embstr 编码则通过调用一次内存分配函数来分配一块连续的空间，空间中依次包含 redisObject 和 sdshdr 两个结构，如图 8-3 所示 embstr 编码的字符串对象在执行命令时，产生的效果和 raw 编码的字符串对象执行命令时产生的效果是相同的，但使用 embstr 编码的字符串对象来保存短字符串值有以下好处： embstr 编码将创建字符串对象所需的内存分配次数从 raw 编码的两次降低为一次。 释放 embstr 编码的字符串对象只需要调用一次内存释放函数，而释放 raw 编码的字符串对象需要调用两次内存释放函数。 因为 embstr 编码的字符串对象的所有数据都保存在一块连续的内存里面，所以这种编码的字符串对象比起 raw 编码的字符串对象能够更好地利用缓存带来的优势。作为例子，以下命令创建了一个 embstr 编码的字符串对象作为 msg 键的值，值对象的样子如图 8-4 所示： 12345redis&gt; SET msg \"hello\"OK redis&gt; OBJECT ENCODING msg\"embstr\" 最后要说的是，可以用 long double 类型表示的浮点数在 Redis 中也是作为字符串值来保存的：如果我们要保存一个浮点数到字符串对象里面，那么程序会先将这个浮点数转换成字符串值，然后再保存起转换所得的字符串值。 举个例子，执行以下代码将创建一个包含 3.14 的字符串表示 &quot;3.14&quot; 的字符串对象： 12345redis&gt; SET pi 3.14OK redis&gt; OBJECT ENCODING pi\"embstr\" 在有需要的时候，程序会将保存在字符串对象里面的字符串值转换回浮点数值，执行某些操作，然后再将执行操作所得的浮点数值转换回字符串值，并继续保存在字符串对象里面。 举个例子，如果我们执行以下代码的话： 12345redis&gt; INCRBYFLOAT pi 2.0\"5.14\" redis&gt; OBJECT ENCODING pi\"embstr\" 那么程序首先会取出字符串对象里面保存的字符串值 &quot;3.14&quot; ，将它转换回浮点数值 3.14 ，然后把 3.14 和 2.0 相加得出的值 5.14 转换成字符串 &quot;5.14&quot; ，并将这个 &quot;5.14&quot; 保存到字符串对象里面。 总结并列出了字符串对象保存各种不同类型的值所使用的编码方式 值 编码 可以用 long 类型保存的整数。 int 可以用 long double 类型保存的浮点数。 embstr 或者 raw 字符串值，或者因为长度太大而没办法用 long 类型表示的整数，又或者因为长度太大而没办法用 long double 类型表示的浮点数。 embstr 或者 raw 编码的转换 int 编码的字符串对象和 embstr 编码的字符串对象在条件满足的情况下，会被转换为 raw 编码的字符串对象。 对于 int 编码的字符串对象来说，如果我们向对象执行了一些命令，使得这个对象保存的不再是整数值，而是一个字符串值，那么字符串对象的编码将从 int 变为 raw 。 在下面的示例中，我们通过 APPEND 命令，向一个保存整数值的字符串对象追加了一个字符串值，因为追加操作只能对字符串值执行，所以程序会先将之前保存的整数值 10086 转换为字符串值 &quot;10086&quot; ，然后再执行追加操作，操作的执行结果就是一个 raw 编码的、保存了字符串值的字符串对象： 1234567891011121314redis&gt; SET number 10086OK redis&gt; OBJECT ENCODING number\"int\" redis&gt; APPEND number \" is a good number!\"(integer) 23 redis&gt; GET number\"10086 is a good number!\" redis&gt; OBJECT ENCODING number\"raw\" 另外，因为 Redis 没有为 embstr 编码的字符串对象编写任何相应的修改程序（只有 int 编码的字符串对象和 raw 编码的字符串对象有这些程序），所以 embstr 编码的字符串对象实际上是只读的：当我们对 embstr 编码的字符串对象执行任何修改命令时，程序会先将对象的编码从 embstr 转换成 raw ，然后再执行修改命令；因为这个原因，embstr 编码的字符串对象在执行修改命令之后，总会变成一个 raw 编码的字符串对象。 以下代码展示了一个 embstr 编码的字符串对象在执行 APPEND 命令之后，对象的编码从 embstr 变为 raw 的例子： 1234567891011redis&gt; SET msg \"hello world\"OK redis&gt; OBJECT ENCODING msg\"embstr\" redis&gt; APPEND msg \" again!\"(integer) 18 redis&gt; OBJECT ENCODING msg\"raw\" 字符串命令的实现 因为字符串键的值为字符串对象，所以用于字符串键的所有命令都是针对字符串对象来构建的， 列举部分字符串命令，以及这些命令在不同编码的字符串对象下的实现方法。 命令 int 编码的实现方法 embstr 编码的实现方法 raw 编码的实现方法 SET 使用 int 编码保存值。 使用 embstr 编码保存值。 使用 raw 编码保存值。 GET 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，然后向客户端返回这个字符串值。 直接向客户端返回字符串值。 直接向客户端返回字符串值。 APPEND 将对象转换成 raw 编码，然后按 raw编码的方式执行此操作。 将对象转换成 raw 编码，然后按 raw编码的方式执行此操作。 调用 sdscatlen 函数，将给定字符串追加到现有字符串的末尾。 INCRBYFLOAT 取出整数值并将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。 取出字符串值并尝试将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。如果字符串值不能被转换成浮点数，那么向客户端返回一个错误。 取出字符串值并尝试将其转换成long double 类型的浮点数，对这个浮点数进行加法计算，然后将得出的浮点数结果保存起来。如果字符串值不能被转换成浮点数，那么向客户端返回一个错误。 INCRBY 对整数值进行加法计算，得出的计算结果会作为整数被保存起来。 embstr 编码不能执行此命令，向客户端返回一个错误。 raw 编码不能执行此命令，向客户端返回一个错误。 DECRBY 对整数值进行减法计算，得出的计算结果会作为整数被保存起来。 embstr 编码不能执行此命令，向客户端返回一个错误。 raw 编码不能执行此命令，向客户端返回一个错误。 STRLEN 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，计算并返回这个字符串值的长度。 调用 sdslen 函数，返回字符串的长度。 调用 sdslen 函数，返回字符串的长度。 SETRANGE 将对象转换成 raw 编码，然后按 raw编码的方式执行此命令。 将对象转换成 raw 编码，然后按 raw编码的方式执行此命令。 将字符串特定索引上的值设置为给定的字符。 GETRANGE 拷贝对象所保存的整数值，将这个拷贝转换成字符串值，然后取出并返回字符串指定索引上的字符。 直接取出并返回字符串指定索引上的字符。 直接取出并返回字符串指定索引上的字符。 列表对象 列表对象的编码可以是 ziplist 或者 linkedlist 。 ziplist 编码的列表对象使用压缩列表作为底层实现，每个压缩列表节点（entry）保存了一个列表元素。 举个例子，如果执行以下 RPUSH 命令，那么服务器将创建一个列表对象作为 numbers 键的值： 12redis&gt; RPUSH numbers 1 \"three\" 5(integer) 3 如果 numbers 键的值对象使用的是 ziplist 编码，这个这个值对象将会是图 8-5 所展示的样子。 另一方面，linkedlist 编码的列表对象使用双端链表作为底层实现，每个双端链表节点（node）都保存了一个字符串对象，而每个字符串对象都保存了一个列表元素。 举个例子，如果前面所说的 numbers 键创建的列表对象使用的不是 ziplist 编码，而是 linkedlist 编码，那么 numbers 键的值对象将是图 8-6 所示的样子 注意 linkedlist 编码的列表对象在底层的双端链表结构中包含了多个字符串对象，这种嵌套字符串对象的行为在稍后介绍的哈希对象、集合对象和有序集合对象中都会出现，字符串对象是 Redis 五种类型的对象中唯一一种会被其他四种类型对象嵌套的对象。 为了简化字符串对象的表示，在图 8-6 使用了一个带有 StringObject 字样的格子来表示一个字符串对象，而 StringObject 字样下面的是字符串对象所保存的值。 比如说，图 8-7 代表的就是一个包含了字符串值 &quot;three&quot; 的字符串对象，它是 8-8 的简化表示。 编码的转换 当列表对象可以同时满足以下两个条件时，列表对象使用 ziplist 编码： 列表对象保存的所有字符串元素的长度都小于 64 字节； 列表对象保存的元素数量小于 512 个；不能满足这两个条件的列表对象需要使用 linkedlist 编码。 以上两个条件的上限值是可以修改的，具体请看配置文件中关于 list-max-ziplist-value 选项和 list-max-ziplist-entries 选项的说明 对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在压缩列表里的所有列表元素都会被转移并保存到双端链表里面，对象的编码也会从 ziplist 变为 linkedlist 。 会转换回来吗？ 以下代码展示了列表对象因为保存了长度太大的元素而进行编码转换的情况： 1234567891011121314# 所有元素的长度都小于 64 字节redis&gt; RPUSH blah \"hello\" \"world\" \"again\"(integer) 3 redis&gt; OBJECT ENCODING blah\"ziplist\" # 将一个 65 字节长的元素推入列表对象中redis&gt; RPUSH blah \"wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\"(integer) 4 # 编码已改变redis&gt; OBJECT ENCODING blah\"linkedlist\" 除此之外，以下代码展示了列表对象因为保存的元素数量过多而进行编码转换的情况： 1234567891011121314151617# 列表对象包含 512 个元素redis&gt; EVAL \"for i=1,512 do redis.call('RPUSH', KEYS[1], i) end\" 1 \"integers\"(nil) redis&gt; LLEN integers(integer) 512 redis&gt; OBJECT ENCODING integers\"ziplist\" # 再向列表对象推入一个新元素，使得对象保存的元素数量达到 513 个redis&gt; RPUSH integers 513(integer) 513 # 编码已改变redis&gt; OBJECT ENCODING integers\"linkedlist\" 列表命令的实现 因为列表键的值为列表对象，所以用于列表键的所有命令都是针对列表对象来构建的，表 8-8 列出了其中一部分列表键命令，以及这些命令在不同编码的列表对象下的实现方法 命令 ziplist 编码的实现方法 linkedlist 编码的实现方法 LPUSH 调用 ziplistPush 函数，将新元素推入到压缩列表的表头。 调用 listAddNodeHead 函数，将新元素推入到双端链表的表头。 RPUSH 调用 ziplistPush 函数，将新元素推入到压缩列表的表尾。 调用 listAddNodeTail 函数，将新元素推入到双端链表的表尾。 LPOP 调用 ziplistIndex 函数定位压缩列表的表头节点，在向用户返回节点所保存的元素之后，调用 ziplistDelete 函数删除表头节点。 调用 listFirst 函数定位双端链表的表头节点，在向用户返回节点所保存的元素之后，调用 listDelNode 函数删除表头节点。 RPOP 调用 ziplistIndex 函数定位压缩列表的表尾节点，在向用户返回节点所保存的元素之后，调用 ziplistDelete 函数删除表尾节点。 调用 listLast 函数定位双端链表的表尾节点，在向用户返回节点所保存的元素之后，调用 listDelNode 函数删除表尾节点。 LINDEX 调用 ziplistIndex 函数定位压缩列表中的指定节点，然后返回节点所保存的元素。 调用 listIndex 函数定位双端链表中的指定节点，然后返回节点所保存的元素。 LLEN 调用 ziplistLen 函数返回压缩列表的长度。 调用 listLength 函数返回双端链表的长度。 LINSERT 插入新节点到压缩列表的表头或者表尾时，使用 ziplistPush 函数；插入新节点到压缩列表的其他位置时，使用 ziplistInsert 函数。 调用 listInsertNode 函数，将新节点插入到双端链表的指定位置。 LREM 遍历压缩列表节点，并调用 ziplistDelete 函数删除包含了给定元素的节点。 遍历双端链表节点，并调用 listDelNode 函数删除包含了给定元素的节点。 LTRIM 调用 ziplistDeleteRange 函数，删除压缩列表中所有不在指定索引范围内的节点。 遍历双端链表节点，并调用 listDelNode 函数删除链表中所有不在指定索引范围内的节点。 LSET 调用 ziplistDelete 函数，先删除压缩列表指定索引上的现有节点，然后调用 ziplistInsert 函数，将一个包含给定元素的新节点插入到相同索引上面。 调用 listIndex 函数，定位到双端链表指定索引上的节点，然后通过赋值操作更新节点的值。 哈希对象 哈希对象的编码可以是 ziplist 或者 hashtable 。 ziplist 编码的哈希对象使用压缩列表作为底层实现，每当有新的键值对要加入到哈希对象时，程序会先将保存了键的压缩列表节点推入到压缩列表表尾，然后再将保存了值的压缩列表节点推入到压缩列表表尾，因此： 保存了同一键值对的两个节点总是紧挨在一起，保存键的节点在前，保存值的节点在后； 先添加到哈希对象中的键值对会被放在压缩列表的表头方向，而后来添加到哈希对象中的键值对会被放在压缩列表的表尾方向。 举个例子，如果我们执行以下 HSET 命令，那么服务器将创建一个列表对象作为 profile 键的值： 12345678redis&gt; HSET profile name \"Tom\"(integer) 1 redis&gt; HSET profile age 25(integer) 1 redis&gt; HSET profile career \"Programmer\"(integer) 1 如果 profile 键的值对象使用 ziplist 编码，那么这个值对象将会是图 8-9 所示的样子，其中对象所使用的压缩列表如图 8-10 所示 另一方面，hashtable 编码的哈希对象使用字典作为底层实现，哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象，对象中保存了键值对的键； 字典的每个值都是一个字符串对象，对象中保存了键值对的值。 举个例子，如果前面 profile 键创建的不是 ziplist 编码的哈希对象，而是 hashtable 编码的哈希对象，那么这个哈希对象应该会是图 8-11 所示的样子。 编码转换 当哈希对象可以同时满足以下两个条件时，哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； 不能满足这两个条件的哈希对象需要使用 hashtable 编码 注意： 两个条件的上限值可以修改，具体请看配置文件中 hash-max-ziplist-value 选项和 hash-max-ziplist-entries 选项的说明 对于使用 ziplist 编码的列表对象来说，当使用 ziplist 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在压缩列表里的所有键值对都会被转移并保存到字典里面，对象的编码也会从 ziplist 变为 hashtable 。 以下代码展示了哈希对象因为键值对的键长度太大而引起编码转换的情况： 1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET book name \"Mastering C++ in 21 days\"(integer) 1 redis&gt; OBJECT ENCODING book\"ziplist\" # 向哈希对象添加一个新的键值对，键的长度为 66 字节redis&gt; HSET book long_long_long_long_long_long_long_long_long_long_long_description \"content\"(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING book\"hashtable\" 除了键的长度太大会引起编码转换之外，值的长度太大也会引起编码转换，以下代码展示了这种情况的一个示例： 1234567891011121314# 哈希对象只包含一个键和值都不超过 64 个字节的键值对redis&gt; HSET blah greeting \"hello world\"(integer) 1 redis&gt; OBJECT ENCODING blah\"ziplist\" # 向哈希对象添加一个新的键值对，值的长度为 68 字节redis&gt; HSET blah story \"many string ... many string ... many string ... many string ... many\"(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING blah\"hashtable\" 最后，以下代码展示了哈希对象因为包含的键值对数量过多而引起编码转换的情况： 1234567891011121314151617181920# 创建一个包含 512 个键值对的哈希对象redis&gt; EVAL \"for i=1, 512 do redis.call('HSET', KEYS[1], i, i) end\" 1 \"numbers\"(nil) redis&gt; HLEN numbers(integer) 512 redis&gt; OBJECT ENCODING numbers\"ziplist\" # 再向哈希对象添加一个新的键值对，使得键值对的数量变成 513 个redis&gt; HMSET numbers \"key\" \"value\"OK redis&gt; HLEN numbers(integer) 513 # 编码改变redis&gt; OBJECT ENCODING numbers\"hashtable\" 哈希命令的实现 因为哈希键的值为哈希对象，所以用于哈希键的所有命令都是针对哈希对象来构建的，下表列出了其中一部分哈希键命令，以及这些命令在不同编码的哈希对象下的实现方法。 命令 ziplist 编码实现方法 hashtable 编码的实现方法 HSET 首先调用 ziplistPush 函数，将键推入到压缩列表的表尾，然后再次调用 ziplistPush 函数，将值推入到压缩列表的表尾。 调用 dictAdd 函数，将新节点添加到字典里面。 HGET 首先调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后调用 ziplistNext 函数，将指针移动到键节点旁边的值节点，最后返回值节点。 调用 dictFind 函数，在字典中查找给定键，然后调用 dictGetVal 函数，返回该键所对应的值。 HEXISTS 调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，如果找到的话说明键值对存在，没找到的话就说明键值对不存在。 调用 dictFind 函数，在字典中查找给定键，如果找到的话说明键值对存在，没找到的话就说明键值对不存在。 HDEL 调用 ziplistFind 函数，在压缩列表中查找指定键所对应的节点，然后将相应的键节点、以及键节点旁边的值节点都删除掉。 调用 dictDelete 函数，将指定键所对应的键值对从字典中删除掉。 HLEN 调用 ziplistLen 函数，取得压缩列表包含节点的总数量，将这个数量除以 2 ，得出的结果就是压缩列表保存的键值对的数量。 调用 dictSize 函数，返回字典包含的键值对数量，这个数量就是哈希对象包含的键值对数量。 HGETALL 遍历整个压缩列表，用 ziplistGet函数返回所有键和值（都是节点）。 遍历整个字典，用 dictGetKey 函数返回字典的键，用 dictGetVal 函数返回字典的值。 集合对象 集合对象的编码可以是 intset 或者 hashtable 。 intset 编码的集合对象使用整数集合作为底层实现，集合对象包含的所有元素都被保存在整数集合里面。 举个例子，以下代码将创建一个如图 8-12 所示的 intset 编码集合对象： 12redis&gt; SADD numbers 1 3 5(integer) 3 另一方面，hashtable 编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象包含了一个集合元素，而字典的值则全部被设置为 NULL 。 举个例子，以下代码将创建一个如图 8-13 所示的 hashtable 编码集合对象： 12redis&gt; SADD fruits \"apple\" \"banana\" \"cherry\"(integer) 3 编码转换 当集合对象可以同时满足以下两个条件时，对象使用 intset 编码： 集合对象保存的所有元素都是整数值； 集合对象保存的元素数量不超过 512 个； 不能满足这两个条件的集合对象需要使用 hashtable 编码。 注意 第二个条件的上限值是可以修改的，具体请看配置文件中关于 set-max-intset-entries 选项的说明。 对于使用 intset 编码的集合对象来说，当使用 intset 编码所需的两个条件的任意一个不能被满足时，对象的编码转换操作就会被执行：原本保存在整数集合中的所有元素都会被转移并保存到字典里面，并且对象的编码也会从 intset 变为 hashtable 。 举个例子，以下代码创建了一个只包含整数元素的集合对象，该对象的编码为 intset ： 12345redis&gt; SADD numbers 1 3 5(integer) 3 redis&gt; OBJECT ENCODING numbers\"intset\" 不过，只要我们向这个只包含整数元素的集合对象添加一个字符串元素，集合对象的编码转移操作就会被执行： 1234redis&gt; SADD numbers \"seven\"(integer) 1 redis&gt; OBJECT ENCODING numbers\"hashtable\" 除此之外，如果我们创建一个包含 512 个整数元素的集合对象，那么对象的编码应该会是 intset ： 123456redis&gt; EVAL \"for i=1, 512 do redis.call('SADD', KEYS[1], i) end\" 1 integers(nil) redis&gt; SCARD integers(integer) 512 redis&gt; OBJECT ENCODING integers\"intset\" 但是，只要我们再向集合添加一个新的整数元素，使得这个集合的元素数量变成 513 ，那么对象的编码转换操作就会被执行 12345678redis&gt; SADD integers 10086(integer) 1 redis&gt; SCARD integers(integer) 513 redis&gt; OBJECT ENCODING integers\"hashtable\" 集合命令的实现 因为集合键的值为集合对象，所以用于集合键的所有命令都是针对集合对象来构建的，表 8-10 列出了其中一部分集合键命令，以及这些命令在不同编码的集合对象下的实现方法。 命令 intset 编码的实现方法 hashtable 编码的实现方法 SADD 调用 intsetAdd 函数，将所有新元素添加到整数集合里面。 调用 dictAdd ，以新元素为键， NULL 为值，将键值对添加到字典里面。 SCARD 调用 intsetLen 函数，返回整数集合所包含的元素数量，这个数量就是集合对象所包含的元素数量。 调用 dictSize 函数，返回字典所包含的键值对数量，这个数量就是集合对象所包含的元素数量。 SISMEMBER 调用 intsetFind 函数，在整数集合中查找给定的元素，如果找到了说明元素存在于集合，没找到则说明元素不存在于集合。 调用 dictFind 函数，在字典的键中查找给定的元素，如果找到了说明元素存在于集合，没找到则说明元素不存在于集合。 SMEMBERS 遍历整个整数集合，使用 intsetGet 函数返回集合元素。 遍历整个字典，使用 dictGetKey 函数返回字典的键作为集合元素。 SRANDMEMBER 调用 intsetRandom 函数，从整数集合中随机返回一个元素。 调用 dictGetRandomKey 函数，从字典中随机返回一个字典键。 SPOP 调用 intsetRandom 函数，从整数集合中随机取出一个元素，在将这个随机元素返回给客户端之后，调用 intsetRemove 函数，将随机元素从整数集合中删除掉。 调用 dictGetRandomKey 函数，从字典中随机取出一个字典键，在将这个随机字典键的值返回给客户端之后，调用 dictDelete 函数，从字典中删除随机字典键所对应的键值对。 SREM 调用 intsetRemove 函数，从整数集合中删除所有给定的元素。 调用 dictDelete 函数，从字典中删除所有键为给定元素的键值对 有序集合对象 有序集合的编码可以是 ziplist 或者 skiplist 。 ziplist 编码的有序集合对象使用压缩列表作为底层实现，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员（member），而第二个元素则保存元素的分值（score）。 压缩列表内的集合元素按分值从小到大进行排序，分值较小的元素被放置在靠近表头的方向，而分值较大的元素则被放置在靠近表尾的方向。 举个例子，如果我们执行以下 ZADD 命令，那么服务器将创建一个有序集合对象作为 price 键的值： 12redis&gt; ZADD price 8.5 apple 5.0 banana 6.0 cherry(integer) 3 如果 price 键的值对象使用的是 ziplist 编码，那么这个值对象将会是图 8-14 所示的样子，而对象所使用的压缩列表则会是 8-15 所示的样子。 skiplist 编码的有序集合对象使用 zset 结构作为底层实现，一个 zset 结构同时包含一个字典和一个跳跃表： 1234567typedef struct zset &#123; zskiplist *zsl; dict *dict; &#125; zset; zset 结构中的 zsl 跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素：跳跃表节点的 object 属性保存了元素的成员，而跳跃表节点的 score 属性则保存了元素的分值。通过这个跳跃表，程序可以对有序集合进行范围型操作，比如 ZRANK 、 ZRANGE 等命令就是基于跳跃表 API 来实现的。 除此之外，zset 结构中的 dict 字典为有序集合创建了一个从成员到分值的映射，字典中的每个键值对都保存了一个集合元素：字典的键保存了元素的成员，而字典的值则保存了元素的分值。通过这个字典，程序可以用 O(1) 复杂度查找给定成员的分值，ZSCORE 命令就是根据这一特性实现的，而很多其他有序集合命令都在实现的内部用到了这一特性。 有序集合每个元素的成员都是一个字符串对象，而每个元素的分值都是一个 double 类型的浮点数。值得一提的是，虽然 zset 结构同时使用跳跃表和字典来保存有序集合元素，但这两种数据结构都会通过指针来共享相同元素的成员和分值，所以同时使用跳跃表和字典来保存集合元素不会产生任何重复成员或者分值，也不会因此而浪费额外的内存。 为什么有序集合需要同时使用跳跃表和字典来实现？ 在理论上来说，有序集合可以单独使用字典或者跳跃表的其中一种数据结构来实现，但无论单独使用字典还是跳跃表，在性能上对比起同时使用字典和跳跃表都会有所降低。 举个例子，如果我们只使用字典来实现有序集合，那么虽然以 O(1) 复杂度查找成员的分值这一特性会被保留，但是，因为字典以无序的方式来保存集合元素，所以每次在执行范围型操作 ——比如 ZRANK 、 ZRANGE 等命令时，程序都需要对字典保存的所有元素进行排序，完成这种排序需要至少 O(N \\log N) 时间复杂度，以及额外的 O(N) 内存空间（因为要创建一个数组来保存排序后的元素）。 另一方面，如果我们只使用跳跃表来实现有序集合，那么跳跃表执行范围型操作的所有优点都会被保留，但因为没有了字典，所以根据成员查找分值这一操作的复杂度将从 O(1) 上升为 O(\\log N) 。 因为以上原因，为了让有序集合的查找和范围型操作都尽可能快地执行，Redis 选择了同时使用字典和跳跃表两种数据结构来实现有序集合。 举个例子，如果前面 price 键创建的不是 ziplist 编码的有序集合对象，而是 skiplist 编码的有序集合对象，那么这个有序集合对象将会是图 8-16 所示的样子，而对象所使用的 zset 结构将会是图 8-17 所示的样子。 注意 为了展示方便，图 8-17 在字典和跳跃表中重复展示了各个元素的成员和分值，但在实际中，字典和跳跃表会共享元素的成员和分值，所以并不会造成任何数据重复，也不会因此而浪费任何内存 编码的转换 当有序集合对象可以同时满足以下两个条件时，对象使用 ziplist 编码： 有序集合保存的元素数量小于 128 个； 有序集合保存的所有元素成员的长度都小于 64 字节； 不能满足以上两个条件的有序集合对象将使用 skiplist 编码。 注意 以上两条件的上限值可修改，具体请看配置文件中 zset-max-ziplist-entries 选项和 zset-max-ziplist-value 选项的说明。 对于使用 ziplist 编码的有序集合对象来说，当使用 ziplist 编码所需的两个条件中的任意一个不能被满足时，程序就会执行编码转换操作，将原本储存在压缩列表里面的所有集合元素转移到 zset 结构里面，并将对象的编码从 ziplist 改为 skiplist 。 以下代码展示了有序集合对象因为包含了过多元素而引发编码转换的情况： 123456789101112131415161718192021# 对象包含了 128 个元素redis&gt; EVAL \"for i=1, 128 do redis.call('ZADD', KEYS[1], i, i) end\" 1 numbers(nil) redis&gt; ZCARD numbers(integer) 128 redis&gt; OBJECT ENCODING numbers\"ziplist\" # 再添加一个新元素redis&gt; ZADD numbers 3.14 pi(integer) 1 # 对象包含的元素数量变为 129 个redis&gt; ZCARD numbers(integer) 129 # 编码已改变redis&gt; OBJECT ENCODING numbers\"skiplist\" 以下代码则展示了有序集合对象因为元素的成员过长而引发编码转换的情况： 1234567891011121314# 向有序集合添加一个成员只有三字节长的元素redis&gt; ZADD blah 1.0 www(integer) 1 redis&gt; OBJECT ENCODING blah\"ziplist\" # 向有序集合添加一个成员为 66 字节长的元素redis&gt; ZADD blah 2.0 oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo(integer) 1 # 编码已改变redis&gt; OBJECT ENCODING blah\"skiplist\" 有序集合命令的实现 因为有序集合键的值为有序集合对象，所以用于有序集合键的所有命令都是针对有序集合对象来构建的，表 8-11 列出了其中一部分有序集合键命令，以及这些命令在不同编码的有序集合对象下的实现方法。 命令 ziplist 编码的实现方法 zset 编码的实现方法 ZADD 调用 ziplistInsert 函数，将成员和分值作为两个节点分别插入到压缩列表。 先调用 zslInsert 函数，将新元素添加到跳跃表，然后调用 dictAdd 函数，将新元素关联到字典。 ZCARD 调用 ziplistLen 函数，获得压缩列表包含节点的数量，将这个数量除以 2 得出集合元素的数量。 访问跳跃表数据结构的 length 属性，直接返回集合元素的数量。 ZCOUNT 遍历压缩列表，统计分值在给定范围内的节点的数量。 遍历跳跃表，统计分值在给定范围内的节点的数量。 ZRANGE 从表头向表尾遍历压缩列表，返回给定索引范围内的所有元素。 从表头向表尾遍历跳跃表，返回给定索引范围内的所有元素。 ZREVRANGE 从表尾向表头遍历压缩列表，返回给定索引范围内的所有元素。 从表尾向表头遍历跳跃表，返回给定索引范围内的所有元素。 ZRANK 从表头向表尾遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 从表头向表尾遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 ZREVRANK 从表尾向表头遍历压缩列表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 从表尾向表头遍历跳跃表，查找给定的成员，沿途记录经过节点的数量，当找到给定成员之后，途经节点的数量就是该成员所对应元素的排名。 ZREM 遍历压缩列表，删除所有包含给定成员的节点，以及被删除成员节点旁边的分值节点。 遍历跳跃表，删除所有包含了给定成员的跳跃表节点。并在字典中解除被删除元素的成员和分值的关联。 ZSCORE 遍历压缩列表，查找包含了给定成员的节点，然后取出成员节点旁边的分值节点保存的元素分值。 直接从字典中取出给定成员的分值。 类型检查和命令多态 Redis 中用于操作键的命令基本上可以分为两种类型。 其中一种命令可以对任何类型的键执行，比如说 DEL 命令、 EXPIRE 命令、 RENAME 命令、 TYPE 命令、 OBJECT 命令，等等。 举个例子，以下代码就展示了使用 DEL 命令来删除三种不同类型的键： 1234567891011121314151617181920# 字符串键redis&gt; SET msg \"hello\"OK # 列表键redis&gt; RPUSH numbers 1 2 3(integer) 3 # 集合键redis&gt; SADD fruits apple banana cherry(integer) 3 redis&gt; DEL msg(integer) 1 redis&gt; DEL numbers(integer) 1 redis&gt; DEL fruits(integer) 1 而另一种命令只能对特定类型的键执行，比如说： SET 、 GET 、 APPEND 、 STRLEN 等命令只能对字符串键执行； HDEL 、 HSET 、 HGET 、 HLEN 等命令只能对哈希键执行； RPUSH 、 LPOP 、 LINSERT 、 LLEN 等命令只能对列表键执行； SADD 、 SPOP 、 SINTER 、 SCARD 等命令只能对集合键执行； ZADD 、 ZCARD 、 ZRANK 、 ZSCORE 等命令只能对有序集合键执行； 举个例子，可以用 SET 命令创建一个字符串键，然后用 GET 命令和 APPEND 命令操作这个键，但如果我们试图对这个字符串键执行只有列表键才能执行的 LLEN 命令，那么 Redis 将向我们返回一个类型错误： 1234567891011121314redis&gt; SET msg \"hello world\"OK redis&gt; GET msg\"hello world\" redis&gt; APPEND msg \" again!\"(integer) 18 redis&gt; GET msg\"hello world again!\" redis&gt; LLEN msg(error) WRONGTYPE Operation against a key holding the wrong kind of value 类型检查的实现 从上面发生类型错误的代码示例可以看出，为了确保只有指定类型的键可以执行某些特定的命令，在执行一个类型特定的命令之前，Redis 会先检查输入键的类型是否正确，然后再决定是否执行给定的命令。 类型特定命令所进行的类型检查是通过 redisObject 结构的 type 属性来实现的： 在执行一个类型特定命令之前，服务器会先检查输入数据库键的值对象是否为执行命令所需的类型，如果是的话，服务器就对键执行指定的命令； 否则，服务器将拒绝执行命令，并向客户端返回一个类型错误。 举个例子，对于 LLEN 命令来说： 在执行 LLEN 命令之前，服务器会先检查输入数据库键的值对象是否为列表类型，也即是，检查值对象 redisObject 结构 type 属性的值是否为 REDIS_LIST ，如果是的话，服务器就对键执行 LLEN 命令； 否则的话，服务器就拒绝执行命令并向客户端返回一个类型错误； 多态命令的实现 Redis 除了会根据值对象的类型来判断键是否能够执行指定命令之外，还会根据值对象编码方式，选择正确的命令实现代码来执行命令。 举个例子，在前面介绍列表对象的编码时我们说过，列表对象有 ziplist 和 linkedlist 两种编码可用，其中前者使用压缩列表 API 来实现列表命令，而后者则使用双端链表 API 来实现列表命令。 现在，考虑这样一个情况，如果我们对一个键执行 LLEN 命令，那么服务器除了要确保执行命令的是列表键之外，还需要根据键的值对象所使用的编码来选择正确的 LLEN 命令实现： 如果列表对象的编码为 ziplist ，那么说明列表对象的实现为压缩列表，程序将使用 ziplistLen 函数来返回列表的长度； 如果列表对象的编码为 linkedlist ，那么说明列表对象的实现为双端链表，程序将使用 listLength 函数来返回双端链表的长度； 借用面向对象方面的术语来说，我们可以认为 LLEN 命令是多态（polymorphism）的：只要执行 LLEN 命令的是列表键，那么无论值对象使用的是 ziplist 编码还是 linkedlist 编码，命令都可以正常执行。 图 8-19 展示了 LLEN 命令从类型检查到根据编码选择实现函数的整个执行过程，其他类型特定命令的执行过程也是类似的。 实际上，我们可以将 DEL 、 EXPIRE 、 TYPE 等命令也称为多态命令，因为无论输入的键是什么类型，这些命令都可以正确地执行。 DEL 、 EXPIRE 等命令和 LLEN 等命令的区别在于，前者是基于类型的多态 —— 一个命令可以同时用于处理多种不同类型的键，而后者是基于编码的多态 —— 一个命令可以同时用于处理多种不同编码 内存回收 因为 C 语言并不具备自动的内存回收功能，所以 Redis 在自己的对象系统中构建了一个引用计数（reference counting）技术实现的内存回收机制，通过这一机制，程序可以通过跟踪对象的引用计数信息，在适当的时候自动释放对象并进行内存回收。 每个对象的引用计数信息由 redisObject 结构的 refcount 属性记录： 12345678910typedef struct redisObject &#123; // ... // 引用计数 int refcount; // ... &#125; robj; 对象的引用计数信息会随着对象的使用状态而不断变化： 在创建一个新对象时，引用计数的值会被初始化为 1 ； 当对象被一个新程序使用时，它的引用计数值会被增一； 当对象不再被一个程序使用时，它的引用计数值会被减一； 当对象的引用计数值变为 0 时，对象所占用的内存会被释放。 函数 作用 incrRefCount 将对象的引用计数值增一。 decrRefCount 将对象的引用计数值减一，当对象的引用计数值等于 0 时，释放对象。 resetRefCount 将对象的引用计数值设置为 0 ，但并不释放对象，这个函数通常在需要重新设置对象的引用计数值时使用。 对象的整个生命周期可以划分为创建对象、操作对象、释放对象三个阶段。 作为例子，以下代码展示了一个字符串对象从创建到释放的整个过程： 12345678// 创建一个字符串对象 s ，对象的引用计数为 1robj *s = createStringObject(...) // 对象 s 执行各种操作 ... // 将对象 s 的引用计数减一，使得对象的引用计数变为 0// 导致对象 s 被释放decrRefCount(s) 谁来负责查看数量以及触发回收呢？ 对象共享 除了用于实现引用计数内存回收机制之外，对象的引用计数属性还带有对象共享的作用。 举个例子，假设键 A 创建了一个包含整数值 100 的字符串对象作为值对象，如图 8-20 所示。 如果这时键 B 也要创建一个同样保存了整数值 100 的字符串对象作为值对象，那么服务器有以下两种做法： 为键 B 新创建一个包含整数值 100 的字符串对象； 让键 A 和键 B 共享同一个字符串对象；以上两种方法很明显是第二种方法更节约内存。 在 Redis 中，让多个键共享同一个值对象需要执行以下两个步骤： 将数据库键的值指针指向一个现有的值对象； 将被共享的值对象的引用计数增一。举个例子，图 8-21 就展示了包含整数值 100 的字符串对象同时被键 A 和键 B 共享之后的样子，可以看到，除了对象的引用计数从之前的 1 变成了 2 之外，其他属性都没有变化。 它是怎么知道有相同的值的？ 共享对象机制对于节约内存非常有帮助，数据库中保存的相同值对象越多，对象共享机制就能节约越多的内存。 比如说，假设数据库中保存了整数值 100 的键不只有键 A 和键 B 两个，而是有一百个，那么服务器只需要用一个字符串对象的内存就可以保存原本需要使用一百个字符串对象的内存才能保存的数据。 目前来说，Redis 会在初始化服务器时，创建一万个字符串对象，这些对象包含了从 0 到 9999 的所有整数值，当服务器需要用到值为 0 到 9999 的字符串对象时，服务器就会使用这些共享对象，而不是新创建对象。 注意：创建共享字符串对象的数量可以通过修改 redis.h/REDIS_SHARED_INTEGERS 常量来修改。 举个例子，如果我们创建一个值为 100 的键 A ，并使用 OBJECT REFCOUNT 命令查看键 A 的值对象的引用计数，我们会发现值对象的引用计数为 2 ： 12345redis&gt; SET A 100OK redis&gt; OBJECT REFCOUNT A(integer) 2 引用这个值对象的两个程序分别是持有这个值对象的服务器程序，以及共享这个值对象的键 A ，如图 8-22 所示。 如果这时再创建一个值为 100 的键 B ，那么键 B 也会指向包含整数值 100 的共享对象，使得共享对象的引用计数值变为 3 ： 12345678redis&gt; SET B 100OK redis&gt; OBJECT REFCOUNT A(integer) 3 redis&gt; OBJECT REFCOUNT B(integer) 3 另外，这些共享对象不单单只有字符串键可以使用，那些在数据结构中嵌套了字符串对象的对象（linkedlist 编码的列表对象、 hashtable 编码的哈希对象、 hashtable 编码的集合对象、以及 zset 编码的有序集合对象）都可以使用这些共享对象 为什么 Redis 不共享包含字符串的对象？ 当服务器考虑将一个共享对象设置为键的值对象时，程序需要先检查给定的共享对象和键想创建的目标对象是否完全相同，只有在共享对象和目标对象完全相同的情况下，程序才会将共享对象用作键的值对象，而一个共享对象保存的值越复杂，验证共享对象和目标对象是否相同所需的复杂度就会越高，消耗的 CPU 时间也会越多： 如果共享对象是保存整数值的字符串对象，那么验证操作的复杂度为 O(1) ； 如果共享对象是保存字符串值的字符串对象，那么验证操作的复杂度为 O(N) ； 如果共享对象是包含了多个值（或者对象的）对象，比如列表对象或者哈希对象，那么验证操作的复杂度将会是 O(N^2) 。 因此，尽管共享更复杂的对象可以节约更多的内存，但受到 CPU 时间的限制，Redis 只对包含整数值的字符串对象进行共享。 对象的空转时长 除了前面介绍过的 type 、 encoding 、 ptr 和 refcount 四个属性之外，redisObject 结构包含的最后一个属性为 lru 属性，该属性记录了对象最后一次被命令程序访问的时间： 123456789typedef struct redisObject &#123; // ... unsigned lru:22; // ... &#125; robj; OBJECT IDLETIME 命令可以打印出给定键的空转时长，这一空转时长就是通过将当前时间减去键的值对象的 lru 时间计算得出的： 123456789101112131415161718redis&gt; SET msg \"hello world\"OK # 等待一小段时间redis&gt; OBJECT IDLETIME msg(integer) 20 # 等待一阵子redis&gt; OBJECT IDLETIME msg(integer) 180 # 访问 msg 键的值redis&gt; GET msg\"hello world\" # 键处于活跃状态，空转时长为 0redis&gt; OBJECT IDLETIME msg(integer) 0 注意: OBJECT IDLETIME 命令的实现是特殊的，这个命令在访问键的值对象时，不会修改值对象的 lru 属性。 除了可以被 OBJECT IDLETIME 命令打印出来之外，键的空转时长还有另外一项作用：如果服务器打开了 maxmemory 选项，并且服务器用于回收内存的算法为 volatile-lru 或者 allkeys-lru ，那么当服务器占用的内存数超过了 maxmemory 选项所设置的上限值时，空转时长较高的那部分键会优先被服务器释放，从而回收内存。 配置文件的 maxmemory 选项和 maxmemory-policy 选项的说明介绍了关于这方面的更多信息 总结 Redis 数据库中的每个键值对的键和值都是一个对象。 Redis 共有字符串、列表、哈希、集合、有序集合五种类型的对象，每种类型的对象至少都有两种或以上的编码方式，不同的编码可以在不同的使用场景上优化对象的使用效率。 服务器在执行某些命令之前，会先检查给定键的类型能否执行指定的命令，而检查一个键的类型就是检查键的值对象的类型。 Redis 的对象系统带有引用计数实现的内存回收机制，当一个对象不再被使用时，该对象所占用的内存就会被自动释放。 Redis 会共享值为 0 到 9999 的字符串对象。 对象会记录自己的最后一次被访问的时间，这个时间可以用于计算对象的空转时间 参考链接 https://www.bookstack.cn/read/redisbook/433f95a1e435a4da.md https://www.cnblogs.com/remcarpediem/p/11755860.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Redis入门1-底层结构","slug":"Redis/Redis入门1-底层结构","date":"2021-09-12T09:24:54.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/09/12/Redis/Redis入门1-底层结构/","link":"","permalink":"http://xboom.github.io/2021/09/12/Redis/Redis%E5%85%A5%E9%97%A81-%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/","excerpt":"","text":"前言 常见的数据结构有： String 特点：整数值、embstr编码的简单动态字符串、简单动态字符串 应用：简单k,v 存储 List 特点：有序可重复 应用：作轻量级别的队列来使用。左推左拉、右推右拉 Hash 特点： 应用：需要两层key的应用场景 Set 特点：无序不重复 应用：标签 Sorted Set 特点：有序不重复 应用：排行榜之类的 底层数据结构实现： 简单动态字符串 Redis 自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将 SDS 用作 Redis 的默认字符串表示。SDS是一种可以被修改的字符串 Redis 也会用到 C 语言传统字符，但只作为字符串字面量（string literal），用在一些无须对字符串值进行修改的地方，比如打印日志： 1redisLog(REDIS_WARNING,\"Redis is now ready to exit, bye bye...\"); 当执行命令： 12redis&gt; SET msg \"hello world\"OK Redis 将在数据库中创建了一个新的键值对，其中： 键值对的键是一个字符串对象，对象的底层实现是一个保存着字符串 &quot;msg&quot; 的 SDS 。 键值对的值也是一个字符串对象，对象的底层实现是一个保存着字符串 &quot;hello world&quot; 的 SDS 。 除了用来保存数据库中的字符串值之外，AOF 模块中的 AOF 缓冲区，以及客户端状态中的输入缓冲区，都是由 SDS 实现的 SDS定义 每个 sds.h/sdshdr 结构表示一个 SDS 值： 123456789struct sdshdr &#123; // 记录 buf 数组中已使用字节的数量 // 等于 SDS 所保存字符串的长度 int len; // 记录 buf 数组中未使用字节的数量 int free; // 字节数组，用于保存字符串 char buf[]; &#125;; free 属性的值为 0 ，表示这个 SDS 没有分配任何未使用空间。 len 属性的值为 5 ，表示这个 SDS 保存了一个五字节长的字符串。 buf 属性是一个 char 类型的数组，数组的前五个字节分别保存了 'R' 、 'e' 、 'd' 、 'i' 、 's' 五个字符，而最后一个字节则保存了空字符 '\\0' 。 SDS 遵循 C 字符串以空字符结尾的惯例，保存空字符的 1 字节空间不计算在 SDS 的 len 属性里面，并且为空字符分配额外的 1 字节空间，以及添加空字符到字符串末尾等操作都是由 SDS 函数自动完成的，这个空字符对于 SDS 的使用者来说是完全透明的。 遵循空字符结尾这一惯例的好处是，SDS 可以直接重用一部分 C 字符串函数库里面的函数。 举个例子，如果我们有一个指向图 2-1 所示 SDS 的指针 s ，那么我们可以直接使用 stdio.h/printf 函数，通过执行以下语句： 这个 SDS 和之前展示的 SDS 一样，都保存了字符串值 &quot;Redis&quot; 。 这个 SDS 和之前展示的 SDS 的区别在于，这个 SDS 为 buf 数组分配了五字节未使用空间，所以它的 free 属性的值为 5（图中使用五个空格来表示五字节的未使用空间） 其中 保存空字符的 1 字节空间不计算在 SDS 的 len 和 free 属性里面 SDS与C字符串的区别 C 语言使用长度为 N+1 的字符数组来表示长度为 N 的字符串，并且字符数组的最后一个元素总是空字符 '\\0' 。 比如说，图 2-3 就展示了一个值为 &quot;Redis&quot; 的 C 字符串 C 语言使用的这种简单的字符串表示方式，并不能满足 Redis 对字符串在安全性、效率、以及功能方面的要求，接下来将说明 SDS 比 C 字符串更适用于 Redis 的原因 常数复杂度获取字符串长度 因为 C 字符串并不记录自身的长度信息，所以为了获取一个 C 字符串的长度，程序必须遍历整个字符串，对遇到的每个字符进行计数，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为 O(N) 。 举个例子，图 2-4 展示了程序计算一个 C 字符串长度的过程 和 C 字符串不同，因为 SDS 在 len 属性中记录了 SDS 本身的长度，所以获取一个 SDS 长度的复杂度仅为 O(1) 。 举个例子，对于图 2-5 所示的 SDS 来说，程序只要访问 SDS 的 len 属性，就可以立即知道 SDS 的长度为 5 字节： 又比如说，对于图 2-6 展示的 SDS 来说，程序只要访问 SDS 的 len 属性，就可以立即知道 SDS 的长度为 11 字节。 设置和更新 SDS 长度的工作是由 SDS 的 API 在执行时自动完成的，使用 SDS 无须进行任何手动修改长度的工作。 通过使用 SDS 而不是 C 字符串，Redis 将获取字符串长度所需的复杂度从 O(N) 降低到了 O(1) ，即使对一个非常长的字符串键反复执行 STRLEN 命令，也不会对系统性能造成任何影响，因为 STRLEN 命令的复杂度仅为 O(1) 杜绝缓冲区溢出 除了获取字符串长度的复杂度高之外，C 字符串不记录自身长度带来的另一个问题是容易造成缓冲区溢出 (buffer overflow) 举个例子，&lt;string.h&gt;/strcat 函数可以将 src 字符串中的内容拼接到 dest 字符串的末尾： 1char *strcat(char *dest, const char *src); 因为 C 字符串不记录自身的长度，所以 strcat 假定用户在执行这个函数时，已经为 dest 分配了足够多的内存，可以容纳 src 字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。 举个例子，假设程序里有两个在内存中紧邻着的 C 字符串 s1 和 s2 ，其中 s1 保存了字符串 &quot;Redis&quot; ，而 s2 则保存了字符串 &quot;MongoDB&quot; ，如图 2-7 所示 如果一个程序员决定通过执行： 1strcat(s1, \" Cluster\"); 将 s1 的内容修改为 &quot;Redis Cluster&quot; ，但粗心的他却忘了在执行 strcat 之前为 s1 分配足够的空间，那么在 strcat 函数执行之后，s1 的数据将溢出到 s2 所在的空间中，导致 s2 保存的内容被意外地修改，如图 2-8 所示 与 C 字符串不同，SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当 SDS API 需要对 SDS 进行修改时，API 会先检查 SDS 的空间是否满足修改所需的要求，如果不满足的话，API 会自动将 SDS 的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用 SDS 既不需要手动修改 SDS 的空间大小，也不会出现前面所说的缓冲区溢出问题。 举个例子，SDS 的 API 里面也有一个用于执行拼接操作的 sdscat 函数，它可以将一个 C 字符串拼接到给定 SDS 所保存的字符串的后面，但是在执行拼接操作之前，sdscat 会先检查给定 SDS 的空间是否足够，如果不够的话，sdscat 就会先扩展 SDS 的空间，然后才执行拼接操作 比如说，如果我们执行： 1sdscat(s, \" Cluster\"); 其中 SDS 值 s 如图 2-9 所示，那么 sdscat 将在执行拼接操作之前检查 s 的长度是否足够，在发现 s 目前的空间不足以拼接 &quot; Cluster&quot; 之后，sdscat 就会先扩展 s 的空间，然后才执行拼接 &quot; Cluster&quot; 的操作，拼接操作完成之后的 SDS 如图 2-10 所示。 注意图 2-10 所示的 SDS ：sdscat 不仅对这个 SDS 进行了拼接操作，它还为 SDS 分配了 13 字节的未使用空间，并且拼接之后的字符串也正好是 13 字节长，分配的空间 SDS 的空间分配策略有关 减少修改带来的内存重分配次数 正如前两个小节所说，因为 C 字符串并不记录自身的长度，所以对于一个包含了 N 个字符的 C 字符串来说，这个 C 字符串的底层实现总是一个 N+1 个字符长的数组（额外的一个字符空间用于保存空字符）。 因为 C 字符串的长度和底层数组的长度之间存在着这种关联性，所以每次增长或者缩短一个 C 字符串，程序都总要对保存这个 C 字符串的数组进行一次内存重分配操作： 如果是增长字符串的操作，比如拼接操作(append)，那么在执行这个操作之前，程序需要先通过内存重分配来扩展底层数组的空间大小 ——如果忘了这一步就会产生缓冲区溢出。 如果是缩短字符串的操作，比如截断操作(trim)，那么在执行这个操作之后，程序需要通过内存重分配来释放字符串不再使用的那部分空间 ——如果忘了这一步就会产生内存泄漏。 问题1：如果是分配，那么每次分配多少呢? 问题2：如果是释放不再使用的空间，那么什么时候释放呢，或者使用什么策略？ 举个例子，如果我们持有一个值为 &quot;Redis&quot; 的 C 字符串 s ，那么为了将 s 的值改为 &quot;Redis Cluster&quot; ，在执行： 1strcat(s, \" Cluster\"); 之前，我们需要先使用内存重分配操作，扩展 s 的空间。 之后，如果我们又打算将 s 的值从 &quot;Redis Cluster&quot; 改为 &quot;Redis Cluster Tutorial&quot; ，那么在执行： 1strcat(s, \" Tutorial\"); 之前，我们需要再次使用内存重分配扩展 s 的空间，诸如此类。 因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作： 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的。 但是 Redis 作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是执行内存重分配的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响。 为了避免 C 字符串的这种缺陷，SDS 通过未使用空间解除了字符串长度和底层数组长度之间的关联：在 SDS 中，buf 数组的长度不一定就是字符数量加一，数组里面可以包含未使用的字节，而这些字节的数量就由 SDS 的 free 属性记录。 通过未使用空间，SDS 实现了空间预分配和惰性空间释放两种优化策略 空间预分配 空间预分配用于优化 SDS 的字符串增长操作：当 SDS 的 API 对一个 SDS 进行修改，并且需要对 SDS 进行空间扩展的时候，程序不仅会为 SDS 分配修改所必须要的空间，还会为 SDS 分配额外的未使用空间。 额外分配的未使用空间数量由以下公式决定： 如果对 SDS 进行修改之后，SDS 的长度（也即是 len 属性的值）将小于 1 MB ，那么程序分配和 len 属性同样大小的未使用空间，这时 SDS len 属性的值将和 free 属性的值相同。举个例子，如果进行修改之后，SDS 的 len 将变成 13 字节，那么程序也会分配 13 字节的未使用空间，SDS 的 buf 数组的实际长度将变成 13 + 13 + 1 = 27 字节（额外的一字节用于保存空字符）。 如果对 SDS 进行修改之后，SDS 的长度将大于等于 1 MB ，那么程序会分配 1 MB 的未使用空间。举个例子，如果进行修改之后，SDS 的 len 将变成 30 MB ，那么程序会分配 1 MB 的未使用空间，SDS 的 buf 数组的实际长度将为 30 MB + 1 MB + 1 byte 。 通过空间预分配策略，Redis 可以减少连续执行字符串增长操作所需的内存重分配次数。 举个例子，对于图 2-11 所示的 SDS 值 s 来说，如果我们执行： 1sdscat(s, \" Cluster\"); 那么 sdscat 将执行一次内存重分配操作，将 SDS 的长度修改为 13 字节，并将 SDS 的未使用空间同样修改为 13 字节，如图 2-12 所示。 如果这时，我们再次对 s 执行： 1sdscat(s, \" Tutorial\"); 那么这次 sdscat 将不需要执行内存重分配：因为未使用空间里面的 13 字节足以保存 9 字节的 &quot; Tutorial&quot; ，执行 sdscat 之后的 SDS 如图 2-13 所示 在扩展 SDS 空间之前，SDS API 会先检查未使用空间是否足够，如果足够的话，API 就会直接使用未使用空间，而无须执行内存重分配。 通过这种预分配策略，SDS 将连续增长 N 次字符串所需的内存重分配次数从必定 N 次降低为最多 N 次。 惰性空间释放 惰性空间释放用于优化 SDS 的字符串缩短操作：当 SDS 的 API 需要缩短 SDS 保存的字符串时，程序并不立即使用内存重分配来回收缩短后多出来的字节，而是使用 free 属性将这些字节的数量记录起来，并等待将来使用。 举个例子，sdstrim 函数接受一个 SDS 和一个 C 字符串作为参数，从 SDS 左右两端分别移除所有在 C 字符串中出现过的字符。 比如对于图 2-14 所示的 SDS 值 s 来说，执行： 1sdstrim(s, &quot;XY&quot;); &#x2F;&#x2F; 移除 SDS 字符串中的所有 &#39;X&#39; 和 &#39;Y&#39; 会将 SDS 修改成图 2-15 所示的样子 注意执行 sdstrim 之后的 SDS 并没有释放多出来的 8 字节空间，而是将这 8 字节空间作为未使用空间保留在了 SDS 里面，如果将来要对 SDS 进行增长操作的话，这些未使用空间就可能会派上用场。 举个例子，如果现在对 s 执行： 1sdscat(s, &quot; Redis&quot;); 那么完成这次 sdscat 操作将不需要执行内存重分配：因为 SDS 里面预留的 8 字节空间已经足以拼接 6 个字节长的 &quot; Redis&quot; ，如图 2-16 所示。 通过惰性空间释放策略，SDS 避免了缩短字符串时所需的内存重分配操作，并为将来可能有的增长操作提供了优化。 什么时候或者有什么接口进行内存释放 二进制安全 C 字符串中的字符必须符合某种编码（比如 ASCII），并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾 ——这些限制使得 C 字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。 有个问题：到底是怎么存进去并计算长度的 如果是先计算长度，然后再存进去，那么可能出现长度误判，导致整体长度不够的情况 如果是先存进去再计算长度(不可能，需要先分配空间) 原来它用的是二进制数据？ 举个例子，如果有一种使用空字符来分割多个单词的特殊数据格式，如图 2-17 所示，那么这种格式就不能使用 C 字符串来保存，因为 C 字符串所用的函数只会识别出其中的 &quot;Redis&quot; ，而忽略之后的 &quot;Cluster&quot; 。 虽然数据库一般用于保存文本数据，但使用数据库来保存二进制数据的场景也不少见，因此，为了确保 Redis 可以适用于各种不同的使用场景，SDS 的 API 都是二进制安全的(binary-safe): 所有 SDS API 都会以处理二进制的方式来处理 SDS 存放在 buf 数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设 ——数据在写入时是什么样的，它被读取时就是什么样。 将 SDS 的 buf 属性称为字节数组的原因 ——Redis 不是用这个数组来保存字符，而是用它来保存一系列二进制数据。 比如说，使用 SDS 来保存之前提到的特殊数据格式就没有任何问题，因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，如图 2-18 所示 通过使用二进制安全的 SDS ，而不是 C 字符串，使得 Redis 不仅可以保存文本数据，还可以保存任意格式的二进制数据 兼容部分C函数 虽然 SDS 的 API 都是二进制安全的，但它们一样遵循 C 字符串以空字符结尾的惯例：这些 API 总会将 SDS 保存的数据的末尾设置为空字符，并且总会在为 buf 数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的 SDS 可以重用一部分 &lt;string.h&gt; 库定义的函数。 举个例子，如图 2-19 所示，如果我们有一个保存文本数据的 SDS 值 sds ，那么我们就可以重用 &lt;string.h&gt;/strcasecmp 函数，使用它来对比 SDS 保存的字符串和另一个 C 字符串： 1strcasecmp(sds-&gt;buf, &quot;hello world&quot;); 这样 Redis 就不用自己专门去写一个函数来对比 SDS 值和 C 字符串值了。 与此类似，我们还可以将一个保存文本数据的 SDS 作为 strcat 函数的第二个参数，将 SDS 保存的字符串追加到一个 C 字符串的后面： 1strcat(c_string, sds-&gt;buf); 这样 Redis 就不用专门编写一个将 SDS 字符串追加到 C 字符串之后的函数了。 通过遵循 C 字符串以空字符结尾的惯例，SDS 可以在有需要时重用 &lt;string.h&gt; 函数库，从而避免了不必要的代码重复。 总结 C 字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 &lt;string.h&gt; 库中的函数。 可以使用一部分 &lt;string.h&gt; 库中的函数。 接口实现：https://www.cnblogs.com/yinbiao/p/10740212.html 链表 链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。 除了链表键之外，发布与订阅、慢查询、监视器等功能也用到了链表，Redis 服务器本身还使用链表来保存多个客户端的状态信息，以及使用链表来构建客户端输出缓冲区（output buffer） 链表和链表节点的实现 每个链表节点使用一个 adlist.h/listNode 结构来表示： 12345678typedef struct listNode &#123; // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;&#125; listNode; 多个 listNode 可以通过 prev 和 next 指针组成双端链表，如图 3-1 所示 虽然仅仅使用多个 listNode 结构就可以组成链表，但使用 adlist.h/list 来持有链表的话，操作起来会更方便 12345678910111213141516171819typedef struct list &#123; // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 链表所包含的节点数量 unsigned long len; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key);&#125; list; list 结构为链表提供了表头指针 head 、表尾指针 tail ，以及链表长度计数器 len ，而 dup 、 free 和 match 成员则是用于实现多态链表所需的类型特定函数： dup 函数用于复制链表节点所保存的值； free 函数用于释放链表节点所保存的值； match 函数则用于对比链表节点所保存的值和另一个输入值是否相等 图 3-2 是由一个 list 结构和三个 listNode 结构组成的链表： Redis 的链表实现的特性可以总结如下： 双端：链表节点带有 prev 和 next 指针，获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ，对链表的访问以 NULL 为终点。 带表头指针和表尾指针：通过 list 结构的 head 指针和 tail 指针，程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器：程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数，程序获取链表中节点数量的复杂度为 O(1) 。 多态：链表节点使用 void* 指针来保存节点值，并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数，所以链表可以用于保存各种不同类型的值。 总结 链表被广泛用于实现 Redis 的各种功能，比如列表键，发布与订阅，慢查询，监视器，等等。 每个链表节点由一个 listNode 结构来表示，每个节点都有一个指向前置节点和后置节点的指针，所以 Redis 的链表实现是双端链表。 每个链表使用一个 list 结构来表示，这个结构带有表头节点指针、表尾节点指针、以及链表长度等信息。 因为链表表头节点的前置节点和表尾节点的后置节点都指向 NULL ，所以 Redis 的链表实现是无环链表。 通过为链表设置不同的类型特定函数，Redis 的链表可以用于保存各种不同类型的值。 字典 在字典中，一个键（key）可以和一个值（value）进行关联（或者说将键映射为值），这些关联的键和值就被称为键值对。 举个例子，当执行命令： 12redis&gt; SET msg \"hello world\"OK 在数据库中创建一个键为 &quot;msg&quot; ，值为 &quot;hello world&quot; 的键值对时，这个键值对就是保存在代表数据库的字典里面的。 除了用来表示数据库之外，字典还是哈希键的底层实现之一：当一个哈希键包含的键值对比较多，又或者键值对中的元素都是比较长的字符串时，Redis 就会使用字典作为哈希键的底层实现。 举个例子，website 是一个包含 10086 个键值对的哈希键，这个哈希键的键都是一些数据库的名字，而键的值就是数据库的主页网址： 123456789redis&gt; HLEN website(integer) 10086 redis&gt; HGETALL website1) \"Redis\"2) \"Redis.io\"3) \"MariaDB\"4) \"MariaDB.org\"5) \"MongoDB\"6) \"MongoDB.org\"# ... website 键的底层实现就是一个字典，字典中包含了 10086 个键值对： 其中一个键值对的键为 &quot;Redis&quot; ，值为 &quot;Redis.io&quot; 。 另一个键值对的键为 &quot;MariaDB&quot; ，值为 &quot;MariaDB.org&quot; ； 还有一个键值对的键为 &quot;MongoDB&quot; ，值为 &quot;MongoDB.org&quot; ； 诸如此类。除了用来实现数据库和哈希键之外，Redis 的不少功能也用到了字典，在后续的章节中会不断地看到字典在 Redis 中的各种不同应用。 字典的实现 Redis 的字典使用哈希表作为底层实现，一个哈希表里面可以有多个哈希表节点，而每个哈希表节点就保存了字典中的一个键值对。 接下来的三个小节将分别介绍 Redis 的哈希表、哈希表节点、以及字典的实现 Hash表 Redis 字典所使用的哈希表由 dict.h/dictht 结构定义 12345678910111213141516typedef struct dictht &#123; // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; &#125; dictht; table 属性是一个数组，数组中的每个元素都是一个指向 dict.h/dictEntry 结构的指针，每个 dictEntry 结构保存着一个键值对。 size 属性记录了哈希表的大小，也即是 table 数组的大小，而 used 属性则记录了哈希表目前已有节点（键值对）的数量。 sizemask 属性的值总是等于 size - 1 ，这个属性和哈希值一起决定一个键应该被放到 table 数组的哪个索引上面。 图 4-1 展示了一个大小为 4 的空哈希表（没有包含任何键值对） 哈希表节点 哈希表节点使用 dictEntry 结构表示，每个 dictEntry 结构都保存着一个键值对： 12345678910111213141516typedef struct dictEntry &#123; // 键 void *key; // 值 union &#123; void *val; uint64_t u64; int64_t s64; &#125; v; // 指向下个哈希表节点，形成链表 struct dictEntry *next; &#125; dictEntry; key 属性保存着键值对中的键，而 v 属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个 uint64_t 整数，又或者是一个 int64_t 整数。 next 属性是指向另一个哈希表节点的指针，可以将多个哈希值相同的键值对连接在一次，以此来解决键冲突（collision）的问题 举个例子，图 4-2 就展示了如何通过 next 指针，将两个索引值相同的键 k1 和 k0 连接在一起。 字典 12345678910111213141516typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ &#125; dict; type 属性和 privdata 属性是针对不同类型的键值对，为创建多态字典而设置的： type 属性是一个指向 dictType 结构的指针，每个 dictType 结构保存了一簇用于操作特定类型键值对的函数，Redis 会为用途不同的字典设置不同的类型特定函数。 而 privdata 属性则保存了需要传给那些类型特定函数的可选参数 123456789101112131415161718192021typedef struct dictType &#123; // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj); &#125; dictType; ht 属性是一个包含两个项的数组，数组中的每个项都是一个 dictht 哈希表，一般情况下，字典只使用 ht[0] 哈希表，ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用。 除了 ht[1] 之外，另一个和 rehash 有关的属性就是 rehashidx ：它记录了 rehash 目前的进度，如果目前没有在进行 rehash ，那么它的值为 -1 。 图 4-3 展示了一个普通状态下（没有进行 rehash）的字典 Hash算法 当要将一个新的键值对添加到字典里面时，程序需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 Redis 计算哈希值和索引值的方法如下： 123456# 使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key); # 使用哈希表的 sizemask 属性和哈希值，计算出索引值# 根据情况不同， ht[x] 可以是 ht[0] 或者 ht[1]index = hash &amp; dict-&gt;ht[x].sizemask; 举个例子，对于图 4-4 所示的字典来说，如果我们要将一个键值对 k0 和 v0 添加到字典里面，那么程序会先使用语句： 1hash &#x3D; dict-&gt;type-&gt;hashFunction(k0); 计算键 k0 的哈希值。 假设计算得出的哈希值为 8 ，那么程序会继续使用语句： 1index &#x3D; hash &amp; dict-&gt;ht[0].sizemask &#x3D; 8 &amp; 3 &#x3D; 0; 计算出键 k0 的索引值 0 ，这表示包含键值对 k0 和 v0 的节点应该被放置到哈希表数组的索引 0 位置上，如图 4-5 所示。 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。 MurmurHash 算法的优点在于，即使输入的键是有规律的，算法仍能给出一个很好的随机分布性，并且算法的计算速度也非常快。 MurmurHash 算法目前的最新版本为 MurmurHash3 ，而 Redis 使用的是 MurmurHash2 ，关于 MurmurHash 算法的更多信息可以参考该算法的主页：http://code.google.com/p/smhasher/ 。 解决键冲突 当有两个或以上数量的键被分配到了哈希表数组的同一个索引上面时，称这些键发生了冲突（collision）。 Redis 的哈希表使用链地址法（separate chaining）来解决键冲突：每个哈希表节点都有一个 next 指针，多个哈希表节点可以用 next 指针构成一个单向链表，被分配到同一个索引上的多个节点可以用这个单向链表连接起来，这就解决了键冲突的问题。 举个例子，假设程序要将键值对 k2 和 v2 添加到图 4-6 所示的哈希表里面，并且计算得出 k2 的索引值为 2 ，那么键 k1 和 k2 将产生冲突，而解决冲突的办法就是使用 next 指针将键 k2 和 k1 所在的节点连接起来，如图 4-7 所示。 因为 dictEntry 节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是将新节点添加到链表的表头位置（复杂度为 O(1)），排在其他已有节点的前面 链表使用的是双向链表所以可以直接添加到链表结尾，但是字典的链表中并只有一个指向next的指针，所以如果发生hash冲突，则直接添加到链表头部 rehash 随着操作的不断执行，哈希表保存的键值对会逐渐地增多或者减少，为了让哈希表的负载因子（load factor）维持在一个合理的范围之内，当哈希表保存的键值对数量太多或者太少时，程序需要对哈希表的大小进行相应的扩展或者收缩。 负载因子是什么时候定义的，怎么去判断什么时候去rehash呢？ rehash过程中会停掉插入和获取吗，新来的值怎么办？ 查询的时候会同时查询 hash[0] 和 hash[1] 吗 扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成，Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的 ht[1] 哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及 ht[0] 当前包含的键值对数量（也即是 ht[0].used属性的值）： 如果执行的是扩展操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作，那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面：rehash 指的是重新计算键的哈希值和索引值，然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后（ht[0] 变为空表），释放 ht[0] ，将 ht[1] 设置为 ht[0] ，并在 ht[1] 新创建一个空白哈希表，为下一次 rehash 做准备。 举个例子，假设程序要对图 4-8 所示字典的 ht[0] 进行扩展操作，那么程序将执行以下步骤： ht[0].used 当前的值为 4 ，4 * 2 = 8 ，而 8 （2^3）恰好是第一个大于等于 4 的 2 的 n 次方，所以程序会将 ht[1] 哈希表的大小设置为 8 。图 4-9 展示了 ht[1] 在分配空间之后，字典的样子。 将 ht[0] 包含的四个键值对都 rehash 到 ht[1] ，如图 4-10 所示。 释放 ht[0] ，并将 ht[1] 设置为 ht[0] ，然后为 ht[1] 分配一个空白哈希表，如图 4-11 所示。 至此，对哈希表的扩展操作执行完毕，程序成功将哈希表的大小从原来的 4 改为了现在的 8 。 hash表的拓展和收缩 当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作： 服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 1 ； 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 5 ； 其中哈希表的负载因子可以通过公式： 1# 负载因子 = 哈希表已保存节点数量 / 哈希表大小load_factor = ht[0].used / ht[0].size 计算得出。比如说，这个哈希表的负载因子为： 12345# 对于一个大小为 `4` ，包含 `4` 个键值对的哈希表来说load_factor = 4 / 4 = 1# 对于一个大小为 `512` ，包含 `256` 个键值对的哈希表来说load_factor = 256 / 512 = 0.5 根据 BGSAVE 命令或 BGREWRITEAOF 命令是否正在执行，服务器执行扩展操作所需的负载因子并不相同 因为在执行 BGSAVE 命令或 BGREWRITEAOF 命令的过程中，Redis 需要创建当前服务器进程的子进程，而大多数操作系统都采用写时复制（copy-on-write）技术来优化子进程的使用效率，所以在子进程存在期间，服务器会提高执行扩展操作所需的负载因子，从而尽可能地避免在子进程存在期间进行哈希表扩展操作，这可以避免不必要的内存写入操作，最大限度地节约内存 渐进式rehash 扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面，但rehash 动作并不是一次性、集中式地完成的，而是分多次、渐进式地完成的。原因在于，如果哈希表里保存大量键值对，要一次性将这些键值对全部 rehash 到 ht[1] 的话，庞大的计算量可能会导致服务器在一段时间内停止服务。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间，让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ，并将它的值设置为 0 ，表示 rehash 工作正式开始。 在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ，当 rehash 工作完成之后，程序将 rehashidx 属性的值增一。 随着字典操作的不断执行，最终在某个时间点上，ht[0] 的所有键值对都会被 rehash 至 ht[1] ，这时程序将 rehashidx 属性的值设为 -1 ，表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式，将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 而带来的庞大计算量 同时在两个ht都存在数据，那么 增删改查 在哪个进行操作的，怎么判断哪个ht才是目的地 图 4-12 至图 4-17 展示了一次完整的渐进式 rehash 过程，注意观察在整个 rehash 过程中，字典的 rehashidx 属性是如何变化的 渐进式 rehash 执行期间的哈希表操作 因为在进行渐进式 rehash 的过程中，字典会同时使用 ht[0] 和 ht[1] 两个哈希表，所以在渐进式 rehash 进行期间，字典的删除（delete）、查找（find）、更新（update）等操作会在两个哈希表上进行：比如说，要在字典里面查找一个键的话，程序会先在 ht[0] 里面进行查找，如果没找到的话，就会继续到 ht[1] 里面进行查找，诸如此类。 另外，在渐进式 rehash 执行期间，新添加到字典的键值对一律会被保存到 ht[1] 里面，而 ht[0] 则不再进行任何添加操作：这一措施保证了 ht[0] 包含的键值对数量会只减不增，并随着 rehash 操作的执行而最终变成空表 总结 字典被广泛用于实现 Redis 的各种功能，其中包括数据库和哈希键。 Redis 中的字典使用哈希表作为底层实现，每个字典带有两个哈希表，一个用于平时使用，另一个仅在进行 rehash 时使用。 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis 使用 MurmurHash2 算法来计算键的哈希值。 哈希表使用链地址法来解决键冲突，被分配到同一个索引上的多个键值对会连接成一个单向链表。 在对哈希表进行扩展或者收缩操作时，程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面，并且这个 rehash 过程并不是一次性地完成的，而是渐进式地完成的 跳跃表 跳跃表（skiplist）是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。 跳跃表支持平均 O(log N) 最坏 O(N) 复杂度的节点查找，还可以通过顺序性操作来批量处理节点 在大部分情况下，跳跃表的效率可以和平衡树相媲美，并且因为跳跃表的实现比平衡树要来得更为简单，所以有不少程序都使用跳跃表来代替平衡树 TODO 跳跃表与平衡树的比较 有序的条件是什么，怎么实现的有序 Redis 使用跳跃表作为有序集合键的底层实现之一：如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员（member）是比较长的字符串时，Redis 就会使用跳跃表来作为有序集合键的底层实现。 举个例子，fruit-price 是一个有序集合键，这个有序集合以水果名为成员，水果价钱为分值，保存了 130 款水果的价钱： 12345678910redis&gt; ZRANGE fruit-price 0 2 WITHSCORES1) \"banana\"2) \"5\"3) \"cherry\"4) \"6.5\"5) \"apple\"6) \"8\" redis&gt; ZCARD fruit-price(integer) 130 fruit-price 有序集合的所有数据都保存在一个跳跃表里面，其中每个跳跃表节点（node）都保存了一款水果的价钱信息，所有水果按价钱的高低从低到高在跳跃表里面排序： 跳跃表的第一个元素的成员为 &quot;banana&quot; ，它的分值为 5 ； 跳跃表的第二个元素的成员为 &quot;cherry&quot; ，它的分值为 6.5 ； 跳跃表的第三个元素的成员为 &quot;apple&quot; ，它的分值为 8 ； 和链表、字典等数据结构被广泛地应用在 Redis 内部不同，Redis 只在两个地方用到了跳跃表，一个是实现有序集合键，另一个是在集群节点中用作内部数据结构，除此之外，跳跃表在 Redis 里面没有其他用途。 跳表的实现 有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位(注意跳表是有序的) Redis 的跳跃表由 redis.h/zskiplistNode 和 redis.h/zskiplist 两个结构定义，其中 zskiplistNode 结构用于表示跳跃表节点，而 zskiplist 结构则用于保存跳跃表节点的相关信息，比如节点的数量，以及指向表头节点和表尾节点的指针，等等 图 5-1 展示了一个跳跃表示例，位于图片最左边的是 zskiplist 结构，该结构包含以下属性： header ：指向跳跃表的表头节点 tail ：指向跳跃表的表尾节点 level ：记录目前跳跃表内，层数最大的那个节点的层数（表头节点的层数不计算在内） length ：记录跳跃表的长度，也即是，跳跃表目前包含节点的数量（表头节点不计算在内） 位于 zskiplist 结构右方的是四个 zskiplistNode 结构，该结构包含以下属性： 层（level）：节点中用 L1 、 L2 、 L3 等字样标记节点的各个层， L1 代表第一层， L2 代表第二层，以此类推。每个层都带有两个属性：前进指针和跨度。前进指针用于访问位于表尾方向的其他节点，而跨度则记录了前进指针所指向节点和当前节点的距离。在上面的图片中，连线上带有数字的箭头就代表前进指针，而那个数字就是跨度。当程序从表头向表尾进行遍历时，访问会沿着层的前进指针进行。 后退（backward）指针：节点中用 BW 字样标记节点的后退指针，它指向位于当前节点的前一个节点。后退指针在程序从表尾向表头遍历时使用。 分值（score）：各个节点中的 1.0 、 2.0 和 3.0 是节点所保存的分值。在跳跃表中，节点按各自所保存的分值从小到大排列。 成员对象（obj）：各个节点中的 o1 、 o2 和 o3 是节点所保存的成员对象。 注意表头节点和其他节点的构造是一样的：表头节点也有后退指针、分值和成员对象，不过表头节点的这些属性都不会被用到，所以图中省略了这些部分，只显示了表头节点的各个层。 本节接下来的内容将对 zskiplistNode 和 zskiplist 两个结构进行更详细的介绍。 跳跃表节点 跳跃表节点的实现由 redis.h/zskiplistNode 结构定义： 1234567891011121314151617181920212223typedef struct zskiplistNode &#123; // 后退指针 struct zskiplistNode *backward; // 分值 double score; // 成员对象 robj *obj; // 层 struct zskiplistLevel &#123; // 前进指针 struct zskiplistNode *forward; // 跨度 unsigned int span; &#125; level[]; &#125; zskiplistNode; 层 跳跃表节点的 level 数组可以包含多个元素，每个元素都包含一个指向其他节点的指针，程序可以通过这些层来加快访问其他节点的速度，一般来说，层的数量越多，访问其他节点的速度就越快。 每次创建一个新跳跃表节点的时候，程序都根据幂次定律（power law，越大的数出现的概率越小）随机生成一个介于 1 和 32 之间的值作为 level 数组的大小，这个大小就是层的“高度”。 图 5-2 分别展示了三个高度为 1 层、 3 层和 5 层的节点，因为 C 语言的数组索引总是从 0 开始的，所以节点的第一层是 level[0] ，而第二层是 level[1] ，以此类推。 前进指针 每个层都有一个指向表尾方向的前进指针（level[i].forward 属性），用于从表头向表尾方向访问节点。 图 5-3 用虚线表示出了程序从表头向表尾方向，遍历跳跃表中所有节点的路径： 迭代程序首先访问跳跃表的第一个节点（表头），然后从第四层的前进指针移动到表中的第二个节点。 在第二个节点时，程序沿着第二层的前进指针移动到表中的第三个节点。 在第三个节点时，程序同样沿着第二层的前进指针移动到表中的第四个节点。 当程序再次沿着第四个节点的前进指针移动时，它碰到一个 NULL ，程序知道这时已经到达了跳跃表的表尾，于是结束这次遍历 跨度 层的跨度（level[i].span 属性）用于记录两个节点之间的距离： 两个节点之间的跨度越大，它们相距得就越远。 指向 NULL 的所有前进指针的跨度都为 0 ，因为它们没有连向任何节点。 初看上去，很容易以为跨度和遍历操作有关，但实际上并不是这样 ——遍历操作只使用前进指针就可以完成了，跨度实际上是用来计算排位（rank）的：在查找某个节点的过程中，将沿途访问过的所有层的跨度累计起来，得到的结果就是目标节点在跳跃表中的排位。 举个例子，图 5-4 用虚线标记了在跳跃表中查找分值为 3.0 、成员对象为 o3 的节点时，沿途经历的层：查找的过程只经过了一个层，并且层的跨度为 3 ，所以目标节点在跳跃表中的排位为 3 。 再举个例子，图 5-5 用虚线标记了在跳跃表中查找分值为 2.0 、成员对象为 o2 的节点时，沿途经历的层：在查找节点的过程中，程序经过了两个跨度为 1 的节点，因此可以计算出，目标节点在跳跃表中的排位为 2 后退指针 节点的后退指针（backward 属性）用于从表尾向表头方向访问节点：跟可以一次跳过多个节点的前进指针不同，因为每个节点只有一个后退指针，所以每次只能后退至前一个节点。 图 5-6 用虚线展示了如果从表尾向表头遍历跳跃表中的所有节点：程序首先通过跳跃表的 tail 指针访问表尾节点，然后通过后退指针访问倒数第二个节点，之后再沿着后退指针访问倒数第三个节点，再之后遇到指向 NULL 的后退指针，于是访问结束 分值与成员 节点的分值（score 属性）是一个 double 类型的浮点数，跳跃表中的所有节点都按分值从小到大来排序。 节点的成员对象（obj 属性）是一个指针，它指向一个字符串对象，而字符串对象则保存着一个 SDS 值。 在同一个跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点保存的分值却可以是相同的：分值相同的节点将按照成员对象在字典序中的大小来进行排序，成员对象较小的节点会排在前面（靠近表头的方向），而成员对象较大的节点则会排在后面（靠近表尾的方向）。 举个例子，在图 5-7 所示的跳跃表中，三个跳跃表节点都保存了相同的分值 10086.0 ，但保存成员对象 o1 的节点却排在保存成员对象 o2 和 o3 的节点之前，而保存成员对象 o2 的节点又排在保存成员对象 o3 的节点之前，由此可见，o1 、 o2 、 o3 三个成员对象在字典中的排序为 o1 &lt;= o2 &lt;= o3 。 跳跃表 虽然仅靠多个跳跃表节点就可以组成一个跳跃表，如图 5-8 所示。 但通过使用一个 zskiplist 结构来持有这些节点，程序可以更方便地对整个跳跃表进行处理，比如快速访问跳跃表的表头节点和表尾节点，又或者快速地获取跳跃表节点的数量（也即是跳跃表的长度）等信息，如图 5-9 所示。 zskiplist 结构的定义如下 123456789101112typedef struct zskiplist &#123; // 表头节点和表尾节点 struct zskiplistNode *header, *tail; // 表中节点的数量 unsigned long length; // 表中层数最大的节点的层数 int level; &#125; zskiplist; header 和 tail 指针分别指向跳跃表的表头和表尾节点，通过这两个指针，程序定位表头节点和表尾节点的复杂度为 O(1) 。 通过使用 length 属性来记录节点的数量，程序可以在 O(1) 复杂度内返回跳跃表的长度。 level 属性则用于在 O(1) 复杂度内获取跳跃表中层高最大的那个节点的层数量，注意表头节点的层高并不计算在内。 跳跃条是怎么增删改查的 总结 跳跃表是有序集合的底层实现之一，除此之外它在 Redis 中没有其他应用。 Redis 的跳跃表实现由 zskiplist 和 zskiplistNode 两个结构组成，其中 zskiplist 用于保存跳跃表信息（比如表头节点、表尾节点、长度），而 zskiplistNode 则用于表示跳跃表节点。 每个跳跃表节点的层高都是 1 至 32 之间的随机数。 在同一个跳跃表中，多个节点可以包含相同的分值，但每个节点的成员对象必须是唯一的。 跳跃表中的节点按照分值大小进行排序，当分值相同时，节点按照成员对象的大小进行排序。 整数集合 整数集合（intset）是集合键的底层实现之一：当一个集合只包含整数值元素，并且这个集合的元素数量不多时，Redis 就会使用整数集合作为集合键的底层实现。 举个例子，如果创建一个只包含五个元素的集合键，并且集合中的所有元素都是整数值，那么这个集合键的底层实现就会是整数集合： 12345redis&gt; SADD numbers 1 3 5 7 9(integer) 5 redis&gt; OBJECT ENCODING numbers\"intset\" 整数集合的实现 整数集合（intset）是 Redis 用于保存整数值的集合抽象数据结构，它可以保存类型为 int16_t 、 int32_t 或者 int64_t 的整数值，并且保证集合中不会出现重复元素。 每个 intset.h/intset 结构表示一个整数集合： 123456789101112typedef struct intset &#123; // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; &#125; intset; contents 数组是整数集合的底层实现：整数集合的每个元素都是 contents 数组的一个数组项（item），各个项在数组中按值的大小从小到大有序地排列，并且数组中不包含任何重复项。 通过什么决定排序？ length 属性记录了整数集合包含的元素数量，也即是 contents 数组的长度。 虽然 intset 结构将 contents 属性声明为 int8_t 类型的数组，但实际上 contents 数组并不保存任何 int8_t 类型的值 ——contents 数组的真正类型取决于 encoding 属性的值： 如果 encoding 属性的值为 INTSET_ENC_INT16 ，那么 contents 就是一个 int16_t 类型的数组，数组里的每个项都是一个 int16_t 类型的整数值（最小值为 -32,768 ，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ，那么 contents 就是一个 int32_t 类型的数组，数组里的每个项都是一个 int32_t 类型的整数值（最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ，那么 contents 就是一个 int64_t 类型的数组，数组里的每个项都是一个 int64_t 类型的整数值（最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ） 这是怎么实现的？ 图 6-1 展示了一个整数集合示例： encoding 属性的值为 INTSET_ENC_INT16 ，表示整数集合的底层实现为 int16_t 类型的数组，而集合保存的都是 int16_t 类型的整数值。 length 属性的值为 5 ，表示整数集合包含五个元素。 contents 数组按从小到大的顺序保存着集合中的五个元素。 因为每个集合元素都是 int16_t 类型的整数值，所以 contents 数组的大小等于 sizeof(int16_t) *5 = 16* 5 = 80 位。 图 6-2 展示了另一个整数集合示例： encoding 属性的值为 INTSET_ENC_INT64 ，表示整数集合的底层实现为 int64_t 类型的数组，而数组中保存的都是 int64_t 类型的整数值。 length 属性的值为 4 ，表示整数集合包含四个元素。 contents 数组按从小到大的顺序保存着集合中的四个元素。 因为每个集合元素都是 int64_t 类型的整数值，所以 contents 数组的大小为 sizeof(int64_t) *4 = 64* 4 = 256 位。 虽然 contents 数组保存的四个整数值中，只有 -2675256175807981027 是真正需要用 int64_t 类型来保存的，而其他的 1 、 3 、 5 三个值都可以用 int16_t 类型来保存，不过根据整数集合的升级规则，当向一个底层为 int16_t 数组的整数集合添加一个 int64_t 类型的整数值时，整数集合已有的所有元素都会被转换成 int64_t 类型，所以 contents 数组保存的四个整数值都是 int64_t 类型的，不仅仅是 -2675256175807981027 升级 每当我们要将一个新元素添加到整数集合里面，并且新元素的类型比整数集合现有所有元素的类型都要长时，整数集合需要先进行升级（upgrade），然后才能将新元素添加到整数集合里面。 升级整数集合并添加新元素共分为三步进行： 根据新元素的类型，扩展整数集合底层数组的空间大小，并为新元素分配空间。 将底层数组现有的所有元素都转换成与新元素相同的类型，并将类型转换后的元素放置到正确的位上，而且在放置元素的过程中，需要继续维持底层数组的有序性质不变。 将新元素添加到底层数组里面。举个例子，假设现在有一个 INTSET_ENC_INT16 编码的整数集合，集合中包含三个 int16_t 类型的元素，如图 6-3 所示。 因为每个元素都占用 16 位空间，所以整数集合底层数组的大小为 3 * 16 = 48 位，图 6-4 展示了整数集合的三个元素在这 48 位里的位置 现在，假设我们要将类型为 int32_t 的整数值 65535 添加到整数集合里面，因为 65535 的类型 int32_t 比整数集合当前所有元素的类型都要长，所以在将 65535 添加到整数集合之前，程序需要先对整数集合进行升级。 升级首先要做的是，根据新类型的长度，以及集合元素的数量（包括要添加的新元素在内），对底层数组进行空间重分配。 整数集合目前有三个元素，再加上新元素 65535 ，整数集合需要分配四个元素的空间，因为每个 int32_t 整数值需要占用 32 位空间，所以在空间重分配之后，底层数组的大小将是 32 * 4 = 128 位，如图 6-5 所示。 虽然程序对底层数组进行了空间重分配，但数组原有的三个元素 1 、 2 、 3 仍然是 int16_t 类型，这些元素还保存在数组的前 48 位里面，所以程序接下来要做的就是将这三个元素转换成 int32_t 类型，并将转换后的元素放置到正确的位上面，而且在放置元素的过程中，需要维持底层数组的有序性质不变。 首先，因为元素 3 在 1 、 2 、 3 、 65535 四个元素中排名第三，所以它将被移动到 contents 数组的索引 2 位置上，也即是数组 64 位至 95 位的空间内，如图 6-6 所示。 接着，因为元素 2 在 1 、 2 、 3 、 65535 四个元素中排名第二，所以它将被移动到 contents 数组的索引 1 位置上，也即是数组的 32 位至 63 位的空间内，如图 6-7 所示。 之后，因为元素 1 在 1 、 2 、 3 、 65535 四个元素中排名第一，所以它将被移动到 contents 数组的索引 0 位置上，也即是数组的 0 位至 31 位的空间内，如图 6-8 所示 然后，因为元素 65535 在 1 、 2 、 3 、 65535 四个元素中排名第四，所以它将被添加到 contents 数组的索引 3 位置上，也即是数组的 96 位至 127 位的空间内，如图 6-9 所示 最后，程序将整数集合 encoding 属性的值从 INTSET_ENC_INT16 改为 INTSET_ENC_INT32 ，并将 length 属性的值从 3 改为 4 ，设置完成之后的整数集合如图 6-10 所示 因为每次向整数集合添加新元素都可能会引起升级，而每次升级都需要对底层数组中已有的所有元素进行类型转换，所以向整数集合添加新元素的时间复杂度为 O(N) 。 其他类型的升级操作，比如从 INTSET_ENC_INT16 编码升级为 INTSET_ENC_INT64 编码，或者从 INTSET_ENC_INT32 编码升级为 INTSET_ENC_INT64 编码，升级的过程都和上面展示的升级过程类似。 升级之后新元素的摆放位置 因为引发升级的新元素的长度总是比整数集合现有所有元素的长度都大，所以这个新元素的值要么就大于所有现有元素，要么就小于所有现有元素： 在新元素小于所有现有元素的情况下，新元素会被放置在底层数组的最开头（索引 0 ）； 在新元素大于所有现有元素的情况下，新元素会被放置在底层数组的最末尾（索引 length-1 ） 升级的好处 提升灵活度 一般只使用 int16_t 类型的数组来保存 int16_t 类型的值，只使用 int32_t 类型的数组来保存 int32_t 类型的值，诸如此类。 整数集合通过自动升级底层数组来适应新元素，可以随意地将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中，而不必担心出现类型错误，这种做法非常灵活。 节约内存 让一个数组可以同时保存 int16_t 、 int32_t 、 int64_t 三种类型的值，最简单的做法就是直接使用 int64_t 类型的数组作为整数集合的底层实现。即使添加到整数集合里面的都是 int16_t 类型或者 int32_t 类型的值，数组都需要使用 int64_t 类型的空间去保存它们，从而出现浪费内存的情况。 整数集合的做法既可以让集合能同时保存三种不同类型的值，又可以确保升级操作只会在有需要的时候进行，这可以尽量节省内存。 如果一直只向整数集合添加 int16_t 类型的值，那么整数集合的底层实现就会一直是 int16_t 类型的数组，只有在将 int32_t 类型或者 int64_t 类型的值添加到集合时，程序才会对数组进行升级 降级 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。 举个例子，对于图 6-11 所示的整数集合来说，即使我们将集合里唯一一个真正需要使用 int64_t 类型来保存的元素 4294967295 删除了，整数集合的编码仍然会维持 INTSET_ENC_INT64 ，底层数组也仍然会是 int64_t 类型的，如图 6-12 所示 总结 整数集合是集合键的底层实现之一。 整数集合的底层实现为数组，这个数组以有序、无重复的方式保存集合元素，在有需要时，程序会根据新添加元素的类型，改变这个数组的类型。 升级操作为整数集合带来了操作上的灵活性，并且尽可能地节约了内存。 整数集合只支持升级操作，不支持降级操作。 压缩列表 压缩列表（ziplist）是列表键和哈希键的底层实现之一。 当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做列表键的底层实现。 比如说，执行以下命令将创建一个压缩列表实现的列表键： 1234redis&gt; RPUSH lst 1 3 5 10086 \"hello\" \"world\"(integer) 6 redis&gt; OBJECT ENCODING lst\"ziplist\" 因为列表键里面包含的都是 1 、 3 、 5 、 10086 这样的小整数值，以及 &quot;hello&quot; 、 &quot;world&quot; 这样的短字符串。 另外，当一个哈希键只包含少量键值对，并且每个键值对的键和值要么就是小整数值，要么就是长度比较短的字符串，那么 Redis 就会使用压缩列表来做哈希键的底层实现。 举个例子，执行以下命令将创建一个压缩列表实现的哈希键： 1234redis&gt; HMSET profile \"name\" \"Jack\" \"age\" 28 \"job\" \"Programmer\"OK redis&gt; OBJECT ENCODING profile\"ziplist\" 因为哈希键里面包含的所有键和值都是小整数值或者短字符串。 本章将对压缩列表的定义以及相关操作进行详细的介绍 压缩列表 压缩列表是 Redis 为了节约内存而开发的，由一系列特殊编码的连续内存块组成的顺序型（sequential）数据结构。 一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值。 图 7-1 展示了压缩列表的各个组成部分，表 7-1 则记录了各个组成部分的类型、长度、以及用途。 属性 类型 长度 用途 zlbytes uint32_t 4 字节 记录整个压缩列表占用的内存字节数：在对压缩列表进行内存重分配，或者计算 zlend 的位置时使用。 zltail uint32_t 4 字节 记录压缩列表表尾节点距离压缩列表的起始地址有多少字节：通过这个偏移量，程序无须遍历整个压缩列表就可以确定表尾节点的地址。 zllen uint16_t 2 字节 记录了压缩列表包含的节点数量：当这个属性的值小于 UINT16_MAX （65535）时，这个属性的值就是压缩列表包含节点的数量；当这个值等于 UINT16_MAX 时，节点的真实数量需要遍历整个压缩列表才能计算得出。 entryX 列表节点 不定 压缩列表包含的各个节点，节点的长度由节点保存的内容决定。 zlend uint8_t 1 字节 特殊值 0xFF （十进制 255 ），用于标记压缩列表的末端。 图 7-2 展示了一个压缩列表示例： 列表 zlbytes 属性的值为 0x50 （十进制 80），表示压缩列表的总长为 80 字节。 列表 zltail 属性的值为 0x3c （十进制 60），这表示如果我们有一个指向压缩列表起始地址的指针 p ，那么只要用指针 p 加上偏移量 60 ，就可以计算出表尾节点 entry3 的地址。 列表 zllen 属性的值为 0x3 （十进制 3），表示压缩列表包含三个节点 图 7-3 展示了另一个压缩列表示例： 列表 zlbytes 属性的值为 0xd2 （十进制 210），表示压缩列表的总长为 210 字节。 列表 zltail 属性的值为 0xb3 （十进制 179），这表示如果我们有一个指向压缩列表起始地址的指针 p ，那么只要用指针 p 加上偏移量 179 ，就可以计算出表尾节点 entry5 的地址。 列表 zllen 属性的值为 0x5 （十进制 5），表示压缩列表包含五个节点。 压缩列表节点构成 每个压缩列表节点可以保存一个字节数组或者一个整数值，其中，字节数组可以是以下三种长度的其中一种： 长度小于等于 63 （2^{6}-1）字节的字节数组； 长度小于等于 16383 （2^{14}-1） 字节的字节数组； 长度小于等于 4294967295 （2^{32}-1）字节的字节数组； 而整数值则可以是以下六种长度的其中一种： 4 位长，介于 0 至 12 之间的无符号整数； 1 字节长的有符号整数； 3 字节长的有符号整数； int16_t 类型整数； int32_t 类型整数； int64_t 类型整数。 每个压缩列表节点都由 previous_entry_length 、 encoding 、 content 三个部分组成，如图 7-4 所示 previous_entry_length 节点的 previous_entry_length 属性以字节为单位，记录了压缩列表中前一个节点的长度。 previous_entry_length 属性的长度可以是 1 字节或者 5 字节： 如果前一节点的长度小于 254 字节，那么 previous_entry_length 属性的长度为 1 字节：前一节点的长度就保存在这一个字节里面。 如果前一节点的长度大于等于 254 字节，那么 previous_entry_length 属性的长度为 5 字节：其中属性的第一字节会被设置为 0xFE （十进制值 254），而之后的四个字节则用于保存前一节点的长度。 图 7-5 展示了一个包含一字节长 previous_entry_length 属性的压缩列表节点，属性的值为 0x05 ，表示前一节点的长度为 5 字节 图 7-6 展示了一个包含五字节长 previous_entry_length 属性的压缩节点，属性的值为 0xFE00002766 ， 值的最高位字节 0xFE 表示这是一个五字节长的 previous_entry_length 属性， 之后的四字节 0x00002766 （十进制值 10086 ）才是前一节点的实际长度 因为节点的 previous_entry_length 属性记录了前一个节点的长度，所以程序可以通过指针运算，根据当前节点的起始地址来计算出前一个节点的起始地址。 举个例子，如果我们有一个指向当前节点起始地址的指针 c ，那么我们只要用指针 c 减去当前节点 previous_entry_length 属性的值，就可以得出一个指向前一个节点起始地址的指针 p ，如图 7-7 所示。 压缩列表的从表尾向表头遍历操作就是使用这一原理实现的：只要我们拥有了一个指向某个节点起始地址的指针，那么通过这个指针以及这个节点的 previous_entry_length 属性，程序就可以一直向前一个节点回溯，最终到达压缩列表的表头节点。 图 7-8 展示了一个从表尾节点向表头节点进行遍历的完整过程： 首先，我们拥有指向压缩列表表尾节点 entry4 起始地址的指针 p1（指向表尾节点的指针可以通过指向压缩列表起始地址的指针加上 zltail 属性的值得出）； 通过用 p1 减去 entry4 节点 previous_entry_length 属性的值，我们得到一个指向 entry4 前一节点 entry3 起始地址的指针 p2 ； 通过用 p2 减去 entry3 节点 previous_entry_length 属性的值，我们得到一个指向 entry3 前一节点 entry2 起始地址的指针 p3 ； 通过用 p3 减去 entry2 节点 previous_entry_length 属性的值，我们得到一个指向 entry2 前一节点 entry1 起始地址的指针 p4 ，entry1 为压缩列表的表头节点； 最终，我们从表尾节点向表头节点遍历了整个列表。 encoding 节点的 encoding 属性记录了节点的 content 属性所保存数据的类型以及长度： 一字节、两字节或者五字节长，值的最高位为 00 、 01 或者 10 的是字节数组编码：这种编码表示节点的 content 属性保存着字节数组，数组的长度由编码除去最高两位之后的其他位记录； 一字节长，值的最高位以 11 开头的是整数编码：这种编码表示节点的 content 属性保存着整数值，整数值的类型和长度由编码除去最高两位之后的其他位记录； 表 7-2 记录了所有可用的字节数组编码，而表 7-3 则记录了所有可用的整数编码。表格中的下划线 _ 表示留空，而 b 、 x 等变量则代表实际的二进制数据，为了方便阅读，多个字节之间用空格隔开。 表7-2: 编码 编码长度 content 属性保存的值 00bbbbbb 1 字节 长度小于等于 63 字节的字节数组。 01bbbbbb xxxxxxxx 2 字节 长度小于等于 16383 字节的字节数组。 10**__** aaaaaaaa bbbbbbbb cccccccc dddddddd 5 字节 长度小于等于 4294967295 的字节数组。 表7-3: 编码 编码长度 content 属性保存的值 11000000 1 字节 int16_t 类型的整数。 11010000 1 字节 int32_t 类型的整数。 11100000 1 字节 int64_t 类型的整数。 11110000 1 字节 24 位有符号整数。 11111110 1 字节 8 位有符号整数。 1111xxxx 1 字节 使用这一编码的节点没有相应的 content 属性，因为编码本身的 xxxx 四个位已经保存了一个介于 0 和 12 之间的值，所以它无须 content 属性。 content 节点的 content 属性负责保存节点的值，节点值可以是一个字节数组或者整数，值的类型和长度由节点的 encoding 属性决定。 图 7-9 展示了一个保存字节数组的节点示例： 编码的最高两位 00 表示节点保存的是一个字节数组； 编码的后六位 001011 记录了字节数组的长度 11 ； content 属性保存着节点的值 &quot;hello world&quot; 。 图 7-10 展示了一个保存整数值的节点示例： 编码 11000000 表示节点保存的是一个 int16_t 类型的整数值； content 属性保存着节点的值 10086 。 连锁更新 前面说过，每个节点的 previous_entry_length 属性都记录了前一个节点的长度： 如果前一节点的长度小于 254 字节，那么 previous_entry_length 属性需要用 1 字节长的空间来保存这个长度值。 如果前一节点的长度大于等于 254 字节，那么 previous_entry_length 属性需要用 5 字节长的空间来保存这个长度值。 考虑这样一种情况：在一个压缩列表中，有多个连续的、长度介于 250 字节到 253 字节之间的节点 e1 至 eN ，如图 7-11 所示。 因为 e1 至 eN 的所有节点的长度都小于 254 字节，所以记录这些节点的长度只需要 1 字节长的 previous_entry_length 属性，换句话说，e1 至 eN 的所有节点的 previous_entry_length 属性都是 1 字节长的。 如果将一个长度大于等于 254 字节的新节点 new 设置为压缩列表的表头节点，那么 new 将成为 e1 的前置节点，如图 7-12 所示。 因为 e1 的 previous_entry_length 属性仅长 1 字节，它没办法保存新节点 new 的长度，所以程序将对压缩列表执行空间重分配操作，并将 e1 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 现在，麻烦的事情来了 ——e1 原本的长度介于 250 字节至 253 字节之间，在为 previous_entry_length 属性新增四个字节的空间之后，e1 的长度就变成了介于 254 字节至 257 字节之间，而这种长度使用 1 字节长的 previous_entry_length 属性是没办法保存的。 因此，为了让 e2 的 previous_entry_length 属性可以记录下 e1 的长度，程序需要再次对压缩列表执行空间重分配操作，并将 e2 节点的 previous_entry_length 属性从原来的 1 字节长扩展为 5 字节长。 正如扩展 e1 引发了对 e2 的扩展一样，扩展 e2 也会引发对 e3 的扩展，而扩展 e3 又会引发对 e4 的扩展……为了让每个节点的 previous_entry_length 属性都符合压缩列表对节点的要求，程序需要不断地对压缩列表执行空间重分配操作，直到 eN 为止。 Redis 将这种在特殊情况下产生的连续多次空间扩展操作称之为“连锁更新”（cascade update），图 7-13 展示了这一过程。 除了添加新节点可能会引发连锁更新之外，删除节点也可能会引发连锁更新。 考虑图 7-14 所示的压缩列表，如果 e1 至 eN 都是大小介于 250 字节至 253 字节的节点，big 节点的长度大于等于 254 字节（需要 5 字节的 previous_entry_length 来保存），而 small 节点的长度小于 254 字节（只需要 1 字节的 previous_entry_length 来保存），那么当我们将 small 节点从压缩列表中删除之后，为了让 e1 的 previous_entry_length 属性可以记录 big 节点的长度，程序将扩展 e1 的空间，并由此引发之后的连锁更新 因为连锁更新在最坏情况下需要对压缩列表执行 N 次空间重分配操作，而每次空间重分配的最坏复杂度为 O(N) ，所以连锁更新的最坏复杂度为 O(N^2) 。 要注意的是，尽管连锁更新的复杂度较高，但它真正造成性能问题的几率是很低的： 首先，压缩列表里要恰好有多个连续的、长度介于 250 字节至 253 字节之间的节点，连锁更新才有可能被引发，在实际中，这种情况并不多见； 其次，即使出现连锁更新，但只要被更新的节点数量不多，就不会对性能造成任何影响：比如说，对三五个节点进行连锁更新是绝对不会影响性能的； 因为以上原因，ziplistPush 等命令的平均复杂度仅为 O(N) ，在实际中，我们可以放心地使用这些函数，而不必担心连锁更新会影响压缩列表的性能。 总结 压缩列表是一种为节约内存而开发的顺序型数据结构。 压缩列表被用作列表键和哈希键的底层实现之一。 压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值。 添加新节点到压缩列表，或者从压缩列表中删除节点，可能会引发连锁更新操作，但这种操作出现的几率并不高。 参考链接 https://www.bookstack.cn/read/redisbook/ https://www.cnblogs.com/yinbiao/p/10740212.html","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Go入门11-基础类型","slug":"Go/Go入门11-基础类型","date":"2021-09-12T05:15:43.000Z","updated":"2021-09-12T05:16:12.655Z","comments":true,"path":"2021/09/12/Go/Go入门11-基础类型/","link":"","permalink":"http://xboom.github.io/2021/09/12/Go/Go%E5%85%A5%E9%97%A811-%E5%9F%BA%E7%A1%80%E7%B1%BB%E5%9E%8B/","excerpt":"","text":"go源码下载地址 golang的数据类型和数据结构的底层实现 Go的类型系统 Golang的类型分为以下几类： 命名类型[named (defined) type]具有名称的类型：int,int64,float32,string,bool。这些GO预先声明好了 通过类型声明(type declaration)创建的所有类型都是命令类型 123var i int //named typetype myInt int //named typevar b bool //named type 一个命名类型一定和其他类型不同 未命名类型(unnamed type):数组，结构体，指针，函数，接口，切片,map,通道都是未命令类型 123[]string // unnamed typemap[string]string // unnamed type[10]int // unnamed type 虽然没有名字，但却有一个类型字面量(type literal)来描述它们由什么构成 基础类型 任何类型T都有基础类型 如果T 是预先声明类型：boolean, numeric, or string（布尔，数值，字符串）中的一个，或者是一个类型字面量(type literal)，他们对应的基础类型就是T自身。 T的基础类型就是T所引用的那个类型的类型声明(type declaration) 基本类型 在/src/runtime/type.go 对 type类型进行了定义 123456789101112131415161718type _type struct &#123; size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; string类型 在 /src/runtime/string.go 对string类型进行了声明 1234type stringStruct struct &#123; str unsafe.Pointer len int&#125; 参考链接 深入研究Go(golang) Type类型系统","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门3-Groutine","slug":"Go/Go入门3-Groutine","date":"2021-09-08T16:08:34.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2021/09/09/Go/Go入门3-Groutine/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go%E5%85%A5%E9%97%A83-Groutine/","excerpt":"","text":"创建一个协程 1go func()//通过go关键字启动一个协程来运行函数 那么它到底干了什么 123456789101112131415func newproc(siz int32, fn *funcval) &#123; argp := add(unsafe.Pointer(&amp;fn), sys.PtrSize) gp := getg() pc := getcallerpc() systemstack(func() &#123; newg := newproc1(fn, argp, siz, gp, pc) _p_ := getg().m.p.ptr() runqput(_p_, newg, true) if mainStarted &#123; wakep() &#125; &#125;)&#125; 关键术语 并发：一个cpu上能同时执行多项任务，在很短时间内，cpu来回切换任务执行(在某段很短时间内执行程序a，然后又迅速得切换到程序b去执行)，有时间上的重叠（宏观上是同时的，微观仍是顺序执行）,这样看起来多个任务像是同时执行，这就是并发 并行：当系统有多个CPU时,每个CPU同一时刻都运行任务，互不抢占自己所在的CPU资源，同时进行，称为并行 进程：cpu在切换程序的时候，如果不保存上一个程序的状态（也就是context–上下文），直接切换下一个程序，就会丢失上一个程序的一系列状态，于是引入了进程这个概念，用以划分好程序运行时所需要的资源。因此进程就是一个程序运行时候的所需要的基本资源单位（也可以说是程序运行的一个实体） 线程：cpu切换多个进程的时候，会花费不少的时间，因为切换进程需要切换到内核态，而每次调度需要内核态都需要读取用户态的数据，进程一旦多起来，cpu调度会消耗一大堆资源，因此引入了线程的概念，线程本身几乎不占有资源，他们共享进程里的资源，内核调度起来不会那么像进程切换那么耗费资源 协程：协程拥有自己的寄存器上下文和栈。 协程与线程 goroutine与thread的不同 内存占用 一个 goroutine 的栈内存消耗为 2 KB(如果栈空间不够用，会自动进行扩容)。 一个 thread 则需要消耗 1 MB 栈内存，还需要一个被称为 “a guard page” 的区域(用于与其他thread隔离) 创建和销毀 goroutine的切换会消耗200ns(用户态，3个寄存器)，相当于2400-3600条指令 除了使用时需要陷入内核，线程切换会消耗1000-1500ns 1ns平均可执行12-18条指令 当 threads 切换时，需要保存各种寄存器，以便将来恢复： 16 general purpose registers: 通用寄存器 PC (Program Counter): 程序计数器 SP (Stack Pointer): 栈指针 segment registers: 段寄存器 16 XMM registers: FP coprocessor state 16 AVX registers all MSRs etc 而 goroutines 切换只需保存三个寄存器 Program Counter Stack Pointer BP：基址指针寄存器，常用于在访问内存时存放内存单元的偏移地址 Thread内存堆栈 创建一个 thread 为了尽量避免极端情况下操作系统线程栈的溢出，默认会为其分配一个较大的栈内存( 1 - 8 MB 栈内存，线程标准 POSIX Thread)，而且还需要一个被称为 guard page 的区域用于和其他 thread 的栈空间进行隔离。而栈内存空间一旦创建和初始化完成之后其大小就不能再有变化，这决定了在某些特殊场景下系统线程栈还是有溢出的风险 调度模型 Go 程序的执行由两层组成：Go Program，Runtime(即用户程序和运行时) Go创建M个线程(CPU执行调度的单元，内核的task_struck)，之后创建N个goroutine会依附在M个线程上执行即M:N模型。当M指定了线程栈，则M.stack-&gt;G.stack，M的PC寄存器指向G提供的函数，然后执行 GMP Go的调度器内部有四个重要的结构：M，P，G，Sched M:M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息 G:代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度。 P:代表一个Processor(Core 虚拟处理器)，它的主要用途就是用来执行goroutine的，维护了一个需要执行goroutine的队列(LRQ Local Run Queue) Sched:代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等 LRQ：本地运行队列，它属于每个处理器，以便管理分配给要执行的goroutines GRQ：全局运行队列，存在于未分配的goroutine中 下面四种情形下，Go scheduler有机会进行调度 使用关键字go: go创建一个新的goroutine, Go scheduler 会考虑调度 GC: 由于进行 GC 的 goroutine 也需要在 M 上运行，因此肯定会发生调度。当然，Go scheduler 还会做很多其他的调度，例如调度不涉及堆访问的 goroutine 来运行。GC 不管栈上的内存，只会回收堆上的内存 系统调用：当 goroutine 进行系统调用时，会阻塞 M，所以它会被调度走，同时一个新的 goroutine 会被调度上来 内存同步访问：atomic，mutex，channel 操作等会使 goroutine 阻塞，因此会被调度走。等条件满足后（例如其他 goroutine 解锁了）还会被调度上来继续运行 M:N 模型 Go runtime 会负责 goroutine 的生老病死，从创建到销毁。Runtime 会在程序启动的时候，创建 M 个线程(CPU 执行调度的单位)，之后创建的 N 个 goroutine 都会依附在这 M 个线程上执行。这就是 M:N 模型： 在同一时刻，一个线程上只能跑一个 goroutine。当 goroutine 发生阻塞(例如向一个 channel 发送数据，被阻塞)时，runtime 会把当前 goroutine 调度走，让其他 goroutine 来执行。目的就是不让一个线程闲着，榨干 CPU 的每一滴油水 工作窃取 Go scheduler 的职责就是将所有处于 runnable 的 goroutines 均匀分布到在 P 上运行的 M。当一个 P 发现自己的 LRQ 已经没有 G 时，会从其他 P “偷” 一些 G 来运行。这被称为 Work-stealing Go scheduler 使用 M:N 模型，在任一时刻，M 个 goroutines（G） 要分配到 N 个内核线程（M），这些 M 跑在个数最多为 GOMAXPROCS 的逻辑处理器（P）上。每个 M 必须依附于一个 P，每个 P 在同一时刻只能运行一个 M。如果 P 上的 M 阻塞了，那它就需要其他的 M 来运行 P 的 LRQ 里的 goroutines Go scheduler 每一轮调度要做的工作就是找到处于 runnable 的 goroutines，并执行它。找的顺序如下： 12345678runtime.schedule() &#123; // only 1/61 of the time, check the global runnable queue for a G. // if not found, check the local queue. // if not found, // try to steal from other Ps. // if not, check the global runnable queue. // if not found, poll network.&#125; 找到一个可执行的 goroutine 后，就会一直执行下去，直到被阻塞 当 P2 上的一个 G 执行结束，就会去 LRQ 获取下一个 G 来执行。如果 LRQ 已经空了，就是说本地可运行队列已经没有 G 需要执行，并且这时 GRQ 也没有 G 了。这时，P2 会随机选择一个 P（称为 P1），P2 会从 P1 的 LRQ “偷”过来一半的 G 状态切换 GPM 共同成就 Go scheduler。G 需要在 M 上才能运行，M 依赖 P 提供的资源，P 则持有待运行的 G。 M 会从与它绑定的 P 的本地队列获取可运行的 G，也会从 network poller 里获取可运行的 G，还会从其他 P 偷 G G 的状态流转： 省略了一些垃圾回收的状态 P 的状态流转： 通常情况下（在程序运行时不调整 P 的个数），P 只会在上图中的四种状态下进行切换。 当程序刚开始运行进行初始化时，所有的 P 都处于 _Pgcstop 状态， 随着 P 的初始化（runtime.procresize），会被置于 _Pidle。 当 M 需要运行时，会 runtime.acquirep 来使 P 变成 Prunning 状态，并通过 runtime.releasep 来释放。 当 G 执行时需要进入系统调用，P 会被设置为 _Psyscall， 如果这个时候被系统监控抢夺（runtime.retake），则 P 会被重新修改为 _Pidle。 如果在程序运行中发生 GC，则 P 会被设置为 _Pgcstop， 并在 runtime.startTheWorld 时重新调整为 _Prunning M 的状态变化： M 只有自旋和非自旋两种状态。自旋的时候，会努力找工作；找不到的时候会进入非自旋状态，之后会休眠，直到有工作需要处理时，被其他工作线程唤醒，又进入自旋状态 调度器 Go调度程序不是抢占式调度程序，而是协作式调度程序。 成为协作调度程序意味着调度程序需要在代码的安全点明确定义用户空间事件，以制定调度决策。 以下是调度的关键点： 启动协程的关键字go 垃圾回收GC 系统调用 对于异步系统调用例如网络请求，将可能阻止的goroutine移到网络轮询器，让处理程序可以执行下一个 处理像文件IO同步请求，当前的G和M对将与G，P，M模型分开。 同时，将创建一台新机器，以保持原始的G，P，M模型正常工作，并且在系统调用完成时将收回块goroutine 地鼠(gopher)用小车运着一堆待加工的砖。M就可以看作图中的地鼠，P就是小车，G就是小车里装的砖。一图胜千言啊，弄清楚了它们三者的关系，下面我们就开始重点聊地鼠是如何在搬运砖块的 启动过程 runtime·schedinit 调度器初始化：主要是根据用户设置的GOMAXPROCS来创建一批小车§,不管设置多大，最多也只能创建256个小车§。这些小车§初始创建好后都是闲置状态，也就是还没开始使用，所以它们都放置在调度器结构(Sched)的pidle字段维护的链表中存储起来了，以备后续之需。 runtime.newproc 创建一个协程 runtime.main runtime·mstart 源码阅读 G 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990type g struct &#123; // goroutine 使用的栈 stack stack // offset known to runtime/cgo // 用于栈的扩张和收缩检查，抢占标志 stackguard0 uintptr // offset known to liblink stackguard1 uintptr // offset known to liblink _panic *_panic // innermost panic - offset known to liblink _defer *_defer // innermost defer //当前与g 绑定的 m m *m // current m; offset known to arm liblink // goroutine 的运行现场 sched gobuf syscallsp uintptr // if status==Gsyscall, syscallsp = sched.sp to use during gc syscallpc uintptr // if status==Gsyscall, syscallpc = sched.pc to use during gc stktopsp uintptr // expected sp at top of stack, to check in traceback // wakeup 时传入的参数 param unsafe.Pointer atomicstatus uint32 stackLock uint32 // sigprof/scang lock; TODO: fold in to atomicstatus goid int64 schedlink guintptr // g 被阻塞之后的近似时间 waitsince int64 // approx time when the g become blocked // g 被阻塞的原因 waitreason waitReason // if status==Gwaiting // 抢占调度标志。这个为 true 时，stackguard0 等于 stackpreempt preempt bool // preemption signal, duplicates stackguard0 = stackpreempt preemptStop bool // transition to _Gpreempted on preemption; otherwise, just deschedule preemptShrink bool // shrink stack at synchronous safe point // asyncSafePoint is set if g is stopped at an asynchronous // safe point. This means there are frames on the stack // without precise pointer information. asyncSafePoint bool paniconfault bool // panic (instead of crash) on unexpected fault address gcscandone bool // g has scanned stack; protected by _Gscan bit in status throwsplit bool // must not split stack // activeStackChans indicates that there are unlocked channels // pointing into this goroutine's stack. If true, stack // copying needs to acquire channel locks to protect these // areas of the stack. activeStackChans bool // parkingOnChan indicates that the goroutine is about to // park on a chansend or chanrecv. Used to signal an unsafe point // for stack shrinking. It's a boolean value, but is updated atomically. parkingOnChan uint8 raceignore int8 // ignore race detection events sysblocktraced bool // StartTrace has emitted EvGoInSyscall about this goroutine tracking bool // whether we're tracking this G for sched latency statistics trackingSeq uint8 // used to decide whether to track this G runnableStamp int64 // timestamp of when the G last became runnable, only used when tracking runnableTime int64 // the amount of time spent runnable, cleared when running, only used when tracking // syscall 返回之后的 cputicks，用来做 tracing sysexitticks int64 // cputicks when syscall has returned (for tracing) traceseq uint64 // trace event sequencer tracelastp puintptr // last P emitted an event for this goroutine // 如果调用了 LockOsThread，那么这个 g 会绑定到某个 m 上 lockedm muintptr sig uint32 writebuf []byte sigcode0 uintptr sigcode1 uintptr sigpc uintptr // 创建该 goroutine 的语句的指令地址 gopc uintptr // pc of go statement that created this goroutine ancestors *[]ancestorInfo // ancestor information goroutine(s) that created this goroutine (only used if debug.tracebackancestors) // goroutine 函数的指令地址 startpc uintptr // pc of goroutine function racectx uintptr waiting *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order cgoCtxt []uintptr // cgo traceback context labels unsafe.Pointer // profiler labels // time.Sleep 缓存的定时器 timer *timer // cached timer for time.Sleep selectDone uint32 // are we participating in a select and did someone win the race? // Per-G GC state // gcAssistBytes is this G's GC assist credit in terms of // bytes allocated. If this is positive, then the G has credit // to allocate gcAssistBytes bytes without assisting. If this // is negative, then the G must correct this by performing // scan work. We track this in bytes to make it fast to update // and check for debt in the malloc hot path. The assist ratio // determines how this corresponds to scan work debt. gcAssistBytes int64&#125; 其中g结构体关联了两个比较简单的结构体，stack 表示 goroutine 运行时的栈 123456789101112131415161718192021// 描述栈的数据结构，栈的范围：[lo, hi)type stack struct &#123; // 栈顶，低地址 lo uintptr // 栈低，高地址 hi uintptr&#125;type gobuf struct &#123; // 存储 rsp 寄存器的值 sp uintptr // 存储 rip 寄存器的值 pc uintptr // 指向 goroutine g guintptr ctxt unsafe.Pointer // this has to be a pointer so that gc scans it // 保存系统调用的返回值 ret sys.Uintreg lr uintptr bp uintptr // for GOEXPERIMENT=framepointer&#125; M 再来看 M，取 machine 的首字母，它代表一个工作线程，或者说系统线程。G 需要调度到 M 上才能运行，M 是真正工作的人。结构体 m 就是我们常说的 M，它保存了 M 自身使用的栈信息、当前正在 M 上执行的 G 信息、与之绑定的 P 信息…… 当 M 没有工作可做的时候，在它休眠前，会“自旋”地来找工作：检查全局队列，查看 network poller，试图执行 gc 任务，或者“偷”工作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// m 代表工作线程，保存了自身使用的栈信息type m struct &#123; // 记录工作线程（也就是内核线程）使用的栈信息。在执行调度代码时需要使用 // 执行用户 goroutine 代码时，使用用户 goroutine 自己的栈，因此调度时会发生栈的切换 g0 *g // goroutine with scheduling stack morebuf gobuf // gobuf arg to morestack divmod uint32 // div/mod denominator for arm - known to liblink // Fields not known to debuggers. procid uint64 // for debuggers, but offset not hard-coded gsignal *g // signal-handling g goSigStack gsignalStack // Go-allocated signal handling stack sigmask sigset // storage for saved signal mask tls [tlsSlots]uintptr // thread-local storage (for x86 extern register) mstartfn func() curg *g // current running goroutine caughtsig guintptr // goroutine running during fatal signal p puintptr // attached p for executing go code (nil if not executing go code) nextp puintptr oldp puintptr // the p that was attached before executing a syscall id int64 mallocing int32 throwing int32 preemptoff string // if != \"\", keep curg running on this m locks int32 dying int32 profilehz int32 spinning bool // m is out of work and is actively looking for work blocked bool // m is blocked on a note newSigstack bool // minit on C thread called sigaltstack printlock int8 incgo bool // m is executing a cgo call freeWait uint32 // if == 0, safe to free g0 and delete m (atomic) fastrand [2]uint32 needextram bool traceback uint8 ncgocall uint64 // number of cgo calls in total ncgo int32 // number of cgo calls currently in progress cgoCallersUse uint32 // if non-zero, cgoCallers in use temporarily cgoCallers *cgoCallers // cgo traceback if crashing in cgo call doesPark bool // non-P running threads: sysmon and newmHandoff never use .park park note alllink *m // on allm schedlink muintptr lockedg guintptr createstack [32]uintptr // stack that created this thread. lockedExt uint32 // tracking for external LockOSThread lockedInt uint32 // tracking for internal lockOSThread nextwaitm muintptr // next m waiting for lock waitunlockf func(*g, unsafe.Pointer) bool waitlock unsafe.Pointer waittraceev byte waittraceskip int startingtrace bool syscalltick uint32 freelink *m // on sched.freem // mFixup is used to synchronize OS related m state // (credentials etc) use mutex to access. To avoid deadlocks // an atomic.Load() of used being zero in mDoFixupFn() // guarantees fn is nil. mFixup struct &#123; lock mutex used uint32 fn func(bool) bool &#125; // these are here because they are too large to be on the stack // of low-level NOSPLIT functions. libcall libcall libcallpc uintptr // for cpu profiler libcallsp uintptr libcallg guintptr syscall libcall // stores syscall parameters on windows vdsoSP uintptr // SP for traceback while in VDSO call (0 if not in call) vdsoPC uintptr // PC for traceback while in VDSO call // preemptGen counts the number of completed preemption // signals. This is used to detect when a preemption is // requested, but fails. Accessed atomically. preemptGen uint32 // Whether this is a pending preemption signal on this M. // Accessed atomically. signalPending uint32 dlogPerM mOS // Up to 10 locks held by this m, maintained by the lock ranking code. locksHeldLen int locksHeld [10]heldLockInfo&#125; P 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140type p struct &#123; id int32 status uint32 // one of pidle/prunning/... link puintptr schedtick uint32 // incremented on every scheduler call syscalltick uint32 // incremented on every system call sysmontick sysmontick // last tick observed by sysmon m muintptr // back-link to associated m (nil if idle) mcache *mcache pcache pageCache raceprocctx uintptr deferpool [5][]*_defer // pool of available defer structs of different sizes (see panic.go) deferpoolbuf [5][32]*_defer // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen. goidcache uint64 goidcacheend uint64 // Queue of runnable goroutines. Accessed without lock. runqhead uint32 runqtail uint32 runq [256]guintptr // runnext, if non-nil, is a runnable G that was ready'd by // the current G and should be run next instead of what's in // runq if there's time remaining in the running G's time // slice. It will inherit the time left in the current time // slice. If a set of goroutines is locked in a // communicate-and-wait pattern, this schedules that set as a // unit and eliminates the (potentially large) scheduling // latency that otherwise arises from adding the ready'd // goroutines to the end of the run queue. // // Note that while other P's may atomically CAS this to zero, // only the owner P can CAS it to a valid G. runnext guintptr // Available G's (status == Gdead) gFree struct &#123; gList n int32 &#125; sudogcache []*sudog sudogbuf [128]*sudog // Cache of mspan objects from the heap. mspancache struct &#123; // We need an explicit length here because this field is used // in allocation codepaths where write barriers are not allowed, // and eliminating the write barrier/keeping it eliminated from // slice updates is tricky, moreso than just managing the length // ourselves. len int buf [128]*mspan &#125; tracebuf traceBufPtr // traceSweep indicates the sweep events should be traced. // This is used to defer the sweep start event until a span // has actually been swept. traceSweep bool // traceSwept and traceReclaimed track the number of bytes // swept and reclaimed by sweeping in the current sweep loop. traceSwept, traceReclaimed uintptr palloc persistentAlloc // per-P to avoid mutex _ uint32 // Alignment for atomic fields below // The when field of the first entry on the timer heap. // This is updated using atomic functions. // This is 0 if the timer heap is empty. timer0When uint64 // The earliest known nextwhen field of a timer with // timerModifiedEarlier status. Because the timer may have been // modified again, there need not be any timer with this value. // This is updated using atomic functions. // This is 0 if there are no timerModifiedEarlier timers. timerModifiedEarliest uint64 // Per-P GC state gcAssistTime int64 // Nanoseconds in assistAlloc gcFractionalMarkTime int64 // Nanoseconds in fractional mark worker (atomic) // gcMarkWorkerMode is the mode for the next mark worker to run in. // That is, this is used to communicate with the worker goroutine // selected for immediate execution by // gcController.findRunnableGCWorker. When scheduling other goroutines, // this field must be set to gcMarkWorkerNotWorker. gcMarkWorkerMode gcMarkWorkerMode // gcMarkWorkerStartTime is the nanotime() at which the most recent // mark worker started. gcMarkWorkerStartTime int64 // gcw is this P's GC work buffer cache. The work buffer is // filled by write barriers, drained by mutator assists, and // disposed on certain GC state transitions. gcw gcWork // wbBuf is this P's GC write barrier buffer. // // TODO: Consider caching this in the running G. wbBuf wbBuf runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point // statsSeq is a counter indicating whether this P is currently // writing any stats. Its value is even when not, odd when it is. statsSeq uint32 // Lock for timers. We normally access the timers while running // on this P, but the scheduler can also do it from a different P. timersLock mutex // Actions to take at some time. This is used to implement the // standard library's time package. // Must hold timersLock to access. timers []*timer // Number of timers in P's heap. // Modified using atomic instructions. numTimers uint32 // Number of timerDeleted timers in P's heap. // Modified using atomic instructions. deletedTimers uint32 // Race context used while executing timer functions. timerRaceCtx uintptr // preempt is set to indicate that this P should be enter the // scheduler ASAP (regardless of what G is running on it). preempt bool // Padding is no longer needed. False sharing is now not a worry because p is large enough // that its size class is an integer multiple of the cache line size (for any of our architectures).&#125; Sched Go scheduler 在源码中的结构体为 schedt，保存调度器的状态信息、全局的可运行 G 队列等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112// 保存调度器的信息type schedt struct &#123; // accessed atomically. keep at top to ensure alignment on 32-bit systems. // 需以原子访问访问。 // 保持在 struct 顶部，以使其在 32 位系统上可以对齐 goidgen uint64 lastpoll uint64 // time of last network poll, 0 if currently polling pollUntil uint64 // time to which current poll is sleeping lock mutex // 由空闲的工作线程组成的链表 midle muintptr // idle m's waiting for work // 空闲的工作线程数量 nmidle int32 // number of idle m's waiting for work // 空闲的且被 lock 的 m 计数 nmidlelocked int32 // number of locked m's waiting for work // 已经创建的工作线程数量 mnext int64 // number of m's that have been created and next M ID // 表示最多所能创建的工作线程数量 maxmcount int32 // maximum number of m's allowed (or die) // goroutine 的数量，自动更新 nmsys int32 // number of system m's not counted for deadlock //累计释放的m数量 nmfreed int64 // cumulative number of freed m's // 由空闲的 p 结构体对象组成的链表 ngsys uint32 // number of system goroutines; updated atomically // 空闲的 p 结构体对象的数量 pidle puintptr // idle p's npidle uint32 nmspinning uint32 // See \"Worker thread parking/unparking\" comment in proc.go. // Global runnable queue. // 全局可运行的 G队列 runq gQueue runqsize int32 // disable controls selective disabling of the scheduler. // // Use schedEnableUser to control this. // // disable is protected by sched.lock. disable struct &#123; // user disables scheduling of user goroutines. user bool runnable gQueue // pending runnable Gs n int32 // length of runnable &#125; // Global cache of dead G's. // dead G 的全局缓存 // 已退出的 goroutine 对象，缓存下来 // 避免每次创建 goroutine 时都重新分配内存 gFree struct &#123; lock mutex stack gList // Gs with stacks noStack gList // Gs without stacks n int32 &#125; // Central cache of sudog structs. // sudog 结构的集中缓存 sudoglock mutex sudogcache *sudog // Central pool of available defer structs of different sizes. // 不同大小的可用的 defer struct 的集中缓存池 deferlock mutex deferpool [5]*_defer // freem is the list of m's waiting to be freed when their // m.exited is set. Linked through m.freelink. freem *m gcwaiting uint32 // gc is waiting to run stopwait int32 stopnote note sysmonwait uint32 sysmonnote note // While true, sysmon not ready for mFixup calls. // Accessed atomically. sysmonStarting uint32 // safepointFn should be called on each P at the next GC // safepoint if p.runSafePointFn is set. safePointFn func(*p) safePointWait int32 safePointNote note profilehz int32 // cpu profiling rate procresizetime int64 // nanotime() of last change to gomaxprocs totaltime int64 // ∫gomaxprocs dt up to procresizetime // sysmonlock protects sysmon's actions on the runtime. // // Acquire and hold this mutex to block sysmon from interacting // with the rest of the runtime. sysmonlock mutex _ uint32 // ensure timeToRun has 8-byte alignment // timeToRun is a distribution of scheduling latencies, defined // as the sum of time a G spends in the _Grunnable state before // it transitions to _Grunning. // // timeToRun is protected by sched.lock. timeToRun timeHistogram&#125; 还有一些重要的常量 1234567891011121314151617181920212223242526// 所有 g 的长度allglen uintptr// 保存所有的 gallgs []*g// 保存所有的 mallm *m// 保存所有的 p，_MaxGomaxprocs = 1024allp [_MaxGomaxprocs + 1]*p// p 的最大值，默认等于 ncpugomaxprocs int32// 程序启动时，会调用 osinit 函数获得此值ncpu int32// 调度器结构体对象，记录了调度器的工作状态sched schedt// 代表进程的主线程m0 m// m0 的 g0，即 m0.g0 = &amp;g0g0 g 参考文献： https://colobu.com/2017/05/04/golang-runtime-scheduler/ http://morsmachine.dk/go-scheduler https://www.zhihu.com/question/20862617 https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html https://www.bookstack.cn/read/go-internals/zh-05.1.md https://golang.design/go-questions/sched/goroutine-vs-thread/ https://learnku.com/articles/41728 https://www.codercto.com/a/38162.html https://www.cnblogs.com/flhs/p/12677335.html","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门6-内存管理","slug":"Go/Go入门6-内存管理","date":"2021-09-08T16:08:22.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2021/09/09/Go/Go入门6-内存管理/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go%E5%85%A5%E9%97%A86-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"程序中的数据和变量都会被分配到程序所在的虚拟内存中，内存空间包含两个重要区域：栈区（Stack）和堆区（Heap）。函数调用的参数、返回值以及局部变量大都会被分配到栈上，这部分内存会由编译器进行管理；不同编程语言使用不同的方法管理堆区的内存。 Golang内存分配算法主要源自 Google 为 C 语言开发的TCMalloc算法，全称Thread-Caching Malloc TCMalloc TCMalloc用固定大小的page(页)来执行内存获取、分配等操作。跟Linux物理内存页的划分有同样的道理。 TCMalloc用固定大小的对象，比如8KB，16KB 等用于特定大小对象的内存分配，这对于内存获取或释放等操作都带来了简化的作用。 TCMalloc还利用缓存常用对象来提高获取内存的速度。 TCMalloc还可以基于每个线程或者每个CPU来设置缓存大小，这是默认设置 TCMalloc基于每个线程独立设置缓存分配策略，减少了多线程之间锁的竞争 内存分配器 设计原理 内存管理一般包含三个不同的组件，分别是用户程序(Mutator)、分配器(Allocator)和收集器(Collector)，当用户程序申请内存时，它会通过内存分配器申请新内存，而分配器会负责从堆中初始化相应的内存区域 分配方法 编程语言的内存分配器一般包含两种分配方法，一种是线性分配器（Sequential Allocator，Bump Allocator），另一种是空闲链表分配器（Free-List Allocator），这两种分配方法有着不同的实现机制和特性，本节会依次介绍它们的分配过程 线性分配器 使用线性分配器(Bump Allocator)，只需要在内存中维护一个指向内存特定位置的指针，如果用户程序向分配器申请内存，分配器只需要检查剩余的空闲内存、返回分配的内存区域并修改指针在内存中的位置，即移动下图中的指针： 缺点：线性分配器无法在内存被释放时重用内存。如下图所示，如果已经分配的内存被回收，线性分配器无法重新利用红色的内存： 因为线性分配器具有上述特性，所以需要与合适的垃圾回收算法配合使用，例如：标记压缩（Mark-Compact）、复制回收（Copying GC）和分代回收（Generational GC）等算法，通过拷贝的方式整理存活对象的碎片，将空闲内存定期合并，这样就能利用线性分配器的效率提升内存分配器的性能了。 线性分配器需要与具有拷贝特性的垃圾回收算法配合，所以 C 和 C++ 等需要直接对外暴露指针的语言就无法使用该策略 空闲链表分配器 空闲链表分配器（Free-List Allocator）可以重用已经被释放的内存，它在内部会维护一个类似链表的数据结构。当用户程序申请内存时，空闲链表分配器会依次遍历空闲的内存块，找到足够大的内存，然后申请新的资源并修改链表： 缺点:分配内存时需要遍历链表，时间复杂度是 𝑂(𝑛)O(n)。空闲链表分配器可以选择不同的策略在链表中的内存块中进行选择，最常见的是以下四种： 首次适应(First-Fit):从链表头开始遍历，选择第一个大小大于申请内存的内存块； 循环首次适应(Next-Fit):从上次遍历的结束位置开始遍历，选择第一个大小大于申请内存的内存块； 最优适应(Best-Fit):从链表头遍历整个链表，选择最合适的内存块； 隔离适应(Segregated-Fit):将内存分割成多个链表，每个链表中的内存块大小相同，申请内存时先找到满足条件的链表，再从链表中选择合适的内存块 如上图所示，该策略会将内存分割成由 4、8、16、32 字节的内存块组成的链表，当我们向内存分配器申请 8 字节的内存时，它会在上图中找到满足条件的空闲内存块并返回。隔离适应的分配策略减少了需要遍历的内存块数量，提高了内存分配的效率 分级分配 线程缓存分配（Thread-Caching Malloc，TCMalloc）是用于分配内存的机制，它比 glibc 中的 malloc 还要快很多。核心思想就是把内存分为多级管理。将可用的堆内存采用二级分配的方式进行管理：每个线程都会自行维护一个独立的内存池，进行内存分配时优先从该内存池中分配，当内存池不足时才会向全局内存池申请，以避免不同线程对全局内存池的频繁竞 对象大小 内存分配器根据申请分配的内存大小选择不同的处理逻辑，运行时根据对象的大小将对象分成微对象、小对象和大对象三种： 微对象：(0,16B) 小对象：[16B, 32KB] 大对象：(32KB, +∞) 多级缓存 内存分配器不仅会区别对待大小不同的对象，还将内存分成不同的级别分别管理，TCMalloc 和 Go 运行时分配器都会引入线程缓存（Thread Cache）、中心缓存（Central Cache）和页堆（Page Heap）三个组件分级管理内存： 线程缓存属于每一个独立的线程，它能够满足线程上绝大多数的内存分配需求，因为不涉及多线程，所以也不需要使用互斥锁来保护内存，这能够减少锁竞争带来的性能损耗。当线程缓存不能满足需求时，运行时会使用中心缓存作为补充解决小对象的内存分配，在遇到 32KB 以上的对象时，内存分配器会选择页堆直接分配大内存。 虚拟内存分布 在 Go 语言 1.10 以前的版本，堆区的内存空间都是连续的；但是在 1.11 版本，Go 团队使用稀疏的堆内存空间替代了连续的内存，解决了连续内存带来的限制以及在特殊场景下可能出现的问题 线性内存 Go 语言程序的 1.10 版本在启动时会初始化整片虚拟内存区域，如下所示的三个区域 spans、bitmap 和 arena 分别预留了 512MB、16GB 以及 512GB 的内存空间，这些内存并不是真正存在的物理内存，而是虚拟内存 spans 区域存储了指向内存管理单元 runtime.mspan 的指针，每个内存单元会管理几页的内存空间，每页大小为 8KB； bitmap 用于标识 arena 区域中的那些地址保存了对象，位图中的每个字节都会表示堆区中的 32 字节是否空闲； arena 区域是真正的堆区，运行时会将 8KB 看做一页，这些内存页中存储了所有在堆上初始化的对象 对于任意一个地址，都可以根据 arena 的基地址计算该地址所在的页数并通过 spans 数组获得管理该片内存的管理单元 runtime.mspan，spans 数组中多个连续的位置可能对应同一个 runtime.mspan 结构。 Go 语言在垃圾回收时会根据指针的地址判断对象是否在堆中，并通过上一段中介绍的过程找到管理该对象的 runtime.mspan。这些都建立在堆区的内存是连续的这一假设上。这种设计虽然简单并且方便，但是在 C 和 Go 混合使用时会导致程序崩溃： 分配的内存地址会发生冲突，导致堆的初始化和扩容失败 没有被预留的大块内存可能会被分配给 C 语言的二进制，导致扩容后的堆不连续 线性的堆内存需要预留大块内存空间，但大块的内存空间不使用是不切实际的，不预留内存空间却会在特殊场景下造成程序崩溃 稀疏内存 稀疏内存是 Go 语言在 1.11 中提出的方案，使用稀疏的内存布局不仅能移除堆大小的上限，还能解决 C 和 Go 混合使用时的地址空间冲突问题。不过因为基于稀疏内存的内存管理失去了内存的连续性这一假设，这也使内存管理变得更加复杂： 如上图所示，运行时使用二维的 runtime.heapArena 数组管理所有的内存，每个单元都会管理 64MB 的内存空间： 123456789101112type heapArena struct &#123; bitmap [heapArenaBitmapBytes]byte spans [pagesPerArena]*mspan pageInUse [pagesPerArena / 8]uint8 pageMarks [pagesPerArena / 8]uint8 pageSpecials [pagesPerArena / 8]uint8 //zeroedBase 标记arena还没被使用的第一页的第一个字节，是相对于 arena 单调增加，直到达到 heapArenaBytes //该字段足以确定是否分配 需要归零，因为页面分配器遵循地址排序的第一个适合策略。 //以原子方式读取并使用原子 CAS 写入 zeroedBase uintptr&#125; 该结构体中的 bitmap 和 spans 与线性内存中的 bitmap 和 spans 区域一一对应，zeroedBase 字段指向了该结构体管理的内存的基地址。上述设计将原有的连续大内存切分成稀疏的小内存，而用于管理这些内存的元信息也被切成了小块 地址空间 Go 语言的运行时构建了操作系统的内存管理抽象层，将运行时管理的地址空间分成以下四种状态 None: 内存没有被保留或者映射，是地址空间的默认状态 Reserved: 运行时持有该地址空间，但是访问该内存会导致错误 Prepared: 内存被保留，一般没有对应的物理内存访问该片内存的行为是未定义的可以快速转换到 Ready 状态 Ready: 可以被安全访问 不同状态之间的转换过程： 运行时中包含多个操作系统实现的状态转换方法，所有的实现都包含在以 mem_ 开头的文件中，本节将介绍 Linux 操作系统对上图中方法的实现： runtime.sysAlloc: 会从操作系统中获取一大块可用的内存空间，可能为几百 KB 或者几 MB； runtime.sysFree: 会在程序发生内存不足（Out-of Memory，OOM）时调用并无条件地返回内存； runtime.sysReserve 会保留操作系统中的一片内存区域，访问这片内存会触发异常； runtime.sysMap 保证内存区域可以快速转换至就绪状态； runtime.sysUsed 通知操作系统应用程序需要使用该内存区域，保证内存区域可以安全访问； runtime.sysUnused 通知操作系统虚拟内存对应的物理内存已经不再需要，可以重用物理内存； runtime.sysFault 将内存区域转换成保留状态，主要用于运行时的调试； 运行时使用 Linux 提供的 mmap、munmap 和 madvise 等系统调用实现了操作系统的内存管理抽象层 内存管理组件 Go 语言的内存分配器包含内存管理单元、线程缓存、中心缓存和页堆几个重要组件 runtime.mspan runtime.mcache runtime.mcentral runtime.mheap Go 语言程序启动时初始化如上图所示的内存布局，每一个处理器都会分配一个线程缓存 runtime.mcache 用于处理微对象和小对象的分配，它们会持有内存管理单元 runtime.mspan 每个类型的内存管理单元都会管理特定大小的对象，当内存管理单元中不存在空闲对象时，它们会从 runtime.mheap 持有的 134 个中心缓存 runtime.mcentral 中获取新的内存单元，中心缓存属于全局的堆结构体 runtime.mheap，它会从操作系统中申请内存。 在 amd64 的 Linux 操作系统上，runtime.mheap 会持有 4,194,304 runtime.heapArena，每个 runtime.heapArena 都会管理 64MB 的内存，单个 Go 语言程序的内存上限也就是 256TB 内存管理单元 runtime.mspan 是 Go 语言内存管理的基本单元,是一个双向链表 123456789101112131415161718192021222324252627282930313233343536373839type mspan struct &#123; next *mspan // next span in list, or nil if none prev *mspan // previous span in list, or nil if none list *mSpanList // For debugging. TODO: Remove. startAddr uintptr // address of first byte of span aka s.base() 起始地址 npages uintptr // number of pages in span span中页的个数 manualFreeList gclinkptr // list of free objects in mSpanManual spans freeindex uintptr nelems uintptr // number of object in the span. allocCache uint64 allocBits *gcBits gcmarkBits *gcBits // sweep generation: // if sweepgen == h-&gt;sweepgen - 2, the span needs sweeping // if sweepgen == h-&gt;sweepgen - 1, the span is currently being swept // if sweepgen == h-&gt;sweepgen, the span is swept and ready to use // if sweepgen == h-&gt;sweepgen + 1, the span was cached before sweep began and is still cached, and needs sweeping // if sweepgen == h-&gt;sweepgen + 3, the span was swept and then cached and is still cached // h-&gt;sweepgen is incremented by 2 after every GC sweepgen uint32 divMul uint16 // for divide by elemsize - divMagic.mul baseMask uint16 // if non-0, elemsize is a power of 2, &amp; this will get object allocation base allocCount uint16 // number of allocated objects spanclass spanClass // size class and noscan (uint8) state mSpanStateBox // mSpanInUse etc; accessed atomically (get/set methods) needzero uint8 // needs to be zeroed before allocation divShift uint8 // for divide by elemsize - divMagic.shift divShift2 uint8 // for divide by elemsize - divMagic.shift2 elemsize uintptr // computed from sizeclass or from npages limit uintptr // end of data in span speciallock mutex // guards specials list specials *special // linked list of special records sorted by offset.&#125; startAddr 和 npages — 确定该结构体管理的多个页所在的内存，每个页的大小都是 8KB； freeindex — 扫描页中空闲对象的初始索引； allocBits 和 gcmarkBits — 分别用于标记内存的占用和回收情况； allocCache — allocBits 的补码，可以用于快速查找内存中未被使用的内存； 串联后的上述结构体会构成如下双向链表 因为相邻的管理单元会互相引用，所以我们可以从任意一个结构体访问双向链表中的其他节点 runtime.mspan 会以两种不同的视角看待管理的内存 当结构体管理的内存不足时，运行时会以页为单位向堆申请内存 当向 runtime.mspan 申请内存时，它会使用 allocCache 字段以对象为单位在管理的内存中快速查找待分配的空间 如果我们能在内存中找到空闲的内存单元会直接返回，当内存中不包含空闲的内存时，上一级的组件 runtime.mcache 会为调用 runtime.mcache.refill 更新内存管理单元以满足为更多对象分配内存的需求 该状态可能处于 mSpanDead、mSpanInUse、mSpanManual 和 mSpanFree 四种情况。当 runtime.mspan 在空闲堆中，它会处于 mSpanFree 状态；当 runtime.mspan 已经被分配时，它会处于 mSpanInUse、mSpanManual 状态，运行时会遵循下面的规则转换该状态： 在垃圾回收的任意阶段，可能从 mSpanFree 转换到 mSpanInUse 和 mSpanManual； 在垃圾回收的清除阶段，可能从 mSpanInUse 和 mSpanManual 转换到 mSpanFree； 在垃圾回收的标记阶段，不能从 mSpanInUse 和 mSpanManual 转换到 mSpanFree； 设置 runtime.mspan 状态的操作必须是原子性的以避免垃圾回收造成的线程竞争问题 runtime.spanClass 是 runtime.mspan 的跨度类，它决定了内存管理单元中存储的对象大小和个数 class bytes/obj bytes/span objects tail waste max waste 1 8 8192 1024 0 87.50% 2 16 8192 512 0 43.75% 3 24 8192 341 0 29.24% 4 32 8192 256 0 46.88% 5 48 8192 170 32 31.52% 6 64 8192 128 0 23.44% 7 80 8192 102 32 19.07% … … … … … … 67 32768 32768 1 0 12.50% 上表展示了对象大小从 8B 到 32KB，总共 67 种跨度类的大小、存储的对象数以及浪费的内存空间，以表中的第四个跨度类为例，跨度类为 5 的 runtime.mspan 中对象的大小上限为 48 字节、管理 1 个页、最多可以存储 170 个对象。因为内存需要按照页进行管理，所以在尾部会浪费 32 字节的内存，当页中存储的对象都是 33 字节时，最多会浪费 31.52% 的资源 除了上述 67 个跨度类之外，运行时中还包含 ID 为 0 的特殊跨度类，它能够管理大于 32KB 的特殊对象，我们会在后面详细介绍大对象的分配过程，在这里就不展开说明了。 跨度类中除了存储类别的 ID 之外，它还会存储一个 noscan 标记位，该标记位表示对象是否包含指针，垃圾回收会对包含指针的 runtime.mspan 结构体进行扫描。我们可以通过下面的几个函数和方法了解 ID 和标记位的底层存储方式： 1234567891011func makeSpanClass(sizeclass uint8, noscan bool) spanClass &#123; return spanClass(sizeclass&lt;&lt;1) | spanClass(bool2int(noscan))&#125;func (sc spanClass) sizeclass() int8 &#123; return int8(sc &gt;&gt; 1)&#125;func (sc spanClass) noscan() bool &#123; return sc&amp;1 !&#x3D; 0&#125; runtime.spanClass 是一个 uint8 类型的整数，它的前 7 位存储着跨度类的 ID，最后一位表示是否包含指针，该类型提供的两个方法能够帮我们快速获取对应的字段 线程缓存 runtime.mcache 是 Go 语言中的线程缓存，与线程上的处理器一一绑定，主要用来缓存用户程序申请的微小对象。每一个线程缓存都持有 68 * 2 个 runtime.mspan，这些内存管理单元都存储在结构体的 alloc 字段中： 线程缓存初始化时不包含 runtime.mspan ，只有当用户程序申请内存，才会从上一级组件获取新的 runtime.mspan 来满足内存分配 123456789101112131415161718192021222324252627282930313233343536type mcache struct &#123; // The following members are accessed on every malloc, // so they are grouped here for better caching. next_sample uintptr // trigger heap sample after allocating this many bytes local_scan uintptr // bytes of scannable heap allocated // Allocator cache for tiny objects w/o pointers. // See \"Tiny allocator\" comment in malloc.go. // tiny points to the beginning of the current tiny block, or // nil if there is no current tiny block. // // tiny is a heap pointer. Since mcache is in non-GC'd memory, // we handle it by clearing it in releaseAll during mark // termination. tiny uintptr tinyoffset uintptr local_tinyallocs uintptr // number of tiny allocs not counted in other stats // The rest is not accessed on every malloc. alloc [numSpanClasses]*mspan // spans to allocate from, indexed by spanClass stackcache [_NumStackOrders]stackfreelist // Local allocator stats, flushed during GC. local_largefree uintptr // bytes freed for large objects (&gt;maxsmallsize) local_nlargefree uintptr // number of frees for large objects (&gt;maxsmallsize) local_nsmallfree [_NumSizeClasses]uintptr // number of frees for small objects (&lt;=maxsmallsize) // flushGen indicates the sweepgen during which this mcache // was last flushed. If flushGen != mheap_.sweepgen, the spans // in this mcache are stale and need to the flushed so they // can be swept. This is done in acquirep. flushGen uint32&#125; 运行时在初始化处理器时会调用 runtime.allocmcache 初始化线程缓存，该函数会在系统栈中使用 runtime.mheap 中的线程缓存分配器初始化新的 runtime.mcache 结构体： 1234567891011121314151617181920212223242526272829// dummy mspan that contains no free objects.var emptymspan mspanfunc allocmcache() *mcache &#123; var c *mcache systemstack(func() &#123; lock(&amp;mheap_.lock) c = (*mcache)(mheap_.cachealloc.alloc()) c.flushGen = mheap_.sweepgen unlock(&amp;mheap_.lock) &#125;) for i := range c.alloc &#123; c.alloc[i] = &amp;emptymspan &#125; c.next_sample = nextSample() return c&#125;func freemcache(c *mcache) &#123; systemstack(func() &#123; c.releaseAll() stackcache_clear(c) lock(&amp;mheap_.lock) purgecachedstats(c) mheap_.cachealloc.free(unsafe.Pointer(c)) unlock(&amp;mheap_.lock) &#125;)&#125; 初始化后的 runtime.mcache 中的所有 runtime.mspan 都是空的占位符 emptymspan runtime.mcache.refill 会为线程缓存获取一个指定跨度类的内存管理单元，被替换的单元不能包含空闲的内存空间，而获取的单元中需要至少包含一个空闲对象用于分配内存 1234567891011121314151617181920212223242526272829303132333435func (c *mcache) refill(spc spanClass) &#123; // Return the current cached span to the central lists. s := c.alloc[spc] if uintptr(s.allocCount) != s.nelems &#123; throw(\"refill of span with free space remaining\") &#125; if s != &amp;emptymspan &#123; // Mark this span as no longer cached. if s.sweepgen != mheap_.sweepgen+3 &#123; throw(\"bad sweepgen in refill\") &#125; if go115NewMCentralImpl &#123; mheap_.central[spc].mcentral.uncacheSpan(s) &#125; else &#123; atomic.Store(&amp;s.sweepgen, mheap_.sweepgen) &#125; &#125; // Get a new cached span from the central lists. s = mheap_.central[spc].mcentral.cacheSpan() if s == nil &#123; throw(\"out of memory\") &#125; if uintptr(s.allocCount) == s.nelems &#123; throw(\"span has no free space\") &#125; // Indicate that this span is cached and prevent asynchronous // sweeping in the next sweep phase. s.sweepgen = mheap_.sweepgen + 3 c.alloc[spc] = s&#125; 中心缓存 runtime.mcentral 是内存分配器的中心缓存，与线程缓存不同，访问中心缓存中的内存管理单元需要使用互斥锁 12345678910111213141516171819202122232425262728293031323334type mcentral struct &#123; lock mutex spanclass spanClass // For !go115NewMCentralImpl. nonempty mSpanList // list of spans with a free object, ie a nonempty free list empty mSpanList // list of spans with no free objects (or cached in an mcache) // partial and full contain two mspan sets: one of swept in-use // spans, and one of unswept in-use spans. These two trade // roles on each GC cycle. The unswept set is drained either by // allocation or by the background sweeper in every GC cycle, // so only two roles are necessary. // // sweepgen is increased by 2 on each GC cycle, so the swept // spans are in partial[sweepgen/2%2] and the unswept spans are in // partial[1-sweepgen/2%2]. Sweeping pops spans from the // unswept set and pushes spans that are still in-use on the // swept set. Likewise, allocating an in-use span pushes it // on the swept set. // // Some parts of the sweeper can sweep arbitrary spans, and hence // can't remove them from the unswept set, but will add the span // to the appropriate swept list. As a result, the parts of the // sweeper and mcentral that do consume from the unswept list may // encounter swept spans, and these should be ignored. partial [2]spanSet // list of spans with a free object full [2]spanSet // list of spans with no free objects // nmalloc is the cumulative count of objects allocated from // this mcentral, assuming all spans in mcaches are // fully-allocated. Written atomically, read under STW. nmalloc uint64&#125; 每个中心缓存都会管理某个跨度类的内存管理单元，它会同时持有两个 runtime.spanSet，分别存储包含空闲对象和不包含空闲对象的内存管理单元。 基础概念 Go在程序启动的时候，会先向操作系统申请一块内存（注意这时还只是一段虚拟的地址空间，并不会真正地分配内存），切成小块后自己进行管理。 申请到的内存块被分配了三个区域，在X64上分别是512MB，16GB，512GB大小 arena区域就是所谓的堆区，Go动态分配的内存都是在这个区域，它把内存分割成8KB大小的页，一些页组合起来称为mspan。 bitmap区标识arena区域哪些地址保存了对象，并且用4bit标志位表示对象是否包含指针、GC标记信息。bitmap中一个byte大小的内存对应arena区域中4个指针大小（指针大小为 8B ）的内存，所以bitmap区域的大小是`512GB/(4*8B)=16GB 从上图其实还可以看到bitmap的高地址部分指向arena区域的低地址部分，也就是说bitmap的地址是由高地址向低地址增长的。 spans区域存放mspan（也就是一些arena分割的页组合起来的内存管理基本单元，后文会再讲）的指针，每个指针对应一页，所以spans区域的大小就是512GB/8KB*8B=512MB。除以8KB是计算arena区域的页数，而最后乘以8是计算spans区域所有指针的大小。创建mspan的时候，按页填充对应的spans区域，在回收object时，根据地址很容易就能找到它所属的mspan 内存管理单元 mspan：Go中内存管理的基本单元，是由一片连续的8KB的页组成的大块内存。注意，这里的页和操作系统本身的页并不是一回事，它一般是操作系统页大小的几倍。一句话概括：mspan是一个包含起始地址、mspan规格、页的数量等内容的双端链表。 每个mspan按照它自身的属性Size Class的大小分割成若干个object，每个object可存储一个对象。并且会使用一个位图来标记其尚未使用的object。属性Size Class决定object大小，而mspan只会分配给和object尺寸大小接近的对象，当然，对象的大小要小于object大小。还有一个概念：Span Class，它和Size Class的含义差不多， 1Size_Class &#x3D; Span_Class &#x2F; 2 这是因为其实每个 Size Class有两个mspan，也就是有两个Span Class。其中一个分配给含有指针的对象，另一个分配给不含有指针的对象。这会给垃圾回收机制带来利好，之后的文章再谈。 如下图，mspan由一组连续的页组成，按照一定大小划分成object。 Go1.9.2里mspan的Size Class共有67种，每种mspan分割的object大小是8*2n的倍数，这个是写死在代码里的： 12345&#x2F;&#x2F; path: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;sizeclasses.goconst _NumSizeClasses &#x3D; 67var class_to_size &#x3D; [_NumSizeClasses]uint16&#123;0, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536,1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768&#125; 根据mspan的Size Class可以得到它划分的object大小。 比如Size Class等于3，object大小就是32B。 32B大小的object可以存储对象大小范围在17B~32B的对象。而对于微小对象（小于16B），分配器会将其进行合并，将几个对象分配到同一个object中。 数组里最大的数是32768，也就是32KB，超过此大小就是大对象了，它会被特别对待，这个稍后会再介绍。顺便提一句，类型Size Class为0表示大对象，它实际上直接由堆内存分配，而小对象都要通过mspan来分配。 对于mspan来说，它的Size Class会决定它所能分到的页数，这也是写死在代码里的： 12345&#x2F;&#x2F; path: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;sizeclasses.goconst _NumSizeClasses &#x3D; 67var class_to_allocnpages &#x3D; [_NumSizeClasses]uint8&#123;0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 2, 3, 4, 5, 6, 1, 7, 6, 5, 4, 3, 5, 7, 2, 9, 7, 5, 8, 3, 10, 7, 4&#125; 比如当我们要申请一个object大小为32B的mspan的时候，在class_to_size里对应的索引是3，而索引3在class_to_allocnpages数组里对应的页数就是1。 mspan结构体定义 12345678910111213141516171819202122232425262728type mspan struct &#123; //链表前向指针，用于将span链接起来 next *mspan //链表前向指针，用于将span链接起来 prev *mspan // 起始地址，也即所管理页的地址 startAddr uintptr // 管理的页数 npages uintptr // 块个数，表示有多少个块可供分配 nelems uintptr //分配位图，每一位代表一个块是否已分配 allocBits *gcBits // 已分配块的个数 allocCount uint16 // class表中的class ID，和Size Classs相关 spanclass spanClass // class表中的对象大小，也即块大小 elemsize uintptr &#125; 将mspan放到更大的视角来看： 上图可以看到有两个S指向了同一个mspan，因为这两个S指向的P是同属一个mspan的。所以，通过arena上的地址可以快速找到指向它的S，通过S就能找到mspan，回忆一下前面我们说的mspan区域的每个指针对应一页。 假设最左边第一个mspan的Size Class等于10，根据前面的class_to_size数组，得出这个msapn分割的object大小是144B，算出可分配的对象个数是8KB/144B=56.89个，取整56个，所以会有一些内存浪费掉了，Go的源码里有所有Size Class的mspan浪费的内存的大小；再根据class_to_allocnpages数组，得到这个mspan只由1个page组成；假设这个mspan是分配给无指针对象的，那么spanClass等于20。 startAddr直接指向arena区域的某个位置，表示这个mspan的起始地址，allocBits指向一个位图，每位代表一个块是否被分配了对象；allocCount则表示总共已分配的对象个数。 这样，左起第一个mspan的各个字段参数就如下图所示： 内存管理元件 内存分配由内存分配器完成。分配器由3种组件构成：mcache, mcentral, mheap mcache mcache：每个工作线程都会绑定一个mcache，本地缓存可用的mspan资源，这样就可以直接给Goroutine分配，因为不存在多个Goroutine竞争的情况，所以不会消耗锁资源。 mcache的结构体定义： 12345type mcache struct &#123; alloc [numSpanClasses]*mspan&#125;numSpanClasses = _NumSizeClasses &lt;&lt; 1 mcache用Span Classes作为索引管理多个用于分配的mspan，它包含所有规格的mspan。它是_NumSizeClasses的2倍，也就是67*2=134，为什么有一个两倍的关系，前面我们提到过：为了加速之后内存回收的速度，数组里一半的mspan中分配的对象不包含指针，另一半则包含指针。 对于无指针对象的mspan在进行垃圾回收的时候无需进一步扫描它是否引用了其他活跃的对象。 后面的垃圾回收文章会再讲到，这次先到这里。 mcache在初始化的时候是没有任何mspan资源的，在使用过程中会动态地从mcentral申请，之后会缓存下来。当对象小于等于32KB大小时，使用mcache的相应规格的mspan进行分配 mcentral mcentral：为所有mcache提供切分好的mspan资源。每个central保存一种特定大小的全局mspan列表，包括已分配出去的和未分配出去的。 每个mcentral对应一种mspan，而mspan的种类导致它分割的object大小不同。当工作线程的mcache中没有合适（也就是特定大小的）的mspan时就会从mcentral获取。 mcentral被所有的工作线程共同享有，存在多个Goroutine竞争的情况，因此会消耗锁资源。结构体定义： 123456789101112131415161718//path: /usr/local/go/src/runtime/mcentral.gotype mcentral struct &#123; // 互斥锁 lock mutex // 规格 sizeclass int32 // 尚有空闲object的mspan链表 nonempty mSpanList // 没有空闲object的mspan链表，或者是已被mcache取走的msapn链表 empty mSpanList // 已累计分配的对象个数 nmalloc uint64 &#125; empty表示这条链表里的mspan都被分配了object，或者是已经被cache取走了的mspan，这个mspan就被那个工作线程独占了。而nonempty则表示有空闲对象的mspan列表。每个central结构体都在mheap中维护。 简单说下mcache从mcentral获取和归还mspan的流程： 获取 加锁；从nonempty链表找到一个可用的mspan；并将其从nonempty链表删除；将取出的mspan加入到empty链表；将mspan返回给工作线程；解锁。 归还 加锁；将mspan从empty链表删除；将mspan加入到nonempty链表；解锁。 mheap mheap：代表Go程序持有的所有堆空间，Go程序使用一个mheap的全局对象_mheap来管理堆内存。 当mcentral没有空闲的mspan时，会向mheap申请。而mheap没有资源时，会向操作系统申请新内存。mheap主要用于大对象的内存分配，以及管理未切割的mspan，用于给mcentral切割成小对象。 同时我们也看到，mheap中含有所有规格的mcentral，所以，当一个mcache从mcentral申请mspan时，只需要在独立的mcentral中使用锁，并不会影响申请其他规格的mspan。 mheap结构体定义： 12345678910111213141516171819202122232425//path: /usr/local/go/src/runtime/mheap.gotype mheap struct &#123; lock mutex // spans: 指向mspans区域，用于映射mspan和page的关系 spans []*mspan // 指向bitmap首地址，bitmap是从高地址向低地址增长的 bitmap uintptr // 指示arena区首地址 arena_start uintptr // 指示arena区已使用地址位置 arena_used uintptr // 指示arena区末地址 arena_end uintptr central [67*2]struct &#123; mcentral mcentral pad [sys.CacheLineSize - unsafe.Sizeof(mcentral&#123;&#125;)%sys.CacheLineSize]byte &#125;&#125; 上图我们看到，bitmap和arena_start指向了同一个地址，这是因为bitmap的地址是从高到低增长的，所以他们指向的内存位置相同 内存分配流程 Go的内存分配器在分配对象时，根据对象的大小，分成三类：小对象（小于等于16B）、一般对象（大于16B，小于等于32KB）、大对象（大于32KB）。 大体上的分配流程： 32KB 的对象，直接从mheap上分配； &lt;=16B 的对象使用mcache的tiny分配器分配； (16B,32KB] 的对象，首先计算对象的规格大小，然后使用mcache中相应规格大小的mspan分配； 如果mcache没有相应规格大小的mspan，则向mcentral申请 如果mcentral没有相应规格大小的mspan，则向mheap申请 如果mheap中也没有合适大小的mspan，则向操作系统申请 使用go SDK 1.15.5 runtime/malloc.go 参考链接 https://qcrao.com/2019/03/13/graphic-go-memory-allocation/ https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-memory-allocator/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门4-Select","slug":"Go/Go入门4-Select","date":"2021-09-08T16:08:10.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/09/09/Go/Go入门4-Select/","link":"","permalink":"http://xboom.github.io/2021/09/09/Go/Go%E5%85%A5%E9%97%A84-Select/","excerpt":"","text":"select 实例 实例1与实例2有什么不同？ 123456789101112131415161718192021222324252627282930313233func main()&#123; var count int for &#123; select &#123; case &lt;-time.Tick(time.Millisecond * 500): fmt.Println(\"hello\") count++ fmt.Println(\"count---&gt;\" , count) case &lt;-time.Tick(time.Millisecond * 499) : fmt.Println(\"world\") count++ fmt.Println(\"count---&gt;\" , count) &#125; &#125;&#125;func main()&#123; t1 := time.Tick(time.Second) t2 := time.Tick(time.Second) var count int for &#123; select &#123; case &lt;-t1: fmt.Println(\"hello\") count++ fmt.Println(\"count---&gt;\" , count) case &lt;-t2 : fmt.Println(\"world\") count++ fmt.Println(\"count---&gt;\" , count) &#125; &#125;&#125; 执行结果为： 实例1：只会输出含有world日志的case 实例2：交替执行t1与t2 select原理 select通过select.go实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314/* cas0 为*scase指针 指向[ncases]*scase的数组 order0 *uint16 ncases 记录case个数 返回选中第几个case以及是否收到值*/func selectgo(cas0 *scase, order0 *uint16, ncases int) (int, bool) &#123; //debug // NOTE: In order to maintain a lean stack size, the number of scases // is capped at 65536. cas1 := (*[1 &lt;&lt; 16]scase)(unsafe.Pointer(cas0)) order1 := (*[1 &lt;&lt; 17]uint16)(unsafe.Pointer(order0)) scases := cas1[:ncases:ncases] pollorder := order1[:ncases:ncases] lockorder := order1[ncases:][:ncases:ncases] //遍历case如果chan为空且不是default则置位空case对象 scase&#123;&#125; for i := range scases &#123; cas := &amp;scases[i] if cas.c == nil &amp;&amp; cas.kind != caseDefault &#123; *cas = scase&#123;&#125; &#125; &#125; // generate permuted order for i := 1; i &lt; ncases; i++ &#123; j := fastrandn(uint32(i + 1)) pollorder[i] = pollorder[j] pollorder[j] = uint16(i) &#125; // sort the cases by Hchan address to get the locking order. // simple heap sort, to guarantee n log n time and constant stack footprint. //根据chan的地址进行重新排序 for i := 0; i &lt; ncases; i++ &#123; j := i // Start with the pollorder to permute cases on the same channel. c := scases[pollorder[i]].c for j &gt; 0 &amp;&amp; scases[lockorder[(j-1)/2]].c.sortkey() &lt; c.sortkey() &#123; k := (j - 1) / 2 lockorder[j] = lockorder[k] j = k &#125; lockorder[j] = pollorder[i] &#125; for i := ncases - 1; i &gt;= 0; i-- &#123; o := lockorder[i] c := scases[o].c lockorder[i] = lockorder[0] j := 0 for &#123; k := j*2 + 1 if k &gt;= i &#123; break &#125; if k+1 &lt; i &amp;&amp; scases[lockorder[k]].c.sortkey() &lt; scases[lockorder[k+1]].c.sortkey() &#123; k++ &#125; if c.sortkey() &lt; scases[lockorder[k]].c.sortkey() &#123; lockorder[j] = lockorder[k] j = k continue &#125; break &#125; lockorder[j] = o &#125; // lock all the channels involved in the select sellock(scases, lockorder) var ( gp *g sg *sudog c *hchan k *scase sglist *sudog sgnext *sudog qp unsafe.Pointer nextp **sudog )loop: // pass 1 - look for something already waiting var dfli int var dfl *scase var casi int var cas *scase var recvOK bool for i := 0; i &lt; ncases; i++ &#123; casi = int(pollorder[i]) cas = &amp;scases[casi] c = cas.c switch cas.kind &#123; case caseNil: continue case caseRecv: sg = c.sendq.dequeue() if sg != nil &#123; goto recv &#125; if c.qcount &gt; 0 &#123; goto bufrecv &#125; if c.closed != 0 &#123; goto rclose &#125; case caseSend: if c.closed != 0 &#123; goto sclose &#125; sg = c.recvq.dequeue() if sg != nil &#123; goto send &#125; if c.qcount &lt; c.dataqsiz &#123; goto bufsend &#125; case caseDefault: dfli = casi dfl = cas &#125; &#125; if dfl != nil &#123; selunlock(scases, lockorder) casi = dfli cas = dfl goto retc &#125; // pass 2 - enqueue on all chans gp = getg() if gp.waiting != nil &#123; throw(\"gp.waiting != nil\") &#125; nextp = &amp;gp.waiting for _, casei := range lockorder &#123; casi = int(casei) cas = &amp;scases[casi] if cas.kind == caseNil &#123; continue &#125; c = cas.c sg := acquireSudog() sg.g = gp sg.isSelect = true // No stack splits between assigning elem and enqueuing // sg on gp.waiting where copystack can find it. sg.elem = cas.elem sg.releasetime = 0 if t0 != 0 &#123; sg.releasetime = -1 &#125; sg.c = c // Construct waiting list in lock order. *nextp = sg nextp = &amp;sg.waitlink switch cas.kind &#123; case caseRecv: c.recvq.enqueue(sg) case caseSend: c.sendq.enqueue(sg) &#125; &#125; // wait for someone to wake us up gp.param = nil // Signal to anyone trying to shrink our stack that we're about // to park on a channel. The window between when this G's status // changes and when we set gp.activeStackChans is not safe for // stack shrinking. atomic.Store8(&amp;gp.parkingOnChan, 1) gopark(selparkcommit, nil, waitReasonSelect, traceEvGoBlockSelect, 1) gp.activeStackChans = false sellock(scases, lockorder) gp.selectDone = 0 sg = (*sudog)(gp.param) gp.param = nil // pass 3 - dequeue from unsuccessful chans // otherwise they stack up on quiet channels // record the successful case, if any. // We singly-linked up the SudoGs in lock order. casi = -1 cas = nil sglist = gp.waiting // Clear all elem before unlinking from gp.waiting. for sg1 := gp.waiting; sg1 != nil; sg1 = sg1.waitlink &#123; sg1.isSelect = false sg1.elem = nil sg1.c = nil &#125; gp.waiting = nil for _, casei := range lockorder &#123; k = &amp;scases[casei] if k.kind == caseNil &#123; continue &#125; if sglist.releasetime &gt; 0 &#123; k.releasetime = sglist.releasetime &#125; if sg == sglist &#123; // sg has already been dequeued by the G that woke us up. casi = int(casei) cas = k &#125; else &#123; c = k.c if k.kind == caseSend &#123; c.sendq.dequeueSudoG(sglist) &#125; else &#123; c.recvq.dequeueSudoG(sglist) &#125; &#125; sgnext = sglist.waitlink sglist.waitlink = nil releaseSudog(sglist) sglist = sgnext &#125; if cas == nil &#123; // We can wake up with gp.param == nil (so cas == nil) // when a channel involved in the select has been closed. // It is easiest to loop and re-run the operation; // we'll see that it's now closed. // Maybe some day we can signal the close explicitly, // but we'd have to distinguish close-on-reader from close-on-writer. // It's easiest not to duplicate the code and just recheck above. // We know that something closed, and things never un-close, // so we won't block again. goto loop &#125; c = cas.c if cas.kind == caseRecv &#123; recvOK = true &#125; selunlock(scases, lockorder) goto retcbufrecv: recvOK = true qp = chanbuf(c, c.recvx) if cas.elem != nil &#123; typedmemmove(c.elemtype, cas.elem, qp) &#125; typedmemclr(c.elemtype, qp) c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.qcount-- selunlock(scases, lockorder) goto retcbufsend: // can send to buffer typedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ selunlock(scases, lockorder) goto retcrecv: // can receive from sleeping sender (sg) recv(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) recvOK = true goto retcrclose: // read at end of closed channel selunlock(scases, lockorder) recvOK = false if cas.elem != nil &#123; typedmemclr(c.elemtype, cas.elem) &#125; goto retcsend: // can send to a sleeping receiver (sg) send(c, sg, cas.elem, func() &#123; selunlock(scases, lockorder) &#125;, 2) if debugSelect &#123; print(\"syncsend: cas0=\", cas0, \" c=\", c, \"\\n\") &#125; goto retcretc: if cas.releasetime &gt; 0 &#123; blockevent(cas.releasetime-t0, 1) &#125; return casi, recvOKsclose: // send on closed channel selunlock(scases, lockorder) panic(plainError(\"send on closed channel\"))&#125; 流程图如下： 参考链接 https://blog.csdn.net/u011957758/article/details/82230316","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门12-Interface","slug":"Go/Go入门12-Interface","date":"2021-09-05T10:36:41.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2021/09/05/Go/Go入门12-Interface/","link":"","permalink":"http://xboom.github.io/2021/09/05/Go/Go%E5%85%A5%E9%97%A812-Interface/","excerpt":"","text":"interface赋值问题 123456789101112131415161718192021package mainimport ( \"fmt\")type People interface &#123; Speak(string) string&#125;type Stduent struct&#123;&#125;func (stu *Stduent) Speak(think string) (talk string) &#123; if think == \"love\" &#123; talk = \"You are a good boy\" &#125; else &#123; talk = \"hi\" &#125; return&#125;func main() &#123; var peo People = Stduent&#123;&#125; think := \"love\" fmt.Println(peo.Speak(think))&#125; 多态的几个要素： 1、有interface接口，并且有接口定义的方法。 2、有子类去重写interface的接口。 3、有父类指针指向子类的具体对象 所以上述代码报错的地方在var peo People = Stduent{}这条语句， Student{}已经重写了父类People{}中的Speak(string) string方法，那么只需要用父类指针指向子类对象即可。 所以应该改成var peo People = &amp;Student{} 即可编译通过。（People为interface类型，就是指针类型） interface的内部构造(非空接口iface情况) 123456789101112131415161718192021package mainimport ( \"fmt\")type People interface &#123; Show()&#125;type Student struct&#123;&#125;func (stu *Student) Show() &#123;&#125;func live() People &#123; var stu *Student return stu&#125;func main() &#123; if live() == nil &#123; fmt.Println(\"AAAAAAA\") &#125; else &#123; fmt.Println(\"BBBBBBB\") &#125;&#125; 结果是: BBBBBB interface在使用的过程中，共有两种表现形式 一种为空接口(empty interface)，定义如下： 1var MyInterface interface&#123;&#125; 另一种为非空接口(non-empty interface), 定义如下： 1type MyInterface interface &#123;function()&#125; 这两种interface类型分别用两种struct表示，空接口为eface, 非空接口为iface. 空接口eface 空接口eface结构，由两个属性构成，一个是类型信息_type，一个是数据信息。其数据结构声明如下： 1234type eface struct &#123; //空接口 _type *_type //类型信息 data unsafe.Pointer //指向数据的指针(go语言中特殊的指针类型unsafe.Pointer类似于c语言中的void*)&#125; _type属性：是GO语言中所有类型的公共描述，Go语言几乎所有的数据结构都可以抽象成 _type，是所有类型的公共描述，**type负责决定data应该如何解释和操作，**type的结构代码如下: 12345678910111213type _type struct &#123; size uintptr //类型大小 ptrdata uintptr //前缀持有所有指针的内存大小 hash uint32 //数据hash值 tflag tflag align uint8 //对齐 fieldalign uint8 //嵌入结构体时的对齐 kind uint8 //kind 有些枚举值kind等于0是无效的 alg *typeAlg //函数指针数组，类型实现的所有方法 gcdata *byte str nameOff ptrToThis typeOff&#125; data属性: 表示指向具体的实例数据的指针，他是一个unsafe.Pointer类型，相当于一个C的万能指针void* 非空接口iface iface 表示 non-empty interface 的数据结构，非空接口初始化的过程就是初始化一个iface类型的结构，其中data的作用同eface的相同，这里不再多加描述。 1234type iface struct &#123; tab *itab data unsafe.Pointer&#125; iface结构中最重要的是itab结构（结构如下），每一个 itab 都占 32 字节的空间。itab可以理解为pair&lt;interface type, concrete type&gt; 。itab里面包含了interface的一些关键信息，比如method的具体实现。 12345678type itab struct &#123; inter *interfacetype // 接口自身的元信息 _type *_type // 具体类型的元信息 link *itab bad int32 hash int32 // _type里也有一个同样的hash，此处多放一个是为了方便运行接口断言 fun [1]uintptr // 函数指针，指向具体类型所实现的方法&#125; 其中值得注意的字段： interface type包含了一些关于interface本身的信息，比如package path，包含的method。这里的interfacetype是定义interface的一种抽象表示。 type表示具体化的类型，与eface的 type类型相同。 hash字段其实是对_type.hash的拷贝，它会在interface的实例化时，用于快速判断目标类型和接口中的类型是否一致。另，Go的interface的Duck-typing机制也是依赖这个字段来实现。 fun字段其实是一个动态大小的数组，虽然声明时是固定大小为1，但在使用时会直接通过fun指针获取其中的数据，并且不会检查数组的边界，所以该数组中保存的元素数量是不确定的 1234func live() People &#123; var stu *Student return stu &#125; stu是一个指向nil的空指针，但是最后return stu 会触发匿名变量 People = stu值拷贝动作，所以最后live()放回给上层的是一个People insterface{}类型，也就是一个iface struct{}类型。 stu为nil，只是iface中的data 为nil而已。 但是iface struct{}本身并不为nil. Interface 内部构造(空接口eface情况) 1234567891011func Foo(x interface&#123;&#125;) &#123; if x == nil &#123; fmt.Println(\"empty interface\") return &#125; fmt.Println(\"non-empty interface\")&#125;func main() &#123; var p *int = nil Foo(p)&#125; 结果为 “non-empty interface” 因为 Fool()的形参 x interface{} 是一个空接口类型eface struct{},在执行Foo(p)的时候，触发x interface{} = p语句的时候，此X的结构为 所以 x 结构体本身不为nil，而是data指针指向的p为nil Interface{}与*Interface{} 1234567891011121314type S struct &#123;&#125;func f(x interface&#123;&#125;) &#123;&#125;func g(x *interface&#123;&#125;) &#123;&#125;func main() &#123; s := S&#123;&#125; p := &amp;s f(s) //A g(s) //B f(p) //C g(p) //D&#125; 结果： 12345B、D两行错误B错误为： cannot use s (type S) as type *interface &#123;&#125; in argument to g: *interface &#123;&#125; is pointer to interface, not interfaceD错误为：cannot use p (type *S) as type *interface &#123;&#125; in argument to g: *interface &#123;&#125; is pointer to interface, not interface 看到这道题需要第一时间想到的是Golang是强类型语言，interface是所有golang类型的父类 函数中func f(x interface{})的interface{}可以支持传入golang的任何类型，包括指针，但是函数func g(x *interface{})只能接受*interface{} 参考链接 https://www.bookstack.cn/read/aceld-golang/4、interface.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门7-垃圾回收","slug":"Go/Go入门7-垃圾回收","date":"2021-09-03T15:37:38.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2021/09/03/Go/Go入门7-垃圾回收/","link":"","permalink":"http://xboom.github.io/2021/09/03/Go/Go%E5%85%A5%E9%97%A87-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"","text":"垃圾回收 GC，全称 Garbage Collection，即垃圾回收，是一种自动内存管理的机制 通常，垃圾回收器的执行过程被划分为两个半独立的组件： 赋值器（Mutator）：这一名称本质上是在指代用户态的代码。因为对垃圾回收器而言，用户态的代码仅仅只是在修改对象之间的引用关系，也就是在对象图（对象之间引用关系的一个有向图）上进行操作。 回收器（Collector）：负责执行垃圾回收的代码 根对象在垃圾回收的术语中又叫做根集合，它是垃圾回收器在标记过程时最先检查的对象，包括： 全局变量：程序在编译期就能确定的那些存在于程序整个生命周期的变量。 执行栈：每个 goroutine 都包含自己的执行栈，这些执行栈上包含栈上的变量及指向分配的堆内存区块的指针。 寄存器：寄存器的值可能表示一个指针，参与计算的这些指针可能指向某些赋值器分配的堆内存区块。 所有的 GC 算法其存在形式可归结为追踪(Tracing)和引用计数(Reference Counting)这两种形式的混合运用 追踪式 GC:从根对象出发，根据对象之间的引用信息，一步步推进直到扫描完毕整个堆并确定需要保留的对象，从而回收所有可回收的对象。Go、 Java、V8 对 JavaScript 的实现等均为追踪式 GC。 引用计数式 GC:每个对象自身包含一个被引用的计数器，当计数器归零时自动得到回收。因为此方法缺陷较多，在追求高性能时通常不被应用。Python、Objective-C 等均为引用计数式 GC。 目前比较常见的 GC 实现方式包括： 追踪式，分为多种不同类型，例如： 标记清扫：从根对象出发，将确定存活的对象进行标记，并清扫可以回收的对象。 标记整理：为了解决内存碎片问题而提出，在标记过程中，将对象尽可能整理到一块连续的内存上。 增量式：将标记与清扫的过程分批执行，每次执行很小的部分，从而增量的推进垃圾回收，达到近似实时、几乎无停顿的目的。 增量整理：在增量式的基础上，增加对对象的整理过程。 分代式：将对象根据存活时间的长短进行分类，存活时间小于某个值的为年轻代，存活时间大于某个值的为老年代，永远不会参与回收的对象为永久代。并根据分代假设（如果一个对象存活时间不长则倾向于被回收，如果一个对象已经存活很长时间则倾向于存活更长时间）对对象进行回收。 引用计数：根据对象自身的引用计数来回收，当引用计数归零时立即回收。 Go 的 GC 目前使用的是无分代(对象没有代际之分)、不整理(回收过程中不对对象进行移动与整理)、并发(与用户代码并发执行)的三色标记清扫算法。原因在于： 对象整理的优势是解决内存碎片问题以及“允许”使用顺序内存分配器。但 Go 运行时的分配算法基于 tcmalloc，基本上没有碎片问题。 并且顺序内存分配器在多线程的场景下并不适用。Go 使用的是基于 tcmalloc 的现代内存分配算法，对对象进行整理不会带来实质性的性能提升。 分代 GC 依赖分代假设，即 GC 将主要的回收目标放在新创建的对象上（存活时间短，更倾向于被回收），而非频繁检查所有对象。但 Go 的编译器会通过逃逸分析将大部分新生对象存储在栈上（栈直接被回收），只有那些需要长期存在的对象才会被分配到需要进行垃圾回收的堆中。也就是说，分代 GC 回收的那些存活时间短的对象在 Go 中是直接被分配到栈上，当 goroutine 死亡后栈也会被直接回收，不需要 GC 的参与，进而分代假设并没有带来直接优势。并且 Go 的垃圾回收器与用户代码并发执行，使得 STW 的时间与对象的代际、对象的 size 没有关系。Go 团队更关注于如何更好地让 GC 与用户代码并发执行（使用适当的 CPU 来执行垃圾回收），而非减少停顿时间这一单一目标上。 三色标记法 理解三色标记法的关键是理解对象的三色抽象以及波面（wavefront）推进这两个概念。三色抽象只是一种描述追踪式回收器的方法，在实践中并没有实际含义，它的重要作用在于从逻辑上严密推导标记清理这种垃圾回收方法的正确性。也就是说，当我们谈及三色标记法时，通常指标记清扫的垃圾回收。 从垃圾回收器的视角来看，三色抽象规定了三种不同类型的对象，并用不同的颜色相称： 白色对象（可能死亡）：未被回收器访问到的对象。在回收开始阶段，所有对象均为白色，当回收结束后，白色对象均不可达。 灰色对象（波面）：已被回收器访问到的对象，但回收器需要对其中的一个或多个指针进行扫描，因为他们可能还指向白色对象。 黑色对象（确定存活）：已被回收器访问到的对象，其中所有字段都已被扫描，黑色对象中任何一个指针都不可能直接指向白色对象。 这样三种不变性所定义的回收过程其实是一个波面不断前进的过程，这个波面同时也是黑色对象和白色对象的边界，灰色对象就是这个波面。 当垃圾回收开始时，只有白色对象。随着标记过程开始进行时，灰色对象开始出现（着色），这时候波面便开始扩大。当一个对象的所有子节点均完成扫描时，会被着色为黑色。当整个堆遍历完成时，只剩下黑色和白色对象，这时的黑色对象为可达对象，即存活；而白色对象为不可达对象，即死亡。这个过程可以视为以灰色对象为波面，将黑色对象和白色对象分离，使波面不断向前推进，直到所有可达的灰色对象都变为黑色对象为止的过程 Mark and Sweep 算法 在Go v1.3之前的的标记-清除(mark and sweep)算法 第一步:标记(Mark phase)， 暂停程序业务逻辑, 找出不可达的对象，然后做上标记。 第二步:清除(Sweep phase)回收标记好的对象 注意：mark and sweep算法在执行的时候需要程序暂停！即 STW(stop the world)。这段时间程序会卡在哪儿 目前程序的可达对象为1，2，3，4，7五个对象，做上标记。如下图所示 标记完了之后，然后开始清除未标记的对象. 结果如下. 最后停止暂停，让程序继续跑。然后循环重复这个过程，直到process程序生命周期结束 缺点： STW, stop the world; 让程序暂停，程序出现卡顿 标记需要扫描整个heap 清楚数据会产生heap碎片 Go V1.3版本之前就是以上来实施的, 流程是 Go V1.3 做了简单的优化,将STW提前, 减少STW暂停的时间范围.如下所示 最主要的问题是会停止程序，于是在Go v1.5的三色标记法 三色标记法 三色标记法 实际上就是通过三个阶段的标记来确定清楚的对象都有哪些. 第一步: 就是只要是新创建的对象,默认的颜色都是标记为“白色”. 这里面需要注意的是, 所谓“程序”, 则是一些对象的跟节点集合。 第二步: 每次GC回收开始, 然后从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色”集合，非递归遍历 第三步: 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，灰色被遍历之后就放入黑色集合 第四步: 重复第三步, 直到灰色中无任何对象 第五步: 回收所有的白色标记表的对象. 也就是回收垃圾 以上就是三色并发标记法，Go是如何解决标记-清除(mark and sweep)算法中的卡顿(stw，stop the world)问题？ 没有STW的三色标记法 一定要依赖STW的。因为如果不暂停程序，程序的逻辑改变对象引用关系, 这种动作如果在标记阶段做了修改，会影响标记结果的正确性。例如 已经标记为灰色的对象2有指针指向白色的对象3 在还没有扫描到对象2，已经标记为黑色的对象4创建指针q，指向对象3 于此同时对象2将指针p移除，对象3就被挂载了已经扫描完成的黑色的对象4下面 正常执行逻辑，对象2和对象7 被标记为黑色，而对象3因为对象4不在被扫描，而等待被回收 对象3被无辜的清除掉了 当下列两个条件同时满足时, 就会出现对象丢失现象! 条件1: 一个白色对象被黑色对象引用**(白色被挂在黑色下)**，它的下游对象也会一并被清理掉 条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏**(灰色同时丢了该白色)** 为了防止这种现象的发生，最简单的方式就是STW，直接禁止掉其他用户程序对对象引用关系的干扰，但是STW的过程有明显的资源浪费，对所有的用户程序都有很大影响，如何能在保证对象不丢失的情况下合理的尽可能的提高GC效率，减少STW时间呢？ 答案就是, 那么我们只要使用一个机制,来破坏上面的两个条件就可以了 屏蔽机制 让GC回收器,满足下面两种情况之一时,可保对象不丢失. 所以引出两种方式. 强-弱三色不变式 强三色不变式: 不存在黑色对象引用到白色对象的指针 弱三色不变式: 所有被黑色对象引用的白色对象都处于灰色保护状态 为了遵循上述的两个方式,Golang团队初步得到了如下具体的两种屏障方式“插入屏障”, “删除屏障”。 插入屏障 具体操作: 在A对象引用B对象的时候，B对象被标记为灰色。(将B挂在A下游，B必须被标记为灰色) 满足: 强三色不变式. (不存在黑色对象引用白色对象的情况了， 因为白色会强制变成灰色) 黑色对象的内存槽有两种位置, 栈和堆. 栈空间的特点是容量小,但是要求相应速度快,因为函数调用弹出频繁使用, 所以“插入屏障”机制,在栈空间的对象操作中不使用. 而仅仅使用在堆空间对象的操作中。 第一步：程序起初创建，全部标记为白色，将所有对象放入白色集合中 第二步：遍历Root Set(非递归形式，只遍历一次，得到灰色节点) 第三步：遍历Grey 灰色标记表。将可达的对象从白色标记为灰色遍历之后的灰色，标记为黑色 第四步：如果此刻外界向对象4添加对象8，对象1添加对象9。对象4在堆区触发插入屏蔽机制，对象1不触发 第五步：由于插入写屏障(黑色对象添加白色，将白色改为灰色)，对象8变为灰色，对象9依然时i白色 第六步：继续循环上述流程进行三色标记，直到没有灰色节点 但是如果栈不添加,当全部三色标记扫描之后,栈上有可能依然存在白色对象被引用的情况(如上图的对象9). 所以要对栈重新进行三色标记扫描, 但这次为了对象不丢失, 要对本次标记扫描启动STW暂停. 直到栈空间的三色标记结束 第七步：在准备回收白色前，重新遍历扫描一次栈空间。此时加STW暂停保护栈，防止外界干扰 第八步：在STW中，将栈中的对象一次三色标记，直到没有灰色标记 第八步：停止STW 第十步: 最后将栈和堆空间 扫描剩余的全部 白色节点清除. 这次STW大约的时间在10~100ms间 删除屏障 具体操作: 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色。 满足: 弱三色不变式. (保护灰色对象到白色对象的路径不会断) 第一步：程序起初创建，全部标记为白色，将所有对象放入白色集合 第二步：遍历Root Set(非递归形式，只遍历一次)，得到灰色节点 第三步：灰色对象1删除对象5，如果不触发删除写屏障，5-2-3路径与主链路断开，最后均会被清除 第四步：触发删除写屏障，被删除对象5，被标记为黑色 第五步：遍历Grey 灰色标记表，将可达的对象，从白色标记为灰色。遍历之后的灰色，标记为黑色 第六步：继续循环上述流程进行三色标记，直到没有灰色节点 第七步：这种方式的回收精度低，一个对象即使被删除了最后一个指向它的指针也依旧可以活过这一轮，在下一轮GC中被清理掉 第八步：清除白色 混合写屏障(hybrid write barrier)机制 插入写屏障和删除写屏障的短板： 插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活； 删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象。 Go V1.8版本引入了混合写屏障机制（hybrid write barrier），避免了对栈re-scan的过程，极大的减少了STW的时间。结合了两者的优点 具体操作: 1、GC开始将栈上的对象全部扫描并标记为黑色(之后不再进行第二次重复扫描，无需STW)， 2、GC期间，任何在栈上创建的新对象，均为黑色。 3、被删除的对象标记为灰色。 4、被添加的对象标记为灰色。 第一步：GC刚刚开始，都标记为白色 第二步：优先扫描栈对象，将可达对象全部标记为黑色 场景一 对象被一个堆对象删除引用，成为栈对象的下游 第一步：将对象7添加到对象1的下游，因为栈不启动写屏障，所以直接挂载下面 第二步：对象4删除对象7的引用关系，因为对象4是堆区，所以触发写屏障，标记被删除的对象7为灰色(删除即赋新值为nil) 参考文献 https://golang.design/go-questions/memgc/principal/ https://www.bookstack.cn/read/aceld-golang/5、Golang三色标记+混合写屏障GC模式全分析.md","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"架构5-缓存一致性","slug":"FrameWork/架构5-缓存一致性","date":"2021-08-29T09:59:00.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/08/29/FrameWork/架构5-缓存一致性/","link":"","permalink":"http://xboom.github.io/2021/08/29/FrameWork/%E6%9E%B6%E6%9E%845-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/","excerpt":"","text":"缓存是一种提高系统读性能的常见技术，主要用于读多写少的应用场景，那么当数据发生变化，就可能出现缓存与数据库不一致的情况。主要的原因有： 并发的场景下，老的DB数据被读取并更新到缓存中导致不一致 缓存和DB的操作，不在一个事务中，只有一个操作成功导致不一致 从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。但是无法准确计算一个过期时间。如果过期时间太短，则需要频繁的查询数据库，如果过期时间太长，则当DB与Cache不一致的时候，异常时间会太长 缓存架构 考虑数据库与缓存一致性方案的时候，通过写操作需要考虑以下几点： 是更新缓存中的数据，还是淘汰缓存中的数据 是先操纵数据库中的数据再操纵缓存中的数据，还是先操纵缓存中的数据再操纵数据库中的数据 假设缓存与数据库都存在 v = 1 的数据， 现在有 A 查询数据V，同时有进程B更新数据为v = 2，同时还有进程C 更新数据 v = 3 写操作先更新DB，然后在更新Cache，则存在问题的情况是 第一步: B 更新 DB 为 v = 2 第二步: C 更新 DB 为 v = 3 第三步: C 更新 Cache 为 v = 3 第四步: B 更新 Cache 为 v = 2 这里由于无法保证B、C的操作缓存时间，可能导致DB与Cache不一致 数据库锁(隔离级别默认可重复读)无法保证业务代码中B和C谁先执行缓存操作 写操作先更新Cache，然后在更新DB，则存在问题的情况是 第一步: B 更新 Cache 为 v = 2 第二步: C 更新 Cache 为 v = 3 第三步: C 更新 Cache 为 v = 3 第四步: B 更新 Cache 为 v = 2 这里由于无法保证B、C的操作缓存时间，可能导致DB与Cache不一致 写操作先更新DB，然后在删除Cache，则可能存在的问题情况是： 第一步: B 更新DB 为 v = 2 第二步: A 查询缓存 为 v = 1 第三步: B 删除Cache 这里由于删除缓存，A 查询到的数据为旧数据，但是下次查询到的就为更新后的数据 第一步: A 查询DB 为 v = 1 第二步: B 更新DB 为 v = 2 第二步: B 删除Cache 第三步: A 更新Cache为 v = 1 这种情况建立在查询比更新还要慢的情况(概率低) 写操作先删除Cache，然后在更新DB，则可能存在的问题情况是： 第一步：B 先删除Cache 第二步：A 查询DB 第三步：A 查询结果后更新Cache 第四步：B 更新DB 由于查询操作在更新DB之前，导致查询的为旧数据，且下次查询到的仍然为旧数据(需要等到过期时间结束) 总结 淘汰缓存操作简单，负作用则是增加了一次cache miss(通用处理方式)，而更新缓存无法保证缓存为最新的 先操作DB 与先操作Cache中，解决问题方向是：如果出现不一致，谁先做对业务的影响较小，就谁先执行。 方法三中：如果删除Cache失败，则后续查询仍然为旧数据，(需要设置过期时间来解决) 方法四中：可能出现操作没有错误，但是在Cache有效期内，一直是旧数据的情况(方法三是删除Cache失败才会出现) 必须设置缓存过期时间 看来最优的方案是更新DB之后在删除Cache DB与Cache一致性 读缓存 在读取缓存方面，一般都是按照下图的流程来进行业务操作 写缓存 Cache Aside Pattern(边缘缓存模型) 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效 Read/Write Through Pattern 在Cache Aside方案中，应用层需要维护两个数据存储，缓存Cache和DB。而Read/Write Through方案是把更新DB 的操作由缓存自己代理了，业务只需要对接单一的存储，由存储自己维护自己的Cache。 MySQL缓存机制原理是缓存sql文本及查询结果，如果运行相同的SQL，服务器直接从缓存中取到结果，而不需要再去解析和执行SQL。如果表更改了，那么使用这个表的所有缓存查询将不再有效，查询缓存中值相关条目被清空。这里的更改指的是表中任何数据或是结构发生改变 Read Through 套路在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside是由调用方负责把数据加载入缓存，而Read Through则用缓存服务自己来加载，从而对应用方是透明的。 Write Through 套路和Read Through相仿，不过是在更新数据时发生。当有数据更新 如果没有命中缓存，直接更新数据库，然后返回。 如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库（这是一个同步操作） Write Behind Caching Pattern Write Behind 又叫 Write Back。write back就是Linux文件系统的Page Cache的算法。 Write Back简单说就是: 在更新数据的时候只更新缓存不更新数据库，缓存会异步地批量更新数据库。 这个设计的好处就是让数据的I/O操作飞快无比（因为直接操作内存嘛 ），因为异步，write backg还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 但数据不是强一致性的，而且可能会丢失(Unix/Linux非正常关机会导致数据丢失，就是因为这个事)。 另外，Write Back实现逻辑比较复杂，因为他需要track有哪数据是被更新了的，需要刷到持久层上。操作系统的write back会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫lazy write。 主从DB与cache一致性 在数据库架构做了一主多从，读写分离时的情况(忽略redis集群)： 主从DB最大的问题是，存在主-从同步时间，也就是说 第一步: 应用A 更新DB-Master 为 v = 2 第二步: 应用A 开始同步数据到备机 第三步: 应用A 删除缓存更新DB 为 v = 2 第四步: 应用B 查询DB-Slave 为 v = 1 第五步: 应用B 更新Cache 为 v = 1 由于备机是通过binlog来读取数据变化写入数据库，也就是binlog变化是在DB-Slave之前的，那么Canel 监听工具也要指定一段时间发送过期请求给Cache 参考文献 分布式之数据库和缓存双写一致性方式解析 Cache-Aside Pattern Scaling Memcache at Facebook 架构师之路","categories":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/categories/FrameWork/"}],"tags":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/tags/FrameWork/"}]},{"title":"Linux深入3-malloc","slug":"Linux/Linux深入3-malloc","date":"2021-07-31T07:28:42.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/07/31/Linux/Linux深入3-malloc/","link":"","permalink":"http://xboom.github.io/2021/07/31/Linux/Linux%E6%B7%B1%E5%85%A53-malloc/","excerpt":"","text":"基础概念 每个进程都有独立的虚拟地址空间，进程访问的虚拟地址并不是真正的物理地址 虚拟地址是通过每个进程的页表(在每个进程的内核虚拟地址空间)与物理地址进行映射，获得真正物理地址； 如果虚拟地址对应物理地址不在物理内存中，则产生缺页中断，真正分配物理地址，同时更新进程的页表；如果此时物理内存已耗尽，则根据内存替换算法淘汰部分页面至物理磁盘中 空间分配 Linux 使用虚拟地址空间， 对32位操作系统而言，它的寻址空间(虚拟存储空间)为4G(2的32次方)，将最高的1G字节(从虚拟地址0xC0000000到0xFFFFFFFF)，供内核使用，称为内核空间，而将较低的3G字节(从虚拟地址0x00000000到0xBFFFFFFF)，供各个进程使用，称为用户空间。 由低地址到高地址分别为： 只读段：该部分空间只能读不可写(包括：代码段、rodata 段(C常量字符串和#define定义的常量) ) 数据段：保存全局变量、静态变量的空间 堆 ：就是平时所说的动态内存， malloc/new 大部分都来源于此。其中堆顶的位置可通过函数 brk 和 sbrk 进行动态调整。 文件映射区域：如动态库、共享内存等映射物理空间的内存，一般是 mmap 函数所分配的虚拟地址空间 栈：用于维护函数调用的上下文空间，一般为 8M ，可通过 ulimit –s 查看，每个线程都有自己专属的栈 内核虚拟空间：用户代码不可见的内存区域，由内核管理(页表就存放在内核虚拟空间) 分配原理 进程分配内存有两种方式，分别由两个系统调用完成：brk 和 mmap (不考虑共享内存) brk 是将数据段（.data）的最高地址指针 _edata 往高地址推 mmap 是在进程的虚拟地址空间中（堆和栈中间，称为“文件映射区域”的地方）找一块空闲的虚拟内存。 两种方式分配的都是虚拟内存，没有分配物理内存。第一次访问已分配的虚拟地址空间的时候，发生缺页中断进程会陷入内核态，执行以下操作： 检查要访问的虚拟地址是否合法 查找/分配一个物理页 填充物理页内容（读取磁盘，或者直接置0，或者什么都不做） 如果需要读取磁盘，那么这次缺页就是 majfit(major fault：大错误) 否则就是 minflt(minor fault：小错误) 建立映射关系（虚拟地址到物理地址的映射关系） 重复执行发生缺页中断的那条指令 使用命令查看缺页中断的次数 分配过程 第一种情况：malloc小于128K的内存，使用brk 将_edata往高地址推(只分配虚拟空间，不对应物理内存(因此没有初始化)，第一次读/写数据时，引起内核缺页中断，内核才分配对应的物理内存，然后虚拟地址空间建立映射关系)，如下图： 进程启动的时候，其（虚拟）内存空间的初始布局(1)所示 进程调用A=malloc(30K)以后，内存空间 (2), malloc函数会调用brk系统调用，将_edata指针往高地址推30K，就完成虚拟内存分配 _edata+30K只是完成虚拟地址的分配，A这块内存现在还是没有物理页与之对应的，等到进程第一次读写A这块内存的时候，发生缺页中断，这个时候，内核才分配A这块内存对应的物理页。也就是说，如果用malloc分配了A这块内容，然后从来不访问它，那么，A对应的物理页是不会被分配的。 进程调用B=malloc(40K)以后，内存空间如(3) 第二种情况：malloc 大于 128K 的内存，使用 mmap 分配（munmap 释放） 进程调用C=malloc(200K)以后，内存空间如(4) 进程调用D=malloc(100K)以后，内存空间如(5) 进程调用free©以后，C对应的虚拟内存和物理内存一起释放 默认情况下，malloc函数分配内存，如果请求内存大于128K（可由M_MMAP_THRESHOLD选项调节），那就不是去推_edata(指向数据段)指针了，而是利用mmap系统调用，从堆和栈的中间分配一块虚拟内存 这样子做是因为 brk分配的内存需要等到高地址内存释放以后才能释放（例如，在B释放之前，A是不可能释放的，因为只有一个_edata 指针，这就是内存碎片产生的原因，先进后出），而mmap分配的内存可以单独释放 进程调用free(B)以后，如(7)所示:B对应的虚拟内存和物理内存都没有释放，因为只有一个_edata指针，如果往回推，那么D这块内存怎么办呢？当然，B这块内存，是可以重用的，如果这个时候再来一个40K的请求，那么malloc很可能就把B这块内存返回回去了 进程调用free(D)以后，如图(8)所示：B和D连接起来，变成一块140K的空闲内存，当最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作(trim)。在上一个步骤free的时候，发现最高地址空闲内存超过128K，于是内存紧缩，变成(9)所示 总结: 当开辟的空间小于 128K 时，调用 brk()函数，malloc 的底层实现是系统调用函数 brk()，其主要移动指针 _enddata(此时的 _enddata 指的是 Linux 地址空间中堆段的末尾地址，不是数据段的末尾地址) 当开辟的空间大于 128K 时，mmap()系统调用函数来在虚拟地址空间中（堆和栈中间，称为“文件映射区域”的地方）找一块空间来开辟 参考链接 malloc 底层实现原理","categories":[{"name":"Linux Depth","slug":"Linux-Depth","permalink":"http://xboom.github.io/categories/Linux-Depth/"}],"tags":[{"name":"Memory","slug":"Memory","permalink":"http://xboom.github.io/tags/Memory/"}]},{"title":"架构3-布隆过滤器","slug":"FrameWork/架构3-布隆过滤器","date":"2021-07-27T16:36:57.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/07/28/FrameWork/架构3-布隆过滤器/","link":"","permalink":"http://xboom.github.io/2021/07/28/FrameWork/%E6%9E%B6%E6%9E%843-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/","excerpt":"","text":"如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路。但是随着集合中元素的增加，我们需要的存储空间越来越大。同时检索速度也越来越慢。三种结构的检索时间复杂度分别为O(n)，O(logn)，O(1) 哈希函数 哈希函数的概念是：将任意大小的输入数据转换成特定大小的输出数据的函数，转换后的数据称为哈希值或哈希编码，也叫散列值。下面是一幅示意图 所有散列函数都有如下基本特性： 如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。 散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“散列碰撞（collision）”。 布隆过滤器 BloomFilter 是由一个固定大小的二进制向量或者位图（bitmap）和一系列映射函数组成的。 在初始状态时，对于长度为 m 的位数组，它的所有位都被置为0，如下图所示： 当有变量被加入集合时，通过 K 个映射函数将这个变量映射成位图中的 K 个点，把它们置为 1（假定有两个变量都通过 3 个映射函数） 查询某个变量的时候我们只要看看这些点是不是都是 1 就可以大概率知道集合中有没有它了 如果这些点有任何一个 0，则被查询变量一定不在； 如果都是 1，则被查询变量很可能存在 Coding 添加元素 将要添加的元素给 k 个哈希函数 得到对应于位数组上的 k 个位置 将这k个位置设为 1 查询元素 将要查询的元素给k个哈希函数 得到对应于位数组上的k个位置 如果k个位置有一个为 0，则肯定不在集合中 如果k个位置全部为 1，则可能在集合中 特点 一个元素如果判断结果为存在的时候元素不一定存在，但是判断结果为不存在的时候则一定不存在。 布隆过滤器可以添加元素，但是不能删除元素。同一个位置可能被多个值引用，删掉元素会导致误判率增加 解决办法： 定时异步重建布隆过滤器 计数Bloom Filter 过滤器的值存储到Redis中 代码实现 总结 优点： 相比于其它的数据结构，布隆过滤器存储空间和插入/查询时间都是常数 O(K)，另外，散列函数相互之间没有关系，方便由硬件并行实现。布隆过滤器不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 布隆过滤器可以表示全集，其它任何数据结构都不能； 缺点： 随着存入的元素数量增加，误算率随之增加。但是如果元素数量太少，则使用散列表足矣。 一般情况下不能从布隆过滤器中删除元素。我们很容易想到把位数组变成整数数组，每插入一个元素相应的计数器加 1, 这样删除元素时将计数器减掉就可以了。然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 怎么降低误判几率： 增加二进制位数 增加Hash次数 布隆过滤器数据其实也是存在Redis中的 应用场景： 数据库防止穿库 Google Bigtable，Apache HBase和Apache Cassandra以及Postgresql 使用BloomFilter来减少不存在的行或列的磁盘查找。避免代价高昂的磁盘查找会大大提高数据库查询操作的性能。 如同一开始的业务场景。如果数据量较大，不方便放在缓存中。需要对请求做拦截防止穿库。 缓存宕机 缓存宕机的场景，使用布隆过滤器会造成一定程度的误判。原因是除了Bloom Filter 本身有误判率，宕机之前的缓存不一定能覆盖到所有DB中的数据，当宕机后用户请求了一个以前从未请求的数据，这个时候就会产生误判。当然，缓存宕机时使用布隆过滤器作为应急的方式，这种情况应该也是可以忍受的。 WEB拦截器 相同请求拦截防止被攻击。用户第一次请求，将请求参数放入BloomFilter中，当第二次请求时，先判断请求参数是否被BloomFilter命中。可以提高缓存命中率 恶意地址检测 chrome 浏览器检查是否是恶意地址。 首先针对本地BloomFilter检查任何URL，并且仅当BloomFilter返回肯定结果时才对所执行的URL进行全面检查（并且用户警告，如果它也返回肯定结果）。 比特币加速 bitcoin 使用BloomFilter来加速钱包同步 参考链接 布隆过滤器 布隆过滤器","categories":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/categories/FrameWork/"}],"tags":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/tags/FrameWork/"}]},{"title":"Linux基础4-进程","slug":"Linux/Linux基础4-进程","date":"2021-07-25T22:54:33.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/07/26/Linux/Linux基础4-进程/","link":"","permalink":"http://xboom.github.io/2021/07/26/Linux/Linux%E5%9F%BA%E7%A1%804-%E8%BF%9B%E7%A8%8B/","excerpt":"","text":"基础知识 进程就是一个程序的执行流程，内部保存程序运行所需的资源 在UNIX系统中，直邮fork系统调用才可以创建进程 12345678910111213#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main() &#123; pid_t id = fork(); if (id &lt; 0) &#123; perror(\"fork\\n\"); &#125; else if (id == 0) &#123; // 子进程 printf(\"子进程\\n\"); &#125; else &#123; // 父进程 printf(\"父进程\\n\"); &#125; return 0;&#125; 进程创建之后，父子进程都有各自不同的地址空间，其中一个进程在其地址空间的修改对另一个进程不可见。子进程的初始化空间是父进程的一个副本，这里涉及两个不同地址空间，不可写的内存区是共享的，某些UNIX的实现使程序正文在两者间共享，因为它是不可修改的。 还有一种写时复制共享技术，子进程共享父进程的所有内存，一旦两者之一想要修改部分内存，则这块内存被复制确保修改发生在当前进程的私有内存区域 PCB 进程控制块(PCB),操作系统为每个进程都维护一个PCB，用来保存与该进程有关的各种状态信息。进程可以抽象理解为就是一个PCB，PCB是进程存在的唯一标志，用PCB来描述进程的基本情况以及运行变化的过程 PCB包含进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号和调度信息，以及其它在进程由运行态转换到就绪态或阻塞态时必须保存的信息，从而保证该进程随后能再次启动，就像从未中断过一样。 中断发生后操作系统最底层做了什么？ 硬件压入堆栈程序计数器 硬件从中断向量装入新的程序计数器 汇编语言过程保存寄存器的值 汇编语言过程设置新的堆栈 C中断服务例程运行（典型的读和缓冲输入） 调度程序决定下一个将运行的进程 C过程返回到汇编代码 汇编语言过程开始运行新的当前进程 进程控制块中存储的信息 进程标识信息：如本进程的标识，本进程的父进程标识，用户标识等。 处理机状态信息保护区：用于保存进程的运行现场信息： 用户可见寄存器：用户程序可以使用的数据，地址等寄存器 控制和状态寄存器：程序计数器，程序状态字 栈指针：过程调用、系统调用、中断处理和返回时需要用到它 进程控制信息： 调度和状态信息：用于操作系统调度进程使用 进程间通信信息：为支持进程间与通信相关的各种标识、信号、信件等，这些信息存在接收方的进程控制块中 存储管理信息：包含有指向本进程映像存储空间的数据结构 进程所用资源：说明由进程打开使用的系统资源，如打开的文件等 有关数据结构连接信息：进程可以连接到一个进程队列中，或连接到相关的其他进程的PCB 进程状态 进程因为等待输入而被阻塞，进程从运行态转换到阻塞态 调度程序选择了另一个进程执行时，当前程序就会从运行态转换到就绪态 被调度程序选择的程序会从就绪态转换到运行态 当阻塞态的进程等待的一个外部事件发生时，就会从阻塞态转换到就绪态，此时如果没有其他进程运行时，则立刻从就绪态转换到运行态！ 12345678910pid=fork(); // 创建一个与父进程一样的子进程pid=waitpid(); // 等待子进程终止s=execve(); // 替换进程的核心映像exit(); // 终止进程运行并返回状态值s=sigaction(); // 定义信号处理的动作s=sigprocmask(); // 检查或更换信号掩码s=sigpending(); // 获得阻塞信号集合s=sigsuspend(); // 替换信号掩码或挂起进程alarm(); // 设置定时器pause(); // 挂起调用程序直到下一个信号出现 进程调度 一个CPU同一时刻只会有一个进程处于运行状态，操作系统通过调度算法选择下一个要运行的进程 什么时候进行调度 系统调用创建一个新进程后，需要决定是运行父进程还是运行子进程 一个进程退出时需要做出调度决策，需要决定下一个运行的是哪个进程 当一个进程阻塞在I/O和信号量或者由于其它原因阻塞时，必须选择另一个进程运行 当一个I/O中断发生时，如果中断来自IO设备，而该设备现在完成了工作，某些被阻塞的等待该IO的进程就成为可运行的就绪进程了，是否让新就绪的进程运行，或者让中断发生时运行的进程继续运行，或者让某个其它进程运行，这就取决于调度程序的抉择了 调度算法可以分类 非抢占式调度算法：让进程运行直至被阻塞，或者直到该进程自动释放CPU。在时钟中断发生时不会进行调度，在处理完时钟中断后，如果没有更高优先级的进程等待，则被中断的进程会继续执行。简单来说，调度程序必须等待事件结束。 非抢占方式引起进程调度的条件： 进程执行结束，或发生某个事件而不能继续执行 正在运行的进程因有I/O请求而暂停执行 进程通信或同步过程中执行了某些原语操作（wait、block等） 抢占式调度算法：进程运行固定时段。如果时段结束时进程仍在运行，就被挂起，而调度程序挑选另一个进程运行，进行抢占式调度处理，需要在时间间隔的末端发生时钟中断，以便CPU控制返回给调度程序，如果没有可用的时钟，那么非抢占式调度就是唯一的选择。简单来说，防止单一进程长时间独占CPU资源。 进程间通信 匿名管道 匿名管道就是pipe，pipe只能在父子进程间通信，且数据是单向流动（半双工通信） 使用方式： 父进程创建管道，会得到两个文件描述符，分别指向管道的两端； 父进程创建子进程，从而子进程也有两个文件描述符指向同一管道； 父进程可写数据到管道，子进程就可从管道中读出数据，从而实现进程间通信，下面的示例代码中通过pipe实现了每秒钟父进程向子进程都发送消息的功能 123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;int main() &#123; int _pipe[2]; int ret = pipe(_pipe); if (ret &lt; 0) &#123; perror(\"pipe\\n\"); &#125; pid_t id = fork(); if (id &lt; 0) &#123; perror(\"fork\\n\"); &#125; else if (id == 0) &#123; // 子进程 close(_pipe[1]); int j = 0; char _mesg[100]; while (j &lt; 100) &#123; memset(_mesg, '\\0', sizeof(_mesg)); read(_pipe[0], _mesg, sizeof(_mesg)); printf(\"%s\\n\", _mesg); j++; &#125; &#125; else &#123; // 父进程 close(_pipe[0]); int i = 0; char *mesg = NULL; while (i &lt; 100) &#123; mesg = \"父进程来写消息了\"; write(_pipe[1], mesg, strlen(mesg) + 1); sleep(1); ++i; &#125; &#125; return 0;&#125; 平时也会使用 | 关于管道的命令行，如 ls | less 创建管道 为ls创建一个进程，设置stdout为管道写端 为less创建一个进程，设置stdin为管道读端 高级管道 通过popen将另一个程序当作一个新的进程在当前进程中启动，它算作当前进程的子进程，高级管道只能用在有亲缘关系的进程间通信，这种亲缘关系通常指父子进程，下面的GetCmdResult函数可以获取某个Linux命令执行的结果，实现方式就是通过popen 123456789101112131415161718192021222324252627282930std::string GetCmdResult(const std::string &amp;cmd, int max_size = 10240) &#123; char *data = (char *)malloc(max_size); if (data == NULL) &#123; return std::string(\"malloc fail\"); &#125; memset(data, 0, max_size); const int max_buffer = 256; char buffer[max_buffer]; // 将标准错误重定向到标准输出 FILE *fdp = popen((cmd + \" 2&gt;&amp;1\").c_str(), \"r\"); int data_len = 0; if (fdp) &#123; while (!feof(fdp)) &#123; if (fgets(buffer, max_buffer, fdp)) &#123; int len = strlen(buffer); if (data_len + len &gt; max_size) &#123; cout &lt;&lt; \"data size larger than \" &lt;&lt; max_size; break; &#125; memcpy(data + data_len, buffer, len); data_len += len; &#125; &#125; pclose(fdp); &#125; std::string ret(data, data_len); free(data); return ret;&#125; 命名管道 匿名管道有个缺点就是通信的进程一定要有亲缘关系，而命名管道就不需要这种限制。 命名管道其实就是一种特殊类型的文件，所谓的命名其实就是文件名，文件对各个进程都可见，通过命名管道创建好特殊文件后，就可以实现进程间通信。 通过mkfifo创建一个特殊的类型的文件，参数读者看名字应该就了解，一个是文件名，一个是文件的读写权限： 1int mkfifo(const char* filename, mode_t mode); 当返回值为0时，表示该命名管道创建成功，至于如何通信，其实就是个读写文件的问题 消息队列 队列想必大家都知道，像FIFO一样，这里可以有多个进程写入数据，也可以有多个进程从队列里读出数据，但消息队列有一点比FIFO还更高级，它读消息不一定要使用先进先出的顺序，每个消息可以赋予类型，可以按消息的类型读取，不是指定类型的数据还存在队列中。本质上MessageQueue是存放在内核中的消息链表，每个消息队列链表会由消息队列标识符表示，这个消息队列存于内核中，只有主动的删除该消息队列或者内核重启时，消息队列才会被删除 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// 创建和访问一个消息队列int msgget(key_t, key, int msgflg);// 用来把消息添加到消息队列中int msgsend(int msgid, const void *msg_ptr, size_t msg_sz, int msgflg);// msg_ptr是结构体数据的指针，结构第一个字段要有个类型：struct Msg &#123; long int message_type; // 想要传输的数据&#125;;// 从消息队列中获取消息int msgrcv(int msgid, void *msg_ptr, size_t msg_st, long int msgtype, int msgflg);// 用来控制消息队列，不同的command参数有不同的控制方式int msgctl(int msgid, int command, struct msgid_ds *buf);#include &lt;errno.h&gt;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/msg.h&gt;#include &lt;chrono&gt;#include &lt;iostream&gt;#include &lt;thread&gt;using namespace std;#define BUFFER_SIZ 20typedef struct &#123; long int msg_type; char text[BUFFER_SIZ];&#125; MsgWrapper;void Receive() &#123; MsgWrapper data; long int msgtype = 2; int msgid = msgget((key_t)1024, 0666 | IPC_CREAT); if (msgid == -1) &#123; cout &lt;&lt; \"msgget error \\n\"; return; &#125; while (true) &#123; if (msgrcv(msgid, (void *)&amp;data, BUFFER_SIZ, msgtype, 0) == -1) &#123; cout &lt;&lt; \"error \" &lt;&lt; errno &lt;&lt; endl; &#125; cout &lt;&lt; \"read data \" &lt;&lt; data.text &lt;&lt; endl; if (strlen(data.text) &gt; 6) &#123; // 发送超过6个字符的数据，结束 break; &#125; &#125; if (msgctl(msgid, IPC_RMID, 0) == -1) &#123; cout &lt;&lt; \"msgctl error \\n\"; &#125; cout &lt;&lt; \"Receive ok \\n\";&#125;void Send() &#123; MsgWrapper data; long int msgtype = 2; int msgid = msgget((key_t)1024, 0666 | IPC_CREAT); if (msgid == -1) &#123; cout &lt;&lt; \"msgget error \\n\"; return; &#125; data.msg_type = msgtype; for (int i = 0; i &lt; 10; ++i) &#123; memset(data.text, 0, BUFFER_SIZ); char a = 'a' + i; memset(data.text, a, 1); if (msgsnd(msgid, (void *)&amp;data, BUFFER_SIZ, 0) == -1) &#123; cout &lt;&lt; \"msgsnd error \\n\"; return; &#125; std::this_thread::sleep_for(std::chrono::seconds(1)); &#125; memcpy(data.text, \"1234567\", 7); if (msgsnd(msgid, (void *)&amp;data, BUFFER_SIZ, 0) == -1) &#123; cout &lt;&lt; \"msgsnd error \\n\"; return; &#125;&#125;int main() &#123; std::thread r(Receive); r.detach(); std::thread s(Send); s.detach(); std::this_thread::sleep_for(std::chrono::seconds(20)); return 0;&#125; 消息队列收发消息自动保证了同步，不需要由进程自己来提供同步方法，而命名管道需要自行处理同步问题 消息队列接收数据可以根据消息类型有选择的接收特定类型的数据，不需要像命名管道一样默认接收数据。 消息队列有一个缺点就是发送和接收的每个数据都有最大长度的限制 共享内存 可开辟中一块内存，用于各个进程间共享，使得各个进程可以直接读写同一块内存空间，就像线程共享同一块地址空间一样，该方式基本上是最快的进程间通信方式，因为没有系统调用干预，也没有数据的拷贝操作，但由于共享同一块地址空间，数据竞争的问题就会出现，需要自己引入同步机制解决数据竞争问题。 共享内存只是一种方式，它的实现方式有很多种，主要的有mmap系统调用、Posix共享内存以及System V共享内存等。通过这三种“工具”共享地址空间后，通信的目的自然就会达到 信号量 信号量semaphore，是操作系统中一种常用的同步与互斥的机制； 信号量允许多个进程（计数值&gt;1）同时进入临界区； 如果信号量的计数值为1，一次只允许一个进程进入临界区，这种信号量叫二值信号量； 信号量可能会引起进程睡眠，开销较大，适用于保护较长的临界区； 与读写自旋锁类似，linux内核也提供了读写信号量的机制； 流程分析 可以将信号量比喻成一个盒子，初始化时在盒子里放入N把钥匙，钥匙先到先得，当N把钥匙都被拿走完后，再来拿钥匙的人就需要等待了，只有等到有人将钥匙归还了，等待的人才能拿到钥匙 1234567891011struct semaphore &#123; raw_spinlock_t lock; //自旋锁，用于count值的互斥访问 unsigned int count; //计数值，能同时允许访问的数量，也就是上文中的N把锁 struct list_head wait_list; //不能立即获取到信号量的访问者，都会加入到等待列表中&#125;;struct semaphore_waiter &#123; struct list_head list; //用于添加到信号量的等待列表中 struct task_struct *task; //用于指向等待的进程，在实际实现中，指向current bool up; //用于标识是否已经释放&#125;; down接口用于获取信号量 如果sem-&gt;count &gt; 0时，也就是盒子里边还有多余的锁，直接自减并返回了 当sem-&gt;count == 0时，表明盒子里边的锁被用完了，当前任务会加入信号量的等待列表中，设置进程的状态，并调用schedule_timeout来睡眠指定时间，实际上这个时间设置的无限等待，也就是只能等着被唤醒，当前任务才能继续运行； up用于释放信号量 如果等待列表为空，表明没有多余的任务在等待信号量，直接将sem-&gt;count自加即可。 如果等待列表非空，表明有任务正在等待信号量，那就需要对等待列表中的第一个任务（等待时间最长）进行唤醒操作，并从等待列表中将需要被唤醒的任务进行删除操作 信号量缺点 Semaphore与Mutex在实现上有一个重大的区别：ownership。Mutex被持有后有一个明确的owner，而Semaphore并没有owner，当一个进程阻塞在某个信号量上时，它没法知道自己阻塞在哪个进程（线程）之上； 没有ownership会带来以下几个问题： 在保护临界区的时候，无法进行优先级反转的处理； 系统无法对其进行跟踪断言处理，比如死锁检测等； 信号量的调试变得更加麻烦； 因此，在Mutex能满足要求的情况下，优先使用Mutex 其他接口 信号量提供了多种不同的信号量获取的接口，介绍如下： 12345678/* 未获取信号量时，进程轻度睡眠：TASK_INTERRUPTIBLE */int down_interruptible(struct semaphore *sem)/* 未获取到信号量时，进程中度睡眠：TASK_KILLABLE */int down_killable(struct semaphore *sem)/* 非等待的方式去获取信号量 */int down_trylock(struct semaphore *sem)/* 获取信号量，并指定等待时间 */int down_timeout(struct semaphore *sem, long timeout) 读写信号量 读写自旋锁，读写信号量的功能类似，它能有效提高并发性，包含以下特点： 允许多个读者同时进入临界区 读者与写者不能同时进入临界区（读者与写者互斥） 写者与写者不能同时进入临界区（写者与写者互斥） 读写信号量的数据结构与信号量的结构比较相似： 12345678910111213141516struct rw_semaphore &#123; atomic_long_t count; //用于表示读写信号量的计数 struct list_head wait_list; //等待列表，用于管理在该信号量上睡眠的任务 raw_spinlock_t wait_lock; //锁，用于保护count值的操作#ifdef CONFIG_RWSEM_SPIN_ON_OWNER struct optimistic_spin_queue osq; /* spinner MCS lock */ //MCS锁，参考上一篇文章Mutex中的介绍 /* * Write owner. Used as a speculative check to see * if the owner is running on the cpu. */ struct task_struct *owner; //当写者成功获取锁时，owner会指向锁的持有者#endif#ifdef CONFIG_DEBUG_LOCK_ALLOC struct lockdep_map dep_map;#endif&#125;; 读写自旋锁，读写自旋锁中的lock字段，bit[31]用于写锁的标记，bit[30:0]用于读锁的统计，而读写信号量的count字段也大体类似； 孤儿进程与僵尸进程 在linux中，正常情况下子进程是通过父进程创建的，父进程无法预测子进程结束时间，需要调用 wait/waitpid 系统调用取的子进程的终止状态 孤儿进程 如果父进程先退出，而它的子进程还在运行，那么子进程被称为孤儿进程。孤儿进程被根进程(进程号为1)所收养，由根进程管理 僵尸进程 任何一个子进程(根进程除外)在退出之后，并非马上消失，内核释放该进程资源，包括打开的文件，占用的内存等。但是仍然为其保留一定的信息 进程号 the process ID 退出状态 the Termination status of the process 运行时间 the amount of CPU time taken by the process 留下一个称为 僵尸进程(Zombie)的数据结构，直到父进程通过 wait/waitpid 来取时才会释放。如果父进程不调用 wait/waitpid 的话，那么保留的信息就不会释放，其进程号就会一直被占用，但系统所使用的进程号时有限的，如果存在大量僵尸进程，可能因为没有可用进程号导致系统不能产生新的进程 通过命令 ulimit -a 查看 max user processes 通过 ps 命令查看，状态为 Z 定位僵尸进程 ps -A -ostat,ppid,pid,cmd |grep -e '^[Zz]’ -A 参数列出所有进程 -o 自定义输出字段 stat（状态）、ppid（进程父id）、pid（进程id）、cmd（命令） 因为状态为z或者Z的进程为僵尸进程，所以我们使用grep抓取stat状态为zZ进程 僵尸进程ID:3457，父进程ID:3425 僵尸进程ID:3533，父进程ID:3511 处理僵尸进程 使用 kill -HUP &lt;僵尸进程ID&gt; 往往无法杀死僵尸进程，-hup会让进程挂起/休眠。此时需要使用 kill -HUP &lt;僵尸进程父ID&gt; 来杀死进程(注意这里有可能杀死根进程，杀的时候注意) 杀死后可通过上述查询命令查看僵尸进程是否存在 僵尸进程代码解决 方法一：wait/waitpid 函数: pid_t wait(int *status) 父进程在调用wait函数之后将自己阻塞，由wait自动分析是否当前进程的某个子进程已经退出 如果找到了变成僵尸的子进程，wait就会收集这个子进程信息，并将它测底销毁后返回 如果没有找到，wait就会一直阻塞在这里，直到有一个出现为止。其中参数 status 用来保存被收集进程退出时的一些状态，是一个指向int类型的指针(不关心如何死掉，直接使用 wait(NULL) )，一个wait也只能处理一个僵尸进程 方法2：将父进程中对SIGCHILD信号的处理函数设为 使用信号，引入 sinal.h，子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait进行处理僵尸进程(SIG_IGN 或直接忽略) 1234567891011121314151617181920212223242526272829303132333435363738394041 1 #include &lt;stdio.h&gt; 2 #include &lt;unistd.h&gt; 3 #include &lt;errno.h&gt; 4 #include &lt;stdlib.h&gt; 5 #include &lt;signal.h&gt; 6 7 static void sig_child(int signo); 8 9 int main()10 &#123;11 pid_t pid;12 //创建捕捉子进程退出信号13 signal(SIGCHLD,sig_child);14 pid = fork();15 if (pid &lt; 0)16 &#123;17 perror(\"fork error:\");18 exit(1);19 &#125;20 else if (pid == 0)21 &#123;22 printf(\"I am child process,pid id %d.I am exiting.\\n\",getpid());23 exit(0);24 &#125;25 printf(\"I am father process.I will sleep two seconds\\n\");26 //等待子进程先退出27 sleep(2);28 //输出进程信息29 system(\"ps -o pid,ppid,state,tty,command\");30 printf(\"father process is exiting.\\n\");31 return 0;32 &#125;33 34 static void sig_child(int signo)35 &#123;36 pid_t pid;37 int stat;38 //处理僵尸进程39 while ((pid = waitpid(-1, &amp;stat, WNOHANG)) &gt;0)40 printf(\"child %d terminated.\\n\", pid);41 &#125; **方法3：**将子进程成为孤儿进程，从而其父进程变为根进程，通过根进程处理子进程 进程P 创建子进程 Pc1 之后，子进程再创建子进程 Pcc1， 父进程等待第一个子进程 Pc1退出，让Pc1退出，Pcc1 处理问题称为孤儿进程，由根进程处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;errno.h&gt; int main()&#123; pid_t pid; //创建第一个子进程 pid = fork(); if (pid &lt; 0) &#123; perror(\"fork error:\"); exit(1); &#125; //第一个子进程 else if (pid == 0) &#123; //子进程再创建子进程 printf(\"I am the first child process.pid:%d\\tppid:%d\\n\",getpid(),getppid()); pid = fork(); if (pid &lt; 0) &#123; perror(\"fork error:\"); exit(1); &#125; //第一个子进程退出 else if (pid &gt;0) &#123; printf(\"first procee is exited.\\n\"); exit(0); &#125; //第二个子进程 //睡眠3s保证第一个子进程退出，这样第二个子进程的父亲就是根进程里 sleep(3); printf(\"I am the second child process.pid: %d\\tppid:%d\\n\",getpid(),getppid()); exit(0); &#125; //父进程处理第一个子进程退出 if (waitpid(pid, NULL, 0) != pid) //等待进程ID为 pid 的子进程，只要子进程没有结束，就一直等下去 &#123; perror(\"waitepid error:\"); exit(1); &#125; exit(0); return 0;&#125; 参考文献 https://www.cnblogs.com/Anker/p/3271773.html https://mp.weixin.qq.com/s/wTicQwTu8Ta8gLv2fZ3rCA https://mp.weixin.qq.com/s/Lu1nqXfrGPsFcFHH_R_rYA","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"架构2一致性Hash","slug":"FrameWork/架构2-一致性Hash","date":"2021-07-25T14:29:14.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/07/25/FrameWork/架构2-一致性Hash/","link":"","permalink":"http://xboom.github.io/2021/07/25/FrameWork/%E6%9E%B6%E6%9E%842-%E4%B8%80%E8%87%B4%E6%80%A7Hash/","excerpt":"","text":"以存储为例，在整个微服务系统中，存储一般是多个节点 一是为了提高稳定，单节点宕机情况下，整个存储就面临服务不可用 二是数据容错，同样单节点数据物理损毁，而多节点情况下，节点有备份，除非互为备份的节点同时损毁 可以从两个方面衡量哈希算法的适用性： 平衡性：指哈希的结果能够尽可能分布到所有的节点的值域中，这样所有节点的值域都能得到利用(例如：9个 key ，3个节点，最理想的就是每个节点处理3个） 单调性：新值域要能从原有分布 key 里面分摊压力，原有值域却要尽量不落到原有已经分布的 key Hash 一个可以将输入值“压缩”并转成更小的值，这个值通常状况下是唯一、格式极其紧凑的，比如uint64 幂等：每次用同一个值去计算 hash 必须保证都能得到同一个值 采取普通的 hash 算法进行路由，如：key % N 。有一个节点由于异常退出了集群或者是心跳异常，这时再进行 hash route ，会造成大量的数据重新 分发到不同的节点 。节点在接受新的请求时候，需要重新处理获取数据的逻辑：如果是在缓存中，容易引起 缓存雪崩 缓存雪崩：指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机 解决方案： 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生 如果缓存数据库是分布式部署，将热点数据均匀分布在不同的缓存数据库中 设置热点数据永远不过期 所以就引入了一致性Hash(Consistent Hash) 一致性哈希-基础类型 最基础的一致性 hash 算法就是把节点直接分布到环上，从而划分出值域， key 经过 hash( x ) 之后，落到不同的值域，则由对应的节点处理。类似下图，物理节点直接映射到环上 最常见的值域空间大小是：2^32 - 1，节点落到这个空间，来划分不同节点所属的值域 当加入一个新的节点时，只会影响该节点附近的数据。其他节点的数据不会收到影响，从而解决了节点变化的问题 这个正是单调性。这也是 normal hash 算法无法满足分布式场景的原因 基本的一致性哈希算法有明显的缺点： 随机分布节点的方式使得很难均匀的分布哈希值域 在动态增加节点后，原先的分布就算均匀也很难再继续保证均匀； 增删节点带来的一个较为严重的缺点是： 当一个节点异常时，该节点的压力全部转移到相邻的一个节点； 当一个新节点加入时只能为一个相邻节点分摊压力； 一致性哈希-虚拟节点 针对基础一致性 hash 的缺点一种改进算法是引入虚节点（virtual node）的概念。这个本质的改动：值域不再由物理节点划分，而是由固定的虚拟节点划分，这样值域的不均衡就不存在了。 步骤： 系统初始时就创建许多虚节点，虚节点的个数一般远大于未来集群中机器的个数，将虚节点均匀分布到一致性哈希值域环上，功能与基本一致性哈希算法中的节点相同； 为每个物理节点分配若干虚节点； 操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点（旁白：所以这部分元数据是需要存下来的）； 使用虚节点改进有多个优点： 首先，一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点承载失效节点的压力； 同理，一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以负载多个原有节点的压力，从全局看，较容易实现扩容时的负载均衡； 但是这样当B不可用时，仍然是临近的两个节点A、C 承接了B所有的请求 所以虚拟节点和物理节点的绑定关系不能这样绑定，最好打散绑定。不然还是做不到上面说的两个优点 增节点的时候要能为多个节点分摊压力 删节点的时候要能让多个节点承担压力 标红的表示假设 B 节点故障，假设流量顺时针后移的话，那么就能用到 A，C 两个节点来分摊流量了 由于有虚拟节点，所以可以保持值域不变，当出现增删节点只需要调整物理节点映射虚拟节点的关系即可，从而达到流量打散的目的。 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859type Hash func(data []byte) uint32type Map struct &#123; hash Hash //计算hash的函数 replicas int //这个是副本数，影响到虚拟节点的个数(nodes * replicas = virtual nodes) keys []int //有序的列表，从大到校排序的，这个很重要 hashMap map[int]string //用来记录虚拟节点和物理节点元数据关系的&#125;//初始化func New(replicas int, fn Hash) *Map &#123; m := &amp;Map&#123; replicas: replicas, hash: fn, hashMap: make(map[int]string), &#125; if m.hash == nil &#123; // 默认可以用 crc32 来计算hash值 m.hash = crc32.ChecksumIEEE &#125; return m&#125;func (m *Map) Add(keys ...string) &#123; // keys =&gt; [ A, B, C ] for _, key := range keys &#123; for i := 0; i &lt; m.replicas; i++ &#123; // hash 值 = hash (i+key) 每个key有 replicas 个虚拟key hash := int(m.hash([]byte(strconv.Itoa(i) + key))) m.keys = append(m.keys, hash) // 虚拟节点 &lt;-&gt; 实际节点 m.hashMap[hash] = key &#125; &#125; sort.Ints(m.keys) //对keys进行排序&#125;func (m *Map) IsEmpty() bool &#123; if m != nil &#123; return len(m.hashMap) == 0 &#125; return true&#125;func (m *Map) Get(key string) string &#123; if m.IsEmpty() &#123; return \"\" &#125; // 根据用户输入key值，计算出一个hash值 hash := int(m.hash([]byte(key))) // 查看值落到哪个值域范围？选择到虚节点 idx := sort.Search(len(m.keys), func(i int) bool &#123; return m.keys[i] &gt;= hash &#125;) if idx == len(m.keys) &#123; idx = 0 &#125; // 选择到对应物理节点 return m.hashMap[m.keys[idx]]&#125; 比如，A，B，C 三个节点，replicas 为3，那么就： 节点输入：keys =&gt; [ A, B, C ] 用来计算 hash 值的输入是：i + key，也就是输入为：[ 0A, 1A, 2A, 0B, 1B, 2B, 0C, 1C, 2C] ； 计算出来的 hash 序列是：m.keys = [ hash(0A), hash(1A), hash(2A), hash(0B), hash(1B), hash(2B), hash(0C), hash(1C), hash(2C) ] ； 我们认为 hash 函数是有比较好的平衡性的，那么计算出的值，应该就是随机均衡打散的，我们认为是符合概率分布的； 最后会把这个 hash 值的序列做一个排序，做完排序之后，其实就完成了值域的打散划分； 一致性hash可以广泛使用在分布式系统中： 分布式缓存。可以在 redis cluster 这种存储系统上构建一个 cache proxy，自由控制路由。而这个路由规则就可以使用一致性hash算法 服务发现 分布式调度任务 参考链接 分布式系统基石一 搞懂一致性Hash","categories":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/categories/FrameWork/"}],"tags":[{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/tags/FrameWork/"}]},{"title":"Mysql入门8-备份与恢复","slug":"MySql/MySql入门8-备份与恢复","date":"2021-07-25T03:43:51.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2021/07/25/MySql/MySql入门8-备份与恢复/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql%E5%85%A5%E9%97%A88-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/","excerpt":"","text":"概述 根据备份方法的不同，备份可以分为： 热备 Hot Backup：指数据库运行中直接备份，对正在运行的数据库没有任何影响，也称在线备份(Online Backup) 冷备 Code Backup: 指备份操作是在数据库停止的情况下，也称离线备份(Offline Backup) 温备份 Warm Backup: 指数据库运行中进行的备份操作，但是对当前数据库操作有影响。如加一个全局读锁保证数据一致性 按照备份后的文件的内容可以分为： 逻辑备份： 指备份出的文件内容是可读的，内容一般是由一条条SQL语句或表内容实际数据组成 一般用于数据库升级与迁移 缺点是 恢复所需要的时间往往比较长 裸文件备份: 复制数据库的物理文件，即可以是在数据库运行中的复制，也可以是数据库停止运行时直接的数据文件复制。 恢复时间往往比逻辑备份短的多 按照备份数据库的内容可分为： 完全备份：对数据库进行一个完全的备份 增量备份：在上次完全备份的基础上，对于更改的数据进行备份 日志备份：主要指对MySQL数据库二进制日志的备份 MySQL数据库复制(replication)的原理就是异步实时的将二进制日志重做传送并应用到从(slave/standby)数据库 冷备 对于InnoDB存储引擎的冷备，只需要备份MySQL数据库的frm文件，共享表空间文件，独立表空间文件(*.idb)，重做日志。 另外建议定期备份MySQL数据库的配置文件my.cnf 最好将本地产生的备份存放到一台远程服务器中，却被不会应为本地数据库的宕机而影响备份文件的使用 优点： 备份简单，只需要复制相关文件 恢复简单，只需要把文件恢复到指定位置即可 备份文件可在不同操作系统，不同MySQL版本上恢复 恢复速度快，不需要执行任何SQL语句，也不需要重建索引 缺点: InnoDB存储引擎冷备的文件通常比逻辑文件大很多，因为表空间存放着很多其他数据，如undo段，插入缓冲等信息 冷备跨平台可能存在操作系统，MySQL的版本，文件大小写敏感和浮点数格式都可能称为问题 热备 ibbackup是InnoDB存储引擎官方提供的热备工具，可以同时备份MyISAM存储引擎。原理是： 记录备份开始时，InnoDB存储引擎重做日志文件检查点的LSN 复制共享表空间文件以及独立表空间文件 记录复制完表空间文件后，InnoDB存储引擎重做日志文件检查点LSN 复制在备份时产生的重做日志 优点： 在线备份，不阻塞任何的SQL语句 备份性能好，备份的实质是复制数据库文件和重做日志文件 支持压缩备份，通过选项，支持不同级别的压缩 跨平台 逻辑备份 123456789101112131415161718#1. 备份所有数据库mysqldump --all-databases &gt; dump.sql#2. 备份指定数据库mysqldump --databases db1 db2 db3 &gt;dump.sql#3. 导出指定数据库指定条件的数据(test数据库，表a)mysqldump --single-transaction --where&#x3D;&#39;b&gt;2&#39; test a &gt; a.sql#4. 备份恢复(直接执行mysql语句)mysql -uroot -p &lt; test_backup.sql#5. 执行使用source导出逻辑备份文件mysql&gt; source &#x2F;home&#x2F;mysql&#x2F;test_bakcup.sql# 二进制日志备份与恢复#1. 推荐二进制日志的服务器配置log-bin &#x3D; mysql-binsync_binlog &#x3D; 1innodb_suuport_xa &#x3D; 1#2. 使用mysqlbinlog恢复日志shell&gt; mysqlbinlog binlog.00001 &gt; &#x2F;tmp&#x2F;statements.sql 注意： mysqldump 无法导出视图，需要独立导出视图的定义或者备份视图定义 frm文件 备份二进制日志文件前，可以通过FLUSH LOGS 命令来生成一个新的二进制日志文件，然后备份 快照备份 指通过文件系统的快照功能进行数据库备份，MySQL本身不支持快照功能 复制(replication) replication的工作原理可以分为3个步骤： 主服务器(master)把数据更改记录到二进制日志(binlog)中 从服务器(slave)把主服务器的二进制日志复制到自己的中继日志(relay log)中 从服务器重做中继日志中的日志，把更改应用到自己的数据库 复制不是完全实时地进行同步，而是异步实时 主服务器上有一个线程负责发送二进制日志 从服务器有2个线程 I/O线程，负责读取主服务器的二进制日志，并将其保存为中继日志 SQL线程负责复制执行中继日志 在主从架构下，当主服务器误操作发生，从库也会跟着执行，这该怎样恢复？ 可对从服务器上的数据库所在分区做快照，避免误操作对复制造成的影响，然后根据二进制日志进行point-in-time的恢复，因此快照+复制的备份架构如下： 参考文献 《MySql技术内幕》","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门5-事务","slug":"MySql/MySql入门5-事务","date":"2021-07-25T03:20:52.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2021/07/25/MySql/MySql入门5-事务/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql%E5%85%A5%E9%97%A85-%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"概述 InnoDB存储引擎中的事务完全符合ACID特性： 原子性(atomicity)：整个事务是不可分割的工作单位，任何一个SQL语句执行失败，已经执行成功SQL语句也必须撤销，数据库状态退回到执行事务前的状态。 一致性(consistency)：一致性事务指数据库从一个状态转变为下一种一致的状态。事务开始前和事务结束后，数据库完整性并没有被破坏。如事务执行后数据库唯一约束被破坏，则自动撤销事务，返回初始化状态。 隔离性(isolation)：事务的隔离性要求事务提交前对其他事务不可见 持久性(durability)：事务一旦提交，其结果就是永久性的。即使数据库崩崩溃而恢复，提交的数据也不会丢失 事务的分类： 隐式事务：事务没有明显的开始和结束的标记。MySql的每一条DML(增删改)语句都是一个单独的事务，每条语句DML执行完毕自动提交事务。MySql默认自动提交事务 SHOW VARIABLES LIKE 'autocommit'; 显示事务：事务具有明显的开启和结束的标记，前提是必须设置自动提交功能为禁用 开启事务 - 执行SQL语句 - 成功 - 提交事务 开启事务 - 执行SQL语句 - 失败 - 回滚事务 还能设置回滚点： 123savepoint &lt;回滚点名字&gt;rollback to &lt;回滚点名字&gt; 隔离级别 并发可能产生的问题： 脏读 dirty read: 读到其他事务未提交的数据(针对其他事务未提交的操作) 不可重复读 non-repeatable read: 前后读取的记录内容不一致(针对其他事务修改或删除的操作) 幻读 phantom read: 前后读取的记录数量不一致(针对其他事务新增的操作) 针对解决上述问题，将事务的隔离级别分为(由低到高)： 读未提交 read uncommitted: 一个事务未提交，改动能被另一个事务看到 =&gt; 都发生 读提交 read committed: 一个事务提交了，改动才能被另一个事务看到 =&gt; 没有赃读 可重复读 repeatable read(默认级别): 一个事务提交了，改动也不能被另一个事务看到 =&gt; 只有幻读 串行化 serializable: 后访问的事务必须等待前一个事务执行完成才能访问 =&gt; 都解决 所以总结下来就是： 隔离级别 脏读 不可重复读 幻读 read-uncommitted读取未提交 会出现 会出现 会出现 read-committed读已提交 解决 会出现 会出现 repeatable-read可重复读 解决 解决 会出现 serializable可串行化 解决 解决 解决 修改隔离级别语句： 123456set session transaction isolation level read uncommitted; #读未提交set session transaction isolation level read committed; #读已提交set session transaction isolation level repeatable read; #可重复读set session transaction isolation level serializable; #可串行化set autocommit &#x3D; 0; #取消自动提交select @@tx_isolation; #查询隔离级别 当你不再需要该表时， 用 drop 当你仍要保留该表，但要删除所有记录时， 用 truncate，不可回滚 当你要删除部分记录时（always with a WHERE clause), 用 delete 事务的基础 数据库的隔离性包括(ACID)，由不同的功能实现 Atomicity 原子性: 使用undo log 实现回滚 Consistency 一致性: 通过原子性、持久性、隔离性实现数据一致性 Lsolation 隔离性: 使用锁以及MVCC 实现读写分离、读读并行、读写并行 Durability 持久性: 通过redo log 恢复，和在并发环境下的隔离做到一致性 redo log 称为重做日志，是物理日志，记录页的物理修改操作，用来恢复提交事务修改的页操作 undo log 称为回滚日志，是逻辑日志，根据每行记录进行记录。用来回滚行记录到某个特定版本 redo log redo log 也做重做日志，日志文件由两部分组成： 重做日志缓冲 redo log buffer 重做日志文件 redo log file 1234567start transaction;select balance from bank where name=\"zhangsan\";#生成 重做日志 balance=600 update bank set balance = balance - 400; # 生成 重做日志 amount=400 update finance set amount = amount + 400;commit; 为了提升性能不会把每次的修改都实时同步到磁盘，而是会先存到Boffer Pool(缓冲池)里头，把这个当作缓存来用。然后使用后台线程去做缓冲池和磁盘之间的同步，如果还没有来得及同步就宕机怎么办？ 通过Force Log at Commit机制实现事务持久性，即当事务提交(COMMIT)时，必须先将该事务的所有日志写入到重做日志进行持久化，待事务的COMMIT操作完成才算完成 为了确保每次日志都写入重做日志，在每次将重做日志缓冲写入重做日志文件后，InnoDB存储引擎还需要调用一次fsync操作。由于重做日志文件打开并没有使用O_DIRECT选项，因此重做日志缓冲先写入文件系统缓存，为了确保重做日志写入磁盘，必须进行一次fsync操作 系统重启之后在读取redo log恢复最新数据 通过 innodb_flush_log_at_trx_commit 来控制充足哦日志刷新到磁盘的策略 1：事务提交时必须调用一次fsync操作(默认) 0：事务提交时不进行写入重做日志，而是通过master thread 每1秒进行一次重做日志文件的fsync操作 2： 事务提交时将重做日志写入重做日志文件，但仅仅写入文件系统缓存，不进行fsync操作 与二进制日志 binlog的区别是： redo log 是在InnoDB存储引擎层产生的，而二进制文件是服务层产生的 binlog记录的是逻辑日志，记录的是对应的SQL日志。而redo log是物理日志，记录的是对每个页的修改 存入磁盘的时间点不一样 binlog 只在事务提交完成后进行一次写入 redo log在事务进行中不断地被写入，表现为日志并不是随事务提交的顺序进行写入的 总结：redo log是用来恢复数据的 用于保障，已提交事务的持久化特性 undo log undo 也叫做回滚日志，为了回滚需要将之前的操作都记录下来 每次写入数据或者修改数据之前都会把修改前的信息记录到 undo log 当发生系统错误或执行回滚的时候使用undo log undo log 记录事务修改之前版本的数据信息，因此假如由于系统错误或者rollback操作而回滚的话可以根据undo log的信息来进行回滚到没被修改前的状态 (1) 如果在回滚日志里有新增数据记录，则生成删除该条的语句 (2) 如果在回滚日志里有删除数据记录，则生成生成该条的语句 (3) 如果在回滚日志里有修改数据记录，则生成修改到原先数据的语句 总结：undo log是用来回滚数据的用于保障 未提交事务的原子性 MVCC MVCC是通过在每行记录的后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存了行的过期时间，存储的不是实际的时间值而是系统版本号，主要实现思想是通过数据多版本来做到读写分离。从而实现不加锁读进而做到读写并行 MVCC 在mysql中的实现依赖的是undo log 与 read view undo log: unlog 中就某行数据的多个版本数据 read view: 用来判断当前版本数据的可行性 原子性的实现 一个事务必须被视为不可分割的最小工作单位，一个事务中的所有操作要么全部成功提交，要么全部失败回滚，对于一个事务来说不可能只执行其中的部分操作，这就是事务的原子性，通过上述undo log 即可以实现 持久性的实现 事务一旦提交，其所作做的修改会永久保存到数据库中，此时即使系统崩溃修改的数据也不会丢失 InnoDB提供了缓冲池(Buffer Pool)，Buffer Pool中包含了磁盘数据页的映射，可以当做缓存来使用 读数据：会首先从缓冲池中读取，如果缓冲池中没有，则从磁盘读取在放入缓冲池 写数据：会首先写入缓冲池，缓冲池中的数据会定期同步到磁盘中 如果数据已提交，但在缓冲池里还未来得及磁盘持久化，需要一种机制保存已提交事务的数据，为恢复数据使用。redo log 即可解决这个问题，既然redo log也需要存储，也涉及磁盘IO为啥还用它？ redo log 的存储是顺序存储，而缓存同步是随机操作 缓存同步是以数据页为单位的，每次传输的数据大小大于redo log 隔离性的实现 读未提交 读未提交可以理解为没有隔离 串行话(读写锁) 串行话读的时候加共享锁，其他事务可以并发读但不能写。写的时候加排它锁，其他事务不能并发写也不能并发读 可重复读 MVCC:多版本并发控制，通过undo log版本链和read-view实现事务隔离 以可重复读为例 每条记录在更新时除了记录一条变更记录到redo log中，还会记录一条变更相反的回滚操作记录在undo log中 例如一个值从1被按顺序改成了2、3、4，在回滚日志里就会有如下记录： 当前值是4，但查询这条记录的时候，不同时刻启动的事务有不同的read-view 在视图 A、B、C 里面，这一个记录的值分别是 1、2、4 对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到 即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图(也叫快照)，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现 日志什么时候删除？ 日志怎么存储 长事务导致undolog一致存在不被删除，什么是长事务 什么是视图、MVCC 对于一个视图(快照)来说，它能够读到那些版本数据，要遵循以下规则： 当前事务内的更新，可以读到； 版本未提交，不能读到； 版本已提交，但是却在快照创建后提交的，不能读到； 版本已提交，且是在快照创建前提交的，可以读到； 可重复读和读提交的区别在于：在快照的创建上，可重复读仅在事务开始是创建一次，而读提交每次执行语句的时候都要重新创建一次 什么时候删除回滚日志undo-log 当没有比回滚日志更早的读视图（读视图在事务开启时创建）的时候，这个数据不会再有谁驱使它回滚了，这个回滚日志也就失去了用武之地，可以删除了 ibdata文件是共享表空间数据文件。 5.7版本支持单独配置undo log的路径和表空间文件。 为什么回滚到清理后，文件还是不会变小？这个“清理”的意思是 “逻辑上这些文件位置可以复用”，但是并没有删除文件，也没有把文件变小。那到底什么时候删除呢？ 不同的事务拥有不同的Read view，如果一个事务长时间没有提交，意味着会存在很多老的事务视图，同时也保存了大量回滚日志，占用磁盘空间，且在mysql5.5之前，回滚日志和字典都保存在ibdata文件中，即使长事务被提交了，回滚段被清理，文件也不会变小。同时长事务还长期占用锁资源，降低并发效率 幻读 并发写问题的解决方式就是行锁，而解决幻读用的也是锁，叫做间隙锁，MySQL 把行锁和间隙锁合并在一起，解决了并发写和幻读的问题，这个锁叫做 Next-Key锁。 假设现在表中有两条记录，并且 age 字段已经添加了索引，两条记录 age 的值分别为 10 和 30 如图所示，分成了3 个区间，(负无穷,10]、(10,30]、(30,正无穷]，在这3个区间是可以加间隙锁的。 之后，我用下面的两个事务演示一下加锁过程 在事务A提交之前，事务B的插入操作只能等待，这就是间隙锁起得作用。当事务A执行update user set name='风筝2号’ where age = 10; 的时候，由于条件 where age = 10 ，数据库不仅在 age =10 的行上添加了行锁，而且在这条记录的两边，也就是(负无穷,10]、(10,30]这两个区间加了间隙锁，从而导致事务B插入操作无法完成，只能等待事务A提交。不仅插入 age = 10 的记录需要等待事务A提交，age&lt;10、10&lt;age&lt;30 的记录页无法完成，而大于等于30的记录则不受影响，这足以解决幻读问题了。 这是有索引的情况，如果 age 不是索引列，那么数据库会为整个表加上间隙锁。所以，如果是没有索引的话，不管 age 是否大于等于30，都要等待事务A提交才可以成功插入 当前读 两个事务，对同一条数据做修改。结果应该时间靠后的结果。且更新之前要先读数据，这里所说的读和上面说到的读不一样，更新之前的读叫做“当前读”：总是当前版本的数据，多版本中最新一次提交的那版。 假设事务A执行 update 操作， update 的时候要对所修改的行加行锁，这个行锁会在提交之后才释放。而在事务A提交之前，事务B也想 update 这行数据，于是申请行锁，但是由于已经被事务A占有，事务B是申请不到的，此时，事务B就会一直处于等待状态，直到事务A提交，事务B才能继续执行，如果事务A的时间太长，那么事务B很有可能出现超时异常。 加锁的过程要分有索引和无索引两种情况，比如下面这条语句 1update user set age&#x3D;11 where id &#x3D; 1 id 是这张表的主键，是有索引的情况，那么 MySQL 直接就在索引数中找到了这行数据，然后干净利落的加上行锁就可以了 而下面这条语句 1update user set age&#x3D;11 where age&#x3D;10 表中并没有为 age 字段设置索引，所以， MySQL 无法直接定位到这行数据。那怎么办呢，当然也不是加表锁了。MySQL 会为这张表中所有行加行锁，没错，是所有行。但是呢，在加上行锁后，MySQL 会进行一遍过滤，发现不满足的行就释放锁，最终只留下符合条件的行。虽然最终只为符合条件的行加了锁。 参考链接 https://time.geekbang.org/column/article/68963 https://blog.csdn.net/youanyyou/article/details/108722263 https://blog.csdn.net/kongliand/article/details/107953656 《MySQL技术内幕》","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门4-锁","slug":"MySql/MySql入门4-锁","date":"2021-07-25T03:20:33.000Z","updated":"2022-06-25T07:00:00.111Z","comments":true,"path":"2021/07/25/MySql/MySql入门4-锁/","link":"","permalink":"http://xboom.github.io/2021/07/25/MySql/MySql%E5%85%A5%E9%97%A84-%E9%94%81/","excerpt":"","text":"lock与latch 数据库中lock与latch都称为锁，但两者有截然不同的意思 latch 称为闩锁(轻量级)，要求锁的时间必须非常短。在Innodb引擎中，有分mutex(互斥锁)和rwlock(读写锁)。用来保证并发线程操作临界资源的正确性，没有死锁检测机制 lock的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit或rollback后进行释放(补孕酮事务隔离级别释放的时间可能不一样) 全局锁 如果数据库需要进行全局逻辑备份 在支持事务的存储引擎中，在可重复读隔离级别下开启一个事务，数据库备份的同时可以更新(mysqldump工具) 在不支持事务的存储引擎中，需要通过FTWRL方法进行全局加锁备份 要全库只读，为什么不使用set global readonly=true? 有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。 且在slave上，用户有超级权限的话readonly是失效的 在异常处理机制上有差异。 如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。 如果将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 全局锁是对整个数据库实例加锁，整个数据库处于只读状态，下面语句将会被阻塞： 数据更新语句(数据的增删改) 数据定义语句(建表、修改表结构等) 更新类事务的提交语句 加锁 Flush tables with read lock(FTWRL) 解锁 unlock tables 整个数据库只读可能存在的问题： 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟 表级锁 MySql里面表级别的锁有两种： 表锁 元数据锁（meta data lock，MDL) 表锁的语法是 lock tables … read/write，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放 需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象 MDL(metadata lock)，MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。 DDL操作对表加MDL读锁，保证的是表结构不能修改，而与表数据无关，是可以CRUD的 DML操作对表加MDL写锁，保证的是表结构不能被并行修改，同时表数据也不能读了 如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的 虽然MDL是默认添加的，但还是会碰到一个问题：给一个小表加个字段，导致整个库挂了 因为sessionB加了MDL读锁，导致后面的sessionC阻塞。如果sessionB一直没有完成select，那么sessionC申请写锁被阻塞，将会导致后面的sessionD等申请读锁都被阻塞。这个时候客户端如果有频繁重试的逻辑就会导致不停的和数据库建立连接，把连接池打满导致库不可用 事务中的 MDL 锁，在语句执行开始时申请，但在语句结束后并不会马上释放，而会等到整个事务提交后释放 如何安全的给数据库加字段？ 首先需要解决长事务问题：事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务 如果要变更的表是一个热点表，虽然数据量不大但请求很频繁，而你不得不加个字段，你该怎么做呢？ 比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程 行锁 行锁是由各个引擎自己实现的，而有的引擎并不支持行锁如MyISAM InnoDB实现了如下两种标准的行级锁： 共享锁(S Lock)，允许事务读一行数据 排他锁(X Lock)，允许事务删除或更新一行数据 InnoDB支持多粒度(granular)锁，允许事务在行级上的锁和表级上的锁同时存在，称为意向锁(Intention Lock)，表级别的锁 意向共享锁(IS Lock)，事务想要获得一张表中某几行的共享锁 意向排他锁(IX Lock)，事务想要获得一张表中某几行的排他锁 由于InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫描以外的任何请求 IS IX S X IS 兼容 兼容 兼容 不兼容 IX 兼容 兼容 不兼容 不兼容 S 兼容 不兼容 兼容 不兼容 X 不兼容 不兼容 不兼容 不兼容 若将上锁的对象看成一颗树，那么对最下层的对象上锁，首先要对上层对象上锁 当事务A commit结束之后，事务B才执行 InnoDB存储引擎有3种行锁的算法，分别是： Record Lock: 单个行记录上的锁 Gap Lock: 间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock: Gap Lock + Record Lock，锁定一个范围，并且锁定记录本身 在REPEATABLE READ下，存储引擎使用Next-Key Locking机制来避免幻读(一个事务中，两次读到的数据数量不一致) 其他锁 一致性非锁写读 在默认配置即事务隔离级别为REPEATABLE READ模式下，InnoDB存储引擎的SELECT操作使用一致性非锁定读 一致性非锁定读(consistent nonlocking read)是指InnoDB存储引擎通过行多版本控制(multi versioning)的方式来读取当前执行时间数据库中的行的数据。如果读取的行正在执行DELETE或UPDATE操作，读取操作不会等待行上锁的释放而是去读取行的一个快照 快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。undo用来在事务中进行回滚的，因此快照数据本身没有额外的开销 一个行记录可能不止一个快照数据，由此的并发控制，称为多版本并发控制(Multi Version Concurrency Control, MVCC) 在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据 在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本 一致性锁定读 某些情况下需要显示对数据库读取操作加锁已保证数据逻辑一致性。对于SELECT的只读操作，InnoDB存储引擎对于SELECT语句支持两种一致性的锁定读(locking read) SELECT … FOR UPDATE : 对读取的行记录加一个X锁，其他事务不能加上任何锁 SELECT … LOCK IN SHARE MODE： 对读取的行记录加一个S锁，其他事务锁定的行加S锁，但是如果加X锁，会被阻塞 自增长与锁 在InnoDB存储引擎的内存结构中，对每个含有自增长值的表都有一个自增长在计数器(auto-increment counter)，对含有自增长的计数器的表进行插入操作时，这个计数器会被初始化，执行如下的语句来得到计数器的值： select MAX(auto_inc_col) FROM t FOR UPDATE; 插入操作会依据这个计数器值加1服务自增长列，称为AUTO-INC Locking。它并不是在一个事务完成后才释放，而是在完成对自增长值插入的SQL语句后立即释放 自增长值的列必须是缩影，同时必须是索引的第一列，不是则抛出异常 死锁和死锁检查 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略： 一种策略是，直接进入等待直到超时。超时时间可通过参数 innodb_lock_wait_timeout 来设置(默认50s)。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑 不能将第一种策略的超时时间设置的太短，可能导致误伤了不是死锁的情况 每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。死锁检测会消耗大量的CPU。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的 怎么解决由这种热点行更新导致的性能问题呢？ 可以考虑将一行改成逻辑上的多行来减少冲突。如果一个账户的金额等于这10条记录的总和，这样给一个账号添加金额的时候可以随机选择一条来进行，这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗 参考链接 https://time.geekbang.org/column/article/69862","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门2-文件","slug":"MySql/MySql入门2-文件","date":"2021-07-24T02:44:44.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/07/24/MySql/MySql入门2-文件/","link":"","permalink":"http://xboom.github.io/2021/07/24/MySql/MySql%E5%85%A5%E9%97%A82-%E6%96%87%E4%BB%B6/","excerpt":"","text":"构成Mysql和InnoDB存储引擎表的各类文件包括： 参数文件：初始化参数、内存结构大小等设置 日志文件：记录MySql实例的各种类型活动 socket文件：当用UNIX域套接字方式进行连接时所需要的文件 pid文件：MySql实例的进程ID文件(作用是？) 存储引擎文件：每个引擎会有自己的文件保存数据(如记录和索引) 参数文件 当MySql实例启动时，会先读参数文件(如果没有则使用默认值)，用来初始化数据库 1234# 1. 查看数据库参数文件mysql --help |grep my.cnf# 2. 查看数据库参数SHOW VARIABLES 日志文件 MySql常见的日志文件有： 错误日志文件(error log) 二进制日志文件(binlog) 慢查询日志文件(slow query log) 查询日志文件(log) 慢查询日志 开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能 可用于定位可能存在问题的SQL语句，默认情况下并不启动慢查询日志 12345678#查询慢查询阀值(等于阀值的并不会记录)SHOW VARIABLES LIKE 'long_query_time'#查询慢查询开关SHOW VARIABLES LIKE 'log_slow_queries'#慢查询没有使用索引(如果运行SQL而没有使用索引)SHOW VARIABLES LIKE 'log_queries_not_using_indexes'#表示每分钟允许记录到slow log且未使用索引的SQL语句(默认为0，防止语句频繁记录到slow log导致文件不断增大)SHOW VARIABLES LIKE 'log_throttle_queries_not_using_indexes' 12#查看慢查询日志mysqldumpslow nh122-190-slow.log MySQL5.1 开始 可以将慢查询的日志记录放入一张表中，名为 slow_log 123456789101112#指定慢查询输出的格式，默认为FILE#TABLE则将日志写入slow_log表中SHOW VARIABLES LIKE &#39;log_output&#39;#指定逻辑IO次数超过的记录写入slow log(默认100)SHOW VARIABLES LIKE &#39;long_query_io&#39;SHOW VARIABLES LIKE &#39;slow_query_type&#39; #表示slow log的启动方式#0 表示不将sql语句记录到slow log#1 表示根据运行时间将sql语句记录到slow log#2 表示根据逻辑IO次数将sql语句记录到slow log#3 表示根据运行时间及逻辑IO次数将sql语句记录到slow log 逻辑IO：包含所有的读取，不管是磁盘还是缓冲池 物理IO：指从磁盘进行IO读取的次数 查看日志文件：记录了对MySql数据库所有请求的信息，无论这些请求是否得到正确的执行。默认文件名为：主机名.log。和慢查询日志一样，可以将查询日志放入mysql架构下的general_log表中 二进制日志 记录了对MySql数据库执行更改的所有操作，但不包括select和show这类操作，因为这类对数据本身并没有修改 1SHOW BINLOG EVENTS #查看二进制日志 主要有以下几点作用： 恢复(recovery)：某些数据的恢复需要二进制日志。例如在一个数据库全备文件恢复后，用户可通过二进制日志进行point-in-time的恢复 复制(replication)：通过复制和执行二进制日志使一台远程MySql数据库(一般为slave或standby)与一台MySql数据库(master或primary)进行实时同步 审计(audit)：用户可以通过二进制日志中的信息来进行审计，判断是否有对数据库进行注入攻击 12#启动二进制文件my.cnflog-bin[=name] 如果不指定name，则默认二进制日志文件名为主机名，后缀为二进制日志的序列号(bin_log.00001)，所在路径为数据库所在目录(datadir) 其他影响二进制日志记录的参数： max_binlog_size：指定单个二进制日志文件的最大值，超过则新建二进制日志文件，后缀序列号+1 并记录到.index文件中(默认1G) binlog_cache_size： 当使用事务的表，所有未提交的二进制日志会被记录到一个缓存中，等该事务提交再将缓冲写入二进制日志文件。缓存大小由binlog_cache_size决定(默认32K) binlog_cache_size是基于会话的，但一个线程开始一个事务，就会自动分配一个binlog_cache_size 当事务记录大小大于缓冲时，回将缓冲中的日志写入一个临时文件中 缓冲区大小的设置可以通过 SHOW GLOBASL STATUS命令查看 binlog_cache_use：记录使用缓冲区二进制日志的次数 binlog_cache_disk_use：记录使用临时文件写二进制日志的次数 binlog-do-db：表示需要写入哪些库的日志 binlog-ignore-db：表示需要忽略写入哪些库的日志 log-slave-update：如果是slave角色，则不会将从master取得并执行的二进制日志写入自己的二进制日志文件中。如果需要写入，则设置log–slave-update。如果搭建master=&gt;slae=&gt;slave架构复制，必须设置该值 Binlog-format：日志存储的不同格式(STATEMENT/ROW/MIX) 删除binlog 12# 查看过期时间(单位day)show variable like 'expire_logs_days' expire_logs_days = 0: 表示所有binlog日志永久都不会失效，不会自动删除 触发条件： 当 binlog大小超过max_binlog_size的时候 手动flush logs 其他操作可参考链接 其他文件 套接字文件：UNIX系统下本地连接MySql可采用UNIX域套接字方式，需要一个套接字文件， 12#查看套接字文件位置SHOW VARIABLES LIKE 'socket' PID文件：当MySQL实例启动，会将自己的进程ID写入到pid文件 12#查看PID文件位置SHOW VARIABLES LIKE 'pid_file' InnoDB存储引擎文件 表空间文件 InnoDB将存储的数据按表空间进行存放。默认配置下会有一个初始大小为10MB的默认表空间文件，名为ibdata1的文件。 1Innodb_data_file_path = /db/ibdata1:2000M;/dr2/db/ibdata2:2000M:autoextend 将/db/ibdata1和/dr2/db/ibdata2两个文件组成表空间。 若两个文件处于不同的磁盘上，磁盘的负载可能被平均。可以提供数据库的整体性能 如果文件ibdata2用完了，则可以继续自动增长(autoextend) 设置innodb_data_file_path之后，所有基于InnoDB存储引擎的表数据都会记录到该共享表空间 设置innodb_file_per_table,将每个基于InnoDB的表都产生一个独立表空间，命令规则为：表名.ibd 注意：单独表空间文件仅存储该表的数据、索引和插入缓冲BITMAP等信息，其余信息还存放在默认表空间中 重做日志文件 默认情况下，在InnoDB存储引擎的数据目录下会有两个名为ib_logfile0和ib_logfile1的文件，称为重做日志文件(redo log file)。用于记录对于InnoDB存储引擎的事务日志。 每个InnoDB存储引擎至少有1个重做日志文件组(group)，每个文件组至少有2个重做日志文件，如默认的ib_logfile0和ib_logfile1。 为了得到更高的可靠性，可设置多个镜像日志组(mirrored log groups)，将不同的文件组放在不同的磁盘上 采用循环写入的方式运行：在三个重做日志文件的重做日志文件组中，InnoDB存储引擎先写重写日志文件ib_logfile0，当到达文件最后时，会切换直重做日志文件1，当文件1也满了切到文件2，最后回到文件0，重复循环 innodb_log_file_size：指定每个日志文件大小(&lt;512GB) innodb_log_files_in_group：日志文件组中重做日志文件的数量，默认为2 innodb_mirrored_log_groups：指定日志镜像文件组的数量，默认为1(若磁盘高可用，如磁盘阵列，可不开) Innodb_log_group_home_dir：指定日志文件组所在的目录 重做日志文件不能设置的太大，如果设置太大，恢复时需要太长时间，如果设置太小，可能导致一个事务的日志需要多次切换重做日志文件，会导致频繁的发生 async checkpoint 重做日志文件与二进制日志区别： 二进制日志记录所有与数据库相关的日志，而InnoDB存储的重做日志只记录该InnoDB本身的事务日志 二进制日志记录的都是关于一个事务的具体操作内容，即该日志为逻辑日志。而重做日志记录的是关于每个页(Page)的更改的物理情况 写入时间不同，二进制日志仅在事务提交前进行提交，即只写磁盘一次，不论事务多大。而在事务进行的过程中，会不断有重做日志条目(redo entry)被写入到重做日志文件中 重做日志由四部分组成： redo_log_type占用1字节，表示重做日志的类型 space表示空间的ID，但采用压缩的方式，占用空间可能小于4字节 page_no表示页的偏移量，铜梁采用压缩的方式 redo_log_body表示每个重做日志的数据部分，恢复时需要调用相应的函数进行解析 重做日志文件的操作不是直接写，而是先写入一个重做日志缓冲(redo log buffer)，再按照一定的条件顺序地写入日志文件 重做日志缓冲进行磁盘写入时，是按512字节也就是一个扇区的大小进行写入。因为扇区是写入的最小单位，因此可以保证写入必定是成功的 写入条件： 主线程中每秒会将重做日志缓冲写入磁盘的重做日志文件中，不论事务是否已经提交 另外一个由参数 innodb_flush_log_at_trx_commit控制，表示提交(commit)操作时，处理重做日志方式 0 代表等待主线程每秒的刷新 1 表示执行commit时将重做日志缓冲同步写到磁盘，伴有fsync的调用 2 表示重做日志异步写到磁盘，即写到文件系统的缓存中，不能保证在执行commit时肯定会写入重做日志文件 为了保证事务的持久性，必须将innodb_flush_log_at_trx_commit设置为1，即使宕机也能通过重做日志文件恢复 0和2都有可能恢复时部分事务丢失，不同在于，为2时如果数据库宕机而操作系统没有宕机，此时日志保存在文件系统缓存中，恢复时同样能保证数据不丢失 写入过程 只要redo log和binlog保证持久化到磁盘，就能确保MySql异常重启后，数据可以恢复 binlog的写入机制 事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写到binlog文件中 系统为每个线程分配了一片binlog cache内存，但共用一份binlog文件 参数binlog_cache_size控制单个线程内binlog cache大小，一个事务的binlog是不能拆分的(不论事务多大，都要确保一次性写入)，如果超过这个大小就要暂存到磁盘 事务提交的时候，执行器把binlog cache 里完整的事务写入binlog中，并清空binlog cache 12ssize_t write(int fd, const void *buf, size_t count);int fsync(int fd); write 是把日志写入到文件系统的page cache，内存中，没有持久化到磁盘，所以速度比较快，函数返回并不代表已经写入磁盘 fsync 是将数据持久化到磁盘，占用磁盘的IOPS，通知内核将数据写到硬盘中 write 和 fsync 的时机，是由参数 sync_binlog 控制的 sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync(如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志) redo log的写入机制 redo log存在三种状态： 存在redo log buffer中，物理上是在MySql进程内存中 写到磁盘(write)，但是没有持久化(fsync)，物理上是在文件系统的page cache 持久化到磁盘，对应的是hard disk 为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数 0 表示每次事务提交时都只是把 redo log 留在 redo log buffer 中，等待主线程每秒刷新 1 表示每次事务提交时都将 redo log 直接持久化到磁盘(fsync)； 2 表示每次事务提交时都只是把 redo log 写到 page cache(文件系统缓存) 另外InnoDB有一个后台线程，每隔1s把redo log buffer中的日志，调用write写到文件系统的page cache,然后调用fsync持久化到磁盘 注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也可能已经持久化到磁盘的 实际上，除了后台线程没每秒一次的轮询操作，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中 redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache 并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘 一条update语句的执行过程： 执行器先找引擎取 ID=2 这一行 如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器 否则，需要先从磁盘读入内存，然后再返回 执行器拿到引擎给的行数据，更新行得到新的一行数据，再调用引擎接口写入这行新数据 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务 执行器生成这个操作的 binlog，并把 binlog 写入磁盘 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成 问题 为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的 一个事务的 binlog 必须连续写，整个事务完成后，再一起写到文件里。而 redo log 记录的是，中间有生成的日志可以写到 redo log buffer 中。其他事务提交的时候可以被一起写到磁盘中 为什么执行器更新数据之后不直接提交事务接口，而是和引擎有一个交互(这里是redo log)，其他引擎并没有redo log 更新的时候数据量太大，内存放不下怎么办 LRU 参考链接 《MySql技术内幕》 https://time.geekbang.org/column/article/76161","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门1-架构","slug":"MySql/MySql入门1-架构","date":"2021-07-21T23:20:02.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/07/22/MySql/MySql入门1-架构/","link":"","permalink":"http://xboom.github.io/2021/07/22/MySql/MySql%E5%85%A5%E9%97%A81-%E6%9E%B6%E6%9E%84/","excerpt":"","text":"MySQL架构 连接器 连接器：负责跟客户端建立连接、获取权限、维持和管理连接 连接命令：mysql -h$ip -P$port -u$user -p 连接默认端口是3306，可通过修改my.cnf配置文件指定端口 如果客户端太长时间没动静，连接器会自动断开。由参数wait_timeout控制的，默认值是8小时 show variables like ‘wait_timeout’; 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可在每次执行一个比较大的操作后，通过执 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态 可以使用TCP/IP套接字，命名管道和共享内存，UNIX域套接字进行连接 查询缓存 查询缓存：连接建立完成后，就可以执行 select 语句了，执行查询缓存。 之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中，key 是查询的语句，value 是查询的结果 逻辑存储结构 InnoDB存储引擎将所有数据都逻辑的存放在一个空间中，称为表空间，表空间又由以下结构组成 段(segment) 区(extent) 页(page) 行(row) 表空间(Tablespace)：默认情况InnoDB有一个共享表空间ibdata1, 即所有数据都存放在和这个表空间内。 如果用户开启 innodb_file_or_table，则每张表的数据(数据、索引、插入缓冲BitMap)可以单独放到一个表空间中，而回滚(undo)信息，插入缓冲索引页、系统事务信息、二次写缓冲(Double write buffer)还是存放在原来的共享表空间 InnoDB数据页由以下7个部分组成 File Header(文件头) Page Heade(页头) Infimun 和 Supermum Records User Records(用户记录) Free Space(空闲空间) Page Directory(页目录) File Trailer(文件结尾信息) 其中 File Header 38 字节, Page Header 56 字节, File Trailer 8 字节，用来标记页信息 而 User Records, Free Sapce, Page Directory 为实际的行存储空间，所以大小是动态的 Infimun 记录的是比任何主键都要小的值， Supermum Records 指可能比任何可能的值还要大的值，这两个值在页创建的时候建立，且永远不会被删除 关键特征 后台线程 后台存在多个线程处理不同的任务 master thread: 负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性 IO Thread: 大量使用AIO(Async IO)来处理写IO请求。而IO Thread的主要工作是负责这些IO请求的回调处理 Purge Thread: 事务提交后，其锁使用的 undolog 可能不再需要，一次通过PurgeThread来回收已经使用并分配的undo页 Page Cleaner Thread: 脏页的刷新操作 内存 缓冲池 InnoDB存储引擎是基于磁盘存储的。使用缓冲池技术降低CPU与磁盘之间的速率鸿沟 读取页的操作，首先将从磁盘读到的页存放在缓冲池中国暖，这个过程称为将页&quot;FIX&quot;在缓冲池中，下一次再读相同的页时，首先判断该页是否存在缓冲池中 修改页的操作，首先修改在缓冲池中的页，然后再已一定的频率刷新到磁盘上，页从缓冲池刷新回磁盘的操作并不是在每次页发生更新时触发，而是通过一种称为 CheckPoint的机制刷新回磁盘。 32位操作系统最大支持64G内存，而64位操作系统支持512G内存，通过 innodb_buffer_pool_size设置 LRU 缓冲池通过LRU(Latest Recent Used，最近最少使用)算法来进行管理的。即最频繁使用的页在LRU列表的前端，而最少使用的页在LRU列表的尾端 InnoDB对LRU进行了一些优化，还加入了midpoint位置，默认在LRU列表长度的5/8处。新读到的页并不是直接放到LRU列表的首部，而是放入到LRU列表的midpoint位置 为什么不直接放到LRU的首部？ 某些操作可能会使缓存的页被刷出，从而影响缓冲池的效率。如索引或数据的扫描操作 使用innodb_old_blocks_time 表示页读取到mid位置后需要等待多久才能加入到LRU列表的热端 Free LRU 用来管理已经读取的页，但当数据库刚启动时，LRU列表是空的，这时页都放在Free列表中 当需要从缓冲池中分页时，首先从Free列表中查找是否有可用的空闲页，若有则从Free列表中删除，放入到LRU列表中。否则淘汰LRU列表末尾的页，将该内存空间分配给新的页。 当页从LRU列表的old部分加入到new部分时，称此时发生的操作为page made young 当页因innodb_old_blocks_time而没有从old部分移动到new部分的操作称为page not made young FLUSH 在LRU列表中的页被修改后，称该页为脏页(dirty page),即缓冲池中的页和磁盘上的页数据不一致。数据库会通过CHECKPOINT机制将脏页刷新回磁盘，而Flush列表中的页即为脏页列表。 注意：脏页既存在于LRU列表中，也存在于Flush列表中。 LRU列表用来管理缓冲池中页的可用性 Flush列表用来管理将页刷新回磁盘，二者互不影响 重做日志缓冲 内存区域除了缓冲池，还有重做日志缓冲(redo log buffer)。InnoDB存储引擎首先将重做日志先放入到这个缓冲区，然后按一定频率将其刷新到重做日志文件 重做日志缓冲区不需要设置太大，因为一般没买哦就会将重做日志缓冲刷新到日志文件 实际上，8M的重做日志缓冲池足矣，因为下列三种情况会将重做日志缓冲内容刷新到外部磁盘的重做日志文件中 Master Thread 每一秒将重做日志缓冲刷新到重做日志文件 每个事务提交时会将重做日志缓冲刷新到重做日志文件 当重做日志缓冲池剩余空间小于1/2时，将重做日志缓冲刷新到重做日志文件 额外内存池 在InnoDB存储引擎中，对内存的管理是通过一种称为内存堆(heap)的方式进行的，在对一些数据结构本身的内存进行分配时，需要从额外的内存池中进行申请，当该区域的内存不够时，会从缓冲池中神行 例如：分配了缓冲池(innodb_buffer_pool)，但每个缓冲池中的帧缓冲(frame buffer)还有对应的缓冲控制对象(buffer control block)需要从额外内存池中申请 CheckPoint技术 当出现脏页，即缓冲池中页的版本比磁盘的新，需要将新版本的页从缓冲池刷新到磁盘。若每次一个页发生变化，就将新页的版本刷新到磁盘，那么这个开销将是非常大。同时如果刷新到磁盘的时候发生宕机，则数据不可恢复 所以事务数据库系统普遍采用Write Ahead Log策略，即当事务提交时，先写重做日志，再修改页。 如果重做日志可以无限地增大，同时缓冲池页足够大，能够缓冲数据库的数据，就不需要将缓冲池中的页的新版本刷新回磁盘。当发生宕机直接使用重做日志恢复整个系统到宕机发生的时刻，这就需要： 缓冲池可以缓存数据库中所有的数据 重做日志可以无限增大 因此提出CheckPoint技术解决： 缩短数据库的恢复时间：当数据库发生宕机，不需要重做所有日志，因为CheckPoint之前的页都已经刷新回磁盘，只需要恢复CheckPoint之后的重做日志 缓冲池不够用时，将脏页刷新到磁盘：根据LRU算法会溢出最近最少使用的页，若此页为脏页，也需要执行CheckPoint刷回磁盘 重做日志不可用时，刷新脏页：因为重做日志是循环使用，不可用是指部分重做日志已经不再需要了 InnoDB存储引擎通过LSN(Log Sequence Number)来标记版本(8字节)，每个页有LSN，重做日志也有LSN，CheckPoint也有LSN 在Innodb引擎内部，有两种CheckPoint: Sharp Checkpoint：发生在数据库关闭时将所有的脏页都刷新回磁盘(默认) Fuzzy Checkpoint: 若在数据库运行时使用 Sharp CheckPoint 将影响数据库使用，所以运行时使用Fuzzy Master Thread CheckPoint: Master Thread 每秒从缓冲池的脏页列表刷新一定比例回磁盘(异步) FLUSH_LRU_LIST checkPoint: 为了保证LRU列表中有大约100个空闲页，检查(阻塞)LRU中是否有足够的空闲列表，没有则从LRU列表尾端的页移除，如果这些页中有脏页，则进行CheckPoint。这个检查后来单独的Page Ceaner线程中进行 Async/Sync Flush CheckPoint: 重做日志不可用，则从脏页列表中选取。将已经写入到重做日志的LSN记为redo_lsn，将已经刷新回磁盘最新页的LSN记为checkpoint_lsn Dirty Page too much CheckPoint：当脏页太多强制执行CheckPoint 插入缓冲 虽然缓冲池中有Inert Buffer信息，但Insert Buffer和数据页一样，也是物理页的一个组成部分 如果对于非聚集索引叶子节点的插入不是顺序的(例如创建时间递增插入，则也是顺序的)，这时就需要离散地访问非聚集索引导致插入性能下降 对于非聚集索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中， 若存在，则直接插入 若不在，则先放入到一个Insert Buffer对象中，然后通过一定频率和情况进行Insert Buffer和辅助索引页子节点的merge操作 Insert Buffer 使用需要同时满足以下两个条件： 索引是辅助索引(secondary index) 索引不是唯一(unique)的 若MySql数据库发生了宕机，势必有大量Insert Buffer并没有合并到实际的非聚集索引中 辅助索引不能是唯一的，因为在插入缓冲时，数据库并不去查找索引页来判断插入记录的唯一性，如果去查找又会有离散读取的情况发生，从而导致Insert Buffer失去意义 Innsert Buffer 的数据结构是一颗B+树，全局有一颗 Insert Buffer B+ 树，负责对所有的表的辅助索引进行Insert Buffer。这课B+树存放在共享表空间中，默认也就是ibdata1中 由此也有叶节点和非叶节点组成，非叶节点存放的是查询的search key(键值) search key 一共占用 9个字节， space(4字节)表示待插入记录所在的表的表空间id,在Innodb存储引擎中，每个表有一个唯一的space id marker(1字节) 用来兼容老版本的Insert Buffer Offsert 表示页所在的偏移量，占用4字节 当一个辅助索引要插入到页(space, offset)时，如果这个页不在缓冲池中，那么InnoDB存储引擎首先根据上述规则构造一个search key,接下来查询Insert Buffer这颗B+树，然后将这条记录插入到Insert Buffer B+树的叶子节点中 两次写 如果Insert Buffer 带给InnoDB存储引擎是性能上的提升，那么doublewrite带来的就是数据页的可靠性 当数据库宕机，InnoDB可能正在写入某个页到表中，而这个页只写了一部分(比如一个页16KB的页，只写了4KB) 重做日志中记录的是对页的物理操作。如果这个页本身已经发生了损坏，在对其进行重做是没有意义的，也就是说，在应用(apply)重做日志前，用户需要一个页的副本，当写入失效发生时，先通过页的副本还原该页，再进行重做，这就是doublewrite doublewrite 由两部分组成: 一部分是内存中的doublewrite buffer 大小为2MB 另一部分是物理磁盘上共享表空间中连续的128个页，即2个区(extent)，大小同样为2MB 在对缓冲池脏页进行刷新时，并不直接写磁盘，而是通过memcpy函数先将脏页复制到内存中的doublewrite buffer，之后通过doublewrite buffer 再分两次，每次1MB顺序地写入共享表空间的物理磁盘是上，马上调用fsync函数，避免缓冲写带来的问题。因为doublewrite页是连续的，这个过程是循序写的。在完成doublewrite页的写入后，再将doublewrite buffer中的页写入各个表空间文件中，此时的写入则是离散的。 自适应哈希索引 哈希(Hash)是一种非常快的查找方法，时间复杂度为O(1)。而B+树在取决于树的高度(在生产环境中，B+树的高度一般为3~4层) InnoDB存储引擎会监控对表上各索引页的查询。通过缓冲池的B+树页构造而来，称为自适应哈希索引(Adaptive Hash Index, AHI)。自动根据访问的频率和模式来自动地为某些热点页建立哈希索引。 建立AHI的要求如下： 访问模式一样是指查询条件一样。如 where a=xxx 和 where a=xxx and b=xx 交替查询则不会构造AHI 以该模式访问了100次 页通过该模式访问了N次，其中N=页中记录 * 1/16 异步IO 当前的数据库系统都采用异步IO(Asynchronous IO, AIO)的方式来处理磁盘操作 Sync IO是每进行一次IO操作需要等待此次操作结束才能继续接下来的操作。如果用户发出的是一条索引扫描的查询，那么SQL查询语句需要扫描多个索引页，每扫描一个页并等待其完成后再进行下一次的扫描没必要。 AIO是用户发出了一个IO请求之后立即再发出另一个IO请求，当全部IO请求发送完毕后，等待所有IO操作的完成。最大的优势就是进行IO Merge操作。例如需要访问页(space, page_no)为(8,6)、(8,7)、(8,8) 每个页的大小为16KB，AIO会判断这三个页是连续的,因此AIO底层会发送一个IO请求，从(8,6)开始，读取48KB的页 刷新邻接页 当刷新一个脏页是，InnoDB存储引擎会检查该页所在区(extent)的所有页，如果是脏页，那么一起进行刷新。这样通过AIO可以将多个IO写入从左合并为一个IO操作。但存在这样的问题： 是不是可能将不怎么脏的页进行了写入，而该页之后又会很快变成脏页 固态硬盘有这较高的IOPS，是否还需要这个特性 可以通过 innodb_flush_neighbors 控制开启 存储引擎比较 特征 MyISAM BDB Memory InnoDB Archive NDB Sotrage Limits Y 64TB Y Transactions Y Y Locking granularity 表 页 表 行 行 行 MVCC/Snapshot Read Y Y Y Geospatial support Y B-Tree indexes Y Y Y Y Y Hash indexes Y Y Full text search index Y Data Caches Y Y Y Index Caches Y Y Y Compressed data Y Y Encrypted data Y Y Y Y Y Y Bulk Insert Speed High High High Low Very High High Storage cost Low Low N/A High VeryLlow Low Memory cost Low Low Medium High Low High Cluster database support Y Replication support Y Y Y Y Y Y Foreign Key support Y Backup/Point-in-time recovery Y Y Y Y Y Y Query cache support Y Y Y Y Y Y Update Statistics for Data Dictionary Y Y Y Y Y Y","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"MySql入门3-索引","slug":"MySql/MySql入门3-索引","date":"2021-07-21T23:18:54.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2021/07/22/MySql/MySql入门3-索引/","link":"","permalink":"http://xboom.github.io/2021/07/22/MySql/MySql%E5%85%A5%E9%97%A83-%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引是一种为了提升查询效率的数据结构,B+树索引并不能找到一个给定键值的具体行。只能找到数据行所在的页，然后数据库通过把页读到内存中进行查找。每个数据页都通过一个双向链表来进行链接 数据页存放的是完整的每行的记录，而在非数据页的索引页中，存放的仅仅是键值及指向数据页的偏移量，而不是一个完整的行记录 索引模型 哈希表：O(1)的时间复杂度，适用于等值查询，不支持范围查询 有序数组：lgN的时间复杂度(二分查找)，支持等值查询和范围查询，但插入效率低，适用于静态存储引擎(数据不再变化) 搜索树：InnoDB使用B+树结构建立索引，父节点左子树所有节点的值小于父节点的值，右子树所有节点的值大于父节点的值。 InnoDB的为N叉树，差不多为1200 MySql默认一个节点的长度为16K，整数(bigint)字段索引长度为 8B，每个索引还跟着6B的指向其子树的指针；所以16K/14B ≈ 1170 在 InnoDB 中，每一张表其实就是多个 B+ 树，即一个主键索引树和多个非主键索引树。 执行查询的效率，使用主键索引 &gt; 使用非主键索引 &gt; 不使用索引 为什么主键比非主键快(主键索引和非主键索引)？ 主键索引的b+树的叶子节点存储的是具体的行数据，非叶子节点存储的是主键的值。叶子节点之间通过链表连接，也称为 聚簇索引 非主键索引的叶子节点存储的是主键的值，所以通过非主键索引查询数据时，先找到主键，再去主键索引上根据主键找到具体的行数据，也称 二级索引。这个过程叫做回表 如果不使用索引进行查询，则从主索引 B+ 树的叶子节点进行遍历 例如：存在下列这样一个表 12345mysql&gt; create table T( id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6) 则 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引(clustered index) 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引(secondary index) 索引维护 在B+树中，所有记录节点都是按键值的大小顺序存放在同一层的叶子节点上，由各叶子节点进行连接 B+树的插入操作 Leaf Page 满 Index Page 满 操作 No No 1. 直接将数据插入叶子节点 Yes No 1. 拆分叶子节点2. 将中间的额节点存放入 Index Page中3. 小于中间节点的记录放左边4. 大于或等于中间节点的记录方右边 Yes Yes 1. 拆分叶子节点2. 小于中间接待你的记录方左边3. 大于或等于中间节点的记录放右边4. 拆分Index Page5. 小于中间节点的记录放左边6. 大于中间节点的记录放右边7. 中间节点放入上一层 Index Page 旋转发生在Leaf Page已经满了，但是其左右兄弟节点并没有满的情况下。B+树并不会急于拆分页的操作，而是将记录移动所在页的兄弟节点上。通常情况下，左兄弟会被首先检查用来做旋转操作 B+树的删除操作 叶子节点小于填充因子 中间节点小于填充因子 操作 No No 1. 直接将记录从叶子节点删除，如果该节点还是Index Page的节点，用该节点的右节点代替 Yes No 1. 合并叶子节点和它的兄弟节点，同时更新Index Page Yes Yes 1. 合并叶子节点和它的兄弟节点2. 更新Index Page3. 合并Index Page 和它的兄弟节点 最好使用自增主键做为主索引，如上图所示：参见 B+树的插入与删除 如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。 如果新插入的 ID 值为 400，需要逻辑上挪动后面的数据，空出位置。而更糟的情况是： 如果 R5 所在数据页满了，则申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂，影响性能 原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约 50%。当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程，影响空间利用率 所以在主键选择的时候： 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 在KV场景下只有一个索引，且必须是唯一索引，在直接使用业务字段做主键 索引要根据表中的每一行的记录值来创建，所以需要全表扫描 加字段或修改字段，也要修改每一行记录中的对应列的数据，所以也要全表扫描 查询过程 执行查询语句 select id from T where id = 5 id为普通索引，查询过程为： 先通过 B+ 树从树根开始，按层搜索到叶子节点及数据页 然后在数据页内部通过二分法来定位记录 对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索 InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB 执行查询语句 select * from T where k between 3 and 5 查询过程为： 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束 执行查询语句 select ID from T where k between 3 and 5 在 k 索引树上找到 k=3 的记录，取得 ID = 300； 在 k 索引树取下一个值 k=5，取得 ID=500； 在 k 索引树取下一个值 k=6，不满足条件，循环结束 ID 的值已经在 k 索引树上了，通过普通索引直接查询到结果不需要回表，索引 k 已经“覆盖了”查询需求，称为覆盖索引 B+树的最左前缀原则 1234567891011# 身份证信息CREATE TABLE `tuser` ( `id` int(11) NOT NULL, `id_card` varchar(32) DEFAULT NULL, `name` varchar(32) DEFAULT NULL, `age` int(11) DEFAULT NULL, `ismale` tinyint(1) DEFAULT NULL, PRIMARY KEY (`id`), KEY `id_card` (`id_card`), KEY `name_age` (`name`,`age`)) ENGINE=InnoDB where name like ‘张 %’。这时，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止 Where b &gt; 10，是无法使用 (a,b) 这个联合索引的，不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引 为了考虑空间，字段长(name,age)的只建立一次，短(age)的建立两次 以联合索引（name, age）为例，检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩&quot; InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次 【索引下推】Index Condition Pushdown，简称 ICP。 是Mysql 5.6版本引入的技术优化。旨在 在“仅能利用最左前缀索的场景”下（而不是能利用全部联合索引），对不在最左前缀索引中的其他联合索引字段加以利用——在遍历索引时，就用这些其他字段进行过滤(where条件里的匹配)。过滤会减少遍历索引查出的主键条数，从而减少回表次数，提示整体性能。 ------------------ 如果查询利用到了索引下推ICP技术，在Explain输出的Extra字段中会有“Using index condition”。即代表本次查询会利用到索引，且会利用到索引下推。 ------------------ 索引下推技术的实现——在遍历索引的那一步，由只传入可以利用到的字段值，改成了多传入下推字段值 更新过程 change buffer 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作 change buffer可以看成也是一个数据页，需要被持久化到 系统表空间（ibdata1），以及把这个change buffer页的改动记录在redo log里，事后刷进系统表空间（ibdata1） 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 change buffer使用场景 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了 普通索引和唯一索引的查询性能几乎一样, 但是写性能是普通索引快, 因为可以用到change buffer, 唯一索引会导致内存命中率下降 change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。 设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50% 设置为0 表示关闭change buffer功能 语句更新过程 如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的 第一种情况当记录要更新的目标页在内存中 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束 第二种情况当记录要更新的目标页不在内存中 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。之前我就碰到过一件事儿，有个 DBA 的同学跟我反馈说，他负责的某个业务的库内存命中率突然从 99% 降低到了 75%，整个系统处于阻塞状态，更新语句全部堵住。而探究其原因后，我发现这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引 innodb 普通索引修改成唯一索引产生的 生产事故, 写多读少使用 changebuffer 可以加快执行速度(减少数据页磁盘 io); 但是,如果业务模型是 写后立马会做查询, 则会触发 changebuff 立即 merge 到磁盘, 这样 的场景磁盘 io 次数不会减少,反而会增加 changebuffer 的维护代价 change buffer 的使用场景 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价 change buffer 和 redo log redo log是物理日志，记录的是数据页的修改，但是如果数据页不在内存中怎么办呢？都没有去修改数据页，redo log中记什么？所以这时候change buffer登场了，如果修改的不是唯一索引，而是普通索引，不用再去磁盘随机IO了，直接将这个修改记录在change buffer中。也可以说，数据页在内存，那就修改数据页，写redo log，如果数据页不在内存，修改的也不是唯一索引，而是普通索引，那就写change buffer。并把这个change buffer写入到redo log，防止更新丢失。 现在让我们看看在表中插入一条数据 1mysql&gt; insert into t(id,k) values(id1,k1),(id2,k2); 假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。如图 2 所示是带 change buffer 的更新状态图 数据表空间：就是一个个的表数据文件，对应的磁盘文件就是“表名.ibd”； 系统表空间：用来放系统信息，如数据字典等，对应的磁盘文件是“ibdata1” 更新语句的操作如下： Page 1在内存中，直接更新内存 Page 2 不在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中 做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘（两次操作合在一起写了一次磁盘），而且还是顺序写的。同时 写入 change buffer 和 数据表空间都是后台空间，不影响更新的响应时间 然后执行查询语句，如果读语句发生在更新语句后不久，内存中的数据都还在，那么此时的这两个读操作就与系统表空间（ibdata1）和 redo log（ib_log_fileX）无关了 1select * from t where k in (k1, k2) 读 Page 1 的时候，直接从内存返回。 WAL 之后如果读数据，是不是一定要读盘，是不是一定要从 redo log 里面把数据更新以后才可以返回？其实是不用的。你可以看一下图 3 的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。 要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。可以看到，直到需要读 Page 2 的时候，这个数据页才会被读入内存 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗 redo日志有分几十种类型的。redo做的事情，简单讲就是记录页的变化（WAL将页变化的乱序写转换成了顺序写）。页是分多种的，比如 B+树索引页（主键 / 二级索引）、undo页（数据的多版本MVCC）、以及现在的change buffer页等等，这些页被redo记录后，就可以不着急刷盘了。 change buffer记录索引页的变化；但是change buffer本身也是要持久化的，而它持久化的工作和其他页一样，交给了redo日志来帮忙完成； redo日志记录的是change buffer页的变化。 change buffer持久化文件是 ibdata1，索引页持久化文件是 t.ibd change buffer 一开始是写内存的，那么如果这个时候机器掉电重启，会不会导致 change buffer 丢失呢？ 1.change buffer有一部分在内存有一部分在ibdata. 做purge操作,应该就会把change buffer里相应的数据持久化到ibdata 2.redo log里记录了数据页的修改以及change buffer新写入的信息 如果掉电,持久化的change buffer数据已经purge,不用恢复。主要分析没有持久化的数据 情况又分为以下几种: change buffer写入,redo log虽然做了fsync但未commit,binlog未fsync到磁盘,这部分数据丢失 change buffer写入,redo log写入但没有commit,binlog以及fsync到磁盘,先从binlog恢复redo log,再从redo log恢复change buffer change buffer写入,redo log和binlog都已经fsync.那么直接从redo log里恢复。 merge 的执行流程是这样的：从磁盘读入数据页到内存（老版本的数据页）；从 change buffer 里找出这个数据页的 change buffer 记录 (可能有多个），依次应用，得到新版数据页；写 redo log。这个 redo log 包含了数据的变更和 change buffer 的变更。 索引选择 在 MySQL 中一张表可以支持多个索引。但写 SQL 语句的时候并没有主动指定使用哪个索引，MySql是怎么决定使用哪个索引的 1force Index(a) #强制使用指定的索引 使用下面三条SQL语句查看查询过程 123set long_query_time&#x3D;0;select * from t where a between 10000 and 20000; &#x2F;*Q1*&#x2F;select * from t force index(a) where a between 10000 and 20000;&#x2F;*Q2*&#x2F; 将慢查询的日志阀值设置为0，则接下来所有查询操作都记录到慢查询日志中 Q1 使用默认查询语句 Q2 强制使用索引a进行查询 优化器的逻辑 行数 临时表 是否排序 扫描行数是怎么判断的？ MySql在执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数 一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好 使用 show index 方法，看到一个索引的基数。 如果正确的使用索引 使用强制语句 force index(a) 引导 MySQL 使用我们期望的索引，如 order by b limit 1 改成 order by b,a limit 1 之前优化器选择使用索引 b，是因为它认为使用索引 b 可以避免排序（b 本身是索引，已经是有序的了，如果选择索引 b 的话，不需要再做排序，只需要遍历），所以即使扫描行数多，也判定为代价更小。现在 order by b,a 这种写法，要求按照 b,a 排序，就意味着使用这两个索引都需要排序。因此，扫描行数成了影响决策的主要条件，于是此时优化器选了只需要扫描 1000 行的索引 a 参考链接 《高性能myql》 《InnoDB技术内幕-索引与算法》 https://time.geekbang.org/column/article/70848","categories":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"}],"tags":[{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"}]},{"title":"Go入门5-Map","slug":"Go/Go入门5-Map","date":"2021-07-11T04:39:49.000Z","updated":"2022-06-25T07:00:00.114Z","comments":true,"path":"2021/07/11/Go/Go入门5-Map/","link":"","permalink":"http://xboom.github.io/2021/07/11/Go/Go%E5%85%A5%E9%97%A85-Map/","excerpt":"","text":"什么是Map Map是一种通过key来获取value的一个数据结构，其底层存储方式为数组。 一种特殊的数据结构：一种元素对（pair）的无序集合，pair 的一个元素是 key，对应的另一个元素是 value 一种引用类型，未初始化的 map 的值是 nil，可以通过如下方式声明： 12var map1 map[keytype]valuetypevar map1 map[string]int key 可以是任意可以用 == 或者 != 操作符比较的类型，比如 string、int、float。所以数组、切片和结构体不能作为 key，但是指针和接口类型可以。如果要用结构体作为 key 可以提供 Key() 和 Hash() 方法，这样可以通过结构体的域计算出唯一的数字或者字符串的 key。 value 可以是任意类型的；通过使用空接口类型,可以存储任意值，但是使用这种类型作为值时需要先做一次类型断言 Hash表 使用Hash需要解决哈希碰撞的问题，常见方法的就是开放寻址法和拉链法。 需要注意的是，这里提到的哈希碰撞不是多个键对应的哈希完全相等，可能是多个哈希的部分相等，例如：两个键对应哈希的前四个字节相同 开放寻址法 开放寻址法核心思想是依次探测和比较数组中的元素以判断目标键值对是否存在于哈希表中，使用开放寻址法来实现哈希表，那么实现哈希表底层的数据结构就是数组，不过因为数组的长度有限，想哈希表写入 (author, draven) 这个键值对时会从如下的索引开始遍历 1index := hash(\"author\") % array.len 向当前哈希表写入新的数据时，如果发生了冲突，就会将键值对写入到下一个索引不为空的位置 当 Key3 与已经存入哈希表中的两个键值对 Key1 和 Key2 发生冲突时，Key3 会被写入 Key2 后面的空闲位置。当我们再去读取 Key3 对应的值时就会先获取键的哈希并取模，这会先帮助我们找到 Key1，找到 Key1 后发现它与 Key 3 不相等，所以会继续查找后面的元素，直到内存为空或者找到目标元素 当需要查找某个键对应的值时，会从索引的位置开始线性探测数组，找到目标键值对或者空内存就意味着这一次查询操作的结束。 开放寻址法中对性能影响最大的是装载因子，它是数组中元素的数量与数组大小的比值。随着装载因子的增加，线性探测的平均用时就会逐渐增加，这会影响哈希表的读写性能。当装载率超过 70% 之后，哈希表的性能就会急剧下降，而一旦装载率达到 100%，整个哈希表就会完全失效，这时查找和插入任意元素的时间复杂度都是 𝑂(𝑛)O(n) 的，这时需要遍历数组中的全部元素，所以在实现哈希表时一定要关注装载因子的变化 拉链法 与开放地址法相比，平均查找的长度也比较短，各个用于存储节点的内存都是动态申请的，可以节省比较多的存储空间。 实现拉链法一般会使用数组加上链表，一些编程语言会在拉链法的哈希中引入红黑树以优化性能，拉链法会使用链表数组作为哈希底层的数据结构，可以将它看成可以扩展的二维数组 如上图所示，当我们需要将一个键值对 (Key6, Value6) 写入哈希表时，键值对中的键 Key6 都会先经过一个哈希函数，哈希函数返回的哈希会帮助我们选择一个桶，和开放地址法一样，选择桶的方式是直接对哈希返回的结果取模： 1index := hash(\"Key6\") % array.len 选择了 2 号桶后就可以遍历当前桶中的链表了，在遍历链表的过程中会遇到以下两种情况： 找到键相同的键值对 — 更新键对应的值； 没有找到键相同的键值对 — 在链表的末尾追加新的键值对； 如果要在哈希表中获取某个键对应的值，会经历如下的过程 Key11 展示了一个键在哈希表中不存在的例子，当哈希表发现它命中 4 号桶时，它会依次遍历桶中的链表，然而遍历到链表的末尾也没有找到期望的键，所以哈希表中没有该键对应的值。 在一个性能比较好的哈希表中，每一个桶中都应该有 0~1 个元素，有时会有 2~3 个，很少会超过这个数量。计算哈希、定位桶和遍历链表三个过程是哈希表读写操作的主要开销，使用拉链法实现的哈希也有装载因子这一概念： 装载因子:=元素数量÷桶数量装载因子:=元素数量÷桶数量 与开放地址法一样，拉链法的装载因子越大，哈希的读写性能就越差。在一般情况下使用拉链法的哈希表装载因子都不会超过 1，当哈希表的装载因子较大时会触发哈希的扩容，创建更多的桶来存储哈希中的元素，保证性能不会出现严重的下降。如果有 1000 个桶的哈希表存储了 10000 个键值对，它的性能是保存 1000 个键值对的 1/10，但是仍然比在链表中直接读写好 1000 倍 Map结构 1234567891011121314151617181920// A header for a Go map.type hmap struct &#123; count int //表示当前hash表元素个数 flags uint8 //记录当前hash表状态，map是非线程安全的 B uint8 //表示当前哈希表持有的 buckets 数量，桶的数量都是2的倍数，该字段存储对数，也就是len(buckets) == 2^B noverflow uint16 // overflow 的 bucket 近似数 hash0 uint32 // 哈希种子，能为哈希函数的结果引入随机性， 值在创建哈希表时确定，并在调用哈希函数时作为参数传入 buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // 哈希在扩容时用于保存之前 buckets 的字段，它的大小是当前 buckets 的一半 nevacuate uintptr // 指示扩容进度，小于此地址的 buckets 迁移完成 extra *mapextra // optional fields&#125;type mapextra struct &#123; overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap&#125; 这里B是map的bucket数组长度的对数，每个bucket里面存储了kv对。buckets是一个指针，指向实际存储的bucket数组的首地址。 bucket的结构体如下: 123456789// A bucket for a Go map.type bmap struct &#123; //top hash通常包含该bucket中每个键的hash值的高八位 //如果tophash[0]小于mintophash，则tophash[0]为桶疏散状态 //bucketCnt 的初始值是8 tophash [bucketCnt]uint8 //注意：将所有键打包在一起，然后将所有值打包在一起，使得代码比交替key/elem/key/elem/...更复杂。 //但它允许我们消除可能需要的填充，例如map[int64]int8./后面跟一个溢出指针&#125;&#125; 上面这个数据结构并不是 golang runtime 时的结构，在编译时候编译器会给它动态创建一个新的结构，如下： 1234567type bmap struct &#123; topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr&#125; bmap 就是我们常说的“bucket”结构，每个 bucket 里面最多存储 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置） 当map的key和value 每个 bmap 都能存储 8 个键值对，当哈希表中存储的数据过多，单个桶无法装满时就会使用 extra.nextOverflow 中桶存储溢出数据 当作为函数传参时候，传递的实例其实是指针 map是一个hash表，数据被存放在一组buckets(滚筒)中，每个buckets包含8个键值对，低位的buckets用来给数据选择存储的buckets，每个存储buckets包含每个哈希的一些高阶位，以区分单个存储buckets中的条目 如果一个buckets中超过8个，那么将使用其他buckets存储 当哈希表增长时，分配一组新buckets是原来的两倍大，并且会将旧数据复制到新buckets中存储 映射迭代器遍历存储桶数组，并按行走顺序返回键（存储桶编号，然后是溢出链顺序，然后是存储桶索引）。 为了维持迭代语义，我们绝不会在键的存储桶中移动键（如果这样做，键可能会返回0或2次）。 在增加表时，迭代器将继续在旧表中进行迭代，并且必须检查新表是否将要迭代的存储桶（“撤离”）到新表中 常用常量 1234567891011121314151617181920212223242526272829303132333435363738394041424344const ( // 一个桶能装的最大键值对 1&lt;&lt;3 bucketCntBits = 3 bucketCnt = 1 &lt;&lt; bucketCntBits //负载因子计算 uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen) loadFactorNum = 13 loadFactorDen = 2 /* if t.key.size &gt; maxKeySize &amp;&amp; (!t.indirectkey() || t.keysize != uint8(sys.PtrSize)) || t.key.size &lt;= maxKeySize &amp;&amp; (t.indirectkey() || t.keysize != uint8(t.key.size)) &#123; throw(\"key size wrong\") &#125; if t.elem.size &gt; maxElemSize &amp;&amp; (!t.indirectelem() || t.elemsize != uint8(sys.PtrSize)) || t.elem.size &lt;= maxElemSize &amp;&amp; (t.indirectelem() || t.elemsize != uint8(t.elem.size)) &#123; throw(\"elem size wrong\") &#125; */ maxKeySize = 128 maxElemSize = 128 //bmap 偏移 int64 topHash dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64 &#125;&#123;&#125;.v) emptyRest = 0 // 这bmap中这一格为空，在更高的索引中没有更多的非空细胞 emptyOne = 1 // 这bmap中这一格为空 evacuatedX = 2 // 元素可得，但已经迁移到新桶的前半部分 evacuatedY = 3 // 元素可得，但已经迁移到新桶的后半部分 evacuatedEmpty = 4 // 格子为空，bucket已经迁移. minTopHash = 5 // ophash 的最小正常值 // flags iterator = 1 // 可能有迭代器在使用buckets oldIterator = 2 // 可能有迭代器在使用oldbuckets hashWriting = 4 // 有协程正在写 sameSizeGrow = 8 // 等量扩容 // sentinel bucket ID for iterator checks noCheck = 1&lt;&lt;(8*sys.PtrSize) - 1) 常用函数 1234//查找下一个overflow 的bmapfunc (b *bmap) overflow(t *maptype) *bmap &#123; return *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))&#125; 创建Map 通过调用make来创建map，底层为 makemap 函数 1234567891011121314151617181920212223242526272829303132333435func makemap(t *maptype, hint int, h *hmap) *hmap &#123; mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem &gt; maxAlloc &#123; hint = 0 &#125; // 初始化hmap if h == nil &#123; h = new(hmap) &#125; h.hash0 = fastrand() // 找到一个 B使 hint &gt; B &amp;&amp; hint &gt; (2^B) * 6.5 为false 否则B就增大 // 当hint为1 则 B == 0 表示存1个数，只需要用一个桶 // 当hint为6 则 B == 0 表示存6个数，也只需要一个桶 // 当hint为7 则 B == 1 表示存7个树，就需要用两个桶 B := uint8(0) for overLoadFactor(hint, B) &#123; B++ &#125; h.B = B // allocate initial hash table // if B == 0, the buckets field is allocated lazily later (in mapassign) // If hint is large zeroing this memory could take a while. if h.B != 0 &#123; var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil &#123; h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow &#125; &#125; return h&#125; 注意： 返回的是*hmap而 func makeslice(et *_type, len, cap int) slice返回的是结构体 虽然Go 语言中的函数传参都是值传递，在函数内部，map参数会影响外部 而slice部分会，因为slice内部的数组是指针类型，对数组的修改会影响外部，但是长度和容量不会 判断装载因子，13 * 1&lt;&lt;B / 2 即 桶数目的6.5倍 1234//判断装载因子 func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125; Key定位 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机忽略），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。如果 B = 5，则桶数量即 buckets 数组的长度是 2^5 = 32 Key的Hash值计算为： 110010111 | 000011110110110010001111001010100010010110010101010 │ 00110 低5 位 00110，6号桶 高 8 位10010111，十进制为151 在6好bucket中找到tophash值(HOBhash)为151的key，对应2号槽位， 这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入 如果冲突了怎么办 是怎么存入的，什么顺序 查找怎么办 如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket 迁移过程中怎么查找 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; //如果map为空，或者map中数量为0 则返回0值 if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]) &#125; //如果写冲突，则直接报错 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; // 计算哈希值，加入hash0引入随机值 hash := t.hasher(key, uintptr(h.hash0)) // 比如 B=5，那 m 就是31，二进制是全 1 // m = 1&lt;&lt;B - 1 m := bucketMask(h.B) // b 就是 bucket 的地址，hash&amp;m*uintptr(t.bucketsize) 第几个bucket b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) //如果oldbuckets不为空，说明发生了扩容 if c := h.oldbuckets; c != nil &#123; // 如果不是同 size 扩容（看后面扩容的内容） // 对应条件 1 的解决方案 if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. // 新 bucket 数量是老的 2 倍 m &gt;&gt;= 1 &#125; // 求出 key 在老的 map 中的 bucket 位置 oldb := (*bmap)(add(c, (hash&amp;m)*uintptr(t.bucketsize))) // 如果 oldb 没有搬迁到新的 bucket // 那就在老的 bucket 中寻找 if !evacuated(oldb) &#123; b = oldb &#125; &#125; // 计算出高 8 位的 hash // 相当于右移 56 位，只取高8位 top := tophash(hash)bucketloop: for ; b != nil; b = b.overflow(t) &#123; //如果b中没有，则查看b的overflow for i := uintptr(0); i &lt; bucketCnt; i++ &#123; //遍历topHash if b.tophash[i] != top &#123; //如果topHash不等于top if b.tophash[i] == emptyRest &#123; //如果位置为空，后面都没有非空元素 break bucketloop //说明不是顺序填充，相等的topHash值会存到下一个bmap &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) //key 是指针 if t.indirectkey() &#123; //解引用 k = *((*unsafe.Pointer)(k)) &#125; //如果key相等(可能存在topHash相等，key不想等的情况) if t.key.equal(key, k) &#123; //定位value的位置 //b bmap 地址 // dataOffset(数据对齐) // bucketCnt*t.keysize 8个key的大小(固定8个) //i*t.elemsize 第i个元素 e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) // val解引用 if t.indirectelem() &#123; e = *((*unsafe.Pointer)(e)) &#125; return e &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0])&#125; 这里的dataOffset 是 key 相对于 bmap 起始地址的偏移，包含了topHash数组 int64 1234dataOffset = unsafe.Offsetof(struct &#123; b bmap v int64&#125;&#123;&#125;.v) 因此 bucket 里 key 的起始地址就是 unsafe.Pointer(b)+dataOffset minTopHash minTopHash当一个 cell 的 tophash 值小于 minTopHash 时，标志这个 cell 的状态。因为这个状态值是放在 tophash 数组里，为了和正常的哈希值区分开，会给 key 计算出来的哈希值一个增量：minTopHash。这样就能区分正常的 tophash 值和表示状态的哈希值 源码里判断是否搬迁完毕使用函数evacuated，当第一个元素状态为迁移标识符中的三个 1234func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; emptyOne &amp;&amp; h &lt; minTopHash&#125; Map获取 Go 语言中读取 map 有两种语法：带 comma 和 不带 comma，两种分别对应不同的函数 123// src/runtime/hashmap.gofunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointerfunc mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) &#123; //这里不会panic if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return unsafe.Pointer(&amp;zeroVal[0]), false &#125; //并发读写错误 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; hash := t.hasher(key, uintptr(h.hash0)) m := bucketMask(h.B) b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + (hash&amp;m)*uintptr(t.bucketsize))) if c := h.oldbuckets; c != nil &#123; if !h.sameSizeGrow() &#123; // There used to be half as many buckets; mask down one more power of two. m &gt;&gt;= 1 &#125; oldb := (*bmap)(unsafe.Pointer(uintptr(c) + (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) &#123; b = oldb &#125; &#125; top := tophash(hash)bucketloop: for ; b != nil; b = b.overflow(t) &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; if b.tophash[i] == emptyRest &#123; break bucketloop &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; if t.key.equal(key, k) &#123; e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() &#123; e = *((*unsafe.Pointer)(e)) &#125; return e, true &#125; &#125; &#125; return unsafe.Pointer(&amp;zeroVal[0]), false&#125; Map扩容 使用哈希表的目的就是要快速查找到目标 key，然而，随着向 map 中添加的 key 越来越多，key 发生碰撞的概率也越来越大。bucket 中的 8 个 cell 会被逐渐塞满，查找、插入、删除 key 的效率也会越来越低。 Go有一个衡量标准 装载因子 来描述存储情况 1loadFactor := count / (2^B) 触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容： 装载因子超过阈值，源码里定义的阈值是 6.5。 overflow 的 bucket 数量过多： 当 B &lt; 15，即 bucket 总数 2^B &lt; 2^15 时，如果 overflow 的 bucket 数量超过 2^B； 当 B &gt;= 15，即 bucket 总数 2^B &gt;= 2^15，如果 overflow 的 bucket 数量超过 2^15 123456789101112131415161718if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again&#125;// 装载因子超过 6.5func overLoadFactor(count int, B uint8) bool &#123; return count &gt; bucketCnt &amp;&amp; uintptr(count) &gt; loadFactorNum*(bucketShift(B)/loadFactorDen)&#125;// overflow buckets 太多func tooManyOverflowBuckets(noverflow uint16, B uint8) bool &#123; if B &gt; 15 &#123; B = 15 &#125; // The compiler doesn't see here that B &lt; 16; mask B to generate shorter shift code. return noverflow &gt;= uint16(1)&lt;&lt;(B&amp;15)&#125; 第 1 点：每个 bucket 有 8 个空位，在没有溢出，且所有的桶都装满了的情况下，装载因子算出来的结果是 8。因此当装载因子超过 6.5 时，表明很多 bucket 都快要装满了。在这个时候进行扩容是有必要的。 扩容策略：将 B 加 1，bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。而且，新 bucket 只是最大数量变为原来最大数量（2^B）的 2 倍（2^B * 2） B 还没有变，新bucket怎么记录大小呢？ 第 2 点：是对第 1 点的补充。当 map 里元素总数少，但是 bucket 数量多（真实分配的 bucket 数量多，包括大量的 overflow bucket） 扩容策略：开辟一个新 bucket 空间，将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。原来在 overflow bucket 中的 key 可以移动到 bucket 中来。 一个极端的情况：如果插入 map 的 key 哈希都一样，就会落到同一个 bucket 里，超过 8 个就会产生 overflow bucket，结果也会造成 overflow bucket 数过多。移动元素其实解决不了问题，因为这时整个哈希表已经退化成了一个链表，操作效率变成了 O(n)。 Go map 的扩容采取了一种称为“渐进式”地方式，并不会一次性搬迁完毕，每次最多只会搬迁 2 个 bucket。 hashGrow() 函数实际上并没有真正地“搬迁”，它只是分配好了新的 buckets，并将老的 buckets 挂到了 oldbuckets 字段上。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。 准备扩容 hashGrow() 函数表示开始扩容前的处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344func hashGrow(t *maptype, h *hmap) &#123; bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) &#123; //判断是否超过负载因子 bigger = 0 h.flags |= sameSizeGrow //没有说明是第二种情况 &#125; oldbuckets := h.buckets //先将桶指向旧的指针 //h.B+bigger 说明是桶是原来的两倍 newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) //&amp;^按位置为0 flags := h.flags &amp;^ (iterator | oldIterator) if h.flags&amp;iterator != 0 &#123; flags |= oldIterator &#125; // 提交 grow 的动作 h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets //搬迁进度为0 h.nevacuate = 0 //overflow buckets数量为0 h.noverflow = 0 //搬迁extra到老的extra if h.extra != nil &amp;&amp; h.extra.overflow != nil &#123; // Promote current overflow buckets to the old generation. if h.extra.oldoverflow != nil &#123; throw(\"oldoverflow is not nil\") &#125; h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil &#125; if nextOverflow != nil &#123; if h.extra == nil &#123; h.extra = new(mapextra) &#125; h.extra.nextOverflow = nextOverflow &#125; // the actual copying of the hash table data is done incrementally // by growWork() and evacuate().&#125; 扩容工作 123456789func growWork(t *maptype, h *hmap, bucket uintptr) &#123; // 确认搬迁老的 bucket 对应正在使用的 bucket evacuate(t, h, bucket&amp;h.oldbucketmask()) // 再搬迁一个 bucket，以加快搬迁进程 if h.growing() &#123; evacuate(t, h, h.nevacuate) &#125;&#125; h.growing 用来判断是否在搬迁 1234func (h *hmap) growing() bool &#123; return h.oldbuckets != nil&#125;//当oldbuckets非空，表示正在扩容 bucket&amp;h.oldbucketmask()是为了确认搬迁的 bucket 是我们正在使用的 bucket。oldbucketmask() 函数返回扩容前的 map 的 bucketmask。 bucketmask，作用就是将 key 计算出来的哈希值与 bucketmask 相与，得到的结果就是 key 应该落入的桶。比如 B = 5，则返回11111 搬迁核心evacuate 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156type evacDst struct &#123; b *bmap // 当前搬迁的桶 i int // b中的k/v索引 k unsafe.Pointer // 指针指向当前k存储位置 e unsafe.Pointer // 指针指向当前v存储位置&#125;func evacuate(t *maptype, h *hmap, oldbucket uintptr) &#123; //定位老的buckets地址(桶的开始位置+桶的数目*桶的大小) b := (*bmap)(add(h.oldbuckets, oldbucket*uintptr(t.bucketsize))) //返回旧的桶数，如果B为5，那么结果为32 newbit := h.noldbuckets() //如果没有搬迁过 if !evacuated(b) &#123; //xy 包含高低两个搬迁目的地 var xy [2]evacDst //低搬迁目的地 x := &amp;xy[0] //记录新的buckets地址(桶开始的位置+桶数目*大小) ==&gt; 地址是连续的? TODO x.b = (*bmap)(add(h.buckets, oldbucket*uintptr(t.bucketsize))) //k的位置为bmap开始的位置+topHash x.k = add(unsafe.Pointer(x.b), dataOffset) //v的位置为k结束的位置 x.e = add(x.k, bucketCnt*uintptr(t.keysize)) //如果不是等量扩容 if !h.sameSizeGrow() &#123; //使用y来进行搬迁 y := &amp;xy[1] //y.b表示获取新桶的地址 y.b = (*bmap)(add(h.buckets, (oldbucket+newbit)*uintptr(t.bucketsize))) //y.b表示key开始的地址 y.k = add(unsafe.Pointer(y.b), dataOffset) //y.e表示v开始的地址 y.e = add(y.k, bucketCnt*uintptr(t.keysize)) &#125; // 遍历所有的 bucket，包括 overflow buckets // b 是老的 bucket 地址 for ; b != nil; b = b.overflow(t) &#123; k := add(unsafe.Pointer(b), dataOffset) e := add(k, bucketCnt*uintptr(t.keysize)) // 遍历 bucket 中的所有 cell for i := 0; i &lt; bucketCnt; i, k, e = i+1, add(k, uintptr(t.keysize)), add(e, uintptr(t.elemsize)) &#123; // 当前 cell 的 top hash 值 top := b.tophash[i] // 如果 cell 为空，即没有 key if isEmpty(top) &#123; //x &lt;= emptyOne b.tophash[i] = evacuatedEmpty // 那就标志它被\"搬迁\"过 continue // 继续下个 cell &#125; // 正常不会出现这种情况 // 未被搬迁的 cell 只可能是 emptyRest 或者 emptyOne // 正常的 top hash（大于 minTopHash） if top &lt; minTopHash &#123; throw(\"bad map state\") &#125; k2 := k // 如果 key 是指针，则解引用 if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; var useY uint8 //如果不是等量扩容 if !h.sameSizeGrow() &#123; //计算hash值 hash := t.hasher(k2, uintptr(h.hash0)) //如果正在遍历map 且 出现过相同key 的两者相同(float变得NAN) if h.flags&amp;iterator != 0 &amp;&amp; !t.reflexivekey() &amp;&amp; !t.key.equal(k2, k2) &#123; useY = top &amp; 1 top = tophash(hash) &#125; else &#123; if hash&amp;newbit != 0 &#123; useY = 1 &#125; &#125; &#125; if evacuatedX+1 != evacuatedY || evacuatedX^1 != evacuatedY &#123; throw(\"bad evacuatedN\") &#125; b.tophash[i] = evacuatedX + useY // evacuatedX + 1 == evacuatedY dst := &amp;xy[useY] // evacuation destination if dst.i == bucketCnt &#123; dst.b = h.newoverflow(t, dst.b) dst.i = 0 dst.k = add(unsafe.Pointer(dst.b), dataOffset) dst.e = add(dst.k, bucketCnt*uintptr(t.keysize)) &#125; dst.b.tophash[dst.i&amp;(bucketCnt-1)] = top // mask dst.i as an optimization, to avoid a bounds check if t.indirectkey() &#123; *(*unsafe.Pointer)(dst.k) = k2 // copy pointer &#125; else &#123; typedmemmove(t.key, dst.k, k) // copy elem &#125; if t.indirectelem() &#123; *(*unsafe.Pointer)(dst.e) = *(*unsafe.Pointer)(e) &#125; else &#123; typedmemmove(t.elem, dst.e, e) &#125; dst.i++ // These updates might push these pointers past the end of the // key or elem arrays. That's ok, as we have the overflow pointer // at the end of the bucket to protect against pointing past the // end of the bucket. dst.k = add(dst.k, uintptr(t.keysize)) dst.e = add(dst.e, uintptr(t.elemsize)) &#125; &#125; // 如果没有协程在使用老的 buckets，就把老 buckets 清除掉，帮助gc if h.flags&amp;oldIterator == 0 &amp;&amp; t.bucket.ptrdata != 0 &#123; b := add(h.oldbuckets, oldbucket*uintptr(t.bucketsize)) // Preserve b.tophash because the evacuation // state is maintained there. ptr := add(b, dataOffset) n := uintptr(t.bucketsize) - dataOffset memclrHasPointers(ptr, n) &#125; &#125; // 更新搬迁进度 // 如果此次搬迁的 bucket 等于当前进度 if oldbucket == h.nevacuate &#123; advanceEvacuationMark(h, t, newbit) &#125;&#125;func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) &#123; //进度+1 h.nevacuate++ // Experiments suggest that 1024 is overkill by at least an order of magnitude. // Put it in there as a safeguard anyway, to ensure O(1) behavior. //尝试往后看 1024 个 bucket stop := h.nevacuate + 1024 if stop &gt; newbit &#123; stop = newbit &#125; // 寻找没有搬迁的 bucket for h.nevacuate != stop &amp;&amp; bucketEvacuated(t, h, h.nevacuate) &#123; h.nevacuate++ &#125; // 现在 h.nevacuate 之前的 bucket 都被搬迁完毕 if h.nevacuate == newbit &#123; // newbit == # of oldbuckets h.oldbuckets = nil // 清除老的 overflow bucket // 回忆一下：[0] 表示当前 overflow bucket // [1] 表示 old overflow bucket if h.extra != nil &#123; h.extra.oldoverflow = nil &#125; // 清除正在扩容的标志位 h.flags &amp;^= sameSizeGrow &#125;&#125; 搬迁的目的就是将老的 buckets 搬迁到新的 buckets。 应对条件 1，新的 buckets 数量是之前的一倍，所以要重新计算 key 的哈希，才能决定它到底落在哪个 bucket。例如，原来 B = 5，计算出 key 的哈希后，只用看它的低 5 位，就能决定它落在哪个 bucket。扩容后，B 变成了 6，因此需要多看一位，它的低 6 位决定 key 落在哪个 bucket。这称为 rehash 应对条件 2，新的 buckets 数量和之前相等。因此可以按序号来搬，比如原来在 0 号 bucktes，到新的地方后，仍然放在 0 号 buckets。 因此，某个 key 在搬迁前后 bucket 序号可能和原来相等，也可能是相比原来加上 2^B（原来的 B 值），取决于 hash 值 第 6 bit 位是 0 还是 1。 那么为什么遍历 map 是无序的？ 第一种情况插入时遍历：map 在扩容后，会发生 key 的搬迁，原来落在同一个 bucket 中的 key，可能就落在了不同的槽中 第二种情况不插入遍历：在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使你是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对了 123456789101112//测试 map中存入6个值，则按照规则，只会有一个桶，通过十次遍历d := make(map[int]int)for i := 0; i &lt; 6; i++ &#123; d[i] = i&#125;for i := 0; i &lt; 10; i++ &#123; fmt.Printf(\"count:%d \\n\", i) for _, v := range d &#123; fmt.Print(v) &#125;&#125;//结果发现，每次开始的位置没有变化，但是整体的前后顺序不变 “迭代 map 的结果是无序的”这个特性是从 go 1.0 开始加入的 再明确一个问题：如果扩容后，B 增加了 1，意味着 buckets 总数是原来的 2 倍，原来 1 号的桶“裂变”到两个桶。 例如，原始 B = 2，1号 bucket 中有 2 个 key 的哈希值低 3 位分别为：010，110。由于原来 B = 2，所以低 2 位 10 决定它们落在 2 号桶，现在 B 变成 3，所以 010、110 分别落入 2、6 号桶 evacuate 函数每次只完成一个 bucket 的搬迁工作会有 2 层循环，外层遍历 bucket 和 overflow bucket，内层遍历 bucket 的所有 cell 源码里提到 X, Y part，桶的数量是原来的 2 倍，前一半桶被称为 X part，后一半桶被称为 Y part。 一个 bucket 中的 key 可能会分裂落到 2 个桶，一个位于 X part，一个位于 Y part。所以在搬迁一个 cell 之前，需要知道这个 cell 中的 key 是落到哪个 Part。很简单，重新计算 cell 中 key 的 hash，并向前“多看”一位，决定落入哪个 Part,如果 tophash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part 有一个特殊情况是：有一种 key，每次对它计算 hash，得到的结果都不一样。这个 key 就是 math.NaN() 的结果，它的含义是 not a number，类型是 float64。当它作为 map 的 key，在搬迁的时候，会遇到一个问题：再次计算它的哈希值和它当初插入 map 时的计算出来的哈希值不一样 你可能想到了，这样带来的一个后果是，这个 key 是永远不会被 Get 操作获取的！当我使用 m[math.NaN()] 语句的时候，是查不出来结果的。这个 key 只有在遍历整个 map 的时候，才有机会现身。所以，可以向一个 map 插入任意数量的 math.NaN() 作为 key 12345678910111213//通过tophash值与新算出来的哈希值进行运算得到if top&amp;1 != 0 &#123; // top hash 最低位为 1 // 新算出来的 hash 值的 B 位置 1 hash |= newbit&#125; else &#123; // 新算出来的 hash 值的 B 位置 0 hash &amp;^= newbit&#125;// hash 值的 B 位为 0，则搬迁到 x part// 当 B = 5时，newbit = 32，二进制低 6 位为 10 0000useX = hash&amp;newbit == 0 确定了要搬迁到的目标 bucket 后，搬迁操作就比较好进行了。将源 key/value 值 copy 到目的地相应的位置。 设置 key 在原始 buckets 的 tophash 为 evacuatedX 或是 evacuatedY，表示已经搬迁到了新 map 的 x part 或是 y part。新 map 的 tophash 则正常取 key 哈希值的高 8 位 下面通过图来宏观地看一下扩容前后的变化 扩容前，B = 2，共有 4 个 buckets，lowbits 表示 hash 值的低位。假设我们不关注其他 buckets 情况，专注在 2 号 bucket。并且假设 overflow 太多，触发了等量扩容（对应于前面的条件 2） 扩容完成后，overflow bucket 消失了，key 都集中到了一个 bucket，更为紧凑了，提高了查找的效率。 假设触发了 2 倍的扩容，那么扩容完成后，老 buckets 中的 key 分裂到了 2 个 新的 bucket。一个在 x part，一个在 y 的 part。依据是 hash 的 lowbits。新 map 中 0-3 称为 x part，4-7 称为 y part 注意，上面的两张图忽略了其他 buckets 的搬迁情况，表示所有的 bucket 都搬迁完毕后的情形。实际上，我们知道，搬迁是一个“渐进”的过程，并不会一下子就全部搬迁完毕。所以在搬迁过程中，oldbuckets 指针还会指向原来老的 []bmap，并且已经搬迁完毕的 key 的 tophash 值会是一个状态值，表示 key 的搬迁去向。 Map遍历 正常情况下，遍历所有的 bucket 以及它后面挂的 overflow bucket，然后挨个遍历 bucket 中的所有 cell。每个 bucket 中包含 8 个 cell，从有 key 的 cell 中取出 key 和 value，这个过程就完成了。 而扩容过程不是一个原子的操作，它每次最多只搬运 2 个 bucket，所以如果触发了扩容操作，那么在很长时间里，map 的状态都是处于一个中间态：有些 bucket 已经搬迁到新家，而有些 bucket 还待在老地方，它又是怎样遍历的呢？ 先是调用 mapiterinit 函数初始化迭代器，然后循环调用 mapiternext 函数进行 map 迭代，其中迭代器的结构体定义是： 1234567891011121314151617181920212223242526272829type hiter struct &#123; // key 指针 key unsafe.Pointer // value 指针 value unsafe.Pointer // map 类型，包含如 key size 大小等 t *maptype // map header h *hmap // 初始化时指向的 bucket buckets unsafe.Pointer // 当前遍历到的 bmap bptr *bmap overflow [2]*[]*bmap // 起始遍历的 bucet 编号 startBucket uintptr // 遍历开始时 cell 的编号（每个 bucket 中有 8 个 cell） offset uint8 // 是否从头遍历了 wrapped bool // B 的大小 B uint8 // 指示当前 cell 序号 i uint8 // 指向当前的 bucket bucket uintptr // 因为扩容，需要检查的 bucket checkBucket uintptr&#125; 之前说到每次遍历都是无序的 12345678910// 生成随机数 rr := uintptr(fastrand())if h.B &gt; 31-bucketCntBits &#123; r += uintptr(fastrand()) &lt;&lt; 31&#125;// 从哪个 bucket 开始遍历it.startBucket = r &amp; (uintptr(1)&lt;&lt;h.B - 1)// 从 bucket 的哪个 cell 开始遍历it.offset = uint8(r &gt;&gt; h.B &amp; (bucketCnt - 1)) 例如，B = 2，那 uintptr(1)&lt;&lt;h.B - 1 结果就是 3，低 8 位为 0000 0011，将 r 与之相与，就可以得到一个 0~3 的 bucket 序号；bucketCnt - 1 等于 7，低 8 位为 0000 0111，将 r 右移 2 位后，与 7 相与，就可以得到一个 0~7 号的 cell。 于是，在 mapiternext 函数中就会从 it.startBucket 的 it.offset 号的 cell 开始遍历，取出其中的 key 和 value，直到又回到起点 bucket，完成遍历过程。 假设我们有下图所示的一个 map，起始时 B = 1，有两个 bucket，后来触发了扩容（这里不要深究扩容条件，只是一个设定），B 变成 2。并且， 1 号 bucket 中的内容搬迁到了新的 bucket，1 号裂变成 1 号和 3 号；0 号 bucket 暂未搬迁。老的 bucket 挂在在 *oldbuckets 指针上面，新的 bucket 则挂在 *buckets 指针上面 这时，我们对此 map 进行遍历。假设经过初始化后，startBucket = 3，offset = 2。于是，遍历的起点将是 3 号 bucket 的 2 号 cell，下面这张图就是开始遍历时的状态 标红的表示起始位置，bucket 遍历顺序为：3 -&gt; 0 -&gt; 1 -&gt; 2。 因为 3 号 bucket 对应老的 1 号 bucket，因此先检查老 1 号 bucket 是否已经被搬迁过。判断方法就是： 1234func evacuated(b *bmap) bool &#123; h := b.tophash[0] return h &gt; empty &amp;&amp; h &lt; minTopHash&#125; 如果 b.tophash[0] 的值在标志值范围内，即在 (0,4) 区间里，说明已经被搬迁过了 在本例中，老 1 号 bucket 已经被搬迁过了。所以它的 tophash[0] 值在 (0,4) 范围内，因此只用遍历新的 3 号 bucket。 依次遍历 3 号 bucket 的 cell，这时候会找到第一个非空的 key：元素 e。到这里，mapiternext 函数返回，这时我们的遍历结果仅有一个元素： 由于返回的 key 不为空，所以会继续调用 mapiternext 函数。 继续从上次遍历到的地方往后遍历，从新 3 号 overflow bucket 中找到了元素 f 和 元素 g。 遍历结果集也因此壮大： 新 3 号 bucket 遍历完之后，回到了新 0 号 bucket。0 号 bucket 对应老的 0 号 bucket，经检查，老 0 号 bucket 并未搬迁，因此对新 0 号 bucket 的遍历就改为遍历老 0 号 bucket。那是不是把老 0 号 bucket 中的所有 key 都取出来呢？ 并没有这么简单，回忆一下，老 0 号 bucket 在搬迁后将裂变成 2 个 bucket：新 0 号、新 2 号。而我们此时正在遍历的只是新 0 号 bucket（注意，遍历都是遍历的 *bucket 指针，也就是所谓的新 buckets）。所以，我们只会取出老 0 号 bucket 中那些在裂变之后，分配到新 0 号 bucket 中的那些 key。 因此，lowbits == 00 的将进入遍历结果集： 和之前的流程一样，继续遍历新 1 号 bucket，发现老 1 号 bucket 已经搬迁，只用遍历新 1 号 bucket 中现有的元素就可以了。结果集变成： 继续遍历新 2 号 bucket，它来自老 0 号 bucket，因此需要在老 0 号 bucket 中那些会裂变到新 2 号 bucket 中的 key，也就是 lowbit == 10 的那些 key。 这样，遍历结果集变成： 最后，继续遍历到新 3 号 bucket 时，发现所有的 bucket 都已经遍历完毕，整个迭代过程执行完毕。 顺便说一下，如果碰到 key 是 math.NaN() 这种的，处理方式类似。核心还是要看它被分裂后具体落入哪个 bucket。只不过只用看它 top hash 的最低位。如果 top hash 的最低位是 0 ，分配到 X part；如果是 1 ，则分配到 Y part。据此决定是否取出 key，放到遍历结果集里。 map 遍历的核心在于理解 2 倍扩容时，老 bucket 会分裂到 2 个新 bucket 中去。而遍历操作，会按照新 bucket 的序号顺序进行，碰到老 bucket 未搬迁的情况时，要在老 bucket 中找到将来要搬迁到新 bucket 来的 key Map插入 通过汇编语言可以看到，向 map 中插入或者修改 key，最终调用的是 mapassign 函数。 插入或修改 key 的语法是一样的，前者操作的 key 在 map 中不存在，而后者操作的 key 存在 map 中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; if h == nil &#123; panic(plainError(\"assignment to entry in nil map\")) &#125; if h.flags&amp;hashWriting != 0 &#123; //表示正在写 throw(\"concurrent map writes\") &#125; hash := t.hasher(key, uintptr(h.hash0)) //获取hash值 //在调用t.hasher之后设置hashWriting，因为t.hasher可能会出现紧急情况，在这种情况下，我们实际上并未执行写操作 h.flags ^= hashWriting //设置标记位，hashWriting //如果bucket数组一开始为空，则初始化 if h.buckets == nil &#123; h.buckets = newobject(t.bucket) // newarray(t.bucket, 1) &#125;again: bucket := hash &amp; bucketMask(h.B) // 定位存储在哪一个bucket中 if h.growing() &#123; //如果现正在扩容，则扩容 growWork(t, h, bucket) &#125; //得到bucket的结构体 b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) //获取高八位hash值 var inserti *uint8 var insertk unsafe.Pointer var elem unsafe.Pointerbucketloop: for &#123; //死循环 for i := uintptr(0); i &lt; bucketCnt; i++ &#123; //循环bucket中的tophash数组 if b.tophash[i] != top &#123; //如果hash不相等 if isEmpty(b.tophash[i]) &amp;&amp; inserti == nil &#123; //判断是否为空，为空则插入 inserti = &amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) &#125; if b.tophash[i] == emptyRest &#123; //为空，且后续也没有非空cell break bucketloop &#125; continue &#125; //到这里说明高八位hash一样，获取已存在的key k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey() &#123; k = *((*unsafe.Pointer)(k)) &#125; //判断两个key是否相等，不相等就循环下一个 if !t.key.equal(key, k) &#123; continue &#125; // already have a mapping for key. Update it. if t.needkeyupdate() &#123; typedmemmove(t.key, k, key) &#125; //获取已存在的value elem = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) goto done &#125; //如果上一个bucket没能插入，则通过overflow获取链表上的下一个bucket ovf := b.overflow(t) if ovf == nil &#123; break &#125; b = ovf &#125; //如果超过负载，则扩容 if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again &#125; //如果当前bmap满了，就新建一个 if inserti == nil &#123; // all current buckets are full, allocate a new one. newb := h.newoverflow(t, b) inserti = &amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) elem = add(insertk, bucketCnt*uintptr(t.keysize)) &#125; //存储元素 if t.indirectkey() &#123; kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem &#125; if t.indirectelem() &#123; vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem &#125; typedmemmove(t.key, insertk, key) *inserti = top h.count++done: //更新状态，返回元素 if h.flags&amp;hashWriting == 0 &#123; //如果是状态异常 throw(\"concurrent map writes\") &#125; h.flags &amp;^= hashWriting //设置标志位状态表示完成 if t.indirectelem() &#123; elem = *((*unsafe.Pointer)(elem)) &#125; return elem&#125; 整体来看，核心还是一个双层循环，外层遍历 bucket 和它的 overflow bucket，内层遍历整个 bucket 的各个 cell：对 key 计算 hash 值，根据 hash 值按照之前的流程，找到要赋值的位置（可能是插入新 key，也可能是更新老 key），对相应位置进行赋值。 注意： 函数首先会检查 map 的标志位 flags。如果 flags 的写标志位此时被置 1 了，说明有其他协程在执行“写”操作，进而导致程序 panic。这也说明了 map 对协程是不安全的。 通过前文我们知道扩容是渐进式的，如果 map 处在扩容的过程中，那么当 key 定位到了某个 bucket 后，需要确保这个 bucket 对应的老 bucket 完成了迁移过程。即老 bucket 里的 key 都要迁移到新的 bucket 中来（分裂到 2 个新 bucket），才能在新的 bucket 中进行插入或者更新的操作。 上面说的操作是在函数靠前的位置进行的，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。 现在到了定位 key 应该放置的位置了，所谓找准自己的位置很重要。准备两个指针，一个（inserti）指向 key 的 hash 值在 tophash 数组所处的位置，另一个(insertk)指向 cell 的位置（也就是 key 最终放置的地址），当然，对应 value 的位置就很容易定位出来了。这三者实际上都是关联的，在 tophash 数组中的索引位置决定了 key 在整个 bucket 中的位置（共 8 个 key），而 value 的位置需要“跨过” 8 个 key 的长度。 在循环的过程中，inserti 和 insertk 分别指向第一个找到的空闲的 cell。如果之后在 map 没有找到 key 的存在，也就是说原来 map 中没有此 key，这意味着插入新 key。那最终 key 的安置地址就是第一次发现的“空位”（tophash 是 empty）。 如果这个 bucket 的 8 个 key 都已经放置满了，那在跳出循环后，发现 inserti 和 insertk 都是空，这时候需要在 bucket 后面挂上 overflow bucket。当然，也有可能是在 overflow bucket 后面再挂上一个 overflow bucket。这就说明，太多 key hash 到了此 bucket。 在正式安置 key 之前，还要检查 map 的状态，看它是否需要进行扩容。如果满足扩容的条件，就主动触发一次扩容操作。 这之后，整个之前的查找定位 key 的过程，还得再重新走一次。因为扩容之后，key 的分布都发生了变化。 最后，会更新 map 相关的值，如果是插入新 key，map 的元素数量字段 count 值会加 1；在函数之初设置的 hashWriting 写标志出会清零。 另外，有一个重要的点要说一下。前面说的找到 key 的位置，进行赋值操作，实际上并不准确。我们看 mapassign 函数的原型就知道，函数并没有传入 value 值，所以赋值操作是什么时候执行的呢？ 1func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer 答案还得从汇编语言中寻找。我直接揭晓答案，有兴趣可以私下去研究一下。mapassign 函数返回的指针就是指向的 key 所对应的 value 值位置，有了地址，就很好操作赋值了。 Map删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) &#123; //如果map为空，直接panic if h == nil || h.count == 0 &#123; if t.hashMightPanic() &#123; t.hasher(key, 0) // see issue 23734 &#125; return &#125; //如果正在写，则异常并发读写 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map writes\") &#125; //计算hash值 hash := t.hasher(key, uintptr(h.hash0)) //设置标志位 h.flags ^= hashWriting //获取槽 bucket := hash &amp; bucketMask(h.B) if h.growing() &#123; //如果在扩容，直接触发扩容 growWork(t, h, bucket) &#125; b := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize))) bOrig := b top := tophash(hash)search: for ; b != nil; b = b.overflow(t) &#123; for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; //双循环，如果topHash不一致，且不为emptyRest则继续 if b.tophash[i] == emptyRest &#123; //否则下一个overflow break search &#125; continue &#125; k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) k2 := k if t.indirectkey() &#123; k2 = *((*unsafe.Pointer)(k2)) &#125; if !t.key.equal(key, k2) &#123; continue &#125; // Only clear key if there are pointers in it. if t.indirectkey() &#123; *(*unsafe.Pointer)(k) = nil &#125; else if t.key.ptrdata != 0 &#123; memclrHasPointers(k, t.key.size) &#125; e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize)) if t.indirectelem() &#123; *(*unsafe.Pointer)(e) = nil &#125; else if t.elem.ptrdata != 0 &#123; memclrHasPointers(e, t.elem.size) &#125; else &#123; memclrNoHeapPointers(e, t.elem.size) &#125; b.tophash[i] = emptyOne // If the bucket now ends in a bunch of emptyOne states, // change those to emptyRest states. // It would be nice to make this a separate function, but // for loops are not currently inlineable. if i == bucketCnt-1 &#123; if b.overflow(t) != nil &amp;&amp; b.overflow(t).tophash[0] != emptyRest &#123; goto notLast &#125; &#125; else &#123; if b.tophash[i+1] != emptyRest &#123; goto notLast &#125; &#125; for &#123; b.tophash[i] = emptyRest if i == 0 &#123; if b == bOrig &#123; break // beginning of initial bucket, we're done. &#125; // Find previous bucket, continue at its last entry. c := b for b = bOrig; b.overflow(t) != c; b = b.overflow(t) &#123; &#125; i = bucketCnt - 1 &#125; else &#123; i-- &#125; if b.tophash[i] != emptyOne &#123; break &#125; &#125; notLast: h.count-- //数量减1 break search &#125; &#125; if h.flags&amp;hashWriting == 0 &#123; throw(\"concurrent map writes\") &#125; //读写恢复 h.flags &amp;^= hashWriting&#125; 当然，我们只关心 mapdelete 函数。它首先会检查 h.flags 标志，如果发现写标位是 1，直接 panic，因为这表明有其他协程同时在进行写操作。 计算 key 的哈希，找到落入的 bucket。检查此 map 如果正在扩容的过程中，直接触发一次搬迁操作。 删除操作同样是两层循环，核心还是找到 key 的具体位置。寻找过程都是类似的，在 bucket 中挨个 cell 寻找。 找到对应位置后，对 key 或者 value 进行“清零”操作 最后，将 count 值减 1，将对应位置的 tophash 值置成 Empty Map类型 12345678910111213141516171819202122232425262728293031type maptype struct &#123; typ _type key *_type elem *_type bucket *_type // internal type representing a hash bucket // function for hashing keys (ptr to key, seed) -&gt; hash hasher func(unsafe.Pointer, uintptr) uintptr keysize uint8 // size of key slot elemsize uint8 // size of elem slot bucketsize uint16 // size of bucket flags uint32&#125;type _type struct &#123; size uintptr ptrdata uintptr // size of memory prefix holding all pointers hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 // function for comparing objects of this type // (ptr to object A, ptr to object B) -&gt; ==? equal func(unsafe.Pointer, unsafe.Pointer) bool // gcdata stores the GC type data for the garbage collector. // If the KindGCProg bit is set in kind, gcdata is a GC program. // Otherwise it is a ptrmask bitmap. See mbitmap.go for details. gcdata *byte str nameOff ptrToThis typeOff&#125; Map进阶 Key可以是float型 从语法上看，是可以的。Go 语言中只要是可比较的类型都可以作为 key。除开 slice，map，functions 这几种类型，其他类型都是 OK 的。具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。这些类型的共同特征是支持 == 和 != 操作符，k1 == k2 时，可认为 k1 和 k2 是同一个 key。如果是结构体，则需要它们的字段值都相等，才被认为是相同的 key chan是怎么比较的？ 删除Key之后，内存是直接释放吗？ 首先看看删除逻辑代码 1234567891011121314if t.indirectkey() &#123; //如果key是指针 *(*unsafe.Pointer)(k) = nil //则将key置位nil&#125; else if t.key.ptrdata != 0 &#123; //key中含有指针 memclrHasPointers(k, t.key.size)&#125;e := add(unsafe.Pointer(b),dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.elemsize))if t.indirectelem() &#123; //如果val是指针 *(*unsafe.Pointer)(e) = nil&#125; else if t.elem.ptrdata != 0 &#123; //如果val含有指针 memclrHasPointers(e, t.elem.size)&#125; else &#123; memclrNoHeapPointers(e, t.elem.size) //执行飞对指针&#125;b.tophash[i] = emptyOne 其中 1234567891011func memclrHasPointers(ptr unsafe.Pointer, n uintptr) &#123; bulkBarrierPreWrite(uintptr(ptr), 0, n) //添加写屏障 memclrNoHeapPointers(ptr, n) //clears n bytes starting at ptr 非堆&#125;func typedmemclr(typ *_type, ptr unsafe.Pointer) &#123; if writeBarrier.needed &amp;&amp; typ.ptrdata != 0 &#123; //类型含有指针字段 bulkBarrierPreWrite(uintptr(ptr), 0, typ.ptrdata) &#125; memclrNoHeapPointers(ptr, typ.size)&#125; 首先 memclrHasPointers 与 typedmemclr 的区别仅仅是 调用者是否直到清理的类型中是否含有堆指针 参考文献 如何设计并实现一个线程安全的Map https://www.kancloud.cn/kancloud/the-way-to-go/72489 https://blog.csdn.net/u010853261/article/details/99699350 https://cloud.tencent.com/developer/article/1468799 https://www.jianshu.com/p/7782d82f5154 https://qcrao.com/2019/05/22/dive-into-go-map/ https://github.com/golang/go/issues/20135","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"算法2-堆排序","slug":"Algorithm/算法2-堆排序","date":"2021-07-04T23:53:00.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/07/05/Algorithm/算法2-堆排序/","link":"","permalink":"http://xboom.github.io/2021/07/05/Algorithm/%E7%AE%97%E6%B3%952-%E5%A0%86%E6%8E%92%E5%BA%8F/","excerpt":"","text":"堆排序是利用堆这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最坏，最好，平均时间复杂度均为O(nlogn)，是不稳定排序 堆 堆是具有以下性质的完全二叉树 每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆 每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆 同时，我们对堆中的结点按层进行编号，将这种逻辑结构映射到数组中就是下面这个样子 则数组从逻辑上讲就是一个堆结构 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] 小顶堆：arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2] 基本思想和步骤 堆排序的基本思想是：将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值。然后将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了 步骤一 构造初始堆。将给定无序序列构造成一个大顶堆（一般升序采用大顶堆，降序采用小顶堆) 假设给定无序序列结构如下 从最后一个非叶子结点开始（叶结点自然不用调整，第一个非叶子结点 arr.length/2-1=5/2-1=1，也就是下面的6结点），从左至右，从下至上进行调整 最后一个非叶子节点 = n/2 - 1 找到第二个非叶节点4，由于[4,9,8]中9元素最大，4和9交换 步骤二 将堆顶元素与末尾元素进行交换，使末尾元素最大。然后继续调整堆，再将堆顶元素与末尾元素交换，得到第二大元素。如此反复进行交换、重建、交换。 将堆顶元素9和末尾元素4进行交换 重新调整结构，使其继续满足堆定义 再将堆顶元素8与末尾元素5进行交换，得到第二大元素8 后续过程，继续进行调整，交换，如此反复进行，最终使得整个序列有序 总结下堆排序的基本思路： 将无需序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆 将堆顶元素与末尾元素交换，将最大元素&quot;沉&quot;到数组末端 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序 完全二叉树：设二叉树的深度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数， 第 h 层所有的结点都连续集中在最左边 满二叉树：深度为k且有2^k-1个结点的二叉树称为满二叉树 为什么 堆排序(完全二叉树)最后一个非叶子节点的序号是 n/2 - 1 (n为节点个数)？ 第一种：堆的最后一个非叶子节点只有左孩子 因为 i 的左孩子为 2 * i + 1，而最后一个节点的序号为 n - 1，所以 i = n / 2 -1 第二种：堆的最后一个非叶子节点有左右两个孩子 因为 i 的左孩子为 2 * i + 2，而最后一个节点的序号为 n - 1，所以 i = (n - 1) / 2 -1 又因为当完全二叉树最后一个节点是其父节点的右孩子时，树的节点数为奇数，所以 i = n / 2 - 1 当 n &lt;= 1的时候，并没有非叶子节点。即 非叶子节点 i = n / 2 - 1 (n &gt; 1) 插入和删除 删除 利用下沉来进行堆顶元素删除，首先要做的是进行交换，再进行下沉 1234567891011121314151617//构建父节点i最大，父节点大于等于子节点func heapify(arr []int, i, arrLen int) &#123; left := 2*i + 1 right := 2*i + 2 largest := i //找到父子节点中三个最大的，如果不是根节点则交换位置 if left &lt; arrLen &amp;&amp; arr[left] &gt; arr[largest] &#123; largest = left &#125; if right &lt; arrLen &amp;&amp; arr[right] &gt; arr[largest] &#123; largest = right &#125; if largest != i &#123;//如果子节点比父节点大，调整之后，子节点的子节点也需要做调整 arr[i], arr[largest] = arr[largest], arr[i] heapify(arr, largest, arrLen) &#125;&#125; 插入 首先将元素添加到末尾，然后通过上浮的方式进行 1234567//上浮代码func swim(nums []int, index int) &#123; for index &gt; 1 &amp;&amp; nums[index/2] &gt; nums[index] &#123; nums[index/2], nums[index] = nums[index], nums[nums/2] index = index/2 &#125;&#125; 代码实现 Go代码实现： 123456789101112131415161718192021222324252627282930313233//堆排序func heapSort(arr []int) []int &#123; arrLen := len(arr) //构建大頂堆，但是数组并不满足降序，只找到最大值 for i := arrLen / 2 - 1; i &gt;= 0; i-- &#123; heapify(arr, i, arrLen) &#125; //满足大頂堆之后，只需要将最大值与最后一个值互换，并将根节点下沉又是一个大顶堆 for i := arrLen - 1; i &gt;= 0; i-- &#123; arr[0], arr[i] = arr[i], arr[0] arrLen -= 1 heapify(arr, 0, arrLen) &#125; return arr&#125;//构建父节点i最大，父节点大于等于子节点func heapify(arr []int, i, arrLen int) &#123; left := 2*i + 1 right := 2*i + 2 largest := i //找到父子节点中三个最大的，如果不是根节点则交换位置 if left &lt; arrLen &amp;&amp; arr[left] &gt; arr[largest] &#123; largest = left &#125; if right &lt; arrLen &amp;&amp; arr[right] &gt; arr[largest] &#123; largest = right &#125; if largest != i &#123;//如果子节点比父节点大，调整之后，子节点的子节点也需要做调整 arr[i], arr[largest] = arr[largest], arr[i] heapify(arr, largest, arrLen) &#125;&#125; C++代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;iostream&gt;using namespace std;void print(int a[], int n)&#123; int i; for(i = 0; i &lt; n; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; \" \"; &#125; cout &lt;&lt; endl;&#125; void heapAdjust(int a[], int low, int high)&#123; int pivotKey = a[low - 1]; int i; for(i = 2 * low; i &lt;= high; i *= 2) &#123; if(i &lt; high &amp;&amp; a[i - 1] &lt; a[i]) &#123; i++; //i指向较大值 &#125; if(pivotKey &gt;= a[i - 1]) &#123; break; &#125; a[low - 1] = a[i - 1]; low = i; &#125; a[low - 1] = pivotKey; &#125; void heapSort(int a[], int n)&#123; int i, tmp; for(i = n/2; i &gt; 0; i--) &#123; heapAdjust(a, i, n); print(a, n); &#125; for(i = n; i &gt; 1; i--) &#123; tmp = a[i -1]; a[i - 1] = a[0]; a[0] = tmp; heapAdjust(a, 1, i - 1); print(a, n); &#125;&#125;int main()&#123; int a[] = &#123;30, 20, 40, 10, 0, 60, 80, 70&#125;; int n = sizeof(a) / sizeof(a[0]); heapSort(a, n); print(a, n); return 0;&#125; 堆排序是一种选择排序，整体主要由构建初始堆+交换堆顶元素和末尾元素并重建堆两部分组成。 其中构建初始堆经推导复杂度为O(n)，在交换并重建堆的过程中，需交换n-1次， 而重建堆的过程中，根据完全二叉树的性质，[log2(n-1),log2(n-2)…1]逐步递减，近似为nlogn。所以堆排序时间复杂度一般认为就是O(nlogn)级。 堆排序应用 Top K问题 从堆的定义来说，很容易想到堆顶元素是TOP 1，那么如何求经典的TOP K问题? 如果求最大的Top K 元素，是建立大顶堆，还是小顶堆 如果用大顶堆，堆顶是最大的元素，新加入的元素必须和所有的堆中元素做比对，显然不够划算； 如果用小顶堆，那堆顶元素是最小的元素，新加入的元素只要和堆顶元素做对比，如果比堆顶元素大，就可以把堆顶的元素删除，将新加入的元素加入到堆中 每次求TopK的问题的时候，只需要直接返回堆中所有元素即可，每次堆化的算法复杂度为O(logK),那么N个元素堆化的时间为O(nlogK),如果不用堆，每次新增元素都要重新排序，再取前面N个 定时器应用 独立线程，以超时时间建立了一个小顶堆，如果堆顶元素为2分钟，清理连接的线程的休眠时间设置为2分钟，2分钟后取堆顶元素，执行连接关闭操作。 求中位数和99%的响应时间 动态数组取中位数： 如果n为偶数，前n/2数据存储在大顶堆中，n/2存储小顶堆中，则大顶堆的堆顶元素为要找的中位数 如果n为奇数，可以将前n/2+1个数据存储在大顶堆中，后n/2存储在小顶堆中 如果添加新数据，则将数据与大顶堆中的堆顶元素比较 如果小于等于大顶堆中的元素，就插入到大顶堆中 如果比大顶堆的堆顶大，那就插入到小顶堆中 如果插入数据后不满足要求两个堆的数量为n/2和n/2 或n/2 和n+1/2 的要求，需要调整两个堆的大小，从大顶堆中删除堆顶元素，或小顶堆中删除堆顶元素，移动到另外一个堆中即可。 99%的响应时间：如果一个接口的有100个访问请求，分别耗时1ms，2ms，3ms…100ms，那么按照访问时间从小到大排列，排在第99位的访问时间，就是99%的访问时间，我们维护两个堆，大顶堆的元素个数为n99%，小顶堆的元素个数为n1%，那么大顶堆中的堆顶元素即是所求的99%的响应时间，和中位数的应用一样，只是中位数中的应用更特殊一点 优先级队列 优先级队列和通常的栈和队列一样，只不过里面的每一个元素都有一个&quot;优先级”，在处理的时候，首先处理优先级最高的。如果两个元素具有相同的优先级，则按照他们插入到队列中的先后顺序处理 参考链接 https://www.cnblogs.com/chengxiao/p/6129630.html https://mp.weixin.qq.com/s/OqpSdFfK12NhPpcsXTMvtA https://blog.csdn.net/weixin_45728685/article/details/105115912","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/categories/Algorithm/"}],"tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"}]},{"title":"Linux深入2-事件模型","slug":"Linux/Linux深入2-事件模型","date":"2021-07-04T14:39:18.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2021/07/04/Linux/Linux深入2-事件模型/","link":"","permalink":"http://xboom.github.io/2021/07/04/Linux/Linux%E6%B7%B1%E5%85%A52-%E4%BA%8B%E4%BB%B6%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"基础知识 同步与异步：关注的是消息通信机制 (synchronous communication/ asynchronous communication) 所谓同步，调用在发出后，在没有得到结果之前该调用不返回。一旦调用返回，就得到返回值 换句话说，就是由调用者主动等待这个调用的结果 所谓异步，调用在发出后，这个调用直接返回了，没有返回结果。而是被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用 阻塞与非阻塞：关注的是程序在等待调用结果消息返回值时的状态 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程 用户空间和内核空间 操作系统的核心是内核，可访问受保护的内存空间和底层硬件设备。为了保证用户进程不能直接操作内核（kernel），将虚拟空间划分为内核空间和用户空间。针对32位linux操作系统而言，寻址空间（虚拟存储空间）为4G（2的32次方），将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 补充：地址空间就是一个非负整数地址的有序集合。如{0,1,2…} 用户态与核心态的切换，一共有三种方式 系统调用：用户态进程通过系统调用申请使用操作系统的服务完成工作，系统调用其实是通过中断来实现 异常：当CPU在执行运行在用户态下的程序时，发生异常会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常 外围设备中断：当外围设备向CPU发出中断信号，CPU会暂停执行下一条即将要执行的指令转而去执行对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等 进程切换：内核挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行 保存处理机上下文，包括程序计数器和其他寄存器 更新PCB信息 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列 选择另一个进程执行，并更新其PCB 更新内存管理的数据结构。 恢复处理机上下文 系统中存放进程的管理和控制信息的数据结构称为进程控制块（PCB Process Control Block) 当进程被阻塞，它是不占用CPU资源的 文件描述符：文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统 缓冲IO：缓存IO又被称作标准IO，在Linux的缓存IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。导致数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作 基本类型 《UNIX网络编程：卷一》第六章——I/O复用，书中提及了5种类UNIX下可用的I/O模型： 阻塞式I/O 非阻塞式I/O I/O复用（select，poll，epoll…） 信号驱动式I/O（SIGIO） 异步I/O（POSIX的aio_系列函数） 阻塞IO模型 进程会一直阻塞，直到数据拷贝完成 应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。数据准备好后，从内核拷贝到用户空间，IO函数返回成功指示 非阻塞IO模型 通过进程反复调用IO函数，在数据拷贝过程中，进程是阻塞的 IO复用模型 主要是select和epoll。一个线程可以对多个IO端口进行监听，当socket有读写事件时分发到具体的线程进行处理 虽然I/O多路复用的函数也是阻塞的，但是其与以上两种还是有不同的，I/O多路复用是阻塞在select，epoll这样的系统调用之上，而没有阻塞在真正的I/O系统调用如recvfrom之上。 select select本质是通过设置或检查存放fd标志位的数据结构来进行下一步处理。缺点是： 单个进程可监视的fd数量被限制，即能监听端口的大小有限。一般来说和系统内存有关，具体数目可以cat /proc/sys/fs/file-max察看。32位默认是1024个，64位默认为2048个 对socket进行扫描时是线性扫描，即采用轮询方法，效率低。当套接字比较多的时候，每次select()都要遍历FD_SETSIZE个socket来完成调度，不管socket是否活跃都遍历一遍。会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，就避免了轮询，这正是epoll与kqueue做的 需要维护一个用来存放大量fd的数据结构，会使得用户空间和内核空间在传递该结构时复制开销大 poll poll本质和select相同，将用户传入的数据拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或主动超时，被唤醒后又要再次遍历fd。它没有最大连接数的限制，原因是它是基于链表来存储的，但缺点是： 大量的fd的数组被整体复制到用户态和内核空间之间，不管有无意义 poll还有一个特点“水平触发”，如果报告了fd后，没有被处理，那么下次poll时再次报告该ffd。 epoll epoll支持水平触发和边缘触发，最大特点在于边缘触发，只告诉哪些fd刚刚变为就绪态，并且只通知一次。还有一特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一量该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。epoll的优点： 没有最大并发连接的限制。 效率提升，只有活跃可用的FD才会调用callback函数。 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递。 水平触发与垂直触发 select、poll、epoll区别总结 信号驱动IO模型 首先允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据 异步IO模型 相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的 5种IO模型比较 阻塞与非阻塞IO的区别是：调用阻塞IO后进程会一直等待对应的进程完成，而非阻塞IO不会等待对应的进程完成，在kernel还在准备数据的情况下直接返回 前四种I/O模型都是同步I/O操作，区别在于第一阶段，第二阶段是一样的：在数据从内核复制到应用缓冲区期间（用户空间），进程阻塞于recvfrom调用。相反，异步I/O模型在这两个阶段都要处理。 阻塞IO和非阻塞IO的区别：数据准备的过程中,进程是否阻塞。 同步IO和异步IO的区别：数据拷贝的过程中,进程是否阻塞 实现原理 123456789101112//创建socketint s = socket(AF_INET, SOCK_STREAM, 0); //绑定bind(s, ...)//监听listen(s, ...)//接受客户端连接int c = accept(s, ...)//接收客户端数据recv(c, ...);//将数据打印出来printf(...) 接收数据 网卡收到网线传来的数据 通过硬件电路传输，将数据写入到内存中的某个地址上 由硬件产生的信号CPU会中断掉正在执行的程序而做出响应。执行完毕再重新执行用户程序。中断过程和函数调用差不多。只不过函数调用是事先定好位置，而中断的位置由“信号”决定 以键盘为例，当用户按下键盘某个按键时，键盘会给cpu的中断引脚发出一个高电平。cpu能够捕获这个信号，然后执行键盘中断程序。 当网卡把数据写入到内存后，网卡向cpu发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据。 先将网络数据写入到对应socket的接收缓冲区里 再唤醒进程A，重新将进程A放入工作队列中 工作队列 当程序运行到recv时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态 计算机中运行着A、B、C三个进程，其中进程A执行着上述基础网络程序，一开始，这3个进程都被操作系统的工作队列所引用，处于运行状态，会分时执行 等待队列 当进程A执行创建socket语句时，操作系统会创建一个由文件系统管理的socket对象（如下图）。 socket对象包含了 发送缓冲区 接收缓冲区 等待队列：指向所有需要等待该socket事件的进程 当程序执行到recv时，操作系统会将进程A从工作队列移动到该socket的等待队列中（如下图）。由于工作队列只剩下了进程B和C，依据进程调度，cpu会轮流执行这两个进程的程序，不会执行进程A的程序。所以进程A被阻塞，不会往下执行代码，也不会占用cpu资源 操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。上图为了方便说明，直接将进程挂到等待队列之下 唤醒进程 当socket接收到数据后，操作系统将该socket等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。也由于socket的接收缓冲区已经有了数据，recv可以返回接收到的数据 问题1：操作系统如何知道网络数据对应于哪个socket？ 因为一个socket对应着一个端口号，而网络数据包中包含了ip和端口的信息，内核可以通过端口号找到对应的socket。当然，为了提高处理速度，操作系统会维护端口号到socket的索引结构，以快速读取 问题2：如何同时监视多个socket的数据？ 多路复用 监视多个socket select 1234567891011121314int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int fds[] = 存放需要监听的socketwhile(1)&#123; int n = select(..., fds, ...) for(int i=0; i &lt; fds.count; i++)&#123; if(FD_ISSET(fds[i], ...))&#123; //fds[i]的数据处理 &#125; &#125;&#125; 假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中 当任何一个socket收到数据后，中断程序将唤起进程。下图展示了sock2接收到了数据的处理流程 所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面。如下图所示 经由这些步骤，当进程A被唤醒后，它知道至少有一个socket接收了数据。程序只需遍历一遍socket列表，就可以得到就绪的socket 当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞 缺点： 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次 epoll epoll改进措施 措施一：功能分离 select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一，epoll将这两个操作分开，先用epoll_ctl维护等待队列，再调用epoll_wait阻塞进程 12345678910111213int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 措施二：就绪列表 select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据 创建epoll对象 当某个进程调用epoll_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列 维护监视列表 创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket。以添加socket为例，如下图，如果通过epoll_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中 当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程 接收数据 当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用。如下图展示的是sock2和sock3收到数据后，中断程序让rdlist引用这两个socket eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。 当程序执行到epoll_wait时，如果rdlist已经引用了socket，那么epoll_wait直接返回，如果rdlist为空，阻塞进程 阻塞和唤醒进程 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化 实现细节 就绪列表引用着就绪的socket，所以它应能够快速的插入数据。 程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。 所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist) 索引结构既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构 因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist并非直接引用socket，而是通过epitem间接引用，红黑树的节点也是epitem对象。同样，文件系统也并非直接引用着socket。为方便理解，本文中省略了一些间接结构 代码分析(还没看懂) epoll 1234567891011121314151617181920212223242526272829303132int init_reactor(int listen_fd,int worker_count)&#123; ...... // 创建多个epoll fd，以充分利用多核 for(i=0;i&lt;worker_count;i++)&#123; reactor-&gt;worker_fd = epoll_create(EPOLL_MAX_EVENTS); &#125; /* epoll add listen_fd and accept */ // 将accept后的事件加入到对应的epoll fd中 int client_fd = accept(listen_fd,(struct sockaddr *)&amp;client_addr,&amp;client_len))); // 将连接描述符注册到对应的worker里面 epoll_ctl(reactor-&gt;client_fd,EPOLL_CTL_ADD,epifd,&amp;event);&#125;// reactor的worker线程static void* rw_thread_func(void* arg)&#123; ...... for(;;)&#123; // epoll_wait等待事件触发 int retval = epoll_wait(epfd,events,EPOLL_MAX_EVENTS,500); if(retval &gt; 0)&#123; for(j=0; j &lt; retval; j++)&#123; // 处理读事件 if(event &amp; EPOLLIN)&#123; handle_ready_read_connection(conn); continue; &#125; /* 处理其它事件 */ &#125; &#125; &#125; ......&#125; eventpoll 核心结构 1234567891011121314151617181920// epoll的核心实现对应于一个epoll描述符 struct eventpoll &#123; spinlock_t lock; struct mutex mtx; wait_queue_head_t wq; // sys_epoll_wait() 等待在这里 // f_op-&gt;poll() 使用的, 被其他事件通知机制利用的wait_address wait_queue_head_t poll_wait; //已就绪的需要检查的epitem 列表 struct list_head rdllist; //保存所有加入到当前epoll的文件对应的epitem struct rb_root rbr; // 当正在向用户空间复制数据时, 产生的可用文件 struct epitem *ovflist; /* The user that created the eventpoll descriptor */ struct user_struct *user; struct file *file; //优化循环检查，避免循环检查中重复的遍历 int visited; struct list_head visited_list_link; &#125; epitem对应一个接入到epoll的文件 12345678910111213141516171819202122232425// 对应于一个加入到epoll的文件 struct epitem &#123; // 挂载到eventpoll 的红黑树节点 struct rb_node rbn; // 挂载到eventpoll.rdllist 的节点 struct list_head rdllink; // 连接到ovflist 的指针 struct epitem *next; /* 文件描述符信息fd + file, 红黑树的key */ struct epoll_filefd ffd; /* Number of active wait queue attached to poll operations */ int nwait; // 当前文件的等待队列(eppoll_entry)列表 // 同一个文件上可能会监视多种事件, // 这些事件可能属于不同的wait_queue中 // (取决于对应文件类型的实现), // 所以需要使用链表 struct list_head pwqlist; // 当前epitem 的所有者 struct eventpoll *ep; /* List header used to link this item to the &amp;quot;struct file&amp;quot; items list */ struct list_head fllink; /* epoll_ctl 传入的用户数据 */ struct epoll_event event; &#125;; eppoll_entry与一个文件上的一个wait_queue_head 相关联，因为同一文件可能有多个等待的事件 12345678910struct eppoll_entry &#123; // List struct epitem.pwqlist struct list_head llink; // 所有者 struct epitem *base; // 添加到wait_queue 中的节点 wait_queue_t wait; // 文件wait_queue 头 wait_queue_head_t *whead; &#125;; epoll_create() 返回一个文件描述符，此描述符挂载在anon_inode_fs(匿名inode文件系统)的根目录下面 1234567//先进行判断size是否&gt;=0，若是则直接调用epoll_create1SYSCALL_DEFINE1(epoll_create, int, size)&#123; if (size &lt;= 0) //当 size&lt;=0 的时候直接直接退出 return -EINVAL; return sys_epoll_create1(0); //参数也没有意义&#125; SYSCALL_DEFINE1是宏用于定义有一个参数的系统调用函数，展开后为： int sys_epoll_create(int size)，即epoll_create系统调用的入口。 为何要用宏而不是直接声明，主要是因为系统调用的参数个数、传参方式都有严格限制，受限于寄存器数量的限制，(80x86下的)kernel限制系统调用最多有6个参数 epoll_create1() 真正的系统调用 1234567891011121314151617181920212223242526272829303132333435363738394041SYSCALL_DEFINE1(epoll_create1, int, flags)&#123; int error, fd; struct eventpoll *ep = NULL; struct file *file; if (flags &amp; ~EPOLL_CLOEXEC) return -EINVAL; /* * Create the internal data structure (\"struct eventpoll\"). */ // kzalloc(sizeof(*ep), GFP_KERNEL),用的是内核空间 error = ep_alloc(&amp;ep); if (error &lt; 0) return error; //获取尚未被使用的文件描述符，即描述符数组的槽位 fd = get_unused_fd_flags(O_RDWR | (flags &amp; O_CLOEXEC)); if (fd &lt; 0) &#123; error = fd; goto out_free_ep; &#125; // 在匿名inode文件系统中分配一个inode,并得到其file结构体 // 且file-&gt;f_op = &amp;eventpoll_fops // 且file-&gt;private_data = ep; file = anon_inode_getfile(\"[eventpoll]\", &amp;eventpoll_fops, ep, O_RDWR | (flags &amp; O_CLOEXEC));//在fd_array中获取一个槽位 if (IS_ERR(file)) &#123; error = PTR_ERR(file); goto out_free_fd; &#125; ep-&gt;file = file; // 将file填入到对应的文件描述符数组的槽里面(下图中files_struct的fd_array) fd_install(fd, file); return fd;out_free_fd: put_unused_fd(fd);out_free_ep: ep_free(ep); return error;&#125; 最后epoll_create生成的文件描述符如下 所有的epoll系统调用都是围绕eventpoll结构体做操作,现简要描述下其中的成员: 123456789101112131415161718192021222324252627/* * 此结构体存储在file-&gt;private_data中 */struct eventpoll &#123; // 自旋锁，在kernel内部用自旋锁加锁，就可以同时多线(进)程对此结构体进行操作 // 主要是保护ready_list spinlock_t lock; // 这个互斥锁是为了保证在eventloop使用对应的文件描述符的时候，文件描述符不会被移除掉 struct mutex mtx; // epoll_wait使用的等待队列，和进程唤醒有关 wait_queue_head_t wq; // file-&gt;poll使用的等待队列，和进程唤醒有关 wait_queue_head_t poll_wait; // 就绪的描述符队列 双向链表 struct list_head rdllist; // 通过红黑树来组织当前epoll关注的文件描述符 红黑树 struct rb_root rbr; // 在向用户空间传输就绪事件的时候，将同时发生事件的文件描述符链入到这个链表里面 struct epitem *ovflist; // 对应的user struct user_struct *user; // 对应的文件描述符 struct file *file; // 下面两个是用于环路检测的优化 int visited; struct list_head visited_list_link;&#125;; epoll_ctl(add) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146/* * The following function implements the controller interface for * the eventpoll file that enables the insertion/removal/change of * file descriptors inside the interest set. */SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)&#123; int error; int full_check = 0; struct fd f, tf; struct eventpoll *ep; struct epitem *epi; struct epoll_event epds; struct eventpoll *tep = NULL; error = -EFAULT; if (ep_op_has_event(op) &amp;&amp; copy_from_user(&amp;epds, event, sizeof(struct epoll_event))) goto error_return; error = -EBADF; f = fdget(epfd); if (!f.file) goto error_return; /* Get the \"struct file *\" for the target file */ tf = fdget(fd); if (!tf.file) goto error_fput; /* The target file descriptor must support poll */ error = -EPERM; if (!tf.file-&gt;f_op-&gt;poll) goto error_tgt_fput; /* Check if EPOLLWAKEUP is allowed */ if (ep_op_has_event(op)) ep_take_care_of_epollwakeup(&amp;epds); /* * We have to check that the file structure underneath the file descriptor * the user passed to us _is_ an eventpoll file. And also we do not permit * adding an epoll file descriptor inside itself. */ error = -EINVAL; if (f.file == tf.file || !is_file_epoll(f.file)) goto error_tgt_fput; /* * At this point it is safe to assume that the \"private_data\" contains * our own data structure. */ ep = f.file-&gt;private_data; /* * When we insert an epoll file descriptor, inside another epoll file * descriptor, there is the change of creating closed loops, which are * better be handled here, than in more critical paths. While we are * checking for loops we also determine the list of files reachable * and hang them on the tfile_check_list, so we can check that we * haven't created too many possible wakeup paths. * * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when * the epoll file descriptor is attaching directly to a wakeup source, * unless the epoll file descriptor is nested. The purpose of taking the * 'epmutex' on add is to prevent complex toplogies such as loops and * deep wakeup paths from forming in parallel through multiple * EPOLL_CTL_ADD operations. */ /* 校验epfd是否是epoll的描述符 */ // 此处的互斥锁是为了防止并发调用epoll_ctl,即保护内部数据结构 // 不会被并发的添加修改删除破坏 mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (op == EPOLL_CTL_ADD) &#123; if (!list_empty(&amp;f.file-&gt;f_ep_links) || is_file_epoll(tf.file)) &#123; full_check = 1; mutex_unlock(&amp;ep-&gt;mtx); mutex_lock(&amp;epmutex); if (is_file_epoll(tf.file)) &#123; error = -ELOOP; if (ep_loop_check(ep, tf.file) != 0) &#123; clear_tfile_check_list(); goto error_tgt_fput; &#125; &#125; else list_add(&amp;tf.file-&gt;f_tfile_llink, &amp;tfile_check_list); mutex_lock_nested(&amp;ep-&gt;mtx, 0); if (is_file_epoll(tf.file)) &#123; tep = tf.file-&gt;private_data; mutex_lock_nested(&amp;tep-&gt;mtx, 1); &#125; &#125; &#125; /* * Try to lookup the file inside our RB tree, Since we grabbed \"mtx\" * above, we can be sure to be able to use the item looked up by * ep_find() till we release the mutex. */ epi = ep_find(ep, tf.file, fd); error = -EINVAL; switch (op) &#123; case EPOLL_CTL_ADD: if (!epi) &#123; epds.events |= POLLERR | POLLHUP; // 插入到红黑树中 error = ep_insert(ep, &amp;epds, tf.file, fd, full_check); &#125; else error = -EEXIST; if (full_check) clear_tfile_check_list(); break; case EPOLL_CTL_DEL: if (epi) error = ep_remove(ep, epi); else error = -ENOENT; break; case EPOLL_CTL_MOD: if (epi) &#123; epds.events |= POLLERR | POLLHUP; error = ep_modify(ep, epi, &amp;epds); &#125; else error = -ENOENT; break; &#125; if (tep != NULL) mutex_unlock(&amp;tep-&gt;mtx); mutex_unlock(&amp;ep-&gt;mtx);error_tgt_fput: if (full_check) mutex_unlock(&amp;epmutex); fdput(tf);error_fput: fdput(f);error_return: return error;&#125; ep_insert 在ep_insert中初始化了epitem，然后初始化了本文关注的焦点,即事件就绪时候的回调函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129static int ep_insert(struct eventpoll *ep, struct epoll_event *event, struct file *tfile, int fd, int full_check)&#123; int error, revents, pwake = 0; unsigned long flags; long user_watches; struct epitem *epi; struct ep_pqueue epq; user_watches = atomic_long_read(&amp;ep-&gt;user-&gt;epoll_watches); if (unlikely(user_watches &gt;= max_user_watches)) return -ENOSPC; if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL))) return -ENOMEM; /* Item initialization follow here ... */ INIT_LIST_HEAD(&amp;epi-&gt;rdllink); INIT_LIST_HEAD(&amp;epi-&gt;fllink); INIT_LIST_HEAD(&amp;epi-&gt;pwqlist); epi-&gt;ep = ep; ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd); epi-&gt;event = *event; epi-&gt;nwait = 0; epi-&gt;next = EP_UNACTIVE_PTR; if (epi-&gt;event.events &amp; EPOLLWAKEUP) &#123; error = ep_create_wakeup_source(epi); if (error) goto error_create_wakeup_source; &#125; else &#123; RCU_INIT_POINTER(epi-&gt;ws, NULL); &#125; /* Initialize the poll table using the queue callback */ /* 初始化epitem */ // &amp;epq.pt-&gt;qproc = ep_ptable_queue_proc epq.epi = epi; init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc); /* * Attach the item to the poll hooks and get current event bits. * We can safely use the file* here because its usage count has * been increased by the caller of this function. Note that after * this operation completes, the poll callback can start hitting * the new item. */ revents = ep_item_poll(epi, &amp;epq.pt); // 在这里将回调函数注入 /* * We have to check if something went wrong during the poll wait queue * install process. Namely an allocation for a wait queue failed due * high memory pressure. */ error = -ENOMEM; if (epi-&gt;nwait &lt; 0) goto error_unregister; /* Add the current item to the list of active epoll hook for this file */ spin_lock(&amp;tfile-&gt;f_lock); list_add_tail_rcu(&amp;epi-&gt;fllink, &amp;tfile-&gt;f_ep_links); spin_unlock(&amp;tfile-&gt;f_lock); /* * Add the current item to the RB tree. All RB tree operations are * protected by \"mtx\", and ep_insert() is called with \"mtx\" held. */ ep_rbtree_insert(ep, epi); /* now check if we've created too many backpaths */ error = -EINVAL; if (full_check &amp;&amp; reverse_path_check()) goto error_remove_epi; /* We have to drop the new item inside our item list to keep track of it */ spin_lock_irqsave(&amp;ep-&gt;lock, flags); /* If the file is already \"ready\" we drop it inside the ready list */ // 如果当前有事件已经就绪，那么一开始就会被加入到ready list // 例如可写事件 // 另外，在tcp内部ack之后调用tcp_check_space,最终调用sock_def_write_space来唤醒对应的epoll_wait下的进程 if ((revents &amp; event-&gt;events) &amp;&amp; !ep_is_linked(&amp;epi-&gt;rdllink)) &#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); ep_pm_stay_awake(epi); /* Notify waiting tasks that events are available */ // wake_up ep对应在epoll_wait下的进程 if (waitqueue_active(&amp;ep-&gt;wq)) wake_up_locked(&amp;ep-&gt;wq); if (waitqueue_active(&amp;ep-&gt;poll_wait)) pwake++; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); atomic_long_inc(&amp;ep-&gt;user-&gt;epoll_watches); /* We have to call this outside the lock */ if (pwake) ep_poll_safewake(&amp;ep-&gt;poll_wait); return 0;error_remove_epi: spin_lock(&amp;tfile-&gt;f_lock); list_del_rcu(&amp;epi-&gt;fllink); spin_unlock(&amp;tfile-&gt;f_lock); rb_erase(&amp;epi-&gt;rbn, &amp;ep-&gt;rbr);error_unregister: ep_unregister_pollwait(ep, epi); /* * We need to do this because an event could have been arrived on some * allocated wait queue. Note that we don't care about the ep-&gt;ovflist * list, since that is used/cleaned only inside a section bound by \"mtx\". * And ep_insert() is called with \"mtx\" held. */ spin_lock_irqsave(&amp;ep-&gt;lock, flags); if (ep_is_linked(&amp;epi-&gt;rdllink)) list_del_init(&amp;epi-&gt;rdllink); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); wakeup_source_unregister(ep_wakeup_source(epi));error_create_wakeup_source: kmem_cache_free(epi_cache, epi); return error;&#125; 其中 tfile-&gt;f_op-&gt;poll的实现，向kernel更底层注册回调函数的是tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt)这一句，我们来看一下对于对应的socket文件描述符，其fd=&gt;file-&gt;f_op-&gt;poll的初始化过程 1234// 将accept后的事件加入到对应的epoll fd中int client_fd = accept(listen_fd,(struct sockaddr *)&amp;client_addr,&amp;client_len)));// 将连接描述符注册到对应的worker里面epoll_ctl(reactor-&gt;client_fd,EPOLL_CTL_ADD,epifd,&amp;event); epoll_wait 12345678910111213141516171819202122232425262728293031323334353637383940414243SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout)&#123; int error; struct fd f; struct eventpoll *ep; &#x2F;* The maximum number of event must be greater than zero *&#x2F; if (maxevents &lt;&#x3D; 0 || maxevents &gt; EP_MAX_EVENTS) return -EINVAL; &#x2F;* Verify that the area passed by the user is writeable *&#x2F; if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct epoll_event))) return -EFAULT; &#x2F;* Get the &quot;struct file *&quot; for the eventpoll file *&#x2F; f &#x3D; fdget(epfd); if (!f.file) return -EBADF; &#x2F;* * We have to check that the file structure underneath the fd * the user passed to us _is_ an eventpoll file. *&#x2F; error &#x3D; -EINVAL; if (!is_file_epoll(f.file)) goto error_fput; &#x2F;* * At this point it is safe to assume that the &quot;private_data&quot; contains * our own data structure. *&#x2F; ep &#x3D; f.file-&gt;private_data; &#x2F;* Time to fish for events ... *&#x2F; &#x2F;* 检查epfd是否是epoll\\_create创建的fd *&#x2F; &#x2F;&#x2F; 调用ep_poll error &#x3D; ep_poll(ep, events, maxevents, timeout);error_fput: fdput(f); return error;&#125; 紧接着，我们看下ep_poll函数: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980tatic int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout)&#123; int res = 0, eavail, timed_out = 0; unsigned long flags; long slack = 0; wait_queue_t wait; ktime_t expires, *to = NULL; if (timeout &gt; 0) &#123; struct timespec end_time = ep_set_mstimeout(timeout); slack = select_estimate_accuracy(&amp;end_time); to = &amp;expires; *to = timespec_to_ktime(end_time); &#125; else if (timeout == 0) &#123; /* * Avoid the unnecessary trip to the wait queue loop, if the * caller specified a non blocking operation. */ timed_out = 1; spin_lock_irqsave(&amp;ep-&gt;lock, flags); goto check_events; &#125;fetch_events: // 获取spinlock spin_lock_irqsave(&amp;ep-&gt;lock, flags); if (!ep_events_available(ep)) &#123; /* * We don't have any available event to return to the caller. * We need to sleep here, and we will be wake up by * ep_poll_callback() when events will become available. */ // 将当前task_struct写入到waitqueue中以便唤醒 // wq_entry-&gt;func = default_wake_function; init_waitqueue_entry(&amp;wait, current); __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); for (;;) &#123; /* * We don't want to sleep if the ep_poll_callback() sends us * a wakeup in between. That's why we set the task state * to TASK_INTERRUPTIBLE before doing the checks. */ set_current_state(TASK_INTERRUPTIBLE); if (ep_events_available(ep) || timed_out) break; if (signal_pending(current)) &#123; res = -EINTR; break; &#125; spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) timed_out = 1; spin_lock_irqsave(&amp;ep-&gt;lock, flags); &#125; __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); set_current_state(TASK_RUNNING); &#125;check_events: /* Is it worth to try to dig for events ? */ eavail = ep_events_available(ep); spin_unlock_irqrestore(&amp;ep-&gt;lock, flags); /* * Try to transfer events to user space. In case we get 0 events and * there's still timeout left over, we go trying again in search of * more luck. */ if (!res &amp;&amp; eavail &amp;&amp; !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; !timed_out) goto fetch_events; return res;&#125; ep_send_events ep_send_events函数主要就是调用了ep_scan_ready_list,顾名思义ep_scan_ready_list就是扫描就绪列表: 12345678910111213141516static int ep_scan_ready_list(struct eventpoll *ep, int (*sproc)(struct eventpoll *, struct list_head *, void *), void *priv, int depth)&#123; ... // 将epfd的rdllist链入到txlist list_splice_init(&amp;ep-&gt;rdllist, &amp;txlist); ... /* sproc = ep_send_events_proc */ error = (*sproc)(ep, &amp;txlist, priv); ... // 处理ovflist,即在上面sproc过程中又到来的事件 ...&#125; 其主要调用了ep_send_events_proc: 123456789101112131415161718192021222324252627static int ep_send_events_proc(struct eventpoll *ep, struct list_head *head, void *priv)&#123; for (eventcnt = 0, uevent = esed-&gt;events; !list_empty(head) &amp;&amp; eventcnt &lt; esed-&gt;maxevents;) &#123; // 遍历ready list epi = list_first_entry(head, struct epitem, rdllink); list_del_init(&amp;epi-&gt;rdllink); // readylist只是表明当前epi有事件，具体的事件信息还是得调用对应file的poll // 这边的poll即是tcp_poll,根据tcp本身的信息设置掩码(mask)等信息 &amp; 上兴趣事件掩码，则可以得知当前事件是否是epoll_wait感兴趣的事件 revents = epi-&gt;ffd.file-&gt;f_op-&gt;poll(epi-&gt;ffd.file, NULL) &amp; epi-&gt;event.events; if(revents)&#123; /* 将event放入到用户空间 */ /* 处理ONESHOT逻辑 */ // 如果不是边缘触发，则将当前的epi重新加回到可用列表中，这样就可以下一次继续触发poll,如果下一次poll的revents不为0，那么用户空间依旧能感知 */ else if (!(epi-&gt;event.events &amp; EPOLLET))&#123; list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist); &#125; /* 如果是边缘触发，那么就不加回可用列表，因此只能等到下一个可用事件触发的时候才会将对应的epi放到可用列表里面*/ eventcnt++ &#125; /* 如poll出来的revents事件epoll_wait不感兴趣(或者本来就没有事件)，那么也不会加回到可用列表 */ ...... &#125; return eventcnt;&#125; eventpoll_init 文件系统初始化 1234567891011121314151617181920212223static int __init eventpoll_init(void) &#123; struct sysinfo si; si_meminfo(&amp;si); // 限制可添加到epoll的最多的描述符数量 max_user_watches = (((si.totalram - si.totalhigh) / 25) &lt;&lt; PAGE_SHIFT) / EP_ITEM_COST; BUG_ON(max_user_watches &lt; 0); // 初始化递归检查队列 ep_nested_calls_init(&amp;poll_loop_ncalls); ep_nested_calls_init(&amp;poll_safewake_ncalls); ep_nested_calls_init(&amp;poll_readywalk_ncalls); // epoll 使用的slab分配器分别用来分配epitem和eppoll_entry epi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem), 0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL); pwq_cache = kmem_cache_create(\"eventpoll_pwq\", sizeof(struct eppoll_entry), 0, SLAB_PANIC, NULL); return 0; &#125; 1.epoll_create中的size参数有什么作用？ 答：size这个参数其实没有任何用处，它只是为了保持兼容，因为之前的fd使用hash表保存，size表示hash表的大小，而现在使用红黑树保存，所以size就没用了。 2.LT和ET的区别（源码级别）？ 答：在源码中，两种模式的区别是一个if判断语句，通过ep_send_events_proc()函数实现，如果没有标上EPOLLET(即默认的LT)且“事件被关注”的fd就会被重新放回了rdllist。那么下次epoll_wait当然会又把rdllist里的fd拿来拷给用户了 参考链接 https://www.cnblogs.com/Yunya-Cnblogs/p/13246517.html https://mp.weixin.qq.com/s/4xqJGsjiSxRHdI80OERHmQ https://blog.csdn.net/baiye_xing/article/details/76352935 https://zhuanlan.zhihu.com/p/63179839 https://my.oschina.net/alchemystar/blog/3008840","categories":[{"name":"Linux Depth","slug":"Linux-Depth","permalink":"http://xboom.github.io/categories/Linux-Depth/"}],"tags":[{"name":"Event","slug":"Event","permalink":"http://xboom.github.io/tags/Event/"}]},{"title":"Go入门10-defer","slug":"Go/Go入门10-defer","date":"2021-07-03T15:07:15.000Z","updated":"2021-07-03T15:07:43.628Z","comments":true,"path":"2021/07/03/Go/Go入门10-defer/","link":"","permalink":"http://xboom.github.io/2021/07/03/Go/Go%E5%85%A5%E9%97%A810-defer/","excerpt":"","text":"defer原理 defer语句会进入一个栈，在函数return前按先进后出的顺序执行(原因是后面定义的函数可能会依赖前面的资源) 在defer函数定义时，对外部变量的引用是有两种方式的: 作为函数参数，则在defer定义时就把值传递给defer，并被cache起来； 作为闭包引用，则会在defer函数真正调用时根据整个上下文确定当前的值。 defer后的语句在执行的时候，函数调用的参数会被复制。如果此变量是一个“值”，那么就和定义的时候是一致的。如果此变量是一个“引用”，那么就可能和定义的时候不一致。 123456789func main() &#123; var whatever [3]struct&#123;&#125; for i := range whatever &#123; defer func() &#123; fmt.Println(i) &#125;() &#125;&#125; 执行结果为： 123222 因为闭包是根据上下文确定当前的值 123456789101112131415type number intfunc (n number) print() &#123; fmt.Println(n) &#125;func (n *number) pprint() &#123; fmt.Println(*n) &#125;func main() &#123; var n number defer n.print() defer n.pprint() defer func() &#123; n.print() &#125;() defer func() &#123; n.pprint() &#125;() n &#x3D; 3&#125; 执行结果是： 12343330 第四个defer语句是闭包，引用外部函数的n, 最终结果是3 第三个defer语句是闭包同第三个 第二个defer语句，n是引用，最终求值是3 第一个defer语句，对n直接求职，开始的时候n=0，所以最后是0 defer返回 返回语句 return xxx编译后编程三条命令 返回值 = xxx 调用defer函数 空的return 1234567func f() (r int) &#123; t :&#x3D; 5 defer func() &#123; t &#x3D; t + 5 &#125;() return t&#125; 可以看成 1234567891011121314func f() (r int) &#123; t :&#x3D; 5 &#x2F;&#x2F; 1. 赋值指令 r &#x3D; t &#x2F;&#x2F; 2. defer被插入到赋值与返回之间执行，这个例子中返回值r没被修改过 func() &#123; t &#x3D; t + 5 &#125; &#x2F;&#x2F; 3. 空的return指令 return&#125; 结果为5 第二例子： 123456func f() (r int) &#123; defer func(r int) &#123; r &#x3D; r + 5 &#125;(r) return 1&#125; 可以看成： 123456789101112func f() (r int) &#123; &#x2F;&#x2F; 1. 赋值 r &#x3D; 1 &#x2F;&#x2F; 2. 这里改的r是之前传值传进去的r，不会改变要返回的那个r值 func(r int) &#123; r &#x3D; r + 5 &#125;(r) &#x2F;&#x2F; 3. 空的return return&#125; 结果为 1 参考链接 https://qcrao.com/2019/02/12/how-to-keep-off-trap-of-defer/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门8-Channel","slug":"Go/Go入门8-Channel","date":"2021-07-03T07:41:44.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/07/03/Go/Go入门8-Channel/","link":"","permalink":"http://xboom.github.io/2021/07/03/Go/Go%E5%85%A5%E9%97%A88-Channel/","excerpt":"","text":"channel底层数据结构 12345678910111213141516171819202122232425type hchan struct &#123; // chan 里元素数量 qcount uint // chan 底层循环数组的长度 dataqsiz uint // 指向底层循环数组的指针 // 只针对有缓冲的 channel buf unsafe.Pointer // chan 中元素大小 elemsize uint16 // chan 是否被关闭的标志 closed uint32 // chan 中元素类型 elemtype *_type // element type // 已发送元素在循环数组中的索引(第几个位置 1 ~ len) sendx uint // send index // 已接收元素在循环数组中的索引第几个位置 1 ~ len) recvx uint // receive index // 等待接收的 goroutine 队列 recvq waitq // list of recv waiters // 等待发送的 goroutine 队列 sendq waitq // list of send waiters // 保护 hchan 中所有字段 lock mutex&#125; 其中需要注意的是： buf指向底层循环数组，如果是非缓冲则指向hchan地址 sendq，recvq 分别表示被阻塞的 goroutine，这些 goroutine 由于尝试读取 channel 或向 channel 发送数据而被阻塞(部分通过直接复制memmove的方式传输未被阻塞，也就不在链表中) waitq 是 sudog 的一个双向链表，而 sudog 实际上是对 goroutine 的一个封装 1234type waitq struct &#123; first *sudog last *sudog&#125; lock 用来保证每个读 channel 或写 channel 的操作都是原子的 chan初创建 chan的创建分为含有缓冲区和非缓冲区，都通过 func makechan(t *chantype, size int64) *hchan实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func makechan64(t *chantype, size int64) *hchan &#123; if int64(int(size)) != size &#123; panic(plainError(\"makechan: size out of range\")) &#125; return makechan(t, int(size))&#125;func makechan(t *chantype, size int) *hchan &#123; elem := t.elem //1. 安全检查 if elem.size &gt;= 1&lt;&lt;16 &#123; throw(\"makechan: invalid channel element type\") &#125; if hchanSize%maxAlign != 0 || elem.align &gt; maxAlign &#123; throw(\"makechan: bad alignment\") &#125; //2. 元素分配是否溢出 mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem &gt; maxAlloc-hchanSize || size &lt; 0 &#123; panic(plainError(\"makechan: size out of range\")) &#125; var c *hchan switch &#123; case mem == 0: //a. 非缓冲或者元素大小为0(struct&#123;&#125;) // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. c.buf = c.raceaddr() case elem.ptrdata == 0: //b. 不含有指针，buf指向数组起始阶段 c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: //c. 元素含有指针 c = new(hchan) c.buf = mallocgc(mem, elem, true) &#125; c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(&amp;c.lock, lockRankHchan) ... return c&#125; 以上有几点注意： 针对创建不同的chan初始化过程 非缓冲chan 只需要分配一个hchanSize 缓冲区chan 不含有指针，则分配一个连续的内存 缓冲区chan 含有指针，分别分配hchan和 指针需要的内存大小 chan 分配在堆中，返回一个指针*hchan sendq 和 recvq 链表由goroutine初始化时候创建，不在这里创建 chan接收 接收的操作有两种写法 123456789// entry points for &lt;- c from compiled codefunc chanrecv1(c *hchan, elem unsafe.Pointer) &#123; chanrecv(c, elem, true)&#125;func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) &#123; _, received = chanrecv(c, elem, true) return&#125; received 反应channel是否被关闭 接收值会放到elem所指向的指针，如果忽略接收值，则elem为nil 第三个参数 都使用 true 表示阻塞模式， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132/* 1. 如果 ep 是 nil，说明忽略了接收值。 2. 如果 block == false，即非阻塞型接收，在没有数据可接收的情况下，返回 (false, false) 3. 否则，如果 c 处于关闭状态，将 ep 指向的地址清零，返回 (true, false)// 否则，用返回值填充 ep 指向的内存地址。返回 (true, true)// 如果 ep 非空，则应该指向堆或者函数调用者的栈*///判断hchan是否有数据func empty(c *hchan) bool &#123; // dataqsiz 一旦初始化chan就不会变化 if c.dataqsiz == 0 &#123; //如果是非缓冲，当存储发送挂起协程的sendq链表为空，表示没有数据 return atomic.Loadp(unsafe.Pointer(&amp;c.sendq.first)) == nil &#125; //缓冲区则判断是否有数据 return atomic.Loaduint(&amp;c.qcount) == 0&#125;// chanbuf(c, i) 指向c的缓冲区第i个元素func chanbuf(c *hchan, i uint) unsafe.Pointer &#123; return add(c.buf, uintptr(i)*uintptr(c.elemsize))&#125;func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; //忽略debug //如果是 nil的channel if c == nil &#123; //如果不阻塞，直接返回 (false, false) if !block &#123; return &#125; //否则，接收一个nil的channel, goroutine 挂起 gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) //不会执行到这里 throw(\"unreachable\") &#125; // 非阻塞模式下，快速检查不用获取锁 if !block &amp;&amp; empty(c) &#123; if atomic.Load(&amp;c.closed) == 0 &#123; //如果chan没有关闭，返回(false, false) return &#125; //这里又做了一次为空判断，防止在关闭检查的时候收到的数据 if empty(c) &#123; // The channel is irreversibly closed and empty. if ep != nil &#123; //如果为空，则返回一个默认值 typedmemclr(c.elemtype, ep) &#125; return true, false &#125; &#125; lock(&amp;c.lock) //获取原子锁 //如果关闭了，且不含有缓冲数据，则ep指向默认值，返回(true, false) if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; unlock(&amp;c.lock) if ep != nil &#123; typedmemclr(c.elemtype, ep) &#125; return true, false &#125; //则获取sendq中阻塞的发送协程进行接收 if sg := c.sendq.dequeue(); sg != nil &#123; //找到一个等待的发送人。 如果缓冲区大小为 0，则接收值直接来自发送者。 //否则，从队列头接收并将发送者的值添加到队列的尾部 recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; //如果没有发送者，且当前缓冲区有数据， //则将数据复制给ep，且清理对应位置缓冲区数据，qcount-- if c.qcount &gt; 0 &#123; //1. 获取对应缓冲区数据 qp := chanbuf(c, c.recvx) //2. 将数据复制给ep if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; //3. 清理对应的缓冲区数据 typedmemclr(c.elemtype, qp) //4. 接收索引+1，如果 等于缓冲区大小，表示结束，从头开始 c.recvx++ if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; //4. 缓冲区数目-1 并释放原子锁 c.qcount-- unlock(&amp;c.lock) return true, true &#125; if !block &#123; unlock(&amp;c.lock) return false, false &#125; //没有数据，也没有发送者，则将要阻塞这个协程 gp := getg() //获取当前协程指针 mysg := acquireSudog() // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. //保存代接收数据的地址 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.isSelect = false mysg.c = c gp.param = nil //进入等待链表中 c.recvq.enqueue(mysg) atomic.Store8(&amp;gp.parkingOnChan, 1) //将当前goroutine 挂起 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2) //当goroutine被唤起 if mysg != gp.waiting &#123; //当前协程等待的不是创建的 mysg throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil gp.activeStackChans = false closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed&#125; 需要注意的是 同一个 chan 不能被重新开启 在缓冲区chan buf满了的情况下，发送协程阻塞，则接收者还是会优先处理缓冲区数据 12345678910111213141516171819202122232425262728293031323334353637383940//如果是非缓冲型的，就直接从发送者的栈拷贝到接收者的栈func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) &#123; src := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125;//接收数据的时候发现有协程阻塞func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; if c.dataqsiz == 0 &#123; if ep != nil &#123; //非缓冲直接从sender拷贝到ep recvDirect(c.elemtype, sg, ep) &#125; &#125; else &#123;//非缓冲区 //1. 获取缓冲区数据 qp := chanbuf(c, c.recvx) // 将数据从缓冲区拷贝给receiver if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 将sender的数据拷贝到缓冲区 typedmemmove(c.elemtype, qp, sg.elem) c.recvx++ //索引+1，则刚才加了数据需要循环一遍才能拿到 if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz &#125; sg.elem = nil gp := sg.g // 解锁 unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒发送的 goroutine。需要等到调度器的光临 goready(gp, skip+1)&#125; chan发送 ch &lt;- 3 最终会转换成 chansend 函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105func full(c *hchan) bool &#123; if c.dataqsiz == 0 &#123; //如果是非缓冲区，且没有接收协程，则表示满了 return c.recvq.first == nil &#125; //如果是缓冲区，当缓冲数据等于缓冲区大小 则表示满了 return c.qcount == c.dataqsiz&#125;func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; //如果为空 if c == nil &#123; if !block &#123; //非阻塞模式下 return false &#125; //阻塞模式下，直接挂起 gopark(nil, nil, waitReasonChanSendNilChan, traceEvGoStop, 2) throw(\"unreachable\") &#125; //... // 如果 channel 未关闭且 channel 没有多余的缓冲空间。直接返回false if !block &amp;&amp; c.closed == 0 &amp;&amp; full(c) &#123; return false &#125; //1. 加锁 lock(&amp;c.lock) //如果channel 关闭，则直接panic if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError(\"send on closed channel\")) &#125; //如果接收协程阻塞存在，说明缓冲区没有数据，直接将sender数据拷贝给receiver if sg := c.recvq.dequeue(); sg != nil &#123; // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; //如果有缓冲空间 if c.qcount &lt; c.dataqsiz &#123; //1. 直接获取缓冲发送位置 qp := chanbuf(c, c.sendx) //2. 将数据直接拷贝进缓冲区 typedmemmove(c.elemtype, qp, ep) //3. 发送位置+1(表示下一个进来的数据存放的位置) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; //3. 数量+1 c.qcount++ unlock(&amp;c.lock) return true &#125; //非阻塞模式直接返回false if !block &#123; unlock(&amp;c.lock) return false &#125; //阻塞当前协程 gp := getg() mysg := acquireSudog() // No stack splits between assigning elem and enqueuing mysg // on gp.waiting where copystack can find it. mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.isSelect = false mysg.c = c gp.waiting = mysg gp.param = nil c.sendq.enqueue(mysg) //将协程加入到等待队列中 atomic.Store8(&amp;gp.parkingOnChan, 1) //挂起当前协程 gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2) // 确保正在发送的值保持活动状态，直到接收者复制它 KeepAlive(ep) // 被唤起 if mysg != gp.waiting &#123; throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil gp.activeStackChans = false if gp.param == nil &#123; if c.closed == 0 &#123; throw(\"chansend: spurious wakeup\") &#125; // 被唤醒后，channel 关闭了。坑爹啊，panic panic(plainError(\"send on closed channel\")) &#125; gp.param = nil mysg.c = nil releaseSudog(mysg) return true&#125; 需要注意的是： KeepAlive 的作用和原理是? 123456789101112131415161718192021222324252627282930313233func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; //sg.elem 指向接收到的值存放的位置，如 val &lt;- ch，指的就是 &amp;val if sg.elem != nil &#123; // 直接拷贝内存（从发送者到接收者） sendDirect(c.elemtype, sg, ep) sg.elem = nil &#125; // sudog 上绑定的 goroutine gp := sg.g unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒接收的 goroutine. skip 和打印栈相关，暂时不理会 goready(gp, skip+1)&#125;// 向一个非缓冲型的 channel 发送数据、从一个无元素的（非缓冲型或缓冲型但空）的 channel// 接收数据，都会导致一个 goroutine 直接操作另一个 goroutine 的栈// 由于 GC 假设对栈的写操作只能发生在 goroutine 正在运行中并且由当前 goroutine 来写// 所以这里实际上违反了这个假设。可能会造成一些问题，所以需要用到写屏障来规避func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) &#123; // src 在当前 goroutine 的栈上，dst 是另一个 goroutine 的栈 // 直接进行内存\"搬迁\" // 如果目标地址的栈发生了栈收缩，当我们读出了 sg.elem 后 // 就不能修改真正的 dst 位置的值了 // 因此需要在读和写之前加上一个屏障 dst := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125; 关闭chan 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func closechan(c *hchan) &#123; if c == nil &#123; //关闭一个空chan会panic panic(plainError(\"close of nil channel\")) &#125; lock(&amp;c.lock) if c.closed != 0 &#123; //关闭一个关闭的chan会panic unlock(&amp;c.lock) panic(plainError(\"close of closed channel\")) &#125; c.closed = 1 //修改chan状态为关闭 var glist gList // release all readers for &#123; //1. 所有接收协程出链表 sg := c.recvq.dequeue() if sg == nil &#123; break &#125; //2. 如果元素不为空，则清除并设置为空 if sg.elem != nil &#123; typedmemclr(c.elemtype, sg.elem) sg.elem = nil &#125; if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil glist.push(gp) &#125; // release all writers (they will panic) for &#123; //从发送链表中获取协程 sg := c.sendq.dequeue() if sg == nil &#123; break &#125; sg.elem = nil if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; gp := sg.g gp.param = nil //形成协程链表 glist.push(gp) &#125; unlock(&amp;c.lock) // Ready all Gs now that we've dropped the channel lock. for !glist.empty() &#123; // 取最后一个 gp := glist.pop() gp.schedlink = 0 //环形协程 goready(gp, 3) &#125;&#125; chan应用 针对不同的chan会有不同的效果： 控制并发数 12345678910111213var limit = make(chan int, 3)func main() &#123; // ………… for _, w := range work &#123; go func() &#123; limit &lt;- 1 w() &lt;-limit &#125;() &#125; // …………&#125; 参考链接 Go问题集 https://qcrao.com/2019/07/22/dive-into-go-channel/","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go编程模式","slug":"Go/Go编程模式","date":"2021-06-18T17:05:45.000Z","updated":"2021-06-21T15:44:16.885Z","comments":true,"path":"2021/06/19/Go/Go编程模式/","link":"","permalink":"http://xboom.github.io/2021/06/19/Go/Go%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"深度比较 12345678910111213141516171819202122import ( \"fmt\" \"reflect\")func main() &#123; v1 := data&#123;&#125; v2 := data&#123;&#125; fmt.Println(\"v1 == v2:\",reflect.DeepEqual(v1,v2)) //prints: v1 == v2: true m1 := map[string]string&#123;\"one\": \"a\",\"two\": \"b\"&#125; m2 := map[string]string&#123;\"two\": \"b\", \"one\": \"a\"&#125; fmt.Println(\"m1 == m2:\",reflect.DeepEqual(m1, m2)) //prints: m1 == m2: true s1 := []int&#123;1, 2, 3&#125; s2 := []int&#123;1, 2, 3&#125; fmt.Println(\"s1 == s2:\",reflect.DeepEqual(s1, s2)) //prints: s1 == s2: true&#125; 接口编程 1234567891011121314151617type Shape interface &#123; Sides() int Area() int&#125;type Square struct &#123; len int&#125;func (s* Square) Sides() int &#123; return 4&#125;func main() &#123; s := Square&#123;len: 5&#125; fmt.Printf(\"%d\\n\",s.Sides())&#125;var _ Shape = (*Square)(nil) //接口实现校验 声明一个 _ 变量（没人用）会把一个 nil 的空指针从 Square 转成 Shape，这样，如果没有实现完相关的接口方法，编译器就会报错： cannot use (*Square)(nil) (type *Square) as type Shape in assignment: *Square does not implement Shape (missing Area method) 性能 如果需要把数字转换成字符串，使用 strconv.Itoa() 比 fmt.Sprintf() 要快一倍左右。 尽可能避免把String转成[]Byte ，这个转换会导致性能下降。 如果在 for-loop 里对某个 Slice 使用 append()，请先把 Slice 的容量扩充到位，这样可以避免内存重新分配以及系统自动按 2 的 N 次方幂进行扩展但又用不到的情况，从而避免浪费内存。 使用StringBuffer 或是StringBuild 来拼接字符串，性能会比使用 + 或 +=高三到四个数量级。 尽可能使用并发的 goroutine，然后使用 sync.WaitGroup 来同步分片操作。 避免在热代码中进行内存分配，这样会导致 gc 很忙。尽可能使用 sync.Pool 来重用对象。 使用 lock-free 的操作，避免使用 mutex，尽可能使用 sync/Atomic包（关于无锁编程的相关话题，可参看《无锁队列实现》或《无锁 Hashmap 实现》）。 使用 I/O 缓冲，I/O 是个非常非常慢的操作，使用 bufio.NewWrite() 和 bufio.NewReader() 可以带来更高的性能。 对于在 for-loop 里的固定的正则表达式，一定要使用 regexp.Compile() 编译正则表达式。性能会提升两个数量级。 需要更高性能的协议，就要考虑使用 protobuf 或 msgp 而不是 JSON，因为 JSON 的序列化和反序列化里使用了反射。 使用 Map 的时候，使用整型的 key 会比字符串的要快，因为整型比较比字符串比较要快 错误检查 1234567891011121314151617181920func parse(r io.Reader) (*Point, error) &#123; var p Point if err := binary.Read(r, binary.BigEndian, &amp;p.Longitude); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.Latitude); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.Distance); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationGain); err != nil &#123; return nil, err &#125; if err := binary.Read(r, binary.BigEndian, &amp;p.ElevationLoss); err != nil &#123; return nil, err &#125;&#125; 太多的 if err != nil {} 12345678910111213141516171819202122func parse(r io.Reader) (*Point, error) &#123; var p Point var err error read := func(data interface&#123;&#125;) &#123; if err != nil &#123; //如果失败则不执行 return &#125; err = binary.Read(r, binary.BigEndian, data) &#125; read(&amp;p.Longitude) read(&amp;p.Latitude) read(&amp;p.Distance) read(&amp;p.ElevationGain) read(&amp;p.ElevationLoss) if err != nil &#123; return &amp;p, err &#125; return &amp;p, nil&#125; 上述代码存在一个变量err和一个内部函数 read 12345678910111213141516171819202122232425262728type Reader struct &#123; r io.Reader err error&#125;func (r *Reader) read(data interface&#123;&#125;) &#123; if r.err == nil &#123; r.err = binary.Read(r.r, binary.BigEndian, data) &#125;&#125;func parse(input io.Reader) (*Point, error) &#123; var p Point r := Reader&#123;r: input&#125; r.read(&amp;p.Longitude) r.read(&amp;p.Latitude) r.read(&amp;p.Distance) r.read(&amp;p.ElevationGain) r.read(&amp;p.ElevationLoss) if r.err != nil &#123; return nil, r.err &#125; return &amp;p, nil&#125; 主要应用于相同业务处理场景 错误包装 1234567891011121314import \"github.com/pkg/errors\" //第三方库//错误包装if err != nil &#123; return errors.Wrap(err, \"read failed\")&#125;// Cause接口switch err := errors.Cause(err).(type) &#123;case *MyError: // handle specificallydefault: // unknown error&#125; 多参数 1234567891011121314func main() &#123; slice1 := make([]interface&#123;&#125;, 0) slice2 := make([]interface&#123;&#125;, 1) //不要这样使用 会初始化1的nil slice1 = append(slice1, 1, \"2\") slice2 = append(slice2, 1, \"2\") test(slice1) //args [[1 2]] num:1 test(slice1...) //args [1 2] num:2 test(slice2) //args [[&lt;nil&gt; 1 2]] num:1 test(slice2...) //args [&lt;nil&gt; 1 2] num:3 &#125;func test(args ...interface&#123;&#125;) &#123; fmt.Printf(\"args %v num:%d \\n\", args, len(args))&#125; Map-Reduce 1234567891011121314151617181920212223242526272829303132333435363738func main() &#123; var list = []string&#123;\"Hao\", \"Chen\", \"MegaEase\"&#125; x := Reduce(list, func(s string) int &#123; return len(s) &#125;) fmt.Printf(\"%v\\n\", x) //15 var intset = []int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125; out := Filter(intset, func(n int) bool &#123; return n%2 == 1 &#125;) fmt.Printf(\"%v\\n\", out) //[1 3 5 7 9] out = Filter(intset, func(n int) bool &#123; return n &gt; 5 &#125;) fmt.Printf(\"%v\\n\", out) //[6 7 8 9 10]&#125;func Reduce(arr []string, fn func(s string) int) int &#123; sum := 0 for _, it := range arr &#123; sum += fn(it) &#125; return sum&#125;func Filter(arr []int, fn func(n int) bool) []int &#123; var newArray = []int&#123;&#125; for _, it := range arr &#123; if fn(it) &#123; newArray = append(newArray, it) &#125; &#125; return newArray&#125; 健壮的泛型Map 123456789101112131415161718192021222324func verifyFuncSignature(fn reflect.Value, types ...reflect.Type) bool &#123; //Check it is a funciton if fn.Kind() != reflect.Func &#123; return false &#125; // NumIn() - returns a function type's input parameter count. // NumOut() - returns a function type's output parameter count. if (fn.Type().NumIn() != len(types)-1) || (fn.Type().NumOut() != 1) &#123; return false &#125; // In() - returns the type of a function type's i'th input parameter. for i := 0; i &lt; len(types)-1; i++ &#123; if fn.Type().In(i) != types[i] &#123; return false &#125; &#125; // Out() - returns the type of a function type's i'th output parameter. outType := types[len(types)-1] if outType != nil &amp;&amp; fn.Type().Out(0) != outType &#123; return false &#125; return true&#125; 健壮的泛型Reduce 123456789101112131415161718192021222324252627282930313233func Reduce(slice, pairFunc, zero interface&#123;&#125;) interface&#123;&#125; &#123; sliceInType := reflect.ValueOf(slice) if sliceInType.Kind() != reflect.Slice &#123; panic(\"reduce: wrong type, not slice\") &#125; len := sliceInType.Len() if len == 0 &#123; return zero &#125; else if len == 1 &#123; return sliceInType.Index(0) &#125; elemType := sliceInType.Type().Elem() fn := reflect.ValueOf(pairFunc) if !verifyFuncSignature(fn, elemType, elemType, elemType) &#123; t := elemType.String() panic(\"reduce: function must be of type func(\" + t + \", \" + t + \") \" + t) &#125; var ins [2]reflect.Value ins[0] = sliceInType.Index(0) ins[1] = sliceInType.Index(1) out := fn.Call(ins[:])[0] for i := 2; i &lt; len; i++ &#123; ins[0] = out ins[1] = sliceInType.Index(i) out = fn.Call(ins[:])[0] &#125; return out.Interface()&#125; 健壮的泛型 Filter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func Filter(slice, function interface&#123;&#125;) interface&#123;&#125; &#123; result, _ := filter(slice, function, false) return result&#125;func FilterInPlace(slicePtr, function interface&#123;&#125;) &#123; in := reflect.ValueOf(slicePtr) if in.Kind() != reflect.Ptr &#123; panic(\"FilterInPlace: wrong type, \" + \"not a pointer to slice\") &#125; _, n := filter(in.Elem().Interface(), function, true) in.Elem().SetLen(n)&#125;var boolType = reflect.ValueOf(true).Type()func filter(slice, function interface&#123;&#125;, inPlace bool) (interface&#123;&#125;, int) &#123; sliceInType := reflect.ValueOf(slice) if sliceInType.Kind() != reflect.Slice &#123; panic(\"filter: wrong type, not a slice\") &#125; fn := reflect.ValueOf(function) elemType := sliceInType.Type().Elem() if !verifyFuncSignature(fn, elemType, boolType) &#123; panic(\"filter: function must be of type func(\" + elemType.String() + \") bool\") &#125; var which []int for i := 0; i &lt; sliceInType.Len(); i++ &#123; if fn.Call([]reflect.Value&#123;sliceInType.Index(i)&#125;)[0].Bool() &#123; which = append(which, i) &#125; &#125; out := sliceInType if !inPlace &#123; out = reflect.MakeSlice(sliceInType.Type(), len(which), len(which)) &#125; for i := range which &#123; out.Index(i).Set(sliceInType.Index(which[i])) &#125; return out.Interface(), len(which)&#125; 反射不适用于高性能的地方 类型检查 Type Assert 对变量进行 .(type)的转型操作返回两个值，分别是 variable 和 error variable 是被转换好的类型，error 表示如果不能转换类型，则会报错 12345678910111213141516171819202122232425//Container is a generic container, accepting anything.type Container []interface&#123;&#125;//Put adds an element to the container.func (c *Container) Put(elem interface&#123;&#125;) &#123; *c = append(*c, elem)&#125;//Get gets an element from the container.func (c *Container) Get() interface&#123;&#125; &#123; elem := (*c)[0] *c = (*c)[1:] return elem&#125;intContainer := &amp;Container&#123;&#125;intContainer.Put(7)intContainer.Put(42)// assert that the actual type is intelem, ok := intContainer.Get().(int)if !ok &#123; fmt.Println(\"Unable to read an int from intContainer\")&#125;fmt.Printf(\"assertExample: %d (%T)\\n\", elem, elem) 反射 1234567891011121314151617181920212223242526type Container struct &#123; s reflect.Value&#125;func NewContainer(t reflect.Type, size int) *Container &#123; if size &lt;=0 &#123; size=64 &#125; return &amp;Container&#123; s: reflect.MakeSlice(reflect.SliceOf(t), 0, size), &#125;&#125;func (c *Container) Put(val interface&#123;&#125;) error &#123; if reflect.ValueOf(val).Type() != c.s.Type().Elem() &#123; return fmt.Errorf(“Put: cannot put a %T into a slice of %s\", val, c.s.Type().Elem())) &#125; c.s = reflect.Append(c.s, reflect.ValueOf(val)) return nil&#125;func (c *Container) Get(refval interface&#123;&#125;) error &#123; if reflect.ValueOf(refval).Kind() != reflect.Ptr || reflect.ValueOf(refval).Elem().Type() != c.s.Type().Elem() &#123; return fmt.Errorf(\"Get: needs *%s but got %T\", c.s.Type().Elem(), refval) &#125; reflect.ValueOf(refval).Elem().Set( c.s.Index(0) ) c.s = c.s.Slice(1, c.s.Len()) return nil&#125; 参考文献 https://coolshell.cn/articles/21128.html https://github.com/robpike","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Redis入门0-设计与实现","slug":"Redis/Redis入门0-设计与实现","date":"2021-04-13T16:07:16.000Z","updated":"2022-06-25T07:00:00.113Z","comments":true,"path":"2021/04/14/Redis/Redis入门0-设计与实现/","link":"","permalink":"http://xboom.github.io/2021/04/14/Redis/Redis%E5%85%A5%E9%97%A80-%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"前言 Redis的设计与实现主要关注以下几点： 高性能：线程模型(CPU)、数据结构(内存)、AOF(IO)、epoll网络框架(网络) 高可靠：主从复制、哨兵模式、RDB 高课拓展：数据分片，负载均衡 AOF AOF是写后日志，先执行命令，把数据写入内存，然后将命令已文本的形式追加到日志文件中(主线程执行) 优点： 只有命令能执行成功，才会被记录到日志中，否则直接返回错误，避免记录错误日志。 不会阻塞当前操作命令 缺点： 会阻塞下一个操作命令 如果执行完操作命令还没来得及写入文件就宕机，则会有丢失的风险 提供三种AOF文件的写回操作： Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘 可以做到基本不丢失数据，但太影响主线程性能 Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘 减少对系统性能的影响，如果发生宕机，上一秒内未落盘的命令操作仍然会丢失 No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 落盘的时机随机，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了 AOF重写 AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。 AOF文件过大存在的问题： 文件系统本身对文件大小有限制，无法保存过大的文件； 如果文件太大，之后再往里面追加命令记录的话，效率也会变低； 如果日志文件太大，整个恢复过程就会非常缓慢，会影响到 Redis 的正常使用 使用AOF重写过程解决文件过大的问题 虽然 AOF 重写后，日志文件会缩小，但把整个数据库的操作日志都写回磁盘是一个非常耗时的过程，所以AOF重写由后台子进程 bgrewriteaof 来完成的，过程为『一次拷贝，两处日志』 一个拷贝(内存)：每次执行重写，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，内存包含数据库最新数据，将其记入重写日志。 两处日志(写时复制)： 第一处日志指正在使用的 AOF 日志，新操作被写到它的缓冲区。即使宕机，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。 第二处日志指新的 AOF 重写日志。新操作也会被写到重写日志的缓冲区，以保证数据库最新状态的记录。此时就可以用新的 AOF 文件替代旧文件了 Redis机器上最好关闭内存大页机制(Huge Page，页面大小2M)，进程申请内存时阻塞的概率降低 缺点是：AOF记录的是操作命令，而不是实际的数据，所以故障恢复时，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。则使用内存快照RDB(Redis Database) RDB RDB记录的是某一时刻的数据(全量快照)，并不是操作。在数据恢复时，可直接把 RDB 文件读入内存完成恢复 RDB提供了两个命令来生成RDB文件：save和bgsave Save:在主线程中执行，会导致阻塞 bgsave:创建一个子进程，专门用于写入RDB文件(默认配置) 生成RDB文件是需要时间的，如果生成的这段时间时间里又有新数据怎么办？ 答：Redis 使用写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作 AOF与RDB混合 跟 AOF 相比，快照的恢复速度快，但快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销 使用参数：aof-use-rdb-preamble yes 过程如下： T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。这样快照不用频繁操作，AOF也只需记录两次快照之间的操作，避免重写开销","categories":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"}]},{"title":"Go入门2-Slice","slug":"Go/Go入门2-Slice","date":"2020-12-12T08:39:51.000Z","updated":"2021-01-10T03:45:36.683Z","comments":true,"path":"2020/12/12/Go/Go入门2-Slice/","link":"","permalink":"http://xboom.github.io/2020/12/12/Go/Go%E5%85%A5%E9%97%A82-Slice/","excerpt":"","text":"Go 语言的函数参数传递，只有值传递，没有引用传递 数组与切片 在 Go 中，与 C 数组变量隐式作为指针使用不同，Go 数组是值类型，赋值和函数传参操作都会复制整个数组数据 123456789101112131415161718192021222324252627func main() &#123; arrayA := [2]int&#123;100, 200&#125; var arrayB [2]int arrayB = arrayA fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB) arrayA[0] = 200 testArray(arrayA) fmt.Println(arrayA) fmt.Println(arrayB) testArray(arrayB)&#125;func testArray(x [2]int) &#123; fmt.Printf(\"func Array : %p , %p, %p, %v\\n\", &amp;x, &amp;x[0], &amp;x[1], x) x[0] = 300&#125;//运行结果为：arrayA : 0xc00010c010 , 0xc00010c010, 0xc00010c018, [100 200]arrayB : 0xc00010c020 , 0xc00010c020, 0xc00010c028, [100 200]func Array : 0xc00010c060 , 0xc00010c060, 0xc00010c068, [200 200][200 200][100 200]func Array : 0xc00010c0a0 , 0xc00010c0a0, 0xc00010c0a8, [100 200] 分析： 当直接使用 arrayB = arrayA 进行的是值复制，也就是说Go数组是值类型 将数组赋值给函数的时候，也是对整个数组进行复制，函数内改变值并不会改变外面的数组 上面的例子换成切片会怎么样？ 1234567891011121314151617181920212223242526func main() &#123; arrayA := []int&#123;100, 200, 300&#125; arrayB := arrayA[0:2] fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v, %d\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB, cap(arrayB)) arrayA[0] = 200 testArray(arrayB) fmt.Printf(\"arrayA : %p , %p, %p, %v\\n\", &amp;arrayA, &amp;arrayA[0], &amp;arrayA[1], arrayA) fmt.Printf(\"arrayB : %p , %p, %p, %v, %d\\n\", &amp;arrayB, &amp;arrayB[0], &amp;arrayB[1], arrayB, cap(arrayB))&#125;func testArray(x []int) &#123; fmt.Printf(\"func Array : %p , %p, %p, %v %d\\n\", &amp;x, &amp;x[0], &amp;x[1], x, cap(x)) x[0] = 300 x = append(x, 400) fmt.Printf(\"func Array : %p , %p, %p, %v %d\\n\", &amp;x, &amp;x[0], &amp;x[1], x, cap(x))&#125;//运行结果为:arrayA : 0xc00000c060 , 0xc000014140, 0xc000014148, [100 200 300]arrayB : 0xc00000c080 , 0xc000014140, 0xc000014148, [100 200], 3func Array : 0xc00000c0e0 , 0xc000014140, 0xc000014148, [200 200] 3func Array : 0xc00000c0e0 , 0xc000014140, 0xc000014148, [300 200 400] 3arrayA : 0xc00000c060 , 0xc000014140, 0xc000014148, [300 200 400]arrayB : 0xc00000c080 , 0xc000014140, 0xc000014148, [300 200], 3 分析： 当直接使用 arrayB = arrayA 进行的是其实是值复制，地址空间不一致，但是指向的内存空间一致 切片做参数传递的时候,但函数内容修改切片内容并不会影响外部切片，改变底层数组会影响外部切片 切片数据结构 切片（slice）切片是一个引用类型，是对数组一个连续片段的引用。这个片段是由起始和终止索引标识的一些项的子集。终止索引标识的项不包括在切片内(左闭右开)。切片提供了一个与指向数组的动态窗口。 给定项的切片索引可能比相关数组的相同元素的索引小。和数组不同的是，切片的长度可以在运行时修改，最小为 0 最大为相关数组的长度：切片是一个长度可变的数组 12345type slice struct &#123; array unsafe.Pointer //指向数组的指针 len int //当前切片长度 cap int //当前切片容量&#125; uintptr is an integer type that is large enough to hold the bit pattern of any pointer unsafe.Pointer 通用指针,类似C中的 void * 切片创建 首先这里会有一个常见函数，math.MulUintptr 将两个参数相乘来判断是否溢出 123456789//MulUintptr返回a * b以及乘法是否溢出。在受支持的平台上，这是编译器固有的功能。//true 越界;false 没有越界func MulUintptr(a, b uintptr) (uintptr, bool) &#123; if a|b &lt; 1&lt;&lt;(4*sys.PtrSize) || a == 0 &#123; return a * b, false &#125; overflow := b &gt; MaxUintptr/a return a * b, overflow&#125; 解析： sys.PtrSize 在64位机器中为8 12const PtrSize = 4 &lt;&lt; (^uintptr(0) &gt;&gt; 63) // unsafe.Sizeof(uintptr(0)) but an ideal constconst MaxUintptr = ^uintptr(0) &lt; 1&lt;&lt;(4*sys.PtrSize)相当于&lt; 1&lt;&lt;32 ,而8位计算机中最大的是 2^64-1,所以判断a和b都小于2^32即可 否则 b &gt; MaxUintptr/a 如果 b 大于最大值除以a,则说明 a*b 超过最大值即越界 make与字面量切片 支持通过make或字面量的方式进行创建切片 12slice1 := make([]int, 4, 6) //makeslice2 := []int&#123;1,2,3,4&#125; //字面量，注意[]内不要写容量，否则是数组 再来看分片函数 makeslice 12345678910111213141516func makeslice(et *_type, len, cap int) unsafe.Pointer &#123; mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem &gt; maxAlloc || len &lt; 0 || len &gt; cap &#123; mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem &gt; maxAlloc || len &lt; 0 &#123; panicmakeslicelen() &#125; panicmakeslicecap() &#125; // 分配内存 // 小对象从当前P 的cache中空闲数据中分配 // 大的对象 (size &gt; 32KB) 直接从heap中分配 // runtime/malloc.go return mallocgc(mem, et, true)&#125; 解析： 根据 数据类型大小 x 切片容量 判断是否会越界 如果 越界或超过最大分配长度maxAlloc(32位与64位不一致)，长度大于容器。则尝试 根据 数据类型大小 x 切片长度 判断溢出 如果 长度溢出则 panicmakeslicelen: panic报错 “makeslice: len out of range” 如果 容量溢出则 panicmakeslicecap: panic报错 “makeslice: cap out of range” 否则分配内存： 1func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer &#123;&#125; 所以切片其实分配了容量大小的内存，只是访问不到，也被初始化 空切片与nil切片 123var slice []int //nil切片silce := make( []int , 0 ) //空切片slice := []int&#123; &#125; //空切片 空切片和 nil 切片的区别在于：空切片指向的地址不是nil，指向的是一个内存地址，但是它没有分配任何内存空间，即底层元素包含0个元素。 不管是使用 nil 切片还是空切片，对其调用内置函数 append，len 和 cap 的效果都是一样的 切片扩容 扩容原理 growslice用来处理在使用append时候的切片扩容，那么它的规则如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100func growslice(et *_type, old slice, cap int) slice &#123; if raceenabled &#123; callerpc := getcallerpc() racereadrangepc(old.array, uintptr(old.len*int(et.size)), callerpc, funcPC(growslice)) &#125; if msanenabled &#123; msanread(old.array, uintptr(old.len*int(et.size))) &#125; //当新容量比旧容量还有小的时候，直接panic报错 if cap &lt; old.cap &#123; panic(errorString(\"growslice: cap out of range\")) &#125; if et.size == 0 &#123; //如果切片元素大小为0，还调用了扩容方法，那么就新生成一个新的容量的切片返回 return slice&#123;unsafe.Pointer(&amp;zerobase), old.len, cap&#125; &#125; newcap := old.cap doublecap := newcap + newcap if cap &gt; doublecap &#123; //1. 当指定容量大于旧切片2倍，则使用指定容量 newcap = cap &#125; else &#123; //2. 指定容量小于旧切片2倍 if old.len &lt; 1024 &#123; newcap = doublecap //2.1. 如果旧切片长度小于1024，则使用容量为旧容量两倍 &#125; else &#123; //2.2. 如果旧切片长度大于1024且小于指定容量，则新容量循环增加自身的四分之一直到大于指定容量 for 0 &lt; newcap &amp;&amp; newcap &lt; cap &#123; newcap += newcap / 4 &#125; if newcap &lt;= 0 &#123; //2.3. 如果旧切片长度等于0，则新容量等于旧容量 newcap = cap &#125; &#125; &#125; var overflow bool var lenmem, newlenmem, capmem uintptr // Specialize for common values of et.size. // For 1 we don't need any division/multiplication. // For sys.PtrSize, compiler will optimize division/multiplication into a shift by a constant. // For powers of 2, use a variable shift. switch &#123; case et.size == 1: lenmem = uintptr(old.len) newlenmem = uintptr(cap) capmem = roundupsize(uintptr(newcap)) overflow = uintptr(newcap) &gt; maxAlloc newcap = int(capmem) case et.size == sys.PtrSize: lenmem = uintptr(old.len) * sys.PtrSize newlenmem = uintptr(cap) * sys.PtrSize capmem = roundupsize(uintptr(newcap) * sys.PtrSize) overflow = uintptr(newcap) &gt; maxAlloc/sys.PtrSize newcap = int(capmem / sys.PtrSize) case isPowerOfTwo(et.size): var shift uintptr if sys.PtrSize == 8 &#123; // Mask shift for better code generation. shift = uintptr(sys.Ctz64(uint64(et.size))) &amp; 63 &#125; else &#123; shift = uintptr(sys.Ctz32(uint32(et.size))) &amp; 31 &#125; lenmem = uintptr(old.len) &lt;&lt; shift newlenmem = uintptr(cap) &lt;&lt; shift capmem = roundupsize(uintptr(newcap) &lt;&lt; shift) overflow = uintptr(newcap) &gt; (maxAlloc &gt;&gt; shift) newcap = int(capmem &gt;&gt; shift) default: lenmem = uintptr(old.len) * et.size newlenmem = uintptr(cap) * et.size capmem, overflow = math.MulUintptr(et.size, uintptr(newcap)) capmem = roundupsize(capmem) newcap = int(capmem / et.size) &#125; if overflow || capmem &gt; maxAlloc &#123; panic(errorString(\"growslice: cap out of range\")) &#125; var p unsafe.Pointer if et.ptrdata == 0 &#123; //先将 P 地址加上新的容量得到新切片容量的地址，然后将新切片容量地址后面的 capmem-newlenmem 个 bytes 这块内存初始化。为之后继续 append() 操作腾出空间 p = mallocgc(capmem, nil, false) // memclrNoHeapPointers clears n bytes starting at ptr memclrNoHeapPointers(add(p, newlenmem), capmem-newlenmem) &#125; else &#123; // 重新申请 capmen 这个大的内存地址，并且初始化为0值 p = mallocgc(capmem, et, true) if lenmem &gt; 0 &amp;&amp; writeBarrier.enabled &#123; // Only shade the pointers in old.array since we know the destination slice p // only contains nil pointers because it has been cleared during alloc. bulkBarrierPreWriteSrcOnly(uintptr(p), uintptr(old.array), lenmem-et.size+et.ptrdata) &#125; &#125; //将 lenmem 这个多个 bytes 从 old.array地址 拷贝到 p 的地址处 memmove(p, old.array, lenmem) return slice&#123;p, old.len, newcap&#125; //返回新切片&#125; 上述就是扩容的实现。主要需要关注的有两点: 扩容时候的策略 首先判断，如果新申请容量（cap）大于2倍的旧容量（old.cap），最终容量（newcap）就是新申请的容量（cap） 否则判断，如果旧切片的长度小于1024，则最终容量(newcap)就是旧容量(old.cap)的两倍，即（newcap=doublecap） 否则判断，如果旧切片长度大于等于1024，则最终容量（newcap）从旧容量（old.cap）开始循环增加原来的 1/4，即（newcap=old.cap,for {newcap += newcap/4}）直到最终容量（newcap）大于等于新申请的容量(cap)，即（newcap &gt;= cap） 如果最终容量（cap）计算值溢出，则最终容量（cap）就是新申请容量（cap） 扩容是生成全新的内存地址还是在原来的地址后追加 扩容实例 实例1： 123456789101112131415161718192021222324252627func main() &#123; s := make([]int, 0) oldCap := cap(s) for i := 0; i &lt; 2048; i++ &#123; s = append(s, i) newCap := cap(s) if newCap != oldCap &#123; fmt.Printf(\"[%d -&gt; %4d] cap = %-4d | after append %-4d cap = %-4d\\n\", 0, i-1, oldCap, i, newCap) oldCap = newCap &#125; &#125;&#125;//运行结果为:[0 -&gt; 0] cap = 1 | after append 1 cap = 2 [0 -&gt; 1] cap = 2 | after append 2 cap = 4 [0 -&gt; 3] cap = 4 | after append 4 cap = 8 [0 -&gt; 7] cap = 8 | after append 8 cap = 16 [0 -&gt; 15] cap = 16 | after append 16 cap = 32 [0 -&gt; 31] cap = 32 | after append 32 cap = 64 [0 -&gt; 63] cap = 64 | after append 64 cap = 128 [0 -&gt; 127] cap = 128 | after append 128 cap = 256 [0 -&gt; 255] cap = 256 | after append 256 cap = 512 [0 -&gt; 511] cap = 512 | after append 512 cap = 1024[0 -&gt; 1023] cap = 1024 | after append 1024 cap = 1280[0 -&gt; 1279] cap = 1280 | after append 1280 cap = 1696[0 -&gt; 1695] cap = 1696 | after append 1696 cap = 2304 分析： 1280/1024 = 1.25 , 1696/1024 = 1.325, 2304/1696=1.3584,为什么每次增加的倍数都不一样呢？ * 实例2： 1234567891011121314func main() &#123; slice := []int&#123;10, 20, 30, 40&#125; newSlice := append(slice, 50) fmt.Printf(\"Before slice %v, Pointer %p, Pointer0 %p, len %d, cap %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"Before newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf(\"After slice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"After newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice))&#125;Before slice [10 20 30 40], Pointer 0xc00000c060, Pointer0 0xc000014140, len 4, cap 4Before newSlice = [10 20 30 40 50], Pointer = 0xc00000c080, Pointer0 0xc000016140, len = 5, cap = 8After slice = [10 20 30 40], Pointer = 0xc00000c060, Pointer0 0xc000014140, len = 4, cap = 4After newSlice = [10 30 30 40 50], Pointer = 0xc00000c080, Pointer0 0xc000016140, len = 5, cap = 8 分析： 当旧切片容量小于1024，新切片容量直接翻倍 这里分配了一个新地址，修改新分片，旧分片并不会改变 实例2： 12345678910111213141516171819func main() &#123; array := [4]int&#123;10, 20, 30, 40&#125; slice := array[0:2] newSlice := append(slice, 50) fmt.Printf(\"Before array %v, Poninter %p, Poninter0 %p \\n\", array, &amp;array, &amp;array[0]) fmt.Printf(\"Before slice %v, Pointer %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"Before newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) newSlice[1] += 10 fmt.Printf(\"After slice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", slice, &amp;slice, &amp;slice[0], len(slice), cap(slice)) fmt.Printf(\"After newSlice = %v, Pointer = %p, Pointer0 %p, len = %d, cap = %d\\n\", newSlice, &amp;newSlice, &amp;newSlice[0], len(newSlice), cap(newSlice)) fmt.Printf(\"After array = %v, Poninter = %p, Pointer0 %p \\n\", array, &amp;array, &amp;array[0])&#125;Before array [10 20 50 40], Poninter 0xc000014140, Poninter0 0xc000014140 Before slice [10 20], Pointer 0xc00000c060, Pointer0 0xc000014140, len = 2, cap = 4Before newSlice = [10 20 50], Pointer = 0xc00000c080, Pointer0 0xc000014140, len = 3, cap = 4After slice = [10 30], Pointer = 0xc00000c060, Pointer0 0xc000014140, len = 2, cap = 4After newSlice = [10 30 50], Pointer = 0xc00000c080, Pointer0 0xc000014140, len = 3, cap = 4After array = [10 30 50 40], Poninter = 0xc000014140, Pointer0 0xc000014140 分析： 与实例1进行对比：由于slice还有容量可以扩容，所以执行 append() 操作以后，会在原数组上直接操作，这种情况下，扩容以后的数组还是指向原来的数组 实例1中由于没有容量进行扩容，所以执行append之后指向的就是一个全新的数组，修改值并不会影响原来的数组 由于数组是值类型，而切片是应引用类型，所以看到 &amp;array 与 &amp;array[0]的地址是一样的。但是 &amp;slice 与 &amp;slice[0]的地址是不一样的。另外两者指向的第一个值地址都是一样的，指向同一片内存。 由于容量导致结果不一致，极易产生bug。 实例3： 123456789101112131415161718192021222324252627func main() &#123; s := make([]int, 0) oldCap := cap(s) for i := 0; i &lt; 2048; i++ &#123; s = append(s, i) newCap := cap(s) if newCap != oldCap &#123; fmt.Printf(\"[%d -&gt; %4d] cap = %-4d | after append %-4d cap = %-4d\\n\", 0, i-1, oldCap, i, newCap) oldCap = newCap &#125; &#125;&#125;//运行结果为:[0 -&gt; -1] cap = 0 | after append 0 cap = 1 [0 -&gt; 0] cap = 1 | after append 1 cap = 2 [0 -&gt; 1] cap = 2 | after append 2 cap = 4 [0 -&gt; 3] cap = 4 | after append 4 cap = 8 [0 -&gt; 7] cap = 8 | after append 8 cap = 16 [0 -&gt; 15] cap = 16 | after append 16 cap = 32 [0 -&gt; 31] cap = 32 | after append 32 cap = 64 [0 -&gt; 63] cap = 64 | after append 64 cap = 128 [0 -&gt; 127] cap = 128 | after append 128 cap = 256 [0 -&gt; 255] cap = 256 | after append 256 cap = 512 [0 -&gt; 511] cap = 512 | after append 512 cap = 1024[0 -&gt; 1023] cap = 1024 | after append 1024 cap = 1280[0 -&gt; 1279] cap = 1280 | after append 1280 cap = 1696[0 -&gt; 1695] cap = 1696 | after append 1696 cap = 2304 分析： 切片拷贝 拷贝原理 123456789101112131415161718192021222324252627282930313233343536func slicecopy(toPtr unsafe.Pointer, toLen int, fmPtr unsafe.Pointer, fmLen int, width uintptr) int &#123; if fmLen == 0 || toLen == 0 &#123; return 0 &#125; n := fmLen if toLen &lt; n &#123; n = toLen &#125; if width == 0 &#123; return n &#125; if raceenabled &#123; //竞争检测 callerpc := getcallerpc() pc := funcPC(slicecopy) racereadrangepc(fmPtr, uintptr(n*int(width)), callerpc, pc) racewriterangepc(toPtr, uintptr(n*int(width)), callerpc, pc) &#125; if msanenabled &#123; // 如果开启了 The memory sanitizer (msan) msanread(fmPtr, uintptr(n*int(width))) msanwrite(toPtr, uintptr(n*int(width))) &#125; size := uintptr(n) * width if size == 1 &#123; // common case worth about 2x to do here // TODO: is this still worth it with new memmove impl? //如果只有一个元素，那么指针直接转换即可 *(*byte)(toPtr) = *(*byte)(fmPtr) // known to be a byte pointer &#125; else &#123; //如果不止一个元素，那么就把 size 个 bytes 从 fm.array 地址开始，拷贝到 to.array 地址之后 memmove(toPtr, fmPtr, size) &#125; return n&#125; 拷贝实例 实例1： 12345678910111213func main() &#123; array := []int&#123;10, 20, 30, 40&#125; slice := make([]int, 6) slice1 := make([]int, 3) n := copy(slice, array) m := copy(slice1, array) fmt.Println(n, slice) fmt.Println(m, slice1)&#125;//运行结果4 [10 20 30 40 0 0]3 [10 20 30] 分析： copy返回复制数目，slicecopy 方法最终结果取决于较短的那个切片，当较短的切片复制完成，整个复制过程就全部完成了 实例2： 1234567891011func main() &#123; slice := []int&#123;10, 20, 30, 40&#125; for index, value := range slice &#123; fmt.Printf(\"value = %d , value-addr = %x , slice-addr = %x\\n\", value, &amp;value, &amp;slice[index]) &#125;&#125;value = 10 , value-addr = c000018088 , slice-addr = c000014140value = 20 , value-addr = c000018088 , slice-addr = c000014148value = 30 , value-addr = c000018088 , slice-addr = c000014150value = 40 , value-addr = c000018088 , slice-addr = c000014158 分析： 如果用 range 的方式去遍历一个切片， Value 其实是切片里面的值拷贝。所以每次打印 Value 的地址都不变 由于 Value 是值拷贝而非引用传递，所以直接改 Value 是达不到更改原切片值的目的的，需通过 &amp;slice[index] 获取真实的地址 参考文献 https://halfrost.com/go_slice/ https://www.kancloud.cn/kancloud/the-way-to-go/72489","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"Go入门1-闭包","slug":"Go/Go入门1-闭包","date":"2020-12-08T14:58:29.000Z","updated":"2021-07-03T07:43:50.659Z","comments":true,"path":"2020/12/08/Go/Go入门1-闭包/","link":"","permalink":"http://xboom.github.io/2020/12/08/Go/Go%E5%85%A5%E9%97%A81-%E9%97%AD%E5%8C%85/","excerpt":"","text":"闭包概念 所谓闭包是指内层函数引用了外层函数中的变量或称为引用了自由变量的函数，其返回值也是一个函数 123闭包 = 函数 + 引用环境接受一个或多个函数作为输入 输出一个函数 函数 函数只是一段可执行代码，编译后就“固化”了，每个函数在内存中只有一份实例，得到函数的入口点便可以执行函数了。 在函数式编程语言中，函数是一等公民（First class value）：第一类对象，我们不需要像命令式语言中那样借助函数指针，委托操作函数，函数可以作为另一个函数的参数或返回值，可以赋给一个变量。函数可以嵌套定义，即在一个函数内部可以定义另一个函数，有了嵌套函数这种结构，便会产生闭包问题 匿名函数 函数可以像普通的类型（整型、字符串等）一样进行赋值、作为函数的参数传递、作为函数的返回值等。 匿名函数可以动态的创建，与之成对比的常规函数必须在包中编译前就定义完毕。匿名函数可以随时改变功能 Golang的函数只能返回匿名函数！ 闭包实例 实例1： 1234567891011121314151617181920212223242526272829303132//函数片段func add(base int) func(int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) //打印变量地址，可以看出来 内部函数时对外部传入参数的引用 f := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base += i return base &#125; return f&#125;//由 main 函数作为程序入口点启动func main() &#123; t1 := add(10) fmt.Println(t1(1), t1(2)) t2 := add(100) fmt.Println(t2(1), t2(2))&#125;//结果为0xc0000b40080xc0000b40080xc0000b400811 130xc0000b40200xc0000b40200xc0000b4020101 103 可以看出： 函数add返回一个函数，返回的这个函数就是闭包 调用同一个闭包函数，都是对同一个环境进行操作 函数add每进入一次，就形成了一个新的环境，对应的闭包中，函数都是同一个函数，环境却是引用不同的环境 实例2.1： 1234567891011121314151617//由 main 函数作为程序入口点启动func main() &#123; x, y := 1,2 defer func(a int)&#123; fmt.Println(\"defer x, y = \", a, y) //y为闭包引用 &#125;(x) //x值拷贝 调用时传入参数 x += 100 y += 200 fmt.Println(x, y)&#125;//结果为：101 202defer x, y = 1 202 实例2.2： 12345678910111213141516171819//由 main 函数作为程序入口点启动func main() &#123; for i := 0; i &lt; 3; i++ &#123; //多次注册延迟调用，相反顺序执行 defer func()&#123; fmt.Println(i) //闭包引用局部变量 &#125;() fmt.Print(i) if i == 2 &#123; fmt.Printf(\"\\n\") &#125; &#125;&#125;//结果为：012333 实例3： 123456789101112131415161718192021222324252627282930313233343536373839404142//返回加减函数，重点：内部函数时对外部变量的引用func calc(base int) (func(int) int, func(int) int) &#123; fmt.Printf(\"%p\\n\", &amp;base) add := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base += i return base &#125; sub := func(i int) int &#123; fmt.Printf(\"%p\\n\", &amp;base) base -= i return base &#125; return add, sub&#125;//由 main 函数作为程序入口点启动func main() &#123; f1, f2 := calc(100) fmt.Println(f1(1), f2(2)) //执行顺序：f1 f2 println fmt.Println(f1(3), f2(4)) fmt.Println(f1(5), f2(6)) fmt.Println(f1(7), f2(8))&#125;//结果为：0xc00001a0880xc00001a0880xc00001a088101 990xc00001a0880xc00001a088102 980xc00001a0880xc00001a088103 970xc00001a0880xc00001a088104 96 可以看出： 可利用go特性，同时返回两个闭包函数 相同父环境得两个闭包函数，使用的是相同的内存变量 实例4： 123456789101112131415161718//由 main 函数作为程序入口点启动func main() &#123; for i:=0; i&lt;5; i++ &#123; go func()&#123; fmt.Println(i) //i变量值也是引用.创建5个线程执行函数， for循环执行过程中可能执行完的时候，线程刚好处于i的某个值。 &#125;() &#125; time.Sleep(time.Second * 1)&#125;//结果为55555 闭包中的值是对源变量的引用。指向的是变量的当前值。当延迟调用函数体内某个变量作为defer匿名函数的参数，则在定义defer时已获得值拷贝，否则引用某个变量的地址(引用拷贝) 实例5(阻塞)： 123456789101112131415161718func main() &#123; //创建slice cs := make([](chan int), 10) for i := 0; i &lt; len(cs); i++ &#123; cs[i] = make(chan int) &#125; for i := range cs &#123; go func() &#123; cs[i] &lt;- i //创建线程，但是i是引用外部变量，不一定等线程执行的时候就是当前i值 &#125;() &#125; for i := 0; i &lt; len(cs); i++ &#123; t := &lt;-cs[i] //读取值的时候，可能会出现一只阻塞的情况 fmt.Println(t) &#125;&#125; 主要功能就是 创建10个线程执行函数，并向channal写入值。由于goroutine还没有开始，i的值已经跑到了最大9，使得这几个goroutine都取的i=9这个值，从而都向cs[9]发消息，导致执行t := &lt;-cs[i]时，cs[0]、cs[1]、cs[2] … 都阻塞起来了，从而导致了死锁 方案1(利用闭包传每次循环得参数)： 12345for i := range cs &#123; go func(index int) &#123; cs[index] &lt;- index &#125;(i)&#125; 方案2(利用阻塞管道，每次等待协程起来再执行下一次)： 12345678ch := make(chan int)for i := range cs &#123; go func() &#123; ch &lt;- 1 cs[i] &lt;- i &#125;() &lt;- ch&#125; 闭包原理 闭包是函数和它所引用的环境。像一个结构体 1234type Closure struct &#123; F func()() i *int&#125; 通过汇编查看闭包底层 1234567891011package mainfunc f(i int) func() int &#123; return func() int &#123; i++ return i &#125;&#125;//执行命令 go tool compile -S demo.go 得出结果为 可以看到LEAQ type.noalg.struct &#123; F uintptr; \"\".i *int &#125;(SB), CX 运行汇编命令得到得结果为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132➜ Go go tool compile -S demo.go&quot;&quot;.f STEXT size&#x3D;157 args&#x3D;0x10 locals&#x3D;0x20 0x0000 00000 (demo.go:3) TEXT &quot;&quot;.f(SB), ABIInternal, $32-16 0x0000 00000 (demo.go:3) MOVQ (TLS), CX 0x0009 00009 (demo.go:3) CMPQ SP, 16(CX) 0x000d 00013 (demo.go:3) PCDATA $0, $-2 0x000d 00013 (demo.go:3) JLS 147 0x0013 00019 (demo.go:3) PCDATA $0, $-1 0x0013 00019 (demo.go:3) SUBQ $32, SP 0x0017 00023 (demo.go:3) MOVQ BP, 24(SP) 0x001c 00028 (demo.go:3) LEAQ 24(SP), BP 0x0021 00033 (demo.go:3) FUNCDATA $0, gclocals·2589ca35330fc0fce83503f4569854a0(SB) 0x0021 00033 (demo.go:3) FUNCDATA $1, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB) 0x0021 00033 (demo.go:3) LEAQ type.int(SB), AX 0x0028 00040 (demo.go:3) MOVQ AX, (SP) 0x002c 00044 (demo.go:3) PCDATA $1, $0 0x002c 00044 (demo.go:3) CALL runtime.newobject(SB) 0x0031 00049 (demo.go:3) MOVQ 8(SP), AX 0x0036 00054 (demo.go:3) MOVQ AX, &quot;&quot;.&amp;i+16(SP) 0x003b 00059 (demo.go:3) MOVQ &quot;&quot;.i+40(SP), CX 0x0040 00064 (demo.go:3) MOVQ CX, (AX) 0x0043 00067 (demo.go:4) LEAQ type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;(SB), CX 0x004a 00074 (demo.go:4) MOVQ CX, (SP) 0x004e 00078 (demo.go:4) PCDATA $1, $1 0x004e 00078 (demo.go:4) CALL runtime.newobject(SB) 0x0053 00083 (demo.go:4) MOVQ 8(SP), AX 0x0058 00088 (demo.go:4) LEAQ &quot;&quot;.f.func1(SB), CX 0x005f 00095 (demo.go:4) MOVQ CX, (AX) 0x0062 00098 (demo.go:4) PCDATA $0, $-2 0x0062 00098 (demo.go:4) CMPL runtime.writeBarrier(SB), $0 0x0069 00105 (demo.go:4) JNE 131 0x006b 00107 (demo.go:4) MOVQ &quot;&quot;.&amp;i+16(SP), CX 0x0070 00112 (demo.go:4) MOVQ CX, 8(AX) 0x0074 00116 (demo.go:4) PCDATA $0, $-1 0x0074 00116 (demo.go:4) MOVQ AX, &quot;&quot;.~r1+48(SP) 0x0079 00121 (demo.go:4) MOVQ 24(SP), BP 0x007e 00126 (demo.go:4) ADDQ $32, SP 0x0082 00130 (demo.go:4) RET 0x0083 00131 (demo.go:4) PCDATA $0, $-2 0x0083 00131 (demo.go:4) LEAQ 8(AX), DI 0x0087 00135 (demo.go:4) MOVQ &quot;&quot;.&amp;i+16(SP), CX 0x008c 00140 (demo.go:4) CALL runtime.gcWriteBarrierCX(SB) 0x0091 00145 (demo.go:4) JMP 116 0x0093 00147 (demo.go:4) NOP 0x0093 00147 (demo.go:3) PCDATA $1, $-1 0x0093 00147 (demo.go:3) PCDATA $0, $-2 0x0093 00147 (demo.go:3) CALL runtime.morestack_noctxt(SB) 0x0098 00152 (demo.go:3) PCDATA $0, $-1 0x0098 00152 (demo.go:3) JMP 0 0x0000 65 48 8b 0c 25 00 00 00 00 48 3b 61 10 0f 86 80 eH..%....H;a.... 0x0010 00 00 00 48 83 ec 20 48 89 6c 24 18 48 8d 6c 24 ...H.. H.l$.H.l$ 0x0020 18 48 8d 05 00 00 00 00 48 89 04 24 e8 00 00 00 .H......H..$.... 0x0030 00 48 8b 44 24 08 48 89 44 24 10 48 8b 4c 24 28 .H.D$.H.D$.H.L$( 0x0040 48 89 08 48 8d 0d 00 00 00 00 48 89 0c 24 e8 00 H..H......H..$.. 0x0050 00 00 00 48 8b 44 24 08 48 8d 0d 00 00 00 00 48 ...H.D$.H......H 0x0060 89 08 83 3d 00 00 00 00 00 75 18 48 8b 4c 24 10 ...&#x3D;.....u.H.L$. 0x0070 48 89 48 08 48 89 44 24 30 48 8b 6c 24 18 48 83 H.H.H.D$0H.l$.H. 0x0080 c4 20 c3 48 8d 78 08 48 8b 4c 24 10 e8 00 00 00 . .H.x.H.L$..... 0x0090 00 eb e1 e8 00 00 00 00 e9 63 ff ff ff .........c... rel 5+4 t&#x3D;17 TLS+0 rel 36+4 t&#x3D;16 type.int+0 rel 45+4 t&#x3D;8 runtime.newobject+0 rel 70+4 t&#x3D;16 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0 rel 79+4 t&#x3D;8 runtime.newobject+0 rel 91+4 t&#x3D;16 &quot;&quot;.f.func1+0 rel 100+4 t&#x3D;16 runtime.writeBarrier+-1 rel 141+4 t&#x3D;8 runtime.gcWriteBarrierCX+0 rel 148+4 t&#x3D;8 runtime.morestack_noctxt+0&quot;&quot;.f.func1 STEXT nosplit size&#x3D;19 args&#x3D;0x8 locals&#x3D;0x0 0x0000 00000 (demo.go:4) TEXT &quot;&quot;.f.func1(SB), NOSPLIT|NEEDCTXT|ABIInternal, $0-8 0x0000 00000 (demo.go:4) FUNCDATA $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (demo.go:4) FUNCDATA $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 00000 (demo.go:4) MOVQ 8(DX), AX 0x0004 00004 (demo.go:5) MOVQ (AX), CX 0x0007 00007 (demo.go:5) INCQ CX 0x000a 00010 (demo.go:5) MOVQ CX, (AX) 0x000d 00013 (demo.go:6) MOVQ CX, &quot;&quot;.~r0+8(SP) 0x0012 00018 (demo.go:6) RET 0x0000 48 8b 42 08 48 8b 08 48 ff c1 48 89 08 48 89 4c H.B.H..H..H..H.L 0x0010 24 08 c3 $..go.cuinfo.packagename. SDWARFINFO dupok size&#x3D;0 0x0000 6d 61 69 6e main&quot;&quot;..inittask SNOPTRDATA size&#x3D;24 0x0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0010 00 00 00 00 00 00 00 00 ........runtime.memequal64·f SRODATA dupok size&#x3D;8 0x0000 00 00 00 00 00 00 00 00 ........ rel 0+8 t&#x3D;1 runtime.memequal64+0runtime.gcbits.01 SRODATA dupok size&#x3D;1 0x0000 01 .type..namedata.*struct &#123; F uintptr; i *int &#125;- SRODATA dupok size&#x3D;32 0x0000 00 00 1d 2a 73 74 72 75 63 74 20 7b 20 46 20 75 ...*struct &#123; F u 0x0010 69 6e 74 70 74 72 3b 20 69 20 2a 69 6e 74 20 7d intptr; i *int &#125;type.*struct &#123; F uintptr; &quot;&quot;.i *int &#125; SRODATA dupok size&#x3D;56 0x0000 08 00 00 00 00 00 00 00 08 00 00 00 00 00 00 00 ................ 0x0010 23 f3 08 44 08 08 08 36 00 00 00 00 00 00 00 00 #..D...6........ 0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0030 00 00 00 00 00 00 00 00 ........ rel 24+8 t&#x3D;1 runtime.memequal64·f+0 rel 32+8 t&#x3D;1 runtime.gcbits.01+0 rel 40+4 t&#x3D;5 type..namedata.*struct &#123; F uintptr; i *int &#125;-+0 rel 48+8 t&#x3D;1 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0runtime.gcbits.02 SRODATA dupok size&#x3D;1 0x0000 02 .type..namedata..F- SRODATA dupok size&#x3D;5 0x0000 00 00 02 2e 46 ....Ftype..namedata.i- SRODATA dupok size&#x3D;4 0x0000 00 00 01 69 ...itype.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125; SRODATA dupok size&#x3D;128 0x0000 10 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 ................ 0x0010 b7 43 25 9a 02 08 08 19 00 00 00 00 00 00 00 00 .C%............. 0x0020 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0030 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0040 02 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 ................ 0x0050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0x0070 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 ................ rel 32+8 t&#x3D;1 runtime.gcbits.02+0 rel 40+4 t&#x3D;5 type..namedata.*struct &#123; F uintptr; i *int &#125;-+0 rel 44+4 t&#x3D;6 type.*struct &#123; F uintptr; &quot;&quot;.i *int &#125;+0 rel 48+8 t&#x3D;1 type..importpath.&quot;&quot;.+0 rel 56+8 t&#x3D;1 type.noalg.struct &#123; F uintptr; &quot;&quot;.i *int &#125;+80 rel 80+8 t&#x3D;1 type..namedata..F-+0 rel 88+8 t&#x3D;1 type.uintptr+0 rel 104+8 t&#x3D;1 type..namedata.i-+0 rel 112+8 t&#x3D;1 type.*int+0gclocals·2589ca35330fc0fce83503f4569854a0 SRODATA dupok size&#x3D;10 0x0000 02 00 00 00 02 00 00 00 00 00 ..........gclocals·9fb7f0986f647f17cb53dda1484e0f7a SRODATA dupok size&#x3D;10 0x0000 02 00 00 00 01 00 00 00 00 01 ..........gclocals·33cdeccccebe80329f1fdbee7f5874cb SRODATA dupok size&#x3D;8 0x0000 01 00 00 00 00 00 00 00 ........ 参考链接 https://segmentfault.com/a/1190000019753885 https://www.cnblogs.com/landv/p/11589074.html","categories":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"}]},{"title":"TCP-5-拥塞控制","slug":"TCP/TCP-5-拥塞控制","date":"2020-11-06T14:46:37.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/11/06/TCP/TCP-5-拥塞控制/","link":"","permalink":"http://xboom.github.io/2020/11/06/TCP/TCP-5-%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/","excerpt":"","text":"引言 有了流量控制为什么还要有拥塞控制？ 流量控制是避免 发送方 的数据填满 接收方 的缓存。 在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大…. 所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。 拥塞控制 是避免 发送方 的数据填满整个网络。 在 发送方 调节所要发送数据的量，定义了 拥塞窗口 拥塞窗口和发送窗口有什么区别？ 拥塞窗口 cwnd 是发送方维护的一个 的状态变量，会根据网络的拥塞程度动态变化的。 前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，加入拥塞窗口概念后，发送窗口的值是swnd = min(cwnd, rwnd)，即拥塞窗口和接收窗口中的最小值。 拥塞窗口 cwnd 变化的规则：只要网络中没有出现拥塞， cwnd 就会增大； 但网络中出现了拥塞， cwnd 就减少； 怎么知道网络出现了阻塞？ 只要 发送方 没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就认为网络 出现了拥塞。 拥塞控制主要是四个算法： 慢启动 拥塞避免 拥塞发生 快速恢复 慢启动 TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据 包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？ 慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，就拥塞窗口 cwnd 的大小就会加 1。 假定拥塞窗口 cwnd 和发送窗口 swnd 相等，下面举个栗子： 连接建立完成后，一开始初始化 cwnd = 1 ，表示可以传一个 MSS 大小的数据。 当收到 1 个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个 当收到 2 个的 ACK 确认应答后，cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个 当收到 4 个的 ACK 确认到来的时候，4 个确认 cwnd 增加 4，于是就可以 比之前多发 4 个，所以这一次能够发送 8 个 可以看出慢启动算法，发包的个数是指数性的增长。 那慢启动涨到什么时候是个头呢？ 有一个叫慢启动门限 ssthresh （slow start threshold）状态变量 当 cwnd &lt; ssthresh 时，使用慢启动算法 当 cwnd &gt;= ssthresh 时，就会使用 拥塞避免算法 拥塞避免 当拥塞窗口 cwnd &gt;= ssthresh 慢启动门限 就会进入拥塞避免算法 一般来说 ssthresh 的大小是 65535 字节 = 2^16 - 1 那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd 接上前面的慢启动的例子，现假定 ssthresh 为 8 ： 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次 能够发送 9 个 MSS 大小的数据，变成了线性增长 拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶 段，但是增长速度缓慢了一些。 就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失 的数据包进行重传。 当触发了重传机制，也就进入了 拥塞发生算法 拥塞发生 当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种： 超时重传 快速重传 发生超时重传则就会使用拥塞发生算法。 这个时候，sshresh 和 cwnd 的值会发生变化： ssthresh 设为 cwnd/2 cwnd 重置为 1 接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦超时重传 ，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。 还有更好的方式 快速重传算法： 当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。 TCP 认为这种情况不严重，因为大部分没丢，只丢了小部分，则 ssthresh 和 cwnd 变化如下： cwnd = cwnd/2 设置为原来的一半; ssthresh = cwnd 进入快速恢复算法 快速恢复 快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也 不那么糟糕，所以没有必要像 RTO 超时那么强烈。 进入快速恢复之前， cwnd 和 ssthresh 已被更新了： cwnd = cwnd/2 ，设置为原来的一半 ssthresh = cwnd 然后进入快速恢复算法如下： 拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了） 重传丢失的数据包 如果再收到重复的 ACK，那么 cwnd 增加 1 如果收到新数据的 ACK 后，设置 cwnd 为 ssthresh，接着就进入了拥塞避免算法 也就是没有像 超时重传 一夜回到解放前，而是还在比较高的值，后续呈线性增长","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-4-流量控制","slug":"TCP/TCP-4-流量控制","date":"2020-11-06T14:02:09.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/11/06/TCP/TCP-4-流量控制/","link":"","permalink":"http://xboom.github.io/2020/11/06/TCP/TCP-4-%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6/","excerpt":"","text":"引言 如果发送方一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。 流量控制可以让发送方根据接收方的实际接收能力控制发送 的数据量 如果窗口固定 客户端是接收方，服务端是发送方， 假设接收窗口和发送窗口相同都为 200 假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响 根据上图的流量控制，说明下每个过程(本次是把服务端作为发送方，所以没有画出服务端的接收窗口)： 客户端向服务端发送请求数据报文 服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 Usable 减少为 120 字 节，同时 SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。 客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节， RCV.NXT 也就指向 321，这意味着 客户端期望的下一个报文的序列号是 321，接着发送确认报文给服务端。 服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法在继续发送数据。 客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节， RCV.NXT 也就指向 441，接着 发送确认报文给服务端。 服务端收到对 80 字节数据的确认报文后， SND.UNA 指针往右偏移后指向 321，于是可用窗口 Usable 增大到 80。 服务端收到对 120 字节数据的确认报文后， SND.UNA 指针往右偏移后指向 441，于是可用窗口 Usable 增大到 200。 服务端可以继续发送了，于是发送了 160 字节的数据后， SND.NXT 指向 601，于是可用窗口 Usable 减少到 40。 客户端收到 160 字节后，接收窗口往右移动了 160 字节， RCV.NXT 也就是指向了 601，接着发送 确认报文给服务端。 服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 SND.UNA 指针 偏移了 160 后指向 601，可用窗口 Usable 也就增大至了 200。 操作系统缓冲区与滑动窗口的关系 前面的流量控制例子假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中 所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。 当应用进程没法及时读取缓冲区的内容时，也会对缓冲区造成影响。 操作系统的缓冲区，是如何影响发送窗口和接收窗口的呢？ 缓冲区与滑动窗口 考虑以下场景： 客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 360 ； 服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。 根据上图的流量控制，说明下每个过程： 客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140） 服务端收到 140 字节数据，但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100），最后发送确认信息时，将窗口大小通过给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 260 客户端发送 180 字节数据，此时可用窗口减少到 80 服务端收到 180 字节数据，但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区， 于是接收窗口收缩到了 80 （260 - 180），并在发送确认信息时，通过窗口大小给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 80 客户端发送 80 字节数据后，可用窗口耗尽 服务端收到 80 字节数据，但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是 接收窗口收缩到了 0，并在发送确认信息时，通过窗口大小给客户端 客户端收到确认和窗口通告报文后，发送窗口减少为 0。 可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变 缩小缓冲区 当服务端系统资源非常紧张，操心系统可能会直接减少接收缓冲区大小，应用程序会因为无法及时读取缓存数据，可能会出现丢包 说明下每个过程： 客户端发送 140 字节的数据，于是可用窗口减少到了 220 服务端因为现在非常的繁忙，操作系统把接收缓存减少了 100 字节，当收到对端 140 数据确认报文后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大 小从 360 收缩成了 100，最后发送确认信息时，通告窗口大小给对方 此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户 端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40 服务端收到了 180 字节数据时，发现数据大小超过了接收窗口的大小，于是就把数据包丢失了 客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗 口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。 所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。所以TCP 规定不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段间再减少缓存 窗口关闭 TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控 制。 如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭 窗口关闭存在潜在危险：接收方向发送方通告窗口大小时，是通过 ACK 报文来通告的。 当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个ACK报文丢失呢？ 这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，就会出现相互等待的死锁现象 TCP 是如何解决窗口关闭时，潜在的死锁现象呢？ 为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 窗口探测 如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器； 如果接收窗口不是 0，那么死锁的局面就可以被打破了。 窗口探查探测的次数一般为 3 此次，每次次大约 30-60 秒（不同的实现可能会不一样，如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接 糊涂窗口综合症 如果接收方太忙了，来不及取走接收窗口里的数据，就会导致发送方的发送窗口越来越小。 到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。 要知道，我们的 TCP + IP 头有 40 个字节，如果只是为了传几个字节不太划算 考虑以下场景： 接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下： 接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据； 在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节； 每个过程的窗口大小不断减少了，并且发送的数据都是比较小的了 糊涂窗口综合症的现象： 接收方可以通告一个小的窗口 而发送方可以发送小数据 解决糊涂窗口综合症的办法是： 让接收方不通告小窗口给发送方：当 窗口大小 小于 min( MSS，缓存空间/2 ) ，也就是小于 MSS 与 1/2 缓存大小中的最小值时，就会 向发送方通告窗口为 0 ，也就阻止了发送方再发数据过来。 等到接收方处理了一些数据后，窗口大小 &gt;= MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。 让发送方避免发送小数据：使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据： 要等到窗口大小 &gt;= MSS 或是 数据大小 &gt;= MSS 收到之前发送数据的 ack 回包 只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件 Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。 可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每 个应用自己的特点来关闭） 1setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&amp;value, sizeof(int))","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-3-滑动窗口","slug":"TCP/TCP-3-滑动窗口","date":"2020-11-02T15:53:18.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/11/02/TCP/TCP-3-滑动窗口/","link":"","permalink":"http://xboom.github.io/2020/11/02/TCP/TCP-3-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/","excerpt":"","text":"引言 TCP 是每发送一个数据，都要进行一次确认应答。如果上一个数据包收到应答再发送下一个。数据包往返时间越长，通信的效率就越低。 为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，也不会降低网络通信的效率 有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值 窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。 假设窗口大小为 3 个 TCP 段，那么发送方就可以连续发送 3 个 TCP 段，若中途若有 ACK 丢失，可以通过下一个确认应答进行确认 图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通话下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据接收方都收到了。这个模式就叫累计确认或者累计应答 窗口的大小由哪一方决定？ TCP 头里有一个字段叫 Window ，即窗口大小。 接收端告知发送端自己还有多少缓冲区，发送方数据大小不能超过这个窗口大小，否则接收方无法正常接收数据 发送方的滑动窗口 下图发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口： #1 是已发送并收到 ACK确认的数据：1~31 字节 #2 是已发送但未收到 ACK确认的数据：32~45 字节 #3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节 #4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后 在下图，当发送方把数据全部都一下发送出去后，可用窗口为 0 ，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据 在下图，当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 52~56 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了 程序是如何表示发送方的四个部分的呢？ TCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移） SND.WND：表示发送窗口的大小（大小是由接收方指定的） SND.UNA：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节 SND.NXT：是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的 第一个字节 指向 #4 的第一个字节是个相对指针，它需要 SND.NXT 指针加上 SND.WND 大小的偏移量，就可以指向 #4 的第一个字节了 可用窗口大小的计算就是： 可用窗口大 = SND.WND -（SND.NXT - SND.UNA） 接收方的滑动窗口 接收窗口相对简单一些，根据处理的情况划分成三个部分： #1 + #2 是已成功接收并确认的数据（等待应用进程读取）； #3 是未收到数据但可以接收的数据； #4 未收到数据并不可以接收的数据； 其中三个接收部分，使用两个指针进行划分: RCV.WND ：表示接收窗口的大小，它会通告给发送方 RCV.NXT ：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节 指向 #4 的第一个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND 大小的偏移量，就可以指向 #4 的第一个字节 接收窗口和发送窗口的大小是相等的吗？ 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。 因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发 送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。 滑动窗口是固定的吗？ 如果是固定窗口，就会存在两个问题： 如果说窗口过小，当传输比较大的数据的时候需要不停的对数据进行确认，会造成很大的延迟。 如果说窗口的大小定义的过大。假设发送方一次发送100个数据。但接收方只能处理50个数据。这样每次都会只对这50个数据进行确认。发送方下一次还是发送100个数据，但是接受方还是只能处理50个数据。这样就有不必要的数据来拥塞我们的链路。 窗口左边沿向右移动称为窗口合拢，发生在数据被发送和确认后(重复ACK会会被丢弃，左边沿不会左移)。 窗口右边沿向右移动称为窗口张开，发生在另一端的接收进程读取已经确认的数据并释放了TCP的接收缓存时 窗口右边沿向左移动称为窗口收缩。RFC建议不使用这种方式。但TCP必须能够在某一端这种情况时进行处理 如果左边沿到达右边沿，则称其为一个零窗口，此时发送方不能够发送任何数据","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"TCP-2-超时与重传","slug":"TCP/TCP-2-超时与重传","date":"2020-11-01T03:00:54.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/11/01/TCP/TCP-2-超时与重传/","link":"","permalink":"http://xboom.github.io/2020/11/01/TCP/TCP-2-%E8%B6%85%E6%97%B6%E4%B8%8E%E9%87%8D%E4%BC%A0/","excerpt":"","text":"引言 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。针对TCP数据包丢失情况，常用以下四种重传机制解决： 超时重传 快速重传 SACK D-SACK 超时重传 在发送时设置一个定时器，当定时器溢出时还没收到ACK确认，它就重传该数据。TCP会在以下两种情况发生超时重传 数据包丢失 确认应答丢失 那么怎样决定超时间隔和重传的频率? RTT(Round-Trip Time 往返时延):表示数据往返两端一次所需要的时间 RTO(Retransmission Timeout 超时重传时间):表示数据包发送超过该时间还没有收到回复，则进行超时重传 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发 所以 超时重传时间 RTO 的值应该略大于报文往返 RTT 的值 由于网络是波动的，报文往返 RTT 是经常波动变化的，那系统是怎么确定RTO的值的呢？ 系统通过采样的方式来估计往返时间RTT： 采样 RTT 的时间进行加权平均，算出一个 SRTT(smoothed round-trip time)的值，而且这个值是不断变化的，因为网络状况不断地变化。 采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况 [RFC6298] 建议使用 Jacobaon/Karels 算法 计算 RTO 123456789# 第一次计算RTO,其中R1为第一次测量的RTT值, SRTT是计算平滑的RTT, DevRTR 是计算平滑的 RTT 与最新 RTT 的差距。SRTT = R1DevRTT = R1/2RTO = μ * SRTT + ∂ * DevRTT = μ * R1 + ∂ * R1 / 2# 后续计算RTO,其中R2为最新测量的RTTSRTT = SRTT + α * (RTT - SRTT) = R1 + α * (R2 - R1)DevRTT = (1 - β) * DevRTT + β * (|RTT - STRR|) = (1 - β) * R1 / 2 + β * (|R2 - R1|)RTO = μ * SRTT + ∂ * DevRTT 在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4 通过大量实验调试出来的 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍：即每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。 快速重传 超时触发重传存在的问题是，超时周期可能相对较长。 就有了快速重传机制来解决超时重发的时间等待 发送方发出了 1，2，3，4，5 份数据 第一份 Seq1 先送到了，于是就 Ack 回 2(表示2以前的都收到了，其他发送方接下来从2开始发送) 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2; 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到; 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失 的Seq2 最后，接收到收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段 SACK **快速重传( Selective Acknowledgment 选择性确认)**重传的时候，怎么知道是重传之前的一个，还是重传所有。发送端并不清楚这连续的三个 Ack 2 是由于哪个请求而传回来的。比如快速重传例子，是重传 Seq2 还是重传 Seq2、Seq3、Seq4、Seq5 。为了解决不知道重传哪些 TCP 报文，就有 SACK。 在 TCP 头部 Option 字段里加一个 SACK 的东西，将缓存的地图(几组不连续块的第一个序列号与不连续块的最后一个序列号)发送给发送方， 这样发送方就可以知道哪些数据收到了，哪些数据没收到，只重传丢失的数据。 如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现 只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。 如果要支持 SACK ，必须双方都要支持。在 Linux 下，通过 net.ipv4.tcp_sack 参数打开这个功能(Linux 2.4 后默认打开) Duplicate SACK Duplicate SACK 又称 D-SACK ，使用 **SACK 来通知发送方有哪些数据被重复接收了 ** 比如1,ACK丢包： 接收方发给发送方的两个 ACK 确认应答都丢失了，发送方超时后，重传第一个数据包(3000 ~ 3499) 接收方发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉发送方 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据 都已收到，所以这个 SACK 就代表着 D-SACK 。 这样发送方就知道了，数据没有丢，是接收方的 ACK 确认报文丢了 比如2,网络延时： 数据包(1000~1499) 被网络延迟了，导致发送方没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但重传后，被延迟的数据包(1000~1499)又到了接收方; 所以接收方回了一个SACK=1000~1500，因为ACK已经到了3000，所以这个SACK是D- SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而 是因为网络延迟了。 可见，D-SACK 有这么几个好处: 可以让发送方知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以判断是不是发送方的数据包被网络延迟了; 可以判断网络中是不是把发送方的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能(Linux 2.4 后默认打开)","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]},{"title":"Linux新手1-内核入门","slug":"Linux/Linux新手1-内核入门","date":"2020-10-25T02:24:18.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/10/25/Linux/Linux新手1-内核入门/","link":"","permalink":"http://xboom.github.io/2020/10/25/Linux/Linux%E6%96%B0%E6%89%8B1-%E5%86%85%E6%A0%B8%E5%85%A5%E9%97%A8/","excerpt":"","text":"编译内核代码 下载内核代码 从国内镜像网站下载最新的内核源码linux-5.9.1，并进行解压 12xz -d linux-5.9.1.tar.xztar xvf linux-5.9.1.tar 配置内核 对内核进行配置是为了得到内核配置文件.config。通过对内核进行配置，可以使未来编译成功的内核增加或减少对一些内核特性的支持。对内核进行配置有多种方法：基于文本/图形等，这里采用make menuconfig方式： 由于该配置方式基于ncurses库，所以在启动配置界面前要先安装ncurses库 12yum install -y ncurses-devel # make menuconfig requires the ncurses librariesmake menuconfig 配置界面成功如下 对内核按照默认的配置方式进行编译，因此当配置菜单启动后直接退出并保存即可。此时就在内核源码根目录下生成了.config文件 编译内核 编译内核包含两部分的工作， 其一是编译内核，即编译配置选项中标记为Y的那部分，这部分内核最终形成bzIamge镜像文件； 其二是编译内核模块，即编译配置选项中标记为M的那部分内核，这部分形成以.ko结尾的内核模块目标文件。 上述两部分编译工作可以依次通过make bzImage和make modules完成，也可以通过一条make命令直接完成。 编译内核的整个过程比较漫长，可以对make加-j参数来提高编译的效率。在make时使用该选项会为编译过程分配n个并发任务，这样可以缩短编译时间。n的取值为cpu个数的二倍。 1make -j4 如果不想看到垃圾信息又不想错过告警和错误信息，可以使用重定向文件写入，编译信息会写到文件中，同时错误和告警信息都会在屏幕上显示 1make &gt; ../detritus 安装内核 安装过程分为两部分，首先对内核模块进行安装，这个过程会将刚刚编译内核模块时生成的内核模块复制到/lib/modules/5.9.1/目录下，其中5.9.1为对应的内核版本。使用的命令如下： 1sudo make modules_install 接着使用下述命令安装编译好的内核： 1sudo make install 安装内核的过程主要完成了以下的工作： 将编译内核时生成的内核镜像bzImage拷贝到/boot目录下，并将这个镜像命名为vmlinuz-5.9.1。如果使用x86的cpu，则该镜像位于arch/x86/boot/目录下（处于正在编译的内核源码下）。 将~/linux-5.9.1/目录下的System.map拷贝到/boot/目录下，重新命名为System.map-5.9.1。该文件中存放了内核的符号表。 将~/linux-5.9.1/目录下的.config拷贝到/boot/目录下，重新命名为config-5.9.1。 创建initrd.img文件 initrd.img即为初始化的ramdisk文件，它是一个镜像文件，将一些最基本的驱动程序和命令工具打包到镜像文件里。该镜像文件的作用是在系统还没有挂载根分区前，系统需要执行一些操作，比如挂载scsi驱动，此时将initrd文件释放到内存中，作为一个虚拟的根分区，然后执行相关脚本，运行insmod命令加载需要的模块。 1sudo mkinitramfs 5.9.1 -o /boot/initrd.img-5.9.1 更新grub 最后一步则是更新grub启动菜单，使用下面的命令则可以自动更新启动菜单： 1sudo update-grub2 这样会将刚才编译好的内核放在启动菜单的首位，如果需要修改启动菜单中默认系统的启动顺序，则修改/boot/grub/grub.cfg文件中的set default=的值即可 内核就这样编译完毕了 Hello Kernel 编写一个Hello.c文件 1234567891011121314151617181920212223242526272829#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;//必选//模块许可声明MODULE_LICENSE(\"GPL\");//模块加载函数static int hello_init(void)&#123; printk(KERN_ALERT \"hello,I am edsionte\\n\"); return 0;&#125;//模块卸载函数static void hello_exit(void)&#123; printk(KERN_ALERT \"goodbye,kernel\\n\");&#125;//模块注册module_init(hello_init);module_exit(hello_exit);//可选MODULE_AUTHOR(\"xboom dove\");MODULE_DESCRIPTION(\"This is a simple example!\\n\");MODULE_ALIAS(\"A simplest example\"); 一个模块程序的中，模块加载函数，模块卸载函数以及模块许可声明是必须有的 编写了模块加载函数后，还必须用module_init(mode_name);的形式注册这个函数。当接下来用insmod加载模块时，内核会自动去寻找并执行内核加载函数，完成一些初始化工作。类似的当使用rmmod命令时，内核会自动去执行内核卸载函数。 注意这里的printk函数，可以简单的理解为它是内核中的printf函数，初次使用很容易将其打成printf 编写Makefile文件 12345678910111213141516171819obj-m += hello.o#generate the pathCURRENT_PATH:=$(shell pwd)#the current kernel version numberLINUX_KERNEL:=$(shell uname -r)#the absolute pathLINUX_KERNEL_PATH:=/lib/modules/$(LINUX_KERNEL)/build#complie objectall: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules#cleanclean: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean 在当前目录下运行make命令，就会生成hello.ko文件，即模块目标文件。接下来： 使用insmod hello.ko 添加该模块 如果报错 “Cannot generate ORC metadata for CONFIG_UNWINDER_ORC=y, please install libelf-dev, libelf-devel or elfutils-libelf-devel” Centos: yum install elfutils-libelf-devel Ubuntu： 12apt install libelf-devapt install libssl-dev 查看已经加载的模块 lsmod 12[root@localhost linuxDemo]# lsmod |grep hellohello 16384 0 使用rmmod hello.ko 卸载该模块 使用 dmesg命令查看printk中显示的语句 12345[339722.818996] hello: loading out-of-tree module taints kernel.[339722.819088] hello: module verification failed: signature and/or required key missing - tainting kernel[339722.819609] hello,I am edsionte[339764.633084] goodbye,kernel[339786.267993] hello,I am edsionte linux内核从3.7 开始加入模块签名检查机制，如果内核选项CONFIG_MODULE_SIG和CONFIG_MODULE_SIG_FORCE打开的话， 当加载模块时内核会检查模块的签名，如果签名不存在或者签名内容不一致，会强制退出模块的加载 提示&quot;module verification failed: signature and/or required key missing - tainting kernel&quot; 关闭方式： 在Makefile中添加 CONFIG_MODULE_SIG=n 然后重新重新编译内核 其实第一步insmod还有另外一种办法： 将hello.ko文件拷贝到/lib/module/#uname -r#/目录下， #uname -r#意思是，在终端中输入 uname -r后显示的内核版本及名称，例如mini2440中#uname -r#就是2.6.32.2-FriendlyARM。 然后depmod（会在/lib/modules/#uname -r#/目录下生成modules.dep和modules.dep.bb文件，表明模块的依赖关系） 最后modprobe hello（注意这里无需输入.ko后缀）即可 两种方法的区别： modprobe和insmod类似，都是用来动态加载驱动模块的，区别在于modprobe可以解决load module时的依赖关系，它是通过/lib/modules/#uname -r/modules.dep(.bb)文件来查找依赖关系的；而insmod不能解决依赖问题。 参考文档 Linux内核新手区 《Linux内核设计与实现》","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"协议4-SMB","slug":"Protocol/协议4-SMB","date":"2020-10-11T03:37:12.018Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/10/11/Protocol/协议4-SMB/","link":"","permalink":"http://xboom.github.io/2020/10/11/Protocol/%E5%8D%8F%E8%AE%AE4-SMB/","excerpt":"","text":"SMB协议简介 早期使用 FTP 来传文件，但是无法直接修改主机上面的档案数据，只能先下载后才能修改 SMB（Server Message Block），SMB1.0又称CIFS(Common Internet File System),一种应用层网络传输协议，主要使网络上的机器能够共享计算机文件、打印机、串行端口和通讯等资源。也提供认证的进程间通讯技能。主要用在Windows的机器上。 Samba让UNIX与Windows的SMB/CIFS（Server Message Block/Common Internet File System）网络协议做链接的自由软件，让两者互通有。 samba是许多服务以及协议的实现，其包括TCP/IP上的NetBIOS、SMB、CIFS等协议。 Samba能够为选定的Unix目录（包括所有子目录）创建网络共享。该功能使得Windows用户可以像访问普通Windows下的文件夹那样来通过网络访问这些Unix目录。Samba服务是由两个进程组成，分别是nmbd和smbd。 nmbd：其功能是进行NetBIOS名解析，并提供浏览服务显示网络上的共享资源列表。 smbd：其主要功能就是用来管理Samba服务器上的共享目录、打印机等，主要是针对网络上的共享资源进行管理的服务。当要访问服务器时，要查找共享文件，这时我们就要依靠smbd这个进程来管理数据传输 SMB原理 下图展示了SMB2和SMB3的报文头部结构，MS-SMB2协议文档。 一个普通的SMB会话从开始到结束一般会经过以下六个生命阶段： SMB协议协商（Negotiate） 在一个SMB还没有开始的时候，由客户端率先发出一个协商请求。在请求中，客户端会列出所有它所支持协议版本以及所支持的一些特性（比如加密Encryption、持久句柄Persistent Handle、客户端缓存Leasing等等）。而服务端在回复中则会指定一个SMB版本且列出客户端与服务端共同支持的特性。 建立SMB会话（Session Setup） 客户端选择一个服务端支持的协议来进行用户认证，可以选择的认证协议一般包括NTLM、Kerberos等。按照选择的认证协议的不同，这个阶段可能会进行一次或多次SESSION_SETOP请求/回复的网络包交换。至于NTLM或Kerberos认证协议的细节，我们会另文再叙。 连接一个文件分享（Tree Connect） 在会话建立之后，客户端会发出连接文件分享的请求。源于文件系统的树形结构，该请求被命名为树连接（Tree Connect）。以SMB协议的阿里云NAS为例，一般的SMB挂载命令为： 文件系统操作 在文件分享连接成功之后，用户通过SMB客户端进行真正的对于目标文件分享的业务操作。这个阶段可以用到的指令有CREATE、CLOSE、FLUSH、READ、WRITE、SETINFO、GETINFO等等。 断开文件分享连接（Tree Disconnect） 当一个SMB会话被闲置一定时间之后，Windows会自动断开文件分享连接并随后中止SMB会话。这个闲置时间可以通过Windows注册表进行设定[9]。当然，用户也可以主动发起断开连接请求。 终止SMB会话（Logoff） 当客户端发出会话中止请求并得到服务端发回的中止成功的回复之后，这个SMB会话至此便正式结束了。 SMB报文分析 SMB1的交互过程如下 SMB2的交换过程如下 Name query &lt;NetBIOS名称&gt;: 根据以上Windows系统的名字的解析机制，如果计算机需要知道一个NetBIOS名称或者域名对应的IP地址，首先会查找本地Hosts文件和NetBIOS缓存，其次会去联系DNS服务器进行解析。如果这几种方式都无法完成解析，则计算机会发出NBNS数据包。 TCP 三次握手：由于SMB是运行在TCP/IP上，这里会先完成三次握手 Negotiate Protocol Request:客户端向服务器发起协商请求，Requested Dialects包含客户端支持的所有SMB协议版本 Negotiate Protocol Reponse:服务器收到该请求后，选择它支持的最新版本 Dialect SMB2 wildcard (0x02ff 表示SMB2及以上版本)回复给客户端 0x0202 SMB 2.002 0x0210 SMB 2.1 0x0300 SMB 3.0 0x0302 SMB 3.02 0x02FF SMB2 Negotiate Protocol Request:上一次协商已经确定使用SMB2及以上版本，所以这次Dialect中只列出了SMB2及以上版本 Negotiate Protocol Reponse:服务器收到该请求后返回具体协议 SMB 3.0 Session Setup Request(NTLMSSP_NEGOTIATE): Negotiation结束之后，客户端请求和服务器建立一个session，在客户端发送的Session Setup Request里，包含了身份验证请求 Session Setup Reponse: 提示 STATUS_MORE_PROCESSING_REQUIRED, 需要进行NTLMSSP_AUTH认证 Session Setup Request(NTLMSSP_AUTH): 进行 NTLMSSP_AUTH认证 Session Setup Reponse: 返回会话创建结果创建会话成功 Tree Connect Request Tree: 客户端发送的 Tree Connect Request 来访问具体的共享，如果前面没有指定共享名（\\服务器名），客户端访问的是命名管道$IPC, 也可以指定访问共享目录 \\服务器\\共享名 Tree Connect Response: 服务器在检查过用户对该路径的权限后，回复Tree Connect Response。检查用户权限是这样进行的：服务器从Session Setup Request中已经得到用户所属的组，再通过和该路径上的ACL对比，即可得到用户权限。至此，用户就进入了共享文件夹 接下来是在会话以及读写权限获取之后使用单独的读取和写入以及事务处理命名管道操作在命名管道上执行事务的步骤 Create Request File:客户端发送 Create Reques File 去打开一个命名管道(这里叫lsarpc) CREATE Reponse File:服务器以Create Reponse File和打开管道作为响应 Ioctl Request(Bind) Ioctl Reponse(Bind_ack) 这里还不清楚具体的功能先写在这里 Close Request File :关闭刚才打开的管道 lsarcp Close Resonse: 服务器端确认关闭 同理开启另外一个隧道执行完毕后关闭(为什么要开启两个隧道) TCP 四次挥手：最后通过TCP四次挥手结束此次会话 可能存在的其他操作可以参考微软文档以及wireshark的SMB2报文分析 TODO: SMB1 和SMB2 的报文分析 SAMBA到底是什么 Winbindd是什么 wireshark的包和自己的包有什么区别 域到底是什么 其中提到的其他协议CIFS\\NBNS\\RPC协议到底是什么 华为加域过程分析 参考链接 SMB2 Protocol SAMBA原理 SMB小专 Microsoft Document Samba SMB.CONF配置说明 Windows帮助与支持","categories":[{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"}],"tags":[{"name":"SMB","slug":"SMB","permalink":"http://xboom.github.io/tags/SMB/"}]},{"title":"Linux性能优化-0-Tools","slug":"Linux/Linux性能优化-0-Tools","date":"2020-09-30T16:00:38.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/10/01/Linux/Linux性能优化-0-Tools/","link":"","permalink":"http://xboom.github.io/2020/10/01/Linux/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-0-Tools/","excerpt":"","text":"前言 Linux性能分析 CPU性能 磁盘I/O性能 内存性能 网络性能 Linux大神Brendan D.Gregg的Linux Performance Tools Virtual Memory vmstat vmstat：Virtual Meomory Statistics(虚拟内存统计)，可对操作系统的虚拟内存、进程、CPU活动进行监控 vmstat 命令详解 使用 man vmstat 查看 vmstat 的使用说明 123456789101112131415161718192021222324252627vmstat [options] [delay [count]]# 参数详解delay 表示间隔更新的时间。如果不指定，则只会打印一次从系统启动到现在的平均值count 更新次数，在没有计数的情况下，定义延迟时，默认值为无限-a, --active -f, --forks 显示从启动到现在forks次数-m, --slabs 显示内核slab缓冲区信息-n, --one-header 标题只显示一次而不是定期显示-s, --state 显示各种事件计数器和内存统计信息的表。此显示不重复-d, --disk 显示磁盘统计-D, --disk-sum 显示磁盘摘要信息-p, --partition device 显示分区统计信息-S, --unit character 在1000(k),1024(k),1000000(m)或1048576(m)字节之间切换输出。不会更改交换(si/so)或块(bi/bo)字段-t, --timestamp 每一行显示时间戳-w, --wide 宽输出模式（适用于内存量较大的系统，其中默认输出模式会出现不需要的列中断）。输出宽度超过80个字符每行NOTES 1. vmstat does not require special permissions. 2. These reports are intended to help identify system bottlenecks. Linux vmstat does not count itself as a running process. 3. All linux blocks are currently 1024 bytes. Old kernels may report blocks as 512 bytes, 2048 bytes, or 4096 bytes. 4. Since procps 3.1.9, vmstat lets you choose units (k, K, m, M). Default is K (1024 bytes) in the default mode. FILES /proc/meminfo /proc/stat /proc/*/stat vmstat常用命令 vmstat 2 1 间隔2s采集一次服务器状态，1表示只采集一次 12345678910111213141516171819202122232425262728293031323334353637[xboom@localhost ~]$ vmstat 2 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 698692 70384 80 136748 159 827 4004 965 250 511 5 4 91 0 0#FIELD DESCRIPTION FOR VM MODEProcs r: The number of runnable processes (running or waiting for run time). b: The number of processes in uninterruptible sleep. Memory swpd: the amount of virtual memory used. free: the amount of idle memory. buff: the amount of memory used as buffers. cache: the amount of memory used as cache. inact: the amount of inactive memory. (-a option) active: the amount of active memory. has been recently userd (-a option) Swap si: Amount of memory swapped in from disk (kb/s) so: Amount of memory swapped to disk (kb/s) IO bi: Blocks received from a block device (blocks/s). default 1024bytes per block bo: Blocks sent to a block device (blocks/s).System in: The number of interrupts per second, including the clock. cs: The number of context switches per second.CPU These are percentages of total CPU time. us: Time spent running non-kernel code. (user time, including nice time) sy: Time spent running kernel code. (system time) id: Time spent idle. Prior to Linux 2.5.41, this includes IO-wait time. wa: Time spent waiting for IO. Prior to Linux 2.5.41, included in idle. st: Time stolen from a virtual machine. Prior to Linux 2.6.11, unknown. Active memory: This information is currently in memory, and has been recently used. Inactive memory: This information in memory is not actively being used, but was recently used. For example, if you’ve been using Mail and then quit it, the RAM that Mail was using is marked as Inactive memory. This Inactive memory is available for use by another application, just like Free memory. However, if you open Mail before its Inactive memory is used by a different application, Mail will open quicker because its Inactive memory is converted to Active memory, instead of loading Mail from the slower hard dis vmstat -d 查看磁盘信息 123456789101112131415161718192021222324[xboom@localhost ~]$ vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur secsda 73171 8499 7041491 19482 12371 181181 1645664 10221 0 23sr0 52 0 2125 55 0 0 0 0 0 0dm-0 46855 0 6696761 13980 5743 0 143320 2504 0 17dm-1 32794 0 266008 6924 187764 0 1502112 308321 0 10#FIELD DESCRIPTION FOR DISK MODEReads total: Total reads completed successfully merged: grouped reads (resulting in one I/O) sectors: Sectors read successfully ms: milliseconds spent readingWrites total: Total writes completed successfully merged: grouped writes (resulting in one I/O) sectors: Sectors written successfully ms: milliseconds spent writingIO cur: I/O in progress s: seconds spent for I/O vmstat -p dev 查看分区情况 12345678[root@localhost ~]# vmstat -p /dev/sda1sda1 reads read sectors writes requested writes 343 68386 34 456#FIELD DESCRIPTION FOR DISK PARTITION MODE reads: Total number of reads issued to this partition,分区的读的次数 read sectors: Total read sectors for partition,分区的读扇区的次数 writes : Total number of writes issued to this partition,分区写的次数 requested writes: Total number of write requests made for partition,分区写请求次数 vmstat -m 查看系统slab信息 123456789101112[root@localhost ~]# vmstat -mCache Num Total Size Pagesisofs_inode_cache 92 92 704 46fuse_inode 36 36 896 36nf_conntrack 102 102 320 51#FIELD DESCRIPTION FOR SLAB MODE cache: Cache name num: Number of currently active objects total: Total number of available objects size: Size of each object pages: Number of pages with at least one active object slab:由于内核会有许多小对象，这些对象构造销毁十分频繁，比如i-node，dentry，这些对象如果每次构建的时候就向内存要一个页(4kb),这样就会非常浪费，为了解决这个问题，就引入了一种新的机制来处理在同一个页框中如何分配小存储区，而slab可以对小对象进行分配,这样就不用为每一个对象分配页框，从而节省了空间，内核对一些小对象创建析构很频繁，slab对这些小对象进行缓冲,可以重复利用,减少内存分配次数 vmstat -f显示从系统启动至今的fork(创建进程的系统调用)数量 12[root@localhost ~]# vmstat -f 4319 forks Block Device Int-块设备IO接口 pidstat pidstat命令用于监视Linux内核当前正在管理的各个任务 pidstat 命令详解 使用 man pidstat 查看 pidstat 的使用说明, 进程(任务)相关统计 1234567891011121314151617181920212223242526272829303132333435363738394041pidstat [ -d ] [ -H ] [ -h ] [ -I ] [ -l ] [ -R ] [ -r ] [ -s ] [ -t ] [ -U [ username ] ] [ -u ] [ -V ] [ -v ] [ -w ] [ -C comm ] [ -G process_name ] [ --human ] [ -p &#123; pid [,...] | SELF | ALL &#125; ] [ -T &#123; TASK | CHILD | ALL &#125; ] [ interval [ count ] ] [ -e program args ]#描述 1. 未指定 -p &lt;pid&gt; 将默认使用-p ALL，仅显示活动任务(统计值非0的任务) 2. 使用 -T 查看子进程信息 3. interval参数表示每次报告的时间间隔，0(或没有参数)表示自系统启动以来的的任务统计信息。如果没有设置为0，则可以与 count 一起使用，count表示指定间隔时间之后生成的报告次数。如果没有则是连续报告#参数详解-C comm 仅显示命令名称包含字符串comm的任务。 该字符串可以是正则表达式-d 报告I/O统计信息（仅内核2.6.20及更高版本）-e program args 给定参数args执行程序，并使用pidstat对其进行监视。当程序终止时，pidstat停止-G process_name 仅显示命令名称包含字符串process_name的进程。该字符串可以是正则表达式。 如果选项-t与选项-G一起使用，则属于该进程的线程为也会显示， 即使其命令名称不包含字符串process_name-H 以秒为单位显示时间戳-h 所有活动都单行显示，报告末尾没有平均统计信息。目的是使它易于被其他程序解析--human 以人类可读的格式打印尺寸（例如1.0k，1.2M等），设置与指标关联的其他任何默认单位（例如，千字节，扇区...）。-I 在多处理器环境中，指示任务CPU使用率（如选项-u所示）应除以处理器总数.-l 显示完成的进程命令和它的参数-p &#123; pid [,...] | SELF | ALL &#125; 选择要报告其统计信息的任务。 pid 指定进程标识号 SELF 报告pidstat进程本身的统计信息 ALL 报告系统管理的所有任务的统计信息。-R 报告实时优先级和调度策略信息 prio 任务的优先级 policy 任务的调度策略-r 报告页面错误和内存利用率。当报告单个任务的统计信息时，可能会显示以下值：-s 报告堆栈利用率-T &#123; TASK | CHILD | ALL &#125; 此选项指定pidstat命令必须监视的内容。 TASK 表示将报告单个任务的统计信息（这是默认选项） CHILD 表示报告所选任务及其子进程 ALL 表示要报告单个任务的统计信息，并针对所选全局报告任务和子进程 注意：任务及其所有子项的全局统计信息不适用于pidstat的所有选项 同样，这些统计信息不一定与当前时间间隔有关：子进程的统计信息仅在完成或被杀死时才收集进程-t 显示与所选任务关联的线程的统计信息 TGID 线程组标识符 TID 线程标识符-U [ username ] 显示正在监视的任务的真实用户名，而不是UID。如果指定了用户名，则仅显示属于指定用户的任务 -u 显示CPU利用率-v 内核表信息-w 显示进程上下文切换情况 pidstat常用命令 pidstat 2 5 每两秒钟显示一次系统中每个活动任务的CPU统计信息的五个报告 1234567891011121314151617181920212223[xboom@localhost ~]$ pidstat 2 5Linux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时10分13秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分16秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分16秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分18秒 0 5569 0.00 0.50 0.00 0.00 0.50 0 kworker/0:1-ata_sff23时10分18秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分18秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分20秒 0 4130 0.50 0.00 0.00 0.00 0.50 0 sssd_kcm23时10分20秒 1000 5605 0.50 0.00 0.00 0.00 0.50 0 pidstat23时10分20秒 UID PID %usr %system %guest %wait %CPU CPU Command23时10分22秒 1000 5605 0.00 0.50 0.00 0.00 0.50 0 pidstat23时10分22秒 UID PID %usr %system %guest %wait %CPU CPU Command平均时间: UID PID %usr %system %guest %wait %CPU CPU Command平均时间: 0 4130 0.10 0.00 0.00 0.00 0.10 - sssd_kcm平均时间: 0 5569 0.00 0.10 0.00 0.00 0.10 - kworker/0:1-ata_sff平均时间: 1000 5605 0.10 0.30 0.00 0.00 0.40 - pidstat pidstat -d 报告I/O统计信息（仅内核2.6.20及更高版本） 123456789101112131415[xboom@localhost ~]$ pidstat -dLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时11分56秒 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command23时11分56秒 1000 2050 0.99 0.00 0.00 12 dbus-daemon23时11分56秒 1000 2052 0.81 0.00 0.00 0 gdm-wayland-ses23时11分56秒 1000 2056 28.68 0.07 0.00 6 gnome-session-b23时11分56秒 1000 2104 197.84 0.05 0.00 199 gnome-shell23时11分56秒 1000 2167 4.47 0.00 0.00 6 XwaylandkB_rd/s 每秒从磁盘读取任务的千字节数kB_wr/s 每秒磁盘写入任务的千字节数kB_ccwr/s 任务已取消其写入磁盘的千字节数。 当任务截断一些脏页缓存时，可能会发生这种情况。 在这种情况下，一些IO另一个任务已被考虑将不会发生iodelay 任务的块I/O延迟，以时钟周期为单位。该指标包括等待同步块I/O完成和交换块I/O完成 pidstat -r 报告页面错误和内存利用率 1234567891011121314151617181920[xboom@localhost ~]$ pidstat -rLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时16分42秒 UID PID minflt/s majflt/s VSZ RSS %MEM Command23时16分42秒 0 1 3.02 0.23 245432 5032 0.62 systemd23时16分42秒 0 725 0.19 0.19 104716 2700 0.33 systemd-journal23时16分42秒 0 757 0.65 0.02 118924 420 0.05 systemd-udevd23时16分42秒 32 885 0.05 0.00 67124 144 0.02 rpcbind23时16分42秒 0 888 0.03 0.01 143108 48 0.01 auditd23时16分42秒 0 890 0.03 0.00 48488 20 0.00 sedispatch23时16分42秒 998 927 6.24 0.12 1634948 2484 0.31 polkitdminflt/s 任务每秒发生的次要错误，不需要从磁盘中加载页majflt/s 任务每秒发生的主要错误，需要从磁盘中加载页VSZ 虚拟地址大小，虚拟内存的使用KBRSS 常驻集合大小，非交换区五里内存使用KB%MEM 任务当前使用的可用物理内存份额当报告任务及其所有子任务的全局统计信息时，可能会显示以下值：minflt-nr 任务及其所有子项所产生的次要错误总数majflt-nr 任务及其所有子项所产生的主要错误总数 pidstat -s 报告堆栈利用率 123456789101112[xboom@localhost ~]$ pidstat -sLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时18分36秒 UID PID StkSize StkRef Command23时18分36秒 1000 2016 132 16 systemd23时18分36秒 1000 2037 132 4 pulseaudio23时18分36秒 1000 2050 132 12 dbus-daemon23时18分36秒 1000 2052 132 4 gdm-wayland-ses23时18分36秒 1000 2056 132 8 gnome-session-bStkSize 为任务保留为堆栈的内存量（以KB为单位），但不一定使用。StkRef 任务引用的用作堆栈的内存量（以KB为单位） pidstat -u 报告CPU利用率 12345678910111213141516171819[xboom@localhost ~]$ pidstat -uLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时20分26秒 UID PID %usr %system %guest %wait %CPU CPU Command23时20分26秒 0 1 0.00 0.03 0.00 0.07 0.03 0 systemd23时20分26秒 0 2 0.00 0.00 0.00 0.00 0.00 0 kthreadd23时20分26秒 0 9 0.00 0.01 0.00 0.03 0.01 0 ksoftirqd/023时20分26秒 0 10 0.00 0.00 0.00 0.81 0.00 0 rcu_sched%usr 用户太CPU占比，不包含虚拟处理器(虚拟处理器)所花费时间%system 内核态CPU占比 %guest 虚拟机CPU占比 %wait 等待IO的CPU占比 %CPU 总的CPU利用率，如果在多处理器环境中，添加参数 -I,该值将除以总的CPU个数CPU 进程运行的CPU id当报告任务及其所有子任务的全局统计信息时，可能会显示以下值：usr-ms 用户态任务及其所有子项花费的毫秒总数system-ms 内核态任务及其所有子项花费的毫秒总数guest-ms 虚拟处理器任务及其所有子项花费的毫秒总数 pidstat -v 内核表信息 12345678910[xboom@localhost ~]$ pidstat -vLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时22分03秒 UID PID threads fd-nr Command23时22分03秒 1000 2016 1 25 systemd23时22分03秒 1000 2037 3 36 pulseaudio23时22分03秒 1000 2050 2 71 dbus-daemonthreads 当前任务关联线程数fd-nr 当前任务关联文件描述符 pidstat -w 显示上下文切换 123456789101112131415[xboom@localhost ~]$ pidstat -wLinux 4.18.0-193.10.el8.x86_64 (localhost.localdomain) 2020年10月10日 _x86_64_ (1 CPU)23时23分28秒 UID PID cswch/s nvcswch/s Command23时23分28秒 0 1 0.79 0.27 systemd23时23分28秒 0 2 0.03 0.00 kthreadd23时23分28秒 0 3 0.00 0.00 rcu_gp23时23分28秒 0 4 0.00 0.00 rcu_par_gp23时23分28秒 0 6 0.00 0.00 kworker/0:0H-kblockdcswch/s 每秒执行的任务的自愿上下文切换总数。 自愿的上下文切换：进程无法获取所需资源导致,如 I/O、内存等系统系统资源不足。nvcswch/s 每秒执行的任务的非自愿上下文切换总数。 非自愿的上下文切换：是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。 比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换 I/O Controller iostat iostat命令用于通过观察设备活动的时间及其平均传输速率来监视系统输入/输出设备的负载。 iostat命令生成可用于更改系统配置的报告，以更好地平衡物理磁盘之间的输入/输出负载 iostat 命令详解 使用 man iostat 查看 iostat 的使用说明, 进程(任务)相关统计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667iostat [ -c ] [ -d ] [ -h ] [ -k | -m ] [ -N ] [ -s ] [ -t ] [ -V ] [ -x ] [ -y ] [ -z ] [ -j &#123; ID | LABEL | PATH | UUID | ... &#125; ] [ -o JSON ] [ [ -H ] -g group_name ] [ --human ] [ -p [ device [,...] | ALL ] ] [ device [...] | ALL ] [ interval [ count ] ]#描述 1.iostat命令生成的第一个报告提供有关自系统启动以来的时间的统计信息，除非使用了-y选项（在这种情况下，将忽略此第一个报告）。 2.每个后续报告涵盖自上一个报告以来的时间。每次运行iostat命令时，都会报告所有统计信息。 该报告由一个CPU标头行和随后的CPU统计信息行组成。 在多处理器系统上，CPU统计信息是系统范围内所有处理器的平均值。将显示设备标题行，后跟配置的每个设备的一行统计信息。 3.interval参数指定每个报告之间的时间间隔（以秒为单位）。可以与interval参数一起指定count参数。如果指定了count参数，count的值将确定间隔间隔秒生成的报告数。如果没有指定count参数而未指定count参数，则iostat命令将连续生成报告。# iostat命令生成两种类型的报告，即CPU利用率报告和设备利用率报告。# CPU使用率报告iostat命令生成的第一个报告是CPU利用率报告。对于多处理器系统，CPU值是所有处理器之间的全局平均值。 该报告具有以下格式： #磁盘利用率报告设备报告提供了基于每个物理设备或分区的统计信息。 如果未输入设备或分区，则将显示系统使用的每个设备的统计信息，并提供内核维护的统计信息。 如果在命令行上指定了ALL关键字，则将显示系统定义的每个设备的统计信息，包括从未使用过的设备。 除非设置了环境变量POSIXLY_CORRECT，否则默认情况下传输速率以1K块为单位，在这种情况下，将使用512字节的块。 sec/s (kB/s, MB/s): 每秒从设备读取或写入设备的扇区数（千字节，兆字节） rqm/s: 每秒排队到设备中的 I/O 请求数 areq-sz: 发出给设备的 I/O 请求的平均大小（以千字节为单位）注意：在以前的版本中，此字段称为avgrq-sz，以扇区表示 await: 发出给要服务的设备的 I/O 请求的平均时间（毫秒）。 这包括队列中的请求所花费的时间以及为请求服务所花费的时间 # 参数详解-c 显示CPU利用率-d 显示设备利用率-g group_name &#123; device [...] | ALL &#125; 显示一组设备的统计信息 iostat命令报告列表中每个设备的统计信息， 然后报告显示为group_name并由列表中所有设备组成的组的全局统计信息。 ALL关键字表示系统定义的所有块设备都应包含在组中。-H 此选项必须与选项-g一起使用，并指示仅显示该组的全局统计信息，而不显示该组中各个设备的统计信息。-h 使设备使用情况报告更易于人阅读。--human使用此选项隐式启用。--human 以可读格式打印的尺寸（例如KB, M等）。使用此选项显示的单位将取代与度量标准关联的任何其他默认单位（例如千字节，扇区...）-j &#123; ID | LABEL | PATH | UUID | ... &#125; [ device [...] | ALL ] 显示永久设备名称。 选项ID，LABEL等指定持久名称的类型。这些选项不受限制，只有先决条件是 /dev/disk 中必须存在具有所需持久名称的目录。 （可选）可以在所选的持久名称类型中指定多个设备。 因为持久性设备名称通常很长，所以选择-k 显示统计信息（KB/s）-m 显示统计信息（M/s）。-N 显示任何设备映射器设备的注册设备映射器名称。用于查看LVM2统计信息。-o JSON 以JSON（JavaScript对象表示法）格式显示统计信息。JSON输出字段顺序未定义，将来可能会添加新字段。-p [ &#123; device [,...] | ALL &#125; ] 显示系统使用的块设备及其所有分区的统计信息。 如果在命令行上输入了设备名称，则对其进行统计并显示其所有分区。 ALL关键字指示必须显示系统定义的所有块设备和分区的统计信息，包括那些从未使用过的 如果在此选项之前定义了选项-j，则可以使用所选的持久名称类型指定在命令行上输入的设备。-s 在80个字符的宽屏幕中显示报告的简短（窄版）版本。-t 打印显示的每个报告的时间。时间戳格式可能取决于S_TIME_FORMAT环境变量的值（请参见下文）。-V 打印版本号然后退出-x 显示扩展统计信息。-y 如果以给定的时间间隔显示多个记录，则自系统启动以来忽略带有统计信息的第一个报告。-z 告诉iostat忽略在采样期间没有任何活动的任何设备的输出。#环境变量：iostat命令考虑以下环境变量：POSIXLY_CORRECT 设置此变量后，传输速率以512字节块而不是默认的1K块显示。#BUG /proc filesystem must be mounted for iostat to work. Kernels older than 2.6.x are no longer supported. The average service time (svctm field) value is meaningless, 1. I/O statistics are now calculated at block level 2， we don't know when the disk driver starts to process a request.#FILES /proc/stat contains system statistics. /proc/uptime contains system uptime. /proc/diskstats contains disks statistics. /sys contains statistics for block devices. /proc/self/mountstats contains statistics for network filesystems. /dev/disk contains persistent device names. iostat常用命令 iostat自启动报告以来，显示所有CPU和设备的单个历史记录 123456Device: 此列提供 /dev 目录中列出的设备（或分区）名称tps: 表示每秒发送到设备的I/O请求次数。多个逻辑请求可以组合成对设备的单个I/O请求。传输的大小不确定。Blk_read/s(kB_read/s, MB_read/s): 表示从设备读取的数据量，以每秒的块数（千字节，兆字节）表示。块等同于扇区，因此大小为512BBlk_wrtn/s(kB_wrtn/s, MB_wrtn/s): 表示写入设备的数据量，以每秒的块数（千字节，兆字节）表示。Blk_read (kB_read, MB_read): 读取的总块数（千字节，兆字节）。Blk_wrtn (kB_wrtn, MB_wrtn): 写入的总块数（千字节，兆字节）。 iostat -x 显示IO拓展信息 123456789101112131415161718Device: 此列提供 /dev 目录中列出的设备（或分区）名称r/s: 每秒设备完成的读取请求数（合并后）w/s: 每秒设备完成的写请求数（合并后）rsec/s (rkB/s, rMB/s): 每秒从设备读取的扇区数（千字节，兆字节）wsec/s (wkB/s, wMB/s): 每秒写入设备的扇区数（千字节，兆字节）rrqm/s: 每秒合并到设备中的读取请求数wrqm/s: 每秒合并到设备中的写入请求数%rrqm: 读取请求(已合并)的百分比%wrqm: 写入请求(已合并)的百分比r_await: 发送给要服务的设备的读取请求的平均时间（以毫秒为单位）包括队列中的请求所花费的时间以及为请求服务所花费的时间w_await: 发布给要服务的设备的写请求的平均时间（以毫秒为单位）包括队列中的请求所花费的时间以及为请求服务所花费的时间aqu-sz: 发出到设备的请求的平均队列长度。注意：在以前的版本中 此字段称为avgqu-szrareq-sz: 发出给设备的读请求的平均大小（以千字节为单位）wareq-sz: 发出给设备的写请求的平均大小（以千字节为单位）svctm: 发出给设备的 I/O 请求的平均服务时间（以毫秒为单位）警告！不再信任此字段。 将来的sysstat版本中将删除此字段%util: 向设备发出 I/O 请求的经过时间的百分比（设备的带宽利用率） 当该值接近100％时，发生设备饱和用于串行服务请求的设备 对于并行处理请求的设备（例如RAID阵列和现代SSD）此数字并不反映其性能限制 参考链接 Linux vmstat命令详解","categories":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"}]},{"title":"TCP-1-传输控制协议","slug":"TCP/TCP-1-传输控制协议","date":"2020-07-26T09:25:09.000Z","updated":"2022-06-25T07:00:00.112Z","comments":true,"path":"2020/07/26/TCP/TCP-1-传输控制协议/","link":"","permalink":"http://xboom.github.io/2020/07/26/TCP/TCP-1-%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"引言 TCP提供一种面向连接的，可靠的字节流服务 面向连接的：指通信通过三次握手建立TCP连接 可靠的： 三次握手建立连接，四次挥手释放连接； ARQ协议(Automatic Repeat-reQuest)即自动重传请求来保证数据传输的正确性； 使用分组窗口和滑动窗口协议来保证接收方能够及时处理接收到的数据，进行流量控制； 使用慢开始、拥塞避免、拥塞发生、快恢复进行拥塞控制 字节流：不在字节流中添加边界标识符 如何唯一确定一个TCP连接？ 答：源地址+源端口+目的地址+目的端口，源地址和目的地址的字段(32位)是在IP头部，源端口和目的端口的字段(16位)在TCP头部。 有一个IP的服务器监听一个端口，它的TCP最大连接数是多少？ 答：首先，存在文件描述符限制，Socket都是文件，所以通过ulimit配置文件描述符的限制；其次，存在内存限制，每个TCP连接都要占用一定内存，操作系统的内存是有限的 TCP数据结构 源端口(Source Port, 2B = 16bits)和目的端口(Destination Port, 2B = 16bit)加上IP层源IP和目的IP唯一确定一个TCP连接，一个IP地址和一个端口号称为一个插口(socket)。 序列号(Sequence Number, 4B = 32bits)标识了TCP发送端到TCP接收端的数据流的一个字节，该字节代表着包含该序列号的报文段的数据报文段的第一个字节(想象两程序直接一个方向上流动的数据流，TCP给每个字节赋予了一个序列号)。如果TCP报文中flags标志位为SYN，该序列号表示初始化序列号(ISN)，此时第一个数据应该是从序列号ISN+1开始(因为SYN标识消耗了一个序号，同理FIN标识也要占用一个序号)。序号是32bits的无符号数，序号到达最大值 2^32 - 1后又从0开始。 初始序列号可被视为一个32位的计数器，该计数器数值每4微妙加1，防止出现与其他连接的序列号重叠 确认序列号(Acknowledgment Number, 4B = 32bits)表示TCP发送确认的一端所期望接受的下一个数据分片的序列号。该序号在TCP分片中Flags标志位为ACK时生效。 TCP为应用层提供全双工服务。这意味数据能在两个方向上独立地进行传输。因此，连接的每一端必须保持每个方向上的传输数据序号。 首部长度(Data Offset/Header Length, 4bits)也叫数据偏移，该字段表示 TCP 所传输的数据部分应该从 TCP 包的哪个位置开始计算，可以看作是 TCP 首部的长度。它表示TCP头包含了多少4字节的Words。因为4bits在十进制中能表示的最大值为15，32bits表示4个字节，那么Data Offset的最大可表示15*4=60个字节。所以TCP报头长度最大为60字节。如果options fields为0的话，报文头长度为20个字节。 预留字段(Reserved field, 6bits)值全为零。预留给以后使用。 标志位(Flags, 6bits)表示TCP包特定的连接状态。一个标签位占1bit，从低位到高位值依次为 FIN: 表示发送者已经发送完数据。通常用在发送者发送完数据的最后一个包中 SYN: 在连接建立时用来同步序号。当SYN=1而ACK=0时，表明这是一个连接请求报文。对方若同意建立连接，则应在响应报文中使SYN=1和ACK=1. 因此,SYN置1就表示这是一个连接请求或连接接受报文。 RST: 表示复位TCP连接 PSH: 通知接收端处理接收的报文，而不是将报文缓存到buffer中 ACK: 只有ACK=1时有效，也规定连接建立后所有发送的报文的ACK必须为1 URG: 表示紧急指针字段有效 补充的标志还有ECE,CWR占用保留字段2bits ECE: Explicit Congestion Notification。ECN回显。 CWR: Congestion Window Reduced。拥塞窗口减(发送方降低它的发送速率)，CWR 标志与后面的 ECE 标志都用于 IP 首部的 ECN 字段，ECE 标志为 1 时，则通知对方已将拥塞窗口缩小。 详见RFC3168。 窗口大小(Window, 2B = 16bits)表示滑动窗口的大小，用来告诉发送端接收端的buffer space的大小。接收端buffer大小用来控制发送端的发送数据数率，从而达到流量控制。最大值为2^16 = 65535B。若窗口为 0，则表示可以发送窗口探测，以了解最新的窗口大小，但这个数据必须是 1 个字节。 校验和(Checksum, 2B = 16bits)奇偶校验是对整个的 TCP 报文段(包括 TCP 头部和 TCP 数据，以 16 位字进行计算所得)由发送端计算和存储，并由接收端进行验证。 紧急指针(Urgent pointer, 2B = 16bits)只有在 URG 控制位为 1 时有效。该字段的数值表示本报文段中紧急数据的指针。从数据部分的首位到紧急指针所在的位置为止是紧急数据。 选项和填充(Option和pading)可变长度。表示TCP可选选项以及填充位。当选项不足32bits时，填充字段加入额外的0填充。最常见的可选字段: 最长报文大小MSS(Maximum Segment Size)在通信的第一个报文段(为建立连接而设置SYN标志为1的那个段)中指明这个选项，表示本端所能接受的最大报文段的长度(不包含TCP与IP头部)。选项长度不一定是32位的整数倍，所以要加填充位，即在这个字段中加入额外的零，以保证TCP头是32的整数倍 最大段大小的均值为1460，加上40字节(TCP头部和IP头部)组成1500字节 最大传输单元典型值 由于IPv6的头部比IPv4的头部长20字节，最大报文大小也减少20字节 TCP选择确认(SACK)选项 窗口缩放选项 时间戳选项和防回绕序列号 用户超时选项 认证选项 数据(Data)长度可变。用来存储上层协议的数据信息。可以为空。比如在连接建立和连接中止时。 TCP三次握手 第一次握手：建立连接时，客户端发送syn包(syn=x)到服务器，并进入SYN_SENT状态，等待服务器确认； 第二次握手：服务器收到syn包，确认客户的SYN(ack=x+1)，同时自己也发送一个SYN包(syn=y)，即SYN+ACK包，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(seq=x+1, ack=y+1），此包发送完毕，客户端和服务器进入ESTABLISHED(TCP连接成功）状态，完成三次握手。 第三次握手是可以带数据的，前两次的握手不能带数据 TCP四次挥手 第一次挥手：客户端进程发出连接释放请求FIN(seq=u)，并且停止发送数据。客户端进入FIN-WAIT-1(终止等待1)状态。FIN报文段即使不携带数据，也要消耗一个序号。 第二次挥手： 服务器收到连接释放请求，发出确认报文ACK(ack=u+1)，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。整个TCP处于半关闭状态，即客户端已经没数据需要发送，但服务器若发送数据，客户端依然要接受。 客户端收到服务器的确认请求后ACK后，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。 第三次挥手： 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。 第四次挥手：服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。 套接字API还提供半关闭操作，使用shutdown()代替close()可以实现TCP半关闭 MSL与MTT的区别？ TCP状态转换 箭头表示因报文段传输、接收以及计时器超时而引发的状态转换 红色箭头表示客户端行为 虚线箭头表示服务器行为 TIME_WAIT状态又称为2MSL等待状态，在此状态TCP会等待2倍的最大段生存期(Maximum Segment Lifetime, MSL)的时间。这样做的目的是： 让TCP重新发送最终的ACK已避免丢失的情况。重新发送最终的ACK并不是因为TCP重传了ACK，而是因为通信另一方重传了它的FIN。TCP总是重传FIN，直到它收到一个最终的ACK。 当TCP处于TIME_WAIT状态的时候，通信双方将该连接(四元组)定义为不可重新使用。只有当2MSL等待结束，或一条新链接使用的初始序列号超过了连接之前的实例所使用的最高序列号时，或允许使用时间戳选项来区分之前连接实例的报文段以避免混淆时，这条连接才能重新使用。 当一个连接处于TIME_WAIT状态时，任何延迟到达的报文都会被丢弃。 FIN_WAIT_2状态可以一直保持这个状态，也就是对端一直处于CLOSE_WAIT状态，在Linux中可以通过调整变量net.ipv4.tcp_fin_timeout来设置计时器的秒数(默认60s) 问题解答 为什么在TCP首部的开始便是源和目的的端口号？ 答：一个ICMP差错报文必须至少返回引起差错的IP数据报中除了IP首部的前8个字节。当TCP收到一个ICMP差错报文时，它需要检查两个端口号以决定差错对应于哪个连接。因此，端口号必须包含在TCP首部的前8个字节里 首部校验和是怎么计算的? 为什么连接的时候是三次握手，关闭的时候却是四次握手？ 答：当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，只有等到Server端所有报文都发送完，才能发送FIN报文，因此不能一起发送。 为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？ 答：网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间，一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 为什么不能用两次握手进行连接？ 答：3次握手完成三个重要的功能 三次握手才可以阻止重复历史连接的初始化(主要原因) 三次握手可以同步双方的初始序列号 三次握手才可以避免资源浪费 两次握手可能存在死锁的情况：计算机S和C之间的通信，假定C给S发送一个连接请求分组，S收到了这个分组，并发送了确认应答分组。按照两次握手的协定，S认为连接已经成功地建立了，可以开始发送数据分组。可是，C在S的应答分组在传输中被丢失的情况下，将不知道S是否已准备好，不知道S建立什么样的序列号，C甚至怀疑S是否收到自己的连接请求分组。在这种情况下，C认为连接还未建立成功，将忽略S发来的任何数据分组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。 如果已经建立了连接，但是客户端突然出现故障了怎么办？ 答：TCP设有保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 TCP与UDP的区别 目标和源端口：告诉UDP协议应该把报文发往哪个进城 包长度：保存了UDP首部的长度和数据的长度之和 校验和：校验和是为了提供可靠的UDP首部和数据 答：如下 连接 TCP是面向链接的传输层协议，传输数据前先要建立连接 UDP是不需要连接，即刻传输数据 服务对象 TCP是一对一的两点服务，即一条连接只有两个端点 UDP支持一对一、一对多、多对多的交互方式 可靠性 TCP是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达 UDP是尽最大努力交付，不保证可靠交付(什么叫尽最大努力？？) 拥塞控制、流量控制 TCP有拥塞控制和流量控制机制，保证数据的可靠性 UDP则没有 首部开销 TCP首部长度较长、会有一定的开销，没有使用选项字段是20字节 UDP首部只有8个字节，并且固定不变 传输方式 TCP是流式传输、没有边界，但保证顺序和可靠 UDP是一个包一个包的发送，是有边界的，可能丢失或乱序 分片不同 TCP的数据大小如果大于MSS大小，则会在传输层进行分片。中途丢失分片，也只会重传分片 UDP的数据大小如果大于MTU大小，则会在IP层分片并在IP层组装。如果中途丢失分片。则需要传输所有，所以UDP包大小最好小于MTU 应用场景 TCP面向链接，保证数据可靠。如 FTP文件传输、HTTP/HTTPS UDP面向无连接，可随时发送。如 DNS、SNMP、视频/音频等多媒体通信、广播通信 为什么UDP头部有包长度 字段，而TCP头部长度则没有 答：TCP数据的长度 = IP总长度 - IP首部 - TCP头部。UDP为啥不用这个公式，可能是为了保证数据包长度为4字节的整数倍 如何在Linux系统中查看TCP状态 答：通过命令 netstat -antp 参考链接 TCP:传输控制协议 TCP的三次握手和四次挥手 TCP状态转换图解析 TCP状态转换解析","categories":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"}]}],"categories":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/categories/go-zero/"},{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/categories/ETCD/"},{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/categories/FrameWork/"},{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/categories/k8s/"},{"name":"Protocol","slug":"Protocol","permalink":"http://xboom.github.io/categories/Protocol/"},{"name":"Algorithm Applied","slug":"Algorithm-Applied","permalink":"http://xboom.github.io/categories/Algorithm-Applied/"},{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/categories/Acwing/"},{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/categories/MySql/"},{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/categories/MQ/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/categories/Algorithm/"},{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/categories/Docker/"},{"name":"gRPC","slug":"gRPC","permalink":"http://xboom.github.io/categories/gRPC/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/categories/Redis/"},{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/categories/Go/"},{"name":"Linux Depth","slug":"Linux-Depth","permalink":"http://xboom.github.io/categories/Linux-Depth/"},{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/categories/Linux/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/categories/TCP-IP/"}],"tags":[{"name":"go-zero","slug":"go-zero","permalink":"http://xboom.github.io/tags/go-zero/"},{"name":"ETCD","slug":"ETCD","permalink":"http://xboom.github.io/tags/ETCD/"},{"name":"FrameWork","slug":"FrameWork","permalink":"http://xboom.github.io/tags/FrameWork/"},{"name":"k8s","slug":"k8s","permalink":"http://xboom.github.io/tags/k8s/"},{"name":"TLS","slug":"TLS","permalink":"http://xboom.github.io/tags/TLS/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://xboom.github.io/tags/Algorithm/"},{"name":"Acwing","slug":"Acwing","permalink":"http://xboom.github.io/tags/Acwing/"},{"name":"MySql","slug":"MySql","permalink":"http://xboom.github.io/tags/MySql/"},{"name":"Quic","slug":"Quic","permalink":"http://xboom.github.io/tags/Quic/"},{"name":"MQ","slug":"MQ","permalink":"http://xboom.github.io/tags/MQ/"},{"name":"Kafka","slug":"Kafka","permalink":"http://xboom.github.io/tags/Kafka/"},{"name":"Redis","slug":"Redis","permalink":"http://xboom.github.io/tags/Redis/"},{"name":"Docker","slug":"Docker","permalink":"http://xboom.github.io/tags/Docker/"},{"name":"HTTP","slug":"HTTP","permalink":"http://xboom.github.io/tags/HTTP/"},{"name":"gRPC","slug":"gRPC","permalink":"http://xboom.github.io/tags/gRPC/"},{"name":"Go","slug":"Go","permalink":"http://xboom.github.io/tags/Go/"},{"name":"Memory","slug":"Memory","permalink":"http://xboom.github.io/tags/Memory/"},{"name":"Linux","slug":"Linux","permalink":"http://xboom.github.io/tags/Linux/"},{"name":"Event","slug":"Event","permalink":"http://xboom.github.io/tags/Event/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"http://xboom.github.io/tags/TCP-IP/"},{"name":"SMB","slug":"SMB","permalink":"http://xboom.github.io/tags/SMB/"}]}